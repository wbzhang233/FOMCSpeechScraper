[
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2006",
        "date": "December 21, 2006",
        "title": "Economic Outlook",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke December 21, 2006 at the  Charlotte Chamber of Commerce Annual Economic Outlook Conference in Charlotte, N.C.",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2006/lacker_speech_20061221",
        "content": "It's a pleasure to be here in Charlotte again this year for the Annual Economic Conference. I am honored to be invited back for a third appearance, particularly after my forecasting performance last year. Before I begin, I owe you the usual disclaimer that these views are my own and are not necessarily shared by my colleagues around the Federal Reserve System. For those of you who have followed my voting record, however, this should come as no surprise.\n\nIn considering the economic outlook, it's important to bear in mind the broader transition that is taking place. In the three-year period leading up to the middle of this year, we've seen above average growth. Real gross domestic product — our best measure of total production in the economy — grew at a 3 ¾ percent annual rate. To appreciate the strength of that performance, note that the trend rate of GDP growth — by which I mean the rate consistent with trend growth in productivity and the labor force — is more like 3 percent. Labor market conditions improved significantly over that period, with 5.4 million new jobs created and the unemployment rate falling by a full 1 ½ percentage points. With jobs increasingly plentiful, household spending surged — real per capita consumption rose at a robust 2.6 percent annual rate. And even as their spending increased, consumers continued to build wealth; household net worth increased by 31 percent to reach a level equal to 5 years of personal income.\n\nBut since we're not in Lake Wobegon, we can't be above average all the time. Indeed, in the second quarter of this year real GDP only grew at a 2.6 percent rate. In the third quarter, growth dropped to a 2.2 percent rate, and growth is likely to be about the same, or perhaps a bit higher in the current quarter. Since growth clearly has slowed, the question on many people's minds is, \"What's next?\"\n\nFor some guidance, we can look back to similar episodes in the past. The long expansions of the 1980s and the 1990s resemble our current expansion in several key respects. Both were unusually long, by historical standards. Both saw substantial increases in production, employment and wealth. And in both cycles there was a somewhat bumpy transition between an early, high-growth phase and a period of several years of more average, trend-like growth. For example, the cyclical expansion of the 1990s was the longest in our nation's history, and yet in the midst of this period of strong, sustained growth, there was a two-quarter period in early 1995 in which real GDP increased by only 0.9 percent at an annual rate, driven in part by weakness in housing investment. That barely perceptible growth was followed by an additional three quarters of growth at a subpar rate, but then real GDP accelerated and grew quite rapidly for the next four years. This example suggests that we should not be discouraged this time around by an uneven transition from rapid to more sustainable growth.\n\nThe distinguishing feature of the current transition is the magnitude of the adjustment in the housing market, which comes at the end of what has been an amazing, decade-long run. The homeownership rate increased by 4 full percentage points from 1995 to 2005, and the number of houses built per year increased by 46 percent over that 10-year period.\n\nSome observers have called this extraordinary behavior of the housing market in recent years a bubble. I don't find that term useful or particularly accurate, since the behavior of housing appears to have been based on solid fundamentals.\n\nFirst, there were good reasons for the homeownership rate to rise and for homeowners to spend more on housing. Before 1995, the prevailing view was that productivity, and by implication real per capita income, was likely to increase at about 1 percent annually. But since then, as is well known, productivity growth has been dramatically higher — about 3 percent in the nonfarm business sector, for example. People base their investment plans on current and anticipated income growth, and it is not surprising that households would move increasingly from renting to buying their own home.\n\nSecond, inflation fell to below 2 percent in the mid-1990s, and over time financial market participants became more confident that inflation would remain low and stable; that confidence, in turn, led to low mortgage interest rates. Thus, at the beginning of 1995, the 30-year mortgage rate was above 9 percent; by 2003, it had fallen below 6 percent, reducing the relative price of housing services and contributing to the increase in demand.\n\nSatisfying the growth in housing demand required new construction and new land. While the supply of construction services appears to be fairly elastic, in some localities geography and zoning regulations can severely limit the supply of buildable lots. Consequently, the overall supply of housing can be highly inelastic. Increases in demand in such locations generate significant price increases, and those priced out of the market look for homes in locations with less desirable features — for example, with longer commutes.\n\nThis is well illustrated within the Fifth Federal Reserve District. In Charlotte, population, income, and employment grew rapidly from 1995 to 2005. With ample supplies of usable land, 224,000 new building permits were issued, and the price of an existing home increased by a relatively modest 4.2 percent per year. The Washington, D.C., area also had rapid growth in population, income, and employment; and 395,000 new houses were built. Unlike Charlotte, however, the supply of new lots was more limited in the Washington area, and accordingly the average price of an existing home increased 10 percent per year from 1995 to 2005.\n\nThe secular increase in housing demand in recent years was apparently satisfied in many markets by the end of 2005. Nationwide, new home sales have fallen by 22 percent through October of this year. The pipeline of new projects under construction was not scaled back as rapidly, however, and we now have excess inventories of new and existing homes in most localities. Production of new homes will have to undershoot demand for a time in order to work off the backlog. Indeed, new housing starts have fallen 28 percent through November of this year. The inventory overhang that remains suggests that homebuilding will be below demand for several more months.\n\nLooking ahead, there are tentative signs that the demand for housing has stabilized. New home sales have bumped around the 1 million unit annual rate for the last four months, and new purchase mortgage applications have risen over 15 percent in the last seven weeks. If these tentative signs are confirmed by more complete data then new home construction only needs to lag new home sales long enough to work off the current bulge in inventories. In this scenario, I would expect housing starts to realign with sales around the middle of 2007. Should new home demand deteriorate instead, the adjustment could take longer.\n\nIn any event, the weakness in housing will continue to be a drag on overall economic activity into the first half of next year, with the effect gradually waning as the year progresses. But I seriously doubt it will be enough of a drag to tip the economy into recession. My doubts stem from the fact that residential investment accounts for 6 percent of GDP, while household consumption accounts for 70 percent, and the outlook for that spending looks quite strong right now. For the first three quarters of this year, consumer spending has increased at a healthy 3.4 percent annual rate, and it looks like the fourth quarter will see something similar. That growth in spending has been underpinned by a strong labor market and solid income growth. Labor markets are fairly tight, overall, as indicated by the 4.5 percent unemployment rate. Real disposable income increased at a strong rate in the third quarter, and there are signs that real wage gains are improving — wages and salaries, as measured by the employment cost index, increased at a 3.8 percent annual rate in the second and third quarters, the best two-quarter increase in almost five years.\n\nCould weakness in the housing market spillover and weaken consumption spending as well? As residential investment contracts, construction employment will certainly decline. So far, residential construction employment has shed 110,000 jobs since the peak in February. At the same time, however, other segments of the economy have been doing well and overall payrolls actually expanded by 1.2 million jobs. This again reflects the small size of the residential construction sector relative to the overall economy. Although the outlook is for construction employment to continue to weaken for at least several more months, a decline commensurate with the fall-off we've already seen in housing starts still would have only a minor effect on total employment.\n\nAs I have said before, consumer spending is largely determined by current and expected future income prospects. I expect the overall job market to continue to expand even after accounting for further job losses in homebuilding, and I expect the tight labor market to continue to generate healthy wage gains. With income prospects looking good for 2007, it seems a pretty safe bet that consumer spending will do well, and again that's by far the largest part of the economy.\n\nWe've discussed residential investment, but what about business investment spending? Here the fundamentals look favorable. Business profitability is high and the cost of capital is low. In many industries, demand looks strong and capacity utilization is high. So, I would expect business investment to continue to contribute positively to growth in overall economic activity.\n\nThe outlook for real growth in 2007, then, is for continued strength in consumer spending and business investment to be partially offset, particularly early next year, by the drag from the housing market. Growth will start the year on the low side, but should be back to about 3 percent by the end of next year. So my best guess right now is that real GDP growth will average between 2 ½ and 2 ¾ percent in 2007.\n\nTwo risks to this outlook deserve mention. First, it's impossible to be sure that housing demand truly has stabilized, so one downside risk is of a further deterioration in the housing market. However, we don't see any signs of this now. Second, I'll note again the substantial uncertainty surrounding oil prices. This is likely to be with us for some time to come, and it cuts both ways, as our experience this fall demonstrated.\n\nWhat about inflation? The past year has been disappointing on this score as well. Inflation, according to our generally preferred measure — the core PCE price index — has been running above 2 percent since early 2004, and has run 2.5 percent so far this year. The longer core inflation persists above 2 percent, the greater the danger of inflation becoming entrenched at too high a rate.\n\nMany forecasters have been saying core inflation will moderate in the near term, and this certainly would be desirable. But such a moderation is not yet evident, despite the two most recent CPI reports. For example, the three-month average rate of change in the core PCE price index has been oscillating between 1.8 percent and 2.9 percent since last year's hurricanes, and stands at 2.7 percent as of October. In view of this recent record, it would take several months worth of data to provide statistically convincing evidence of a moderation in inflation. In the meantime, the risk that core inflation surges again, or does not subside as desired, clearly remains the predominant macroeconomic policy risk.\n\nAgain, thank you. It's been a pleasure to be here."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2006",
        "date": "December 01, 2006",
        "title": "How Should Regulators Respond to Financial Innovation?",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke December 1, 2006 at The Philadelphia Fed Policy Forum in Philadelphia, Pa.",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2006/lacker_speech_20061201",
        "content": "The subject of this panel is \"Financial Markets and Growth.\" There is now quite a substantial literature devoted to understanding how improvements in the effectiveness of the financial sector can and do contribute to growth and economic well-being in developing countries. My focus will be on the innovations in financial markets and practices that have been particularly striking in the United States over the last couple of decades, and the key benefits of those innovations. We've seen tremendous changes in financial arrangements in recent years, particularly with regard to the ways in which financial markets allocate risk; derivative markets have made risks increasingly divisible and tradable, and consumers have seen vastly expanded opportunities in credit markets. I believe these changes have produced noteworthy economic benefits. Many observers, however, acknowledge the benefits but believe the recent wave of financial innovation also has contributed to increasing financial fragility. The proliferation of new instruments seems to have made it easier for someone to accumulate large risk exposures and harder for counterparties to evaluate them.\n\nIn my remarks today, I will offer up the perspective of an economic policymaker from a more developed country, out of a belief that such a perspective has at least some relevance to policymakers in the developing world. I will speak at a fairly broad and abstract level, and will not address specific policy questions. I also will speak as an ex-research economist, which means I am entitled to leave it to others to validate or refute the hypotheses I advance here.\n\nMy main hypothesis is that one of the most difficult challenges posed by financial innovation has to do with the interplay between institutions that are relatively closely regulated and institutions that operate less constrained by government intervention. In many developing countries, the real dilemma in financial development has been how to foster growth in institutions and market segments that are more credibly distanced from the government than are the institutions that have tended to be government controlled or protected. In some Asian economies, for instance, the role of the banking system in lending to historically state-run enterprises makes it hard to liberalize the set of saving options available to households, for fear of a destabilizing flight of funds from the banking system.\n\nIn developed economies, much of the financial innovation taking place in the last 20 years has been associated with the movement of credit risk and other exposures off of the balance sheets of regulated banks and into such less-regulated entities as hedge funds. The presence of a sector that is less regulated (or, one might say, \"regulated primarily via market discipline\") has proven useful as a testing ground for new financial products and practices, but some have argued that the ability of such entities to amass concentrated exposures poses a potential threat to the stability of regulated institutions.\nLet me again emphasize that I offer up just one policymaker's views. As always, those views are not necessarily shared by any of my colleagues in the Federal Reserve.\n\nFinancial innovation\n\nIn my discussion of the effects of developments in financial markets, I want to focus on the period since the early 1980s. I think we can look to the 1980s as a rough starting point for a broad wave of innovation in financial markets and instruments, driven by advances in information and communication technologies. And this wave affected the financial opportunities of both households and businesses. On the household side, what can be fairly called a revolution in unsecured credit began in the 1980s and accelerated in the 1990s. Mortgage and home equity lending also benefited in this period from the same fundamental forces. Falling costs of computing and telecommunications facilitated advances in credit evaluation and the pricing of risk, which facilitated more finely partitioned credit origination decisions and improved intermediation of the resulting financial claims through securitization. Credit became available to more borrowers and on better terms.1\n\n\n\nIn the world of business finance, this period saw a significant expansion in the set of contingent claims available to market participants, and a significant expansion in the set of claims that are actively traded in secondary markets. Derivative contracts, swaps, loan sales, credit derivatives and securities backed by various types of assets all proliferated during this period. These developments increased the divisibility and marketability of specific risks, and greatly enhanced the ability of businesses and intermediaries to transfer particular risks to other market participants. For example, banks now seem to have a greater ability to move corporate credit exposures off of their books and into the hands of other banks and, increasingly, nonbank intermediaries such as institutional investors and hedge funds. Banks, however, have remained important in the origination of credits and they remain major providers of lending facilities through which exposures could flow back into the banking system under some circumstances.\n\nThe period since the early 1980s was also one of markedly diminished macroeconomic volatility. This change, which has been dubbed the \"great moderation,\" shows up in virtually all aggregate time series for real variables. For example, expansions have been longer and recessions have been shallower and less frequent. This phenomenon has been noted by many authors and the relevant facts were described by Chairman Bernanke in a 2004 speech.2\n\n\n\nThere are natural reasons to expect a connection between the performance of financial markets and the variability of real macroeconomic variables. One of the most fundamental economic purposes of financial markets and institutions is to facilitate household smoothing of consumption against both life-cycle variations and unexpected shocks to income. In an idealized, perfectly frictionless financial market, households would be able to shed all idiosyncratic risks and to achieve a consumption profile that is at least as smooth as average income. (Rob Townsend's research has emphasized the usefulness of this idealized world as a benchmark for evaluating financial market performance.)3\n\n\n\nBut households' ability to smooth consumption appears to be more limited than in such ideal markets. Most notably, direct insurance against some of the most significant individual shocks that households face – especially persistent income shocks – does not appear to be readily available. (Moral hazard or other informational frictions presumably limit the feasibility of such insurance.) In fact, households seem to achieve much of their smoothing with a relatively limited set of financial instruments. Households build up stocks of savings that they can draw on to smooth consumption when faced with unexpected reductions in income or increases in expenses. And households also smooth through such shocks by borrowing against future income.\n\nThe rising use of debt by U.S. households since the 1980s suggests that previously, borrowing was a relatively expensive tool for consumption-smoothing. If so, then one might have expected households to rely somewhat more heavily on savings before the innovations that reduced borrowing costs in the 1980s. As borrowing costs fell, the need for savings to smooth consumption fell also. With easier access to credit, many households may have found themselves with more savings than they needed for smoothing purposes. As access to credit and the amount of borrowing grew, one might have expected household savings rates to decline. And this is exactly what happened.4\n\n\n\nFinancial innovation could contribute to growth, therefore, by reducing the volatility of consumption relative to income and expense shocks. While the intuition for this is straightforward at the level of an individual household, the effect of improved consumption-smoothing opportunities on aggregate volatility is not unambiguous. A decrease in the aggregate consumption volatility associated with a given process for fundamental shocks could be offset by greater volatility in hours worked or through investment. A complicated set of interacting forces is at work in a general equilibrium setting, and the net outcome depends on fundamentals of technology, preferences and the nature of the fundamental shocks. And a variety of other causes have been offered to explain the macroeconomic moderation, including better monetary policy and the good fortune of receiving smaller shocks. Nonetheless, a causal link between the great moderation and the simultaneous wave of financial innovation would seem to be a plausible conjecture.\n\nThe basic story for households, then, appears to be one of reduced credit constraints leading to improved consumption-smoothing opportunities. A similar story might apply to businesses. A large literature has argued that many firms face credit constraints, and that these constraints result in firms' investment spending being more tied to available internal cash flow than would otherwise be the case. Such a mechanism would have the potential to amplify and propagate more fundamental shocks. But if such a mechanism is at work, and if financial innovation has reduced borrowing costs and expanded access to credit for business firms, then we would expect the amplifying effect of credit market constraints to have fallen as well. This, too, could have contributed to the great moderation.\nThe arguments I've presented here suggest that financial innovation may have a role in explaining the great moderation. But these arguments do not address the concerns often expressed about the volatility- or fragility-increasing effects of financial innovation. One line of reasoning underlying such concerns is that while financial innovation has enhanced the divisibility of risks and made it easier to allocate risks across a broader array of investors, these innovations also have facilitated greater concentrations of risk. The effectiveness of markets for new financial claims depends to some extent on the presence of entities willing and able to arbitrage away pricing misalignments, should they arise. That ability goes along with an ability to acquire relatively large positions in a relatively narrow set of claims, and thus to accumulate substantial risk exposures. This is arguably a good description of the role that hedge funds have come to play in financial markets. The flexibility that hedge funds have in responding to what they perceive to be pricing misalignments stems in part from their nature as entities free of much of the regulation facing other financial firms. The efficiency-enhancing benefits of financial innovations thus might be difficult to disentangle from the rise of less regulated intermediaries.\n\nConcerns about possible fragility-increasing effects of financial innovation tend to revolve around low-frequency events – financial crises in which losses incurred by one financial market participant have repercussions for other market participants. Such events might be economically costly if, for example, they caused some positive net present value investment opportunities to be missed or some ongoing, economically viable projects to be shut down. Alternatively, concentration of exposures within hedge funds or other entities could prove complicated and costly to resolve in situations of financial distress. In particular, if such a resolution were costly enough, its effects on the prices of financial assets might have the effect of curtailing the flow of capital to productive purposes, resulting in a disruption to real economic activity.\n\nTwo Views on the Role of Regulation\n\nTo talk about the implications of financial innovation for regulation, I think it is useful to first be clear about the underlying reasons for financial regulation. There are two broad views on this question, and each tends to be associated with a corresponding view on how policy should respond to the risks associated with the financial activities of less-regulated intermediaries.\n\nOne view sees the government financial safety net as the central motivation for the regulation of financial intermediaries. The safety net has the potential to distort risk-taking incentives of protected institutions, and supervisory oversight attempts to prevent excessive risks from accumulating in sectors supported by the safety net. In this view, regulators might be thought of as playing a role similar to that played by private providers of insurance, financial guarantees, or other credit enhancements, who by various means monitor and constrain risk-taking by their clients.\n\nThe safety net reduces the incentives of private financial counterparties to manage the exposures they take on. And these incentive effects arise not just from such explicit safety net guarantees as deposit insurance. They may also result from the expectations of private market participants about actions that the central bank or other public sector entity might take during a financial crisis. The mere possibility of public sector action to stem so-called \"systemic\" losses, such as central bank lending, can provide an implicit safety net that makes some participants more willing to hold concentrated exposures. Hence, under this moral hazard view of the need for regulation, the safety net itself can be a source of \"systemic\" risk.5\n\n\n\nHow does the financial innovation process I have sketched affect the potential for moral hazard induced by the safety net? By expanding the variety of risks that a supported institution is capable of taking on, the development of new instruments could provide new means to accumulate excessive exposures. Left unchecked, this could exacerbate the moral-hazard costs of the safety net. Enhancements in supervisory practice in the U.S. and elsewhere since the early 1990s seem to have made significant progress in constraining the distortionary effects of the safety net.\n\nThe second view, while not discounting the importance of moral hazard related to the safety net, sees a more fundamental justification for regulatory intervention. In this view, there are inherent market failures in financial markets – leading some risks, especially those that might be labeled \"systemic,\" to be mispriced. Often this market failure is portrayed as an externality. Systemic risks are said to distort choices because a counterparty does not take into account the effect of its own possible losses on its counterparty's counterparties. Alternatively, market failures are attributed to such market frictions as imperfect information, the idea being that if it's impossible to know all of the risk-relevant information about a counterparty's characteristics and past and future actions, then credit to that party cannot be priced as precisely as it would under full information.\n\nCoordination failures are a closely related type of market imperfection in which multiple market participants take the same action, such as attempting to sell a particular exposure or withdraw funds from an institution, causing losses to all that might have been avoided. The canonical example of a financial coordination failure is the Diamond-Dybvig bank run.6\n\n\n\nUnder the market failure view, the safety net acts to ameliorate friction-induced distortions and coordination problems, and financial innovation raises an array of concerns. Since distortions arise in the context of transactions, a dramatic rise in the volume of gross transactions relative to real economic activity would raise the level of risk and expand the need for safety net protection and the associated risk-taking constraints. Moreover, growth in the number of distinct market-traded financial instruments would multiply the potential for coordination failures. Offsetting these adverse effects, however, is the fact that innovation improves the ability to assess, measure and price risk, and thus could reduce the incidence of mispricing. A market failure amounts to the deviation of a market price from the normative fundamental value of the underlying claim, as when systemic effects are undervalued and lead to wrongly priced risks, or when coordination failures induce \"firesale\" liquidations at prices below fundamental values. The occurrence of such mispricing relies on the inability of any market participant to recognize and act on the deviation of price from fundamental value. It is exactly the ability to identify and exploit such deviations, however, that financial innovation has tended to enhance.\n\nSo which of these two views do I align myself with? I think it is useful to bring a healthy skepticism to the table about the extent of inherent market failures in financial markets. First, I would point out that work some 20 years ago by my co-panelist Rob Townsend (together with Edward C. Prescott) made clear that information imperfections – moral hazard, asymmetric information, and the like – do not constitute market failures.7\n Rather, the financial instruments and contracts we actually observe, and the rich variety of contractual features they display, should be understood as the market's adaptation to information limits. And the logic that says markets can allocate risk optimally subject to informational constraints is essentially identical to the logic that says markets are efficient when there is perfect information.\n\n\n\nSecond, I think that the notion of \"systemic risk\" as an externality in the classical sense is fundamentally flawed. If I take on a credit-risk exposure to a counterparty who has material exposure to another counterparty, then surely that should figure in my risk assessment. And similarly with that counterparty's exposure to others, and so on. Now, it might be quite costly to know everything one would like to know about one's counterparties' counterparties. But as I've just argued, limits to information do not imply a market failure.\n\nSkepticism regarding market failures does not imply a Panglossian stance, however. Actual markets are complex, and are evolving in ways that are difficult to predict. Measuring and assessing risk in such an environment is an intellectually challenging endeavor. Moreover, not all market participants will acquire sophistication and proficiency with new products and practices at the same pace. As a result, mistakes inevitably will be made, some of which could result in high-profile losses to some market participants – indeed we see these with some regularity, whether in households, business firms or financial institutions. The occurrence of such mistakes does not represent a form of market failure, but rather is an integral part of the innovation process. While market participants should certainly be encouraged to ensure that their own risk-measurement and risk-management practices keep pace with market developments as much as possible – and supervisors should certainly help in this regard with regulated financial institutions – reducing the probability of such mistakes to zero is unlikely to be optimal and could well inhibit beneficial innovation.\n\nSo What Should Regulators Do?\n\nThe picture that I have painted here today leads me to a few general principles about the role of supervision and regulation in the face of financial innovation.\n\nFirst, we must always remain mindful that reducing constraints and freeing up institutions to pursue new products and processes can have tremendous benefits. In part, these benefits stem from removing constraints to innovation, and I've devoted some time today to the hypothesis that the fruits of financial innovation can be seen at the macroeconomic level in the form of reduced real volatility. But reducing regulatory constraints can also more directly improve the allocation of credit and risk. Recent research has used the varying times at which states deregulated their banking industries to find that state-level deregulation was associated with improvements in income-smoothing for people within the state.8\n\n\n\nSecond, to effectively carry out their role of monitoring risks in financial institutions, it is essential for regulators to keep pace with changes and advances in the marketplace. If supervisors are to assess the adequacy of banks' risk-management practices, they must have a thorough understanding of emerging instruments and practices. This task is all the more challenging if innovations originate outside of the regulated banking sector.\n\nThird, it is possible for regulation itself to be a driver of innovation. The advent of the one-year, \"364-day\" credit facility was prompted by the 1988 Basel capital rules. This imposed a higher capital charge on credit lines with maturities of one year or more – hence, a facility lasting a day less than a year. Similarly, concentration limits on loan portfolios spurred the development of the secondary loan market, and the prohibition of interest on corporate deposits spurred the development of \"sweep accounts.\" While it may often be the case that innovations designed to \"bypass\" regulations ultimately lead to wider benefits for the market, this motivation generally makes innovation less likely to enhance efficiency.\n\nFinally, when innovation occurs outside of the banking industry, regulators' main concern should be with the interactions between the regulated and unregulated sectors. For example, supervisors and institutions have focused heavily in recent years on strengthening counterparty risk management practices and the settlement infrastructures undergirding important new financial markets. As I noted earlier, supervising this boundary requires that regulators broadly understand the activities of the unregulated sector, but perhaps even more important, it also requires regulators to understand how innovations change the ways in which exposures can flow back into the banking sector.\n\nThese observations point to a regulatory approach that avoids being overly proscriptive, but that attempts to ensure that regulated institutions' practices for measuring and managing risks are appropriate for the changing environment. Regulators should avoid extending constraints motivated by safety-net considerations to entities that do not receive safety-net support. And regulators should scrupulously avoid any actions or practices that would contribute to the perception that there is a probability of safety net support being extended into sectors that are now governed chiefly by market discipline.\n\nIn summary, I believe there is a strong case that financial innovation in the U.S. has brought real, tangible benefits for macroeconomic performance and growth, and I am drawn to the hypothesis that financial innovation can bring similar benefits to economies at different stages in the growth process. At the same time, some observers have expressed concerns that this wave of innovation also has resulted in concentrations of risk that add to financial market fragility. But at least as persuasive is the notion that the same advances that have made it easier for market participants to evaluate and exchange various risks have also made it possible for markets to respond more resiliently to disruptions by allowing market allocations to change more flexibly in response to changing market circumstances. As a consequence, I believe regulators serve their mission best, not by second-guessing observed risk allocations, but by assuring that individual institutions with access to a public sector safety net conduct their businesses using risk measurement and management practices that keep pace with the ever-changing market.\n\nReferences\n\nAthreya, K., 2004. \"Shame as it Ever Was: Stigma and Personal Bankruptcy,\" Federal Reserve Bank of Richmond Economic Quarterly, 90 (Spring), 1-19.\nBernanke, B., 2004. \"The Great Moderation,\" speech to the Eastern Economic Association in Washington D.C., Board of Governors of the Federal Reserve System, http://www.federalreserve.gov/BOARDDOCS/SPEECHES/2004/20040220/default.htm\n\nDemyanyk, Y., C. Ostergaard, and B. Sorensen, forthcoming. \"U.S. Banking Regulation, Small Business, and Interstate Insurance of Personal Income,\" Journal of Finance.\nDiamond, D., and P. Dybvig, 1983. \"Bank Runs, Deposit Insurance and Liquidity,\" Journal of Political Economy, 91 (June), 401-19.\n\nGoodfriend, M. and J. Lacker, 1999. \"Limited Commitment and Central Bank Lending,\" Federal Reserve Bank of Richmond Economic Quarterly, 85 (Fall), 1-28.\n\nLacker, J., 2005. \"Retail Financial Innovation,\" speech to Virginia Bankers Association, Hot Springs, Va., Federal Reserve Bank of Richmond.\n\nPrescott, E. C., and R. Townsend, 1984. \"Pareto Optima and Competitive Equilibria with Moral Hazard and Adverse Selection,\" Econometrica, 52 (January), 21-46.\n\nTownsend R., 1987. \"Arrow-Debreu Programs as Microfoundations of Macroeconomics,\" in T.F. Bewley, Advances in Advances in Economic Theory: Fifth World Congress, Econometric Society Monograph Series no. 12, New York and Melbourne, Cambridge University Press, 379-428.\n\nWeinberg, J. 2006. \"Borrowing by U.S. Households,\" Federal Reserve Bank of Richmond 2005 Annual Report, 4-16."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2006",
        "date": "October 30, 2006",
        "title": "Monetary Policy Tactics and Strategy",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke October 30, 2006 at the Baltimore Convention Center in Baltimore, Md.",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2006/lacker_speech_20061030",
        "content": "Thank you for that kind introduction, Don. It is a pleasure to be with this esteemed group today. This morning, I’d like to talk about monetary policy, but before I do, I need to note that, as always, the views I express are my own, and do not necessarily coincide with the views of my colleagues within the Federal Reserve. I’d like to talk about monetary policy from two different perspectives: tactics and strategy. These obviously are two pertinent aspects of any sustained planning or decision-making endeavor, whether it involves public policy or the private goals of businesses or households. By tactics, I mean the decisions we make and the actions we take on a day-to-day, month-to-month, or, in the case of the Federal Open Market Committee, twice-a-quarter basis. Our most visible tactical decision is our choice of the federal funds rate. You have probably noticed that I have disagreed with many of my colleagues on this tactical choice at recent meetings, and I will say a few words later on about why.\n\nTactics are in a sense reactive – for us, the choice of appropriate policy actions as economic conditions unfold. Strategy, on the other hand, is the more forward-looking part of a decision-making problem: The process by which you establish specific goals and objectives, and think through the types of actions – that is, tactical choices – that are likely to be required to move you toward your goal. A strategy doesn’t pin down all of your actions in advance. Rather, a strategy guides your thinking about how to make tactical choices in response to incoming information in a way that is consistent with achievement of your long-run goals.\n\nTactics\n\nIn the first part of my remarks this morning, I would like to review the tactical situation facing U.S. monetary policymakers. To set the stage, let me start with the broader context. The U.S. economy currently is in a period of transition. In the three years leading up to the second quarter, real gross domestic product – our broadest measure of total economic activity – grew at a 3.75 percent annual rate. That’s a very healthy growth rate to sustain over a number of years, and it significantly cut into the underutilization of labor resources that emerged during the recession earlier in this decade. Over 5 million new jobs were created over this period and the unemployment rate fell by a full 1.5 percentage points. Labor market conditions are fairly firm now, and the economy is transitioning to a period of growth at a rate consistent with job creation roughly matching the growth in the number of workers over time. Although there is some uncertainty about exactly how fast that is, it is probably somewhere around 3 percent per year, and it would probably involve creating roughly 100,000 jobs per month.\n\nIt would not be unusual for the transition to trend growth to be a little bumpy, however. That occurred back in 1995, for example. Growth in the first half of that year dipped below 1 percent at an annual rate before returning to a healthy pace that was sustained for the next five years. And this time around there is an obvious reason to expect growth to drop below average for a time, namely, the end of the housing boom. I’ll offer a couple of observations about the boom itself before I talk about its aftermath.\n\nFirst, the recent housing boom was very large by historical standards; a couple of numbers will help illustrate. In 2005 almost 2 million new homes were built in the U.S., which is about 50 percent more than the average number built each year in the 1990s. And last year the average price of a home sold in the U.S. rose 13 percent; versus an average increase of less than 3 percent per year back in the 1990s.\n\nSecond, it’s important to remember that the recent housing market boom was driven by fundamental factors that were – and still are – quite favorable. Population continues to expand; for example, last year the number of households increased by one percent nationwide. Income is growing – so far this year, inflation-adjusted disposable income per person has increased at a 2.3 percent annual rate. Household net worth is 53 trillion dollars, which represents over five-and-a-half years of disposable personal income. The tax treatment of housing remains highly favorable. And finally, mortgage interest rates were extremely low for many years, and even now are quite reasonable by historical standards.\n\nThis multi-year surge in housing investment was bound to come to an end, as the demand for upgrades and first homes became satiated. In addition, the rise in mortgage interest rates since 2004 has helped dampen demand. In fact, it seems likely that much of the increase in interest rates was anticipated, and thus probably gave an extra boost to demand in 2005 as consumers took advantage of what they saw then as the waning days of lower mortgage rates. A return to more normal housing market conditions is well under way. New home sales are down about 20 percent from last year’s peak, and housing starts have fallen by a similar magnitude. The rate of price appreciation has fallen substantially as well, to the point that average prices were lower in September than they were a year earlier, although data on average sale prices are distorted by changes in the composition of sales.\n\nSome further retrenchment seems likely in the months ahead, as housing market activity returns to a more sustainable level in which volume, inventories and time-on-market are closer to historical averages. This adjustment naturally involves a fair amount of uncertainty for market participants. Both buyers and sellers are probably more unsure than usual right now about where prices need to settle in order to clear markets. In the meantime, they are collectively engaged in a time-consuming process of discovering the prices at which expectations and plans of buyers and sellers are mutually consistent. But while there is substantial uncertainty about where the bottoming out will occur, I don’t think a catastrophic collapse in housing activity is likely, since the fundamental determinants of housing demand that I listed earlier remain favorable – prospects for population and real income growth look good, net worth remains high, and after-tax mortgage interest rates are still historically low. In fact, tentative signs are emerging that housing markets may be stabilizing, although because housing data are notoriously choppy, one should treat month-to-month numbers with more than the usual amount of care right now.\n\nOutside of housing, the rest of the economy is in reasonably good health. Business capital spending, for example, has been quite a bright spot in recent years. Since early 2003, business fixed investment has grown at over a 6.5 percent annual rate, and since the beginning of this year has grown at an 8.8 percent rate. This more than offset the 10 percent contraction in residential investment over the same time period. The fundamental underpinnings of near-term investment demand are encouraging as well. Profitability is high, capacity utilization has been steadily rising, and many firms see strong demand for their products. So I expect capital spending to continue to be a source of strength over the next several quarters.\n\nMany economic analysts are concerned about the potential fallout of a weakening housing market on consumer spending. Could falling housing prices cause consumers to rein in spending? It’s important to begin with fundamentals. While fluctuations in household wealth are capable of affecting spending at the margin, consumers’ spending behavior is predominantly determined by their current and future income prospects. And those prospects are looking pretty good right now. With the unemployment rate at 4.6 percent, the labor market is looking fairly tight. Despite large increases in gasoline prices earlier this year, inflation-adjusted incomes have been rising, as I noted earlier. And now that we’ve seen some relief at the gas pump, it would not be surprising to see a modest pickup in real income growth in the next couple of months. The deceleration and fall in housing prices certainly has cut into household net worth to some extent, and consumer spending did decelerate at the beginning of this year. But so far, such wealth effects have been relatively limited – consumer spending rose a healthy 3.1 percent in the third quarter.\n\nTaking all these considerations into account, I would look for consumer spending to continue to expand at a reasonably good pace even if housing prices come in weaker than expected.\n\nThe labor market is another widely-cited arena for potentially adverse spillover effects from the housing market. We have seen employment in the residential construction sector fall this year as residential building activity has declined. Fortunately, however, nonresidential construction is on an upswing – over the four quarters ending in September, investment in nonresidential structures rose over 13 percent in real terms. This has allowed many home construction workers to simply change construction jobs rather than become unemployed. Indeed, although in September residential construction employment had fallen by 54,000 since peaking in February, nonresidential construction employment was up by 95,000.\n\nSo the outlook for overall spending looks reasonably good: consumer spending is on track, and business investment is robust. The downturn in housing activity has and will subtract from headline GDP growth, but it is not likely to cancel out these sources of strength.\n\nIn contrast, the outlook for inflation is discomforting. Over the last two years, there have been several episodes in which energy prices have surged and pushed up the overall inflation rate. More troubling is the fact that we have seen significant increases in “core inflation” – the measure of inflation that strips out food and energy prices. According to our preferred index, the price index for personal consumption expenditures, core inflation ran close to 3 percent this past spring.\n\nWhile core PCE inflation has settled down to around 2.25 percent, that is a rate that would be unacceptable on a sustained basis. Here is where tactics have to be driven by strategy. The Federal Reserve’s strategic goal, as a central bank, is price stability. We are the only institution that can achieve this, and attaining and maintaining price stability is the best contribution we can make to maximizing economic growth.\n\nI and several other members of the FOMC have expressed the view that our price stability objective is equivalent to a core PCE inflation rate in a band between 1 and 2 percent, that is, a band centered around 1.5 percent. You might think that price stability should mean inflation equal to zero, that is, prices not changing over time, on average, but there are known upward biases in our available price indexes, and targeting a band above zero is a way of taking those biases into account.\n\nCore inflation has been above this 1 to 2 percent band for over two years now, since April 2004, and is running at 2.5 percent so far this year. The longer inflation remains elevated, the more difficult it will be to bring it back down. As people observe actual core inflation between 2.25 and 2.5 percent, and as they observe the FOMC’s tactical reactions to those numbers, they form expectations regarding future inflation and those expectations become the basis for price setting in product and labor markets. (By the way, it was for his contributions to economic research on exactly this phenomenon that Professor Edmund Phelps was awarded the Nobel Prize in economics several weeks ago. Some of his cited work emphasized the extent to which the public’s expectations will shift over time as they observe policymakers actual tactical choices.) The strategic issue here is that if the Fed allows inflation to remain above target for too long, inflation expectations could become tightly centered around the higher rate.\n\nThis danger is what prompted me to vote at recent FOMC meetings for tactics aimed at bringing inflation down more rapidly, and in a way that convinces the public of our strategic intent to keep inflation low and stable. Against this risk, one must weigh the risk that a further increase in the federal funds rate might exacerbate the housing-related slowdown. My assessment at recent meetings has been that the economy is resilient enough right now to withstand further policy tightening.\n\nStrategy\n\nThere is another way for the public to learn about our intent, however, beyond simply observing our tactical choices. We can try to communicate more directly with the public about our monetary policy strategy. Households, businesses and financial market participants form their expectations about future inflation from several sources: past inflation experience, their understanding of the economic outlook, their observation of the Fed’s monetary policy actions, and their beliefs about the Fed’s inflation strategy. A key component of monetary policy strategy is our long-term goal for inflation – what the Fed would like to see as an average rate of inflation over long periods of time. While it is difficult to perfectly control inflation quarter to quarter, the Fed can pin down long-run average inflation very well.\n\nInflation targeting has been adopted by many other central banks: the European Central Bank and the central banks of the United Kingdom, Sweden and New Zealand, for example. There are many aspects of inflation targeting as it has been implemented abroad – inflation reports, consultations with Finance Ministers, supporting legislation, and so on. But the core feature of inflation targeting everywhere is communicating an explicit numerical inflation objective. So I think it makes sense to talk about inflation targets in the context of the broader subject of “communications” – how we as a central bank communicate about monetary policy.\n\nI’d like to start by suggesting that we should not think of “conducting monetary policy” and “communicating about monetary policy” as two different things. It is certainly tempting to think of setting a target for a short-term interest rate and issuing policy statements as two separate acts that raise two separate sets of considerations. But modern monetary economics and common sense both tell us that the two are inseparable. People will always try to figure out what the central bank is going to do with its policy instrument in the future, no matter how much or how little the central bank actually says about these things. If the central bank says nothing, it still implicitly communicates via its actions, because people will always try to infer the central bank’s future conduct from their current and past actions. In fact, modern monetary economics teaches that there is a very real sense in which “monetary policy is all about communication.”\n\nThe logic behind this statement is not complex or arcane. First, money is intrinsically useless; it has value only for what it can purchase in the future. People accept money in exchange for valuable resources only because they expect others to accept it in exchange in the future. Therefore, the current value of money depends on the value people expect money to have in the future. So expected future inflation can give rise to inflation pressures today.\n\nA corollary to this principle is that controlling current inflation requires controlling people’s expectations for future inflation. This is the sense in which monetary policy is all about communications, because anything we do to shape people’s perceptions and expectations amounts to communications, whether we’re communicating by words or by deeds. The central implication here is the importance of managing and stabilizing the public’s inflation expectations.\n\nThe history of the 1970s provides a vivid illustration. The Federal Reserve allowed inflation to rise during economic expansions and following oil price shocks. Expectations regarding future inflation subsequently rose as well, as the public observed our tolerance for rising inflation. This provided a further impetus to inflation, as those expectations influenced wage bargains, and price-setting by firms. A large part of the battle to reduce inflation in the 1980s and ’90s was a battle to dampen the “inflation psychology” that had taken hold, that is, a battle to convince the public that we would achieve and maintain price stability. Over the course of those decades, we and other central bankers around the world learned another important lesson relating to communications: Namely, that words and subsequent deeds must ultimately be consistent.\n\nThe economic term for this principle is “time consistency,” which simply means that your tactical choices have to be consistent with people’s expectations of those choices over time. (By the way, the 2004 Nobel Prize in economics honored Professors Finn Kydland and Ed Prescott for their pioneering work applying exactly this principle to, among other things, monetary policy.) The more common term for this principle is “credibility,” and a popular slang expression is “walking the talk.” The 1970s again provide a vivid illustration: All throughout the 1970s, the Fed said it was against inflation, but our actions spoke differently and people came to believe our actions rather than our words. In the early 1980s, the battle to reduce inflation required costly policy actions to convince people of our intentions. It took time and effort to establish our credibility.\n\nWhat does this mean for inflation targeting? If we adopt an inflation target, we will have to be sure that we back up our commitment with appropriate monetary policy actions. Otherwise, our target would just be viewed as “cheap talk.” One way to appreciate the potential value of an explicit inflation target is to consider how it might have helped us cope with inflation dynamics over the last few years. On several occasions, usually in response to energy price shocks, questions have arisen about where inflation was headed, that is, about what inflation rate we were willing to tolerate. After Hurricane Katrina, for example, when retail gasoline prices rose above $3 a gallon, there was widespread speculation that the Fed would pause in order to protect growth rather than protect price stability. Measures of inflation expectations rose noticeably as a result. That speculation was off-base, though. Forceful public statements by Committee members tamped down those expectations, but core inflation did bump up for a time as firms were able to pass on energy price increases to buyers who may have been anticipating a broader upswing in inflation.\n\nA similar episode occurred this past spring in response to another round of energy price increases. Inflation expectations rose, and were subsequently tamped down by Committee member communications, but not before another bulge in core inflation emerged, a bulge that has now only partly subsided. I take both these episodes as mini-inflation-scares. In both cases, and others as well in recent years, I believe some financial market volatility can fairly be attributed to public uncertainty about our intentions for inflation.\n\nIf we had had a credible inflation target in place, I believe that market reactions most likely would have been different. People would have known that we intended to return core inflation to our target. Maintaining the credibility of a target, however, would impose constraints on our tactical choices. If core inflation drifts substantially above target, I believe that the Committee would feel compelled to explain how long it was likely to take for inflation to return to target and to comment on the policy actions that would likely be required to get there. Moreover, if we see evidence that markets do not view our target as credible, we may feel compelled to take further policy actions to enhance our credibility.\n\nThere are differences of opinion among economists about short-run inflation dynamics, and about how fast the central bank should seek to return inflation to target. But there is virtual unanimity that the central bank can bring about any average inflation it likes over the horizon of a decade or more. Moreover, it is important to recognize that this is not the case with regard to real economic quantities such as output growth or the unemployment rate. The central bank can influence the path of output and employment over short horizons, but in the long-run, real economic variables are determined by the fundamental forces of productivity growth, population growth, labor force participation decisions, savings behavior, and the like. There is virtual unanimity that these are ultimately beyond the control of the central bank, and so to set an explicit objective for growth or employment would be a mistake.\n\nI have talked about monetary policy tactics and strategy, and have touched on the interplay between the two. Tactical policy decisions should be guided by strategic objectives; this is an obvious and widely applicable principle. But in the case of monetary policy, the public’s expectations regarding future tactical decisions play a crucial role in determining current outcomes, because inflation expectations play such a crucial role in determining current inflation. Without having credibly and explicitly communicated our strategic goals, tactical decision-making is more challenging than it needs to be. Policymakers are in that case forced to resort to policy actions – that is, funds rate changes – to influence the public’s expectations. Accordingly, one factor contributing to my voting decisions at the last few FOMC meetings was a sense that inflation expectations were somewhat higher than would be consistent with my definition of price stability. As communications tools go, however, funds rate changes are relatively blunt. I believe, therefore, and I hope to have convinced you, that an explicit numerical objective for inflation would improve the effectiveness of both the strategy and tactics of monetary policy."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2006",
        "date": "October 11, 2006",
        "title": "The Regional Economic Outlook",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke October 11, 2006 at the District of Columbia Chamber of Commerce.",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2006/lacker_speech_20061011",
        "content": "It is a pleasure to be with you today to discuss the economic outlook for the region. I work, as Barbara’s kind introduction noted, at the Federal Reserve Bank of Richmond. The fact that our nation’s capital lies within the Richmond Federal Reserve District, rather than the other way around, is an odd byproduct of decisions made over 90 years ago. When establishing the Federal Reserve System as the nation’s central bank, Congress created a confederation of regional banks, rather than a single, centrally located bank. The founding organizers then made Richmond the headquarters for the Fifth Federal Reserve District, which covers the area from West Virginia and Maryland in the North down to the Carolinas in the South. The founders’ motivating vision was that the nation was better served by an institution that was closely linked to the diverse economies that make up our country. And so, one of our key responsibilities at the Reserve Banks is to understand local economic conditions around our Districts. Of course, the Fed is well represented inside the beltway, since Washington is the home of the Board of Governors of the Federal Reserve System, the entity that oversees Reserve Bank activities. They are kind enough to let me roam Washington at will, and we are kind enough to cut their paychecks for them.\n\nI plan to discuss economic conditions in our Federal Reserve District, with a particular focus on conditions in the Washington metropolitan area. But because our region’s economy is so tightly linked with national economic trends, I will spend some time talking about the overall economic picture as well. As always, my remarks reflect my own views, and do not necessarily reflect the views of my colleagues within the Federal Reserve System, but you probably gathered as much from my recent voting record. To set the stage, let us start at the national level. The U.S. economy currently is in a period of transition. Looking back over the last three years, real gross domestic product — our broadest measure of total economic activity — grew at a 3.75 percent annual rate. That’s a very healthy growth rate to sustain over a number of years, and it coincided with a significant improvement in the labor market, with 5.3 million new jobs created and the unemployment rate falling by a full 1.5 percentage points. Now that labor market conditions are fairly firm, the economy is transitioning to growth at a trend rate of around 3 percent per year — a pace at which job growth will match the growth in the number of workers over time.\n\nI should note at this time that it would not be unusual for the transition to trend growth to be a little bumpy. That occurred back in 1995, for example. Growth in the first half of that year dipped below 1 percent at an annual rate before returning to a healthy pace that was sustained for the next five years. And this time around, there is an obvious reason to expect growth to drop below average for a time, namely, the end of the boom in residential housing. I’ll talk more about this later on, because how the adjustment in the housing market plays out is an important source of uncertainty in the outlook, both regionally and nationally.\n\nThe other major source of uncertainty in the outlook is inflation. Price stability is the central responsibility of the Federal Reserve — we contribute best to economic growth when we keep inflation low and stable. That is widely viewed as requiring inflation to average between 1 percent and 2 percent, as measured by the core price index for personal consumption expenditures. Inflation, by that measure, has drifted up to 2.5 percent this year. Inflation is likely to moderate over the near term, but there is some uncertainty as to how long that will take. Should inflation persist around the current elevated level, firmer monetary policy would be required to restore price stability. As a result, I believe policymakers will need to remain quite vigilant in the period ahead, to ensure that inflation moderates at a sufficient pace.\n\nThe overall outlook for the Fifth District’s economy is positive, though perhaps somewhat less so than if I had been giving this talk earlier in the year. Employment has grown over the twelve months ending in August at a solid 1.7 percent rate, better than the 1.3 percent growth rate for the U.S. Employment growth has slowed since the spring, similar to the national pattern. The combined unemployment rate for the six jurisdictions in our District has been steady near the current 4.5 percent rate, which is comparable to the 4.6 percent unemployment rate for the U.S. as a whole. Household financial conditions are also consistent with the national figures, with second quarter personal income expanding 2.2 percent against a 2.7 percent annual rate of growth nationally. And recent data on household finances indicate that those living in our region are not facing any unusual difficulties servicing mortgages or otherwise meeting financial obligations.\n\nThis overview of the District’s economic picture masks significant differences between several unique economies within the Fifth District. The Carolinas, along with the southern edge of Virginia, for example, is a region that has historically been dependent on manufacturing, particularly textiles and furniture. These industries have shifted much of their production overseas in recent years. Six years ago, employment in textiles and apparel comprised 23 percent of all manufacturing employment in the region. As of August of this year, that share is down to just 16 percent. Despite this tremendous structural change, economies in these areas are currently experiencing job growth at over 2 percent per year, greater than the national average, and employment in the Carolinas has recently reached new peaks. Driving this rapid growth is the continuing expansion of construction jobs — I’ll say more about this later — added strength in service sector jobs, and a lessening of manufacturing job losses. In fact, new manufacturing operations requiring more highly skilled workers are offsetting some of the decline of the old-line, low-skill industries. On the service side, we are seeing strong growth in the education and health services category. This mirrors the national picture, likely reflecting greater need for health services as we age.\n\nAnother big factor, though, is financial services. In the year ending in August (the latest data available), growth in financial sector employment has been about double the rate of overall job growth in the Carolinas. Some of this is attributable to the recent rapid growth of jobs in Charlotte, which is by some measures the nation’s second largest banking center. The Charlotte area accounts for about one-fifth of all the jobs created in North Carolina in the past two-and-a-half years. In contrast, South Carolina still has counties — mostly rural — with double-digit unemployment rates, signaling perhaps that the state still has some catching up to do and that growth in the services sector has not necessarily been as robust in those parts of the state that have been losing manufacturing jobs. In many ways, the southern tier of Virginia is broadly similar to the economy of the Carolinas and it shares some of the economic issues of those states. An exception is the greater Norfolk — Virginia Beach area, which has a large military component in addition to tourism and services. This area has recently posted strong employment growth of 1.7 percent.\n\nLooking westward within the Fifth District, West Virginia has also been shifting to a services economy, with nearly half of the state’s residents residing in officially designated metropolitan areas — those in the eastern panhandle being part of the Washington metropolitan area. Outside of the panhandle, however, many areas of West Virginia still rely heavily on manufacturing or mining. Those industries continue to exhibit a pronounced cyclical nature, making the state’s economy somewhat more volatile than other areas of our District. Nevertheless, recent expansions by Japanese car manufacturers, combined with the boost to natural resource industries from relatively high energy prices, have paid dividends to the state in recent years.\n\nCloser to home for you, northern Virginia, Washington, D.C., Maryland and portions of eastern West Virginia make up the third broad economic region of our District. This region is composed primarily of service-oriented urban areas that have historically outperformed other regions in our District, as well as the national economy. The federal government has served as a major source of job growth for the region, and has traditionally acted as an economic stabilizer. This region was affected much less than the rest of the country by the recession earlier in this decade, for example. And employment growth has continued to outpace the nation; Washington metro area employment grew 2.5 percent in the year ending August, adding more jobs than any other large metro area outside of New York and Phoenix. Growth in recent years has been powered by defense spending and the ramping up of homeland security. The Washington area receives a substantial share of federal government procurement dollars, and a substantial portion of that spending is technology-related. The job growth has been concentrated in government and professional services and has been in relatively high-skilled, high-pay occupations. But the appropriate question to ask is whether job growth will continue at a breakneck pace. I hesitate to forecast federal policy with a hotly contested election right around the corner, but it seems reasonable to suppose that the real growth in defense and security spending will taper off somewhat in the years ahead. Fortunately, a substantial portion of the recent job increases have come in the private sector, and a substantial portion of these have been in sectors associated with technological innovation. We believe that total metro area employment should continue to expand at a healthy pace, but at a rate that gradually declines over the next couple of years.\n\nOn balance, even with the regional differences, the broad outlook across the Fifth District economy remains solid going forward, though there are risks for the regional economy, much as there are for the national economy. Housing is easily the most widely discussed economic risk at the moment. Because home prices in the Washington area have been notoriously high in recent years, I would imagine that the housing market commands even more attention hereabouts. And with good reason. Washington experienced a more rapid price appreciation than most of the nation during the recent housing boom, and has seen a sharper downturn in recent months.\n\nBut to understand any given local housing market, it is important to understand several macroeconomic factors affecting housing markets nationwide. So I would like to talk a bit about these macro factors, and then come back to the local housing market. First, remember that the recent housing boom has been very large by historical standards. A couple of numbers help illustrate the magnitudes involved. In 2005, almost 2 million new homes were built in the U.S., which is about a 50 percent increase from the average number built each year in the 1990s. Last year the average price of a home sold in the U.S. rose 13.3 percent; back in the 1990s, the average increase was 2.8 percent per year. The acceleration was even greater in the Washington area, where price appreciation ran as high as 25 percent last year versus around 2 percent in the 1990s.\n\nSome of the concern about the housing outlook is motivated by the observation that large swings in residential housing activity, often in response to movements in interest rates, have played a big part in the post-war business cycles. But if you look carefully at the data, you see a big change beginning in the 1980s. Before that time, the way financial institutions were regulated contributed to extreme volatility in housing markets. Most home purchases then were financed by thrift institutions, who raised the bulk of their funding through retail deposits that were subject to interest rate caps. When interest rates rose in the course of a business cycle, money would tend to flow out of regulated financial institutions in search of higher returns, a process known as disintermediation, which caused severe disruption to the home financing system. This regulatory structure made housing activity much more interest rate sensitive, and much more volatile, than it otherwise would have been. The regulations capping deposit rates were eliminated in 1980, and the housing market’s role in the business cycle has been quite different since then. In addition, cyclical interest rate movements were substantially larger prior to the mid-1980s. One should be cautious, therefore, about comparing the current housing cycle to historical episodes, particularly to episodes prior to the mid-1980s.\n\nIt’s important to remember that the recent housing market boom was driven by fundamental factors that were — and still are — quite favorable. I’ll just briefly list a few for you. Population continues to expand; for example, last year the number of households increased by 1 percent nationwide. Income is growing — so far this year, inflation-adjusted disposable income per person has increased at a 2.8 percent annual rate. We are a wealthy nation; household net worth is 53 trillion dollars, which represents over five-and-a-half years of disposable personal income. The tax treatment of housing remains highly favorable. Finally, mortgage interest rates were extremely low for many years, and even now are quite reasonable by historical standards.\n\nGiven these solid fundamentals, it is not surprising that the demand for housing has risen so strongly in recent years. As one would expect, we saw both higher production and higher prices in response to the sustained rise in demand. The rise in mortgage interest rates since 2004 has helped dampen the demand for housing, but it seems likely that much of the increase in rates was anticipated. In fact, the upward move in rates may have given an extra boost to demand in 2005 as consumers took advantage of the waning days of lower mortgage rates. With the surge in demand apparently satisfied now, we can expect to see a “return to normalcy” in the housing market, if I can borrow a phrase from a former Washington resident. Such a return to normalcy would involve lower production than we saw at the peak, and certainly a lower trajectory for housing prices.\n\nThis transition in the housing market is well under way. New home sales are down 17 percent, housing starts have fallen 20 percent, and the rate of price appreciation has fallen substantially, to the point that average prices were slightly lower in August than they were a year ago. These are national figures, of course, and more dramatic swings can be seen in some localities, particularly in areas that saw the strongest increases in housing prices and activity. The Washington area is a good case in point. Prices shot up more rapidly than elsewhere because strong area job growth created demand for housing that area builders had difficulty meeting. Builders say that the availability of building lots limited housing production in many localities within the region. And this makes sense as a simple matter of supply and demand: if supply does not expand elastically to meet a rise in demand, then prices have to rise instead. Indeed, looking across regions both within our District and around the U.S., home price appreciation was greatest where the supply of buildable lots seems to have been least elastic. One would expect most of the price increases in such regions to show up as rising land values, rather than as increases in the value of structures. And when demand subsides, land values reverse course. In contrast, prices did not accelerate as much during the boom in other metro areas in the Fifth District, where the supply of buildable lots was more elastic. And as a result, prices are not decelerating by as much in those areas either. Looking ahead, forecasts by area economists suggest that the Washington metropolitan area will continue to experience relatively rapid job growth. If correct, and if the supply of building lots remains inelastic, these forecasts suggest that the ongoing correction in area housing markets will find a floor sooner rather than later.\n\nAt the national level, some further retrenchment in housing markets is likely in the months ahead. But while there is substantial uncertainty about where the bottoming out will occur, I don’t think a catastrophic collapse in housing activity is likely, since the fundamental determinants of housing demand that I listed earlier remain favorable: prospects for population and real income growth look good, net worth remains high, and after-tax mortgage interest rates are still historically low. Instead, I believe we are seeing a return to a more conventional level of housing market activity in which volume, inventories and time-on-market are closer to historical averages. This adjustment naturally involves a fair amount of uncertainty for market participants. Both buyers and sellers are probably more unsure than usual right now about where prices need to settle in order to clear markets. In the meantime, they are collectively engaged in a time-consuming process of discovering the prices at which expectations and plans of buyers and sellers are mutually consistent.\n\nMany macroeconomic analysts are concerned about the potential fallout of a weakening housing market. The direct impact of the housing market on overall economic activity is easy to calculate. The measure of residential investment spending that is included in real GDP has now fallen for three consecutive quarters. In the second quarter it fell at an annual rate of 11.1 percent, and appears likely to decline even more rapidly in the second half of this year. Since residential investment accounts for less than 6 percent of GDP, that lowered the real GDP growth rate by about seven-tenths of 1 percent in the second quarter. It would not be surprising to see housing reduce growth by even more for a few quarters. That would be a significant drag on the economy, but it would not end the expansion either, especially in light of offsetting strength in business investment spending, a topic I will touch on later.\n\nWhile the direct effect of housing on GDP may not be overly large, some analysts worry about indirect effects, such as lower housing wealth leading to lower consumer spending. Again, it’s important to begin with fundamentals. While fluctuations in household wealth are capable of affecting spending at the margin, the behavior of consumers is predominantly determined by their current and future income prospects. And those prospects are looking pretty good right now. With the unemployment rate below 5 percent, the labor market is looking fairly tight right now. Despite large increases in gasoline prices earlier this year, inflation-adjusted incomes are rising, as I noted earlier. And now that we’ve seen some relief at the gas pump, it would not be surprising to see a modest pickup in real income growth in the next couple of months.\n\nThe deceleration and fall in housing prices certainly will cut in to household net worth to some extent, but so far, such wealth effects have done little to slow household spending.\n\nCould housing prices end up falling sharply enough to cause consumers to rein in spending? Perhaps, but consumers’ balance sheets generally are not as fragile as some commentary might lead one to believe. Housing debt is only 44 percent of the value of household real estate. With that substantial equity position, most homeowners who are not planning to move for other reasons can pretty much ignore transient price fluctuations. And with relatively high levels of financial net worth, most households are well buffered against price fluctuations. Moreover, as I emphasized earlier, household spending is driven mainly by current and future income prospects. Taking all these considerations into account, I would look for consumer spending to continue to expand at a reasonably good pace, even if housing prices come in weaker than I expect.\n\nI should note that the end of the housing boom could not have been a complete surprise to most participants. Sure, it’s nice to sell your home when bidding wars and escalator clauses are common, as they were in 2005. But these conditions were fairly unusual in most markets, and it’s hard to believe many people seriously thought they would persist indefinitely. This is another reason to believe that most people are likely to be reasonably well-positioned for the end of the boom.\n\nAnother potential spillover that some analysts like to mention involves mortgage lending, especially with new financing options available to consumers. My sense is that the underwriting and pricing of mortgages has on the whole been sound, despite some individual anecdotes that suggest otherwise. The broad range of households that have taken out nontraditional mortgages are going to find them advantageous, even if, as with many financial products, a small fraction end up regretting their choice after the fact. Moreover, the banking industry looks healthy right now, with strong profitability and high levels of capital. Loan delinquencies are quite low by historical standards, as are chargeoffs of real estate loans. So it looks to me as if the end of the housing boom is unlikely to have any broader spillovers as a result of financial repercussions. Nor is it likely to be exacerbated by financial disintermediation of the type we saw earlier in the postwar era.\n\nThe labor market is another potential arena for adverse spillover effects from the housing market. We have seen employment in the residential construction sector fall this year as residential building activity has declined. Fortunately, however, nonresidential construction is on an upswing — over the four quarters ending in June, real nonresidential investment rose 7.2 percent. Further increases in nonresidential construction will allow many workers to simply change construction jobs rather than become unemployed. Indeed, over the last year overall construction employment has actually risen by nearly 210,000 jobs even as housing activity has softened.\n\nAs I mentioned earlier, the expected further weakening in housing activity is likely to be largely offset by business capital spending. Over the last three years, business fixed investment has grown at a quite solid 6.6 percent annual rate. Since business fixed investment is over 10 percent of GDP, this means that is has added about two-thirds of a percentage point to GDP growth, which has counteracted the drag from housing that I cited earlier. Indeed, when business investment demand fell sharply following the technology boom of the late 1990s, and the FOMC lowered interest rates in response, the anticipation was that interest rate-sensitive sectors such as housing and consumer durables would take up some of the slack until business investment spending rebounded. Now that business investment has substantially recovered, it makes sense for housing activity to subside in turn.\n\nThe fundamental underpinnings of near term investment demand are encouraging. Profitability is high, capacity utilization has been steadily rising, and many firms see strong demand for their products. Thus, it is not surprising that new orders for capital equipment increased 7.5 percent over the last year, and I see a solid outlook for capital spending over the next several quarters.\n\nSo, the outlook for overall spending looks reasonably good — consumer spending is on track, business investment is robust, and the softening in the housing market is not likely to be large enough to cancel out those sources of strength. To round out the picture on the national economy, let me say a few words about the labor market and the inflation outlook. Last year we added almost 2 million jobs, or about 165,000 jobs per month. We maintained that rapid expansion in the first quarter of 2006, but over the last six months job creation has averaged 118,000 jobs per month. While that sounds low, it’s actually pretty close to what we would need to keep employment growth in line with population growth. And with the unemployment rate fairly low, it’s appropriate that employment growth is close to its trend value. So what we’re seeing in the labor market looks quite consistent with trend growth in overall economic activity.\n\nThe inflation outlook, on the other hand, is less appealing, and that is quite important to us at the Federal Reserve because as I mentioned earlier price stability is our central responsibility. I’ve said on several occasions that I would like to see inflation average about 1.5 percent over time, as measured by our preferred statistic, the price index for core personal consumption expenditures (often referred to as just “the core PCE index.”) Moreover, I have also said that I would be comfortable if inflation was a little higher or lower, coming in between 1 percent and 2 percent. Several other policymakers and economists have also endorsed that range as a functional definition of price stability. But inflation has been outside that comfort zone for over two years now. It was 2.2 percent in 2004, 2.1 percent in 2005, and has come in at a 2.5 percent annual rate so far this year. And inflation looks worse if, instead of using the core PCE index, we were to use the overall index, which includes energy prices. That measure of inflation was 3.2 percent over the last 12 months.\n\nMy concern regarding these inflation readings is straightforward. On a month-to-month basis, practically anything that affects the supply or demand for particular commodities can unexpectedly move a price index around, and the financial press dutifully highlights a different special factor each month. But over a period of several years, a central bank can achieve whatever average inflation rate it chooses. And while there really is no benefit to high inflation, keeping inflation low and stable has a wide range of benefits to society. Low inflation helps people make better plans and commitments, since they will have a better idea about the future purchasing power of their money. And when inflation is low, people don’t have to devote time and effort to protecting their wealth from being eroded by inflation. Thus, it is quite important to keep inflation from drifting away from target over time.\n\nMoreover, the longer inflation remains elevated, the more difficult it will be to bring it back down. As people observe actual core inflation of 2.5 percent, along with the FOMC’s reactions, they adjust expectations regarding future inflation, and those expectations become the basis for price setting in product and labor markets. (By the way, it was for his contributions to economic research on exactly this phenomenon that Professor Edmund Phelps was awarded the Nobel Prize in economics a few days ago.) If the Fed were to allow inflation to remain above target for too long, inflation expectations could become centered around the higher rate. Once that occurs, history tells us that strong and more costly policy actions would be needed to bring inflation and inflation expectations back down. We don’t have any perfect measures of inflation expectations, but what we do have suggests that market participants do not foresee a rapid fall in core inflation. This is why I have argued for further policy actions to convincingly restore price stability."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2006",
        "date": "May 18, 2006",
        "title": "The Evolution of Consumer Finance",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke May 18, 2006 Conference of State Bank Supervisors in Norfolk, Va. ",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2006/lacker_speech_20060518",
        "content": "At the outset this morning I would like to congratulate you: 2005 marked the first year in the history of the federal deposit insurance program in which there were no failures of FDIC-insured institutions. While the banking industry clearly has benefited from a relatively healthy macroeconomic environment, I think the lack of bank failures last year is strong evidence that supervisory agencies — both state and federal — have been doing an outstanding job. In fact, one could argue that we have done too good a job, since, as our recently retired Fed Chairman was fond of observing, the optimal number of bank failures is certainly not zero, the point being that risk-taking is an essential part of banking, and even if our banking system as a general matter is taking only prudent, well-managed risks, there may still be some failures from time to time. Furthermore, risk is an inherent part of innovation, and when banks are trying new things, some will succeed and some will not. But innovation is vital to the continued growth and progress of the industry and its ability to provide the public with ever more useful and efficient financial services over time. I would like to note, as usual, that the views expressed are my own and are not necessarily those of my colleagues in the Federal Reserve System.\n\nThe theme of innovation and its relationship to risk permeates contemporary banking and finance. We certainly see it in the growing use of interest rate and credit derivatives for risk management. While such tools can assist organizations in limiting their exposures to risk, they can also, if used imprudently, be a source of increased risk. Although the term “financial innovation” typically conjures up images of what we might call “high finance,” innovation has also been arguably the most important driver for the evolution of the market for consumer credit, the subject of my talk to you this morning. Because much of the transformation of consumer credit markets in recent decades amounts to the development of new and more varied loan products for a growing set of consumers with more variable pricing.\n\nMany recent innovations in consumer credit products have garnered a fair amount of attention both in the popular press and in supervisory and legislative arenas. Rather than address any of the specifics surrounding these issues, I would like take two steps back and look at these innovations from a broader perspective. Over the last two decades, we have witnessed what can arguably be called a revolution in retail consumer finance. Perhaps the hallmark of this revolution has been the dramatic expansion of unsecured lending through the proliferation of credit cards. This growth has not been limited to unsecured credit, but also includes innovative mortgage and home equity lending.\n\nThe wave of innovation that has driven these trends has brought widespread change to the financial services industry. While this expansion of credit has been broadly beneficial to a wide array of consumers, it has also brought with it a growth in the number and frequency of consumer defaults and bankruptcies, a trend that has contributed to a growing concern in some quarters that American households have “lost control” of their finances to a dizzying array of new products and options and that lenders are taking on new risks for which they may not be adequately prepared.\n\nMy main purpose today is to discuss the nature of innovation in lending, particularly at the retail end. As regulators, it is important for us to understand that innovation is an inherently risky activity, whether we are talking about a large, sophisticated institution that “invents” new products or processes, or a smaller community bank that innovates by putting new products or processes to use. As supervisors, it is important for us to understand that innovation in financial products is likely to be a feature of the banking landscape for many years to come. Supervisory practice and philosophy should view financial innovation as a relatively permanent feature of the banking industry, rather than treat each burst of innovation as an episodic, one-off occurrence. Thus, as a general principle, supervisors place a great deal of emphasis on the ability of bankers to measure, monitor and manage their risk profiles over time and make informed risk acceptance decisions, since risk is intrinsic to the banking business. Supervisory guidance regarding innovative consumer finance products should recognize their potential benefits to a wide array of households in addition to their risk management implications.\n\nThe expansion of consumer credit in the U. S. over the last decade and a half has been truly astonishing. This expansion occurred across a number of product lines. Probably most prominent has been the expansion of credit card lending. Home mortgage lending, including home equity credit, has also seen robust growth. Much new mortgage lending recently has taken the form of so-called “non-traditional mortgages,” such as “interest-only” mortgages that allow the borrower to defer principal payments for the first few years, or “option ARMs” that provide the borrower with several flexible payment options.\n\nAn especially prominent feature of the secular expansion of consumer credit has been the growth in lending to lower-income consumers, many of whom had in the past been unable to obtain credit on as favorable terms from the financial sector. Indeed, many had been unable to obtain credit except from fringe lenders such as pawnbrokers, payday lenders, or through informal arrangements with friends and family. Some borrowers that take out non-traditional mortgages would not have qualified for traditional mortgage products otherwise, or would not have qualified for loans as large. The growth in retail credit, thus, has brought expanded social benefits to a wide array of consumers.\n\nI would argue that this broad expansion of consumer credit has been driven by advances in information and communication technologies, which have reduced the cost of obtaining, evaluating and monitoring consumer account information. At a basic level, this dramatically increased the productivity of the back-office functions associated with all phases of the banking business. Payments processing and the associated book-keeping tasks became much cheaper. The result was a general decline in the intermediation costs that lenders ultimately must recover on top of their funding costs. Competition forced lenders to pass on these cost savings to borrowers in the form of lower lending rates. More consumers could afford to borrow, and the market expanded.\n\nThe application of information technology to the lending process itself has been evident in a number of ways, from the automation of underwriting to the use of credit scoring. Lenders have been able to access and utilize an expanded array of information about potential borrowers. These advances did not change the fundamental nature of underwriting, however; they just made it more effective. Underwriting is about making distinctions between people in order to decide whether to lend, and if so, how much to charge. When lenders were limited to reviewing paper loan applications and old-fashioned credit reports, they could sort potential borrowers into only a few broad categories. New technologies allowed lenders to bring more and more consumer-specific information to bear on lending and pricing decisions, and thus make finer distinctions between consumers. The result was that the lending decision and loan terms could be more closely tailored to individual borrowers. Lenders were able to offer lower interest rates to less-risky customers, and were able to pluck out the creditworthy from among the group of customers that formerly were unable to qualify for credit at all.\n\nBeyond the direct impact of new technologies on underwriting, innovations also contributed to the spread of loan securitization, by making it easier for investors to assess the risk characteristics of the underlying loans. Securitization separates loan origination from holding loans on a balance sheet, thereby allowing originating institutions to economize on funding and capital costs. And the development of quantitative methods for the pricing of options — combining the massive computing power that came online in the late 1980s with the models that finance theorists pioneered in the mid-1970s — opened the door for banks and other financial intermediaries to properly price the put feature inherent in a long-term mortgage, and the option value of loan commitments such as home equity lines of credit and credit cards.\n\nThe technology-driven expansion of retail credit is evident in the growth of household debt and in the proliferation of entirely new credit products. For example, many non-traditional mortgage products, particularly those offered to less-creditworthy borrowers, would not have been economically feasible without the new, more advanced pricing models and underwriting techniques. Likewise, home equity lines of credit became more widely available in part because of the lower cost of accurately assessing creditworthiness.\n\nThese developments have also had important consequences for the structure of retail lending markets and for the activities of community banks. Much of the innovation I have been describing involves realizing economies from standardization and from the application of technologies with substantial economies of scale. One implication has been the increased concentration of some consumer lending activity at relatively large lenders. This appears to be particularly true in the case of credit cards. On the other hand, the spread of securitization, especially in home lending, allows smaller institutions to remain in the market as originators of credits that then get securitized. On the whole, however, technological advances in lending based on quantitative underwriting seem to have favored larger institutions. Consequently, these changes have tended to move community banks toward lending based on less quantitative and more judgmental underwriting techniques where they appear to have a comparative advantage — exploiting the fact that quantitative measures of creditworthiness will always be to some extent incomplete, and that other information available to local lenders can be difficult to commoditize. Thus, loans where it’s hard to quantify risk assessment, as in “character lending” and commercial real estate, have become increasingly important to community banks.\n\nThis is also a recurring theme — the idea that technological advances in banking enhance economies of scale and therefore result in more concentrated market shares. I think that it’s possible to make too much of this idea, since it is often possible for the benefits of scale to be brought to mid-sized and even smaller institutions through the provision of services by third parties. Still, the consequences of innovation for the structure of the industry are important to bear in mind, and the fact that banks of different sizes might implement technology in different ways — some doing it in-house and some acquiring it from third parties — creates another dimension of change — organizational change — of which supervisors must be aware.\n\nThe usual presumption regarding innovations that successfully penetrate markets — from high-speed Internet to high-definition TV — is that they are broadly beneficial to consumers, and I believe this to be true of retail financial market innovations as well. The expansion of retail credit has allowed consumers greater flexibility in managing their household finances, responding to fluctuations in household financial conditions, and accumulating durable goods. At the same time, the expansion of retail credit has brought an increase in what one might call “bad outcomes” — households that face high debt burdens, have trouble meeting payment commitments, and perhaps even default and resort to bankruptcy. The popular media regularly recount horror stories of unsuspecting consumers who find themselves in dire straits following an encounter with the retail credit industry. Consumer advocates have publicized accounts of abusive practices by unscrupulous lenders and have charged regulators with lax enforcement of existing consumer protections. It appears that a significant constituency now favors tighter legislative and regulatory constraints on retail credit providers of all types, from banks to payday lenders. And new credit market products and practices have given rise to supervisory concerns about the associated risks.\n\nThe rising incidence of credit problems and the resulting sentiment for new or tighter regulation are natural byproducts, I would argue, of the wave of innovation in financial products and services. A credit expansion naturally brings with it an increase in the incidence of “bad outcomes.” As credit becomes available to a broader array of borrowers, there is an inevitable increase in the number and even the rate of delinquencies, defaults and bankruptcies. Moreover, the new borrowers being drawn in to credit markets are likely to be, on average, less financially savvy and more vulnerable to the unscrupulous as they struggle to learn about unfamiliar credit products. The popular focus on cases of fraud and financial distress often drives the politics of consumer finance. The very nature of innovation in consumer credit markets thus gives rise to compelling stories that are powerful motivators for attempts to regulate or restrain new practices.\n\nProposed restraints on retail lending often are intended to protect consumers from their own poor judgment, which makes them vulnerable to abusive lending practices. In fact, in many historical episodes of expanding credit, the desire to regulate has been accompanied by a popular belief that growing debt is a sign of decaying values and thrift. One historian has referred to this persistent belief as the “myth of lost economic virtue.” Belief in this myth can easily hide from view the positive economic role of credit in household financial management and the benefits that come from expanded access to credit.\n\nIncreases in the number or frequency of bad credit outcomes could also expose some banks to increased losses. Of course part of the reason for a credit expansion is improvements in the ability of lenders to measure, price for, monitor and absorb such losses, so the mere increase in losses after the fact should not, in itself, be cause for concern, especially if they are expected losses that are priced and reserved for. As long as a lender’s entry into a new product line is appropriately managed, increased loss rates are in a sense a measure of the extent to which access to credit has been provided to new borrowers. But just as new financial products expose inexperienced borrowers to the possibility of mistaken credit market choices, so too may lenders bring different levels of experience to the adoption of new financial products. These banks, like the new consumer borrowers, will inevitably have varied experiences in their new ventures — some will be more successful than others. But the learning that comes along with the trial and error inherent in these diverse experiences is a vital part of the innovation process. Because innovation and progress don’t just come from people inventing new products; they also come from people learning how to manage new processes.\n\nThis last observation may have implications for how regulators think about banks’ ventures into new activities — whether in consumer finance or any other line of business. Examiners are naturally going to pay close attention to the risks associated with new activities, as well they should. But we also have to recognize the value of the learning that is inherent in the process of financial innovation, as in any other innovative endeavor. As always, we need to be mindful of benefits as well as risks when evaluating banks’ activities, and be careful not to stifle worthwhile financial innovations. Seeking such a balance suggests that robust risk practices should include sound processes for approving new product offerings.\n\nPerhaps a more important implication of retail credit expansion for banks and their supervisors stems from the fact that standardization and automation have moved much of this lending — or the credit exposure, in the case of home loan securitization — off the books of community bank organizations either to large banks or to securities markets in the case of home loan securitization. I mentioned earlier that this seems to have driven many community banks toward lending based on softer, less quantitative risk assessment — commercial real estate, for example. Could this broad movement have made community bank lending inherently riskier, or harder to monitor and assess? If so, we would have reason to ensure that the banks moving in this direction understand and are prepared to manage the risks they take on. The proposed guidance on commercial real estate lending issued by the federal financial agencies this past January seeks to do just that. As always, however, regulators need to be mindful of the extent to which observed trends in bank lending may be driven by productive innovation rather than risk management myopia.\nMuch of the popular response to consumer credit expansion and its byproducts has been less about prudential supervision, however, and more about consumer protection. Many proposals amount to calls for lending restrictions or the outright prohibition of some lending practices. This strikes me as a dangerous approach. In the long run, it would tend to slow innovation and constrain the availability of financial products to a broad range of consumers in order to protect the relatively few who use a credit product inappropriately or unadvisedly. It would be analogous to limiting homeowners insurance on the grounds that many homeowners never file claims in excess of their premium payments and thus end up regretting their purchase of insurance.\n\nSince the rise in undesirable borrowing outcomes is related to the increased participation of inexperienced and uninformed borrowers as the credit market expands, a more promising approach would seem to be to improve the knowledge and expertise of borrowers. One method to address this problem is through careful and more thoughtful design of lender disclosures. The supervisory community ought to encourage disclosure statements written for real consumers, rather than lawyers, as now seems to be the case. More broadly, moving from strict rules-based consumer protection regulations towards more principles-based disclosure expectations might allow for more effective customer communications in the face of complex and ever-changing products.\n\nMore broadly, everyone in the financial services industry should understand their interest in enhancing consumers’ financial literacy. We train our young people in how to use automobiles, after all, a product whose early market penetration drew in less savvy users. In my opinion, we could do a better of job of training people in how to use financial products.\n\nOn a supervisory level, we would not want our response to the spread of new lending products to squelch innovations that are useful to consumers. So while we need to ensure that banks understand sufficiently well the risk characteristics of new lending products they deploy and that banks at the forefront of innovation have particularly robust risk management practices, any guidance we provide on such products should recognize that innovations that meet a market test are generally beneficial, and that the learning generated by early experience with new products often results in significant product improvements over time. Balancing these considerations is the challenge.\n\nJohn Weinberg and Jennifer Zara provided valuable assistance in preparing this speech."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2006",
        "date": "April 04, 2006",
        "title": "The Economic Outlook",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke April 4, 2006 at the Economic Roundtable of the Ohio Valley in Parkersburg, W. Va.",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2006/lacker_speech_20060404",
        "content": "It is a pleasure to speak on the economic outlook this morning, in part due to this distinguished Ohio Valley audience, and in part because the outlook is so encouraging. Growth is proceeding on a solid pace this year, and inflation is low and stable. Moreover, our economy has withstood several substantial shocks over the last several years, and yet has remained on course. So, I think we have abundant reason to be grateful for a quite positive economic outlook. Before I begin reviewing that outlook, however, I would like to note, as usual, that the views expressed are my own and are not necessarily those of my colleagues in the Federal Reserve System.\n\nIt has now become uncontroversial to say that the outlook for overall economic activity is quite healthy. But six months ago, you may recall that many pundits were decidedly less optimistic. In the wake of the destruction caused by two hurricanes, energy prices had surged. From the end of 2004 to the peak last fall, crude oil prices rose 56 percent, wholesale natural gas prices rose 129 percent, and retail gasoline prices rose 70 percent. To some, it seemed obvious that the high energy prices would lead to a significant and persistent reduction in consumer spending, which would bring overall economic activity to the edge of recession. That didn’t happen. It is true that the growth rate of real GDP in the fourth quarter fell by about 2 percentage points from its trend over the previous two years, but a closer look reveals that transitory factors played a large role there. Other data have remained robust, and the consensus forecast is now that real growth in the first half of this year will be at about a 4 percent rate.\n\nLet’s take a closer look at some of the recent data that support this healthy outlook. Starting with the national labor market, payroll employment has grown rapidly, adding almost a million new jobs in the last four months, through February. This is more than double the rate that would simply keep pace with population growth. As you might expect, this has driven the overall unemployment rate down under 5 percent. Another indicator of a strong demand for labor is wage growth, which has been steadily increasing lately. Over the same four months, average hourly earnings have risen at a 3.5 percent annual rate, markedly above the 3 percent growth we had seen in the previous 12 months.\n\nThe combination of rising employment and rising wage gains has supported substantial income growth – over the last four months for which we have data, real personal income has risen at a healthy 5.4 percent annual rate. And that, in turn, helps explain the resilience of consumer spending. The conventional view of economists has long been that consumer spending is governed predominantly by a household’s assessment of their own future real income streams. Thus, despite rising energy prices and surveys last fall that suggested sagging consumer confidence, inflation-adjusted consumer spending increased at a booming 8.0 percent annual rate over the holiday season, and now runs at about 3.2 percent ahead of a year ago.\n\nLooking ahead, to assess the outlook for consumers’ spending, you begin with their income prospects. Expectations are that the overall labor market will continue to be strong: continued job growth, a moderate unemployment rate, and further real wage gains should lead to healthy advances in incomes and, thus, overall consumer spending.\n\nBefore turning away from households, I’d like to touch on residential housing activity. As I’m sure you know, the housing market has had an amazing run in recent years. To cite one measure, new housing starts rose from 1.57 million units in 2000 to 2.07 million units in 2005, a remarkable 5.7 percent average annual rate of increase. And that’s just the number of housing units; on top of that, the size and quality of the average new home has been steadily increasing. Another indicator of strong demand was rising prices for existing homes. For the nation as a whole, the price of a typical single-family home rose 55 percent over the same time period.\n\nYou won’t hear me use the B-word to describe this remarkable activity. Instead, I believe fundamental factors can fully explain the expansion we’ve seen in the demand for housing, particularly rising incomes, rising population, favorable tax treatment, and very low interest rates. At the present time, mortgage interest rates are not as favorable as they were a few years ago, and so it is not surprising that we are seeing some signs of a tapering off of residential activity in many markets. For example, there were 1.28 million new single-family home sales last year, but so far this year the sales rate has averaged 1.14 million. I see this not as a precipitous decline, but rather as a return to more normal conditions in many markets. This return to normalcy is especially pronounced in the informal evidence we receive. The multiple first-day bids and final sales at above-asking prices that were observed in many markets have become increasingly rare. Also, the amount of time that a home remains on the market has risen back up to more typical levels. Looking ahead, it seems reasonable to expect the housing market to remain strong, even as some further tapering off in sales and production takes place.\n\nThe key point I would like to emphasize is that the housing phenomenon was not a mysterious, independent boost to the economy, driven by some sort of animal spirits, but instead was a rational response by households to the economic fundamentals, especially very low real interest rates. Thus, going forward, the adjustment of the housing market to evolving fundamentals will continue to fit comfortably within the standard economic framework. My assessment is that plausible rates of moderation in housing activity will not pose a problem for overall activity this year or next. Moreover, I don’t see diminished housing price appreciation as a major problem for consumer spending, since again, the primary determinant of spending is income, and we see solid and improving prospects for real incomes for the nation as a whole.\n\nTurning to firms, the fundamentals for business investment appear to be quite sound. Capital formation, particularly investment in information and communications technology, played an instrumental role in the widely noted surge in productivity growth that took place in the late 1990s. The unique fundamental driving force then was the rapid and sustained fall in the relative price of new computing equipment and associated products. This investment boom resulted in a growing capital stock, and as a result rising productivity growth. Indeed, productivity growth had only averaged 1.38 percent per year for over two decades, but from 1995 to 2000 averaged 2.52 percent per year. That may sound like a small difference, but remember that over time, productivity growth is the foundation of rising standards of living, and that compounding over many years can transform small differences in growth rates into substantial differences in incomes. Thus if productivity growth remained at 1.38 percent, it would take around 50 years for average incomes to double. But with productivity growth at 2.52 percent, the doubling time is cut to almost 28 years.\n\nLooking at more recent numbers, productivity growth since 2000 has averaged 3.3 percent per year, which incidentally would double average incomes in less than 21 years. This is an astonishing performance over a time period with significantly lower rates of capital formation than in the late 1990s. Thus, recent productivity gains appear to owe somewhat more to the re-organization of business processes than to the application of additional capital. But as business investment continues to grow, productivity growth is likely to be driven more by capital formation. We should therefore pay special attention to current prospects for investment spending.\n\nIn my view, the fundamentals for investment are encouraging. In the high-tech area, we are still seeing declining relative prices for many products. Business sales are strong. New orders for capital equipment have been on a pronounced uptrend for 2 ½ years. The cost of capital remains favorable. Capacity utilization in manufacturing has recovered from the recession and any capital overhang is largely behind us. And business profitability is unusually high. Putting these all together, I expect investment spending to be quite robust this year. Falling relative prices should continue to support technology upgrades that enhance efficiency for many firms. In addition, rising capacity utilization rates suggest that many firms will need to add capacity to keep up with demand growth. And if I am correct, this capital spending should be enough to support overall demand in the economy, even as the housing market cools down.\n\nThis is a good time to review the bidding. It looks like we’re on track for continued expansion, with real GDP growing at about a 3 ½ percent annual rate this year. Consumer spending should grow in line with GDP and will be supported by job growth and real wage gains. Residential investment will flatten or slow, but business capital spending should remain robust. And that capital spending will support productivity growth going forward, which in turn will support the future income growth that keeps household spending healthy. And while there are risks to this forecast, as there are with any forecast, I do not see any single scenario that is compelling enough to alter the central tendency of this outlook.\n\nLet’s turn now to the inflation picture, where again things are looking better now than many had expected six months ago. Back then, the energy price surge had led some observers to expect to see those prices pass through to a broad range of prices of goods and services, much like what happened in the 1970s. But that hasn’t happened. Core inflation has been low and relatively steady in the last several years. Our preferred inflation measure, the price index for core personal consumption expenditures, has risen 1.8 percent over the last 12 months. Despite rising energy prices, core inflation actually fell slightly last year, since the core price index had risen 2.2 percent in 2004. Similarly, we are not seeing any sign of rising inflation in the most recent data. Over the last two months, for example, the core index has increased at a 1.8 percent annual rate as well. To put that number in perspective, it lies close to the 1 ½ percent figure that I and several others have proposed as an announced numerical objective for inflation.\n\nWhy haven’t high energy prices boosted other prices as much as many had feared? Probably because those fears are based on looking back at the 1970s and seeing that similar energy price increases had been followed by broader increases in overall inflation. I would argue that any analogy with the 1970s is badly flawed. Back then, monetary policy failed to respond effectively to rising inflationary pressures and the public’s expectations of future inflation had consequently become unanchored. Thus, at that time, higher energy prices became a signal for firms to raise prices and for workers to demand higher wages in order not to fall behind a prospective inflationary surge.\n\nToday, however, the Fed places its highest priority on keeping inflation low, and our ability and willingness to follow through on our announced intentions appears to be widely understood. Thus, longer-term expectations of inflation have remained moderate even as energy prices have moved up over the last couple of years. Looking ahead, short-term movements in the inflation rate can be hard to predict. But what is important is to stabilize inflation over medium- and longer-term horizons. And here the indicators about what the public expects look fairly good. Both survey data and the market prices of inflation-protected Treasury securities tell us that the public expects inflation to continue to be contained. I am confident that we at the Fed have the knowledge and the will to validate those expectations."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2006",
        "date": "March 29, 2006",
        "title": "Central Bank Credit in the Theory of Money and Payments",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke March 29, 2006 at the Federal Reserve Bank of New York's The Economics of Payments II Conference in New York, N.Y.",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2006/lacker_speech_20060329",
        "content": "I’m honored to have the opportunity to speak at this conference, although I must admit that I find the conference’s title a bit puzzling. I can certainly think of more than two conferences on payment economics. Why, the Richmond Fed alone has sponsored two; one in 2000 and one way back in 1987.\n\nBut provenance aside, Jamie and Will and everyone else at the New York and Atlanta Reserve Banks who have contributed to organizing and staging these two conferences deserve our grateful commendations. Indeed, I’m quite heartened by the proliferation of gatherings like this, at which economic theory, econometric evidence and lessons from history are all brought to bear on questions surrounding payments systems. These conferences have been vital to the maturation of payment economics — the study of the mechanics of market exchange — as a distinct field of inquiry. Payment economics is no narrow technical specialty, either: it builds on monetary theory, since use in payments defines monetary instruments. It also draws on banking theory, based on the observation that virtually all institutions that we usually think of as banks are significantly involved in payments intermediation.\n\nI want to talk this morning about the role of central bank credit in payments arrangements. There is a voluminous practitioner literature that touches on this subject. Much of it focuses on some of the terms on which credit is provided, and some recent discussion centers on the relative advantages of collateralization versus overdraft fees in managing the risks that arise from the provision of intraday central bank credit. I want to offer some thoughts on these issues, but my main purpose today is to explore what payment economics has to say about the role of central bank credit in the payments system.\n\nOne theme I will emphasize is that payments system policy — specifically, the terms on which daylight credit is offered — ought to be analyzed within the broader context of the array of central bank policies surrounding the provision of deposit accounts. This viewpoint naturally connects daylight credit policy to the lender of last resort function as well as the operational mechanics of setting the overnight interbank interest rate. This viewpoint leads me quite naturally to a modest proposal for improving payments system policy and the operations of monetary policy. I should emphasize at the outset that my policy proposal is offered up in the spirit of academic inquiry, with the aim of stimulating discussion that will enhance our understanding of how best to achieve our policy goals. I should also emphasize that the thoughts I’ll be sharing with you this morning are my own, and do not necessarily represent the views of the Federal Reserve System.\n\nCentral Banks in the Payment System\n\nCentral banks play a variety of roles in the payment system. The most fundamental, I would argue, is providing banks with deposits and a means of transferring them to make interbank payments. Modern central banks provide electronic transfer systems, but systems based on paper or face-to-face payment orders go back centuries. Indeed, prototype central banks — that is, public sector institutions providing transferable deposits to banks for use in settlement — are documented in the early 1400s in the northern Mediterranean (van Dillen, 1964 1\n, and Mueller, 1997 2\n). I recommend to you a recent Atlanta Fed working paper by Stephen Quinn and Will Roberds that provides an excellent description and analysis of the 17th century Bank of Amsterdam, a prominent and well-documented example of this type of institution (Quinn and Roberds 2005 3\n).\n\n\n\n\n\n\n\nAdvances in the theory of payments have emphasized the role of communication and record-keeping in conveying information about the participants in an economic transaction. The economic function of a payment instrument is to communicate reliably (that is, in an incentive compatible way) about the buyer’s past transactions (Townsend, 1989 4\n, and Kocherlakota, 1998 5\n). Banks, from this perspective, are fundamentally specialized institutions for issuing widely accepted payment instruments, in contrast to their traditionally emphasized role as balance-sheet intermediaries. Indeed, the provision of payments services, I believe, better defines banking than balance-sheet intermediation, which has been the traditional focus of banking economics, but which many other nonbanking institutions engage in as well.\n\n\n\n\n\nIssuing, clearing and settling payment instruments are essentially communication and record-keeping activities. The central role of communication technologies in payment arrangements points, in modern settings, to the importance of economies of scale, common costs and joint production. These conditions can give rise to “network effects” in which much of the benefits and costs are shared among multiple participants. Private organizations that deal effectively with such technologies can be described as clubs, and the theory of clubs teaches us that terms of membership are just as important as unit service prices in inducing efficient participation in the presence of network effects.\n\nEfficient communication arrangements often take the form of networks in which many paths connect through a central node. A clearinghouse can be viewed as a natural club arrangement for such centralized settlement activities. A central bank then represents a nationalized central settlement node for interbank payments. Contemporary legal restrictions more or less compel most banks to settle through the central bank. Some economists have argued that such nationalization was efficiency-enhancing (Goodhart, 1988 6\n). For club goods, however, there is often a range of allocations consistent with efficiency — that is, with Pareto optimality. The formation of central banks may represent the pursuit of a politically favored allocation of the net benefits of clearinghouse arrangements. For example, research into the Federal Reserve’s entry into check collection suggests that it was less about cost-efficiency than it was about shifting the cost of collecting checks drawn on country banks.\n\n\n\nIf the fundamental core of central banking consists of interbank deposit services, then the fundamental core of central bank policy consists of all of the terms and conditions under which those deposit services are offered. These include the obvious pricing terms, such as the nominal rate paid on deposits (zero at your Federal Reserve Bank) and the fee charged for transferring funds. It also includes legal restrictions, such as reserve requirements, that impose constraints on deposit holdings. The determination of the quantity of deposit liabilities supplied is also a component of central bank policy. Under current U.S. arrangements, the New York Fed’s trading desk conducts daily open market operations so as to supply an amount of deposits expected to result in an interest rate on overnight interbank loans equal to the target rate set by the Federal Open Market Committee. Thus the phrase “central bank policy” should be construed here to include the monetary policy operational regime, since, as I’ll argue below, it affects banks’ payment system choices. This connection between the Fed’s daylight credit arrangements and the broader monetary regime implies that attempts to optimize each separately may not deliver the best policies.\n\nCentral Bank Credit in the Payment System\n\nCentral banks have traditionally viewed the provision of credit to the banking system as an essential tool for achieving their goals. The lender-of-last-resort function is a widely accepted role for central banks to play in responding to emergency liquidity needs. Bagehot’s prescription — to lend freely but only at a high rate on good collateral to solvent institutions — is one of the most well-known maxims in central banking, although some of these distinctions can be tricky to apply in practice.\n\nIt is important to recognize, however, that central bank lending involves two distinct actions. The first is an increase in the deposit account liabilities of the central bank. The second is the acquisition of a private liability. In Bagehot’s time, acquiring private liabilities was the main method of altering the aggregate supply of central bank deposit liabilities, and so the lender-of-last-resort policy he prescribed was the natural way to provide for an elastic supply of deposits when demand for those deposits spiked. The founding of the Federal Reserve System was motivated by a similar desire to prevent interest rate spikes when the demand for reserves surged. The advent of open market operations in liquid government securities, however, made it less obvious that acquiring private liabilities was the best way to manage the supply of central bank deposits. Most central banks now treat open market operations aimed at pegging overnight interbank interest rates as distinct from lending to individual banking institutions. In fact, pegging interest rates automatically sterilizes the effect of such lending on aggregate deposit supply. Discount window lending now represents a form of fiscal policy — a public sector loan to a private entity. It is no longer necessary to the provision of an elastic supply of reserves.\n\nThe lender-of-last-resort function typically involves overnight credit. Many central banks provide intraday credit in the course of operating interbank payment systems that provide payment finality. Of course, daylight credit that is not extinguished by the end of the processing day becomes overnight central bank credit of some form or another. Central banks have taken different approaches to the provision of daylight credit. The Swiss used to just say no; now they lend via intraday repurchase agreements. Most central banks provide daylight credit on fairly liberal terms. Many insist that such credit be fully collateralized. The Fed currently allows daylight credit to be uncollateralized, but charges a fee equivalent to 36 basis points at an annual rate on daylight credit above a certain threshold.\n\nPayments Theory\n\nWhat does economics have to say about the role of central bank credit in the payments system? The nature of the problem provides some guidance, I believe, regarding the methodology one needs to bring to bear. To evaluate the role of central bank credit, one needs to assess the costs and benefits of alternative policy regimes governing the provision of that credit. To do that, one needs to understand how bank behavior will change when one changes central bank credit policy. In other words, how will deposit balances and the timing and magnitude of payment flows differ from one regime to another? Empirical analysis of payments systems data can provide some assistance here by providing an understanding of the underlying patterns of payment flows among banks. But such analyses invariably run into the “Lucas Critique” — that is, that estimated relationships from the status quo regime may shift dramatically in response to a change in regime. To the extent that one is evaluating an alternative regime that differs substantially from current policy, one must identify the “structural” determinants of bank behavior that are invariant across regimes. Thus, evaluating alternative payment policy regimes calls for a theoretical framework, although observations from history or across countries might also provide some insights. The analysis of a system’s likely response to a major shift in policy requires a plausible model that incorporates the effects of central bank policy on equilibrium private sector behavior.\n\nWhat should we look for in models of payment activity? One important principle is embodied in William Baxter’s (1983) Dictum—that the issuance, use, clearing and settlement of a payment instrument is a service of joint benefit to the buyer and the seller and that service is provided jointly by all parties to clearing and settlement. As a result, a sound economic evaluation of alternative payment policies requires assessing the effect of those alternatives on the well-being of and costs incurred by all of the parties involved. Models that omit the parties for whom banks are clearing and settling payments — the “end-users” — will fail to satisfy Baxter’s Dictum, and will be potentially misleading.\n\nAs I mentioned earlier, payments arrangements are communications networks, and these often take the form of club goods. Private agents that find themselves in such environments will tend to create multilateral institutions to efficiently cope with their interdependencies. A good payments model should recognize that payment instruments and institutions are not exogenous, but are determined by the nature of the information and other frictions facing traders in the model environment. This endogeneity of payment behavior is what makes the application of carefully specified models essential for thinking about the consequences of significant changes in central bank policies.\n\nViewing instruments and institutions as endogenous adaptations to the structure of the economy has important methodological implications. First, whenever possible, models of payment behavior should be fully articulated general equilibrium models, specified, in the words of an old but useful slogan, at the level of preferences, endowments and technologies. This is essential for drawing welfare conclusions about alternative policies. Second, the endogeneity of institutions places mechanism design at the heart of payments theory, as is true for modern monetary theory. Under a mechanism design approach, payment instruments are seen as messages that embody contingent contracts, and one can model the information and risk allocation characteristics in a way that takes into account the limitations imposed by real-world payment technologies — for example, the costliness and falsifiability of communication, verification and authentication.\n\nThe Freeman Model\n\nScott Freeman (1996) developed a model that meets these criteria and has proven useful for studying the role of central bank credit in settlement arrangements. In the environment of the Freeman Model, both fiat money and private liabilities serve as means of payment. Moreover, each period many agents meet at a central location, some bearing private payment instruments that they want to exchange for money, and others bearing money with which they will redeem their debt. The (exogenous) timing of agents’ arrivals and departures are such that early in the meeting there is an imbalance between agents bearing debt they wish to redeem for money and agents with money to offer for debt. Without central bank intervention, the debt sells at a discount early in the period, an inefficiency relative to frictionless settlement. In this model, the central bank can purchase debt for newly minted money and later retire that money by presenting the debt to issuers for payment.\n\nThe Freeman Model was developed to study the central bank’s ability to accommodate a temporary bulge in the demand for money in connection with settlement in a way that does not create inflation. Freeman makes reference to Milton Friedman and Anna Schwartz’s (1963) discussion of seasonal movements in money demand in the U.S. during the 19th century. But the series of central bank transactions described above can be interpreted as a short-term or even intraday loan from the central bank to the issuer of the debts. Under this interpretation, Ruilin Zhou (2000) has shown that the optimal terms for the central bank transaction are equivalent to daylight credit at a zero interest rate.\n\nOne important observation on the Freeman model is due to Ed Green (1997). By constructing a mechanism by which a coalition of private agents can achieve the same outcome as central bank intervention in the Freeman Model, Green showed that central bank credit was not essential for achieving an optimal allocation. In fact, the coalition described by Green’s Theorem resembles the private clearinghouses which stood at the apex of the U.S. clearing and settlement systems before the creation of the Federal Reserve. This result highlights the lesson that the need for (perhaps quite complicated) multilateral coordination does not by itself create a need for public sector involvement in a payments system. This lesson is buttressed by the observation that many private net settlement arrangements exist alongside central bank gross settlement systems. As I noted earlier, this reasoning suggests that the question of the central bank role in payments is less about efficiency and more about the distribution of costs and benefits.\n\nAnother important observation on the Freeman Model involves its finding that the optimal interest rate on intraday central bank credit is equal to zero. A key feature of his environment that helps deliver this result is the absence of intraday discounting. An interest rate is the intertemporal price of consumption, and in the Freeman model, consumption is discounted only period-to-period, not within the settlement period. This amounts to saying that there is no within period (intraday) opportunity cost of consumption or money, an assumption that may or may not be a good approximation to the operation of large value payments systems. Whether it makes sense to posit that all discounting takes place overnight is an important open research question, especially when the overnight period lasts just 2 ½ hours, as it does for Fedwire.\n\nIt is worth noting that the motivation for daylight credit in the Freeman Model is unrelated to any risk of so-called “gridlock.” People describe gridlock as occurring when banks strategically delay payments within the day, thereby increasing the system’s processing burden late in the day. The option to delay payment is not available in the Freeman Model. In more general settings, one important question regarding the potential for gridlock is the extent to which repeated interaction can constrain the incentive for strategic misbehavior. It is also worth considering whether gridlock could itself be a consequence of the status quo policy regime.\nYet another noteworthy feature of the Freeman Model is that the central bank’s extension of daylight credit is risk-free. From this perspective, one might view a daylight overdraft fee as compensation for risk. Ideally, one would want to set this fee in Pigovian fashion so as to eliminate banks’ incentives to overuse daylight credit. One might think that setting the fee at a level that compensates the central bank for its credit-risk exposure would do the trick, but this would ignore the role of the deposit insurance fund. A central bank’s claim on a failing bank’s collateral simply reduces the liquidation value of the institution and thereby increases the cost to the deposit insurance fund. Moreover, central bank lending can allow the chartering agency to delay closure and facilitate the exit of uninsured creditors, further shifting losses from private counterparties to the public sector and exacerbating moral hazard. Either way, Federal Reserve risk exposure is the wrong metric against which to benchmark overdraft fees. It is essential, in my view, to evaluate the risks associated with central bank credit from the comprehensive perspective of the consolidated fiscal balance sheet rather than from a purely central bank point of view.\n\nWhile it is widely recognized that credit risk is an element of the benefit-cost calculus surrounding daylight credit, assessment of this risk is fraught with difficulty. When financial conditions are generally strong, the risk of actual loss due to daylight credit exposure is likely to be small, and even a small benefit in the form of a smoother functioning payment system might appear to make the provision of central bank credit worthwhile. Daylight credit is often particularly useful during a severe operational disruption, as illustrated by the aftermath of the terrorist attacks of September 11, 2001. (Lacker, 2004 12\n) While some banks delayed payments out of concerns about incoming funds, the availability of daylight credit built confidence that payments would flow. (McAndrews, 2002 13\n) On September 11, the general condition of the banking system was quite strong. Should a major operational disruption occur when some financial institutions are generally more fragile, then an expansion of central bank credit could involve a substantial increase in exposure.\n\n\n\n\n\nOperational disruptions aside, weak banking institutions can create broader moral hazard problems regarding daylight credit. Large banks build sophisticated payment processing systems assuming the availability of automatic daylight credit. Reconfiguring a bank’s operations to cope with a denial of daylight credit can be very costly and highly visible to counterparties. This makes it difficult for the Federal Reserve to withdraw daylight credit in the case of weak or failing institutions, and this in turn can substantially weaken market and supervisory discipline.\n\nReserves versus Payments Credit\n\nThe Freeman Model has been cited as support for minimal daylight overdraft fees, but I would like to explore an alternative central bank policy regime that involves no daylight credit at all. Under this regime, the Fed would automatically “sweep” the overnight excess reserve balances of banks into reverse repurchase agreements. Specifically, at the close of Fedwire ( 6:30 p.m.) we would sell them U.S. Treasury securities in exchange for all of their excess reserve balance. At the opening of Fedwire on “the following day” (actually 9:00 p.m. the same night) the transaction would be reversed; we would buy back the securities and credit their account for the purchase amount, plus interest. Upon initiation of the service, the Fed would conduct a large one-time open market purchase of securities during the day to start the program up with abundant daylight reserves.\n\nIf the interest rate were set close to or at the target fed funds rate, this scheme would allow us to curtail daylight credit without imposing much cost on banks. For every dollar of daylight credit we withdraw, we could supply an additional dollar of daylight reserves via the initial open market purchase. In the limit, we could withdraw all access to daylight credit and increase the aggregate supply of daylight reserves by the maximum amount of daylight credit usage. In principle, any pattern of intraday payments that is feasible under current policy would still be feasible; no change in the timing of payments would be necessary.\n\nThe obvious cost to a bank of substituting overnight balances for daylight credit is the foregone interest on overnight balances. A Fed sweeps service would virtually eliminate the opportunity cost of holding large daylight balances if the interest rate was set at the overnight federal funds target rate. This illustrates the extent to which the demand for daylight credit can be viewed as driven by the tax on Fed deposits due to the lack of interest on reserves. Banks could hold large overnight balances now if they so desired, but they prefer to use daylight credit and hold quite minimal balances beyond those needed to meet reserve requirements.\n\nNote that this policy is equivalent to the optimal policy recommended by the Freeman Model of intraday purchases of securities that are reversed at the end of the day. The sweeps service would withdraw substantial balances at the end of the settlement day and then inject them back in at the beginning of the next day. But the sweeps plan I described would not be feasible in the Freeman Model, because different agents participate in the settlement meeting each period. Thus, the Freeman Model does not provide opportunities to substitute overnight balances for daylight credit. This illustrates the source of the Freeman Model’s crisp prediction regarding daylight credit interest rates: the market for daylight credit is sharply segmented from overnight asset markets. This suggests that to fully understand the economics of daylight central bank credit we need models that allow for nontrivial substitution between overnight balances and daylight credit.\n\nIn the Freeman Model, the central bank acquires the private payment liabilities that give rise to the daylight demand for money, while the sweeps proposal is agnostic on the debt used in the overnight reverse repurchase agreements, although U.S. Treasury securities are the natural candidate given the existing Fed book-entry securities service. Of course, private payment liabilities are the only debt in the Freeman Model, so no meaningful question arises there, but this points to what might be the most essential difference between various central bank daylight credit policy regimes: namely, the nature of the financial claims the central bank acquires. Under current Fed policy, the Federal Reserve Banks take unsecured claims when they provide daylight credit, although operating circulars create a lien on any bank collateral that happens to be pledged for use in overnight borrowing, so perhaps it is best to describe Fed daylight credit as partially secured. Central banks that require full collateralization of daylight overdrafts often allow a range of assets to serve as collateral — similar to the Fed’s policies for discount window collateral. It is beyond my scope here, but the question of the appropriate collateral for central bank credit exposure is an open question that involves deeper issues surrounding the financial safety net and related moral hazard considerations. But note that the sweeps service I have described is nearly equivalent to collateralized daylight credit, if the eligible collateral and repurchase transactions are limited to the same set of assets.\n\nAside on simplifying monetary policy implementation\n\nOne side benefit of the sweeps service I have described is that it would allow us to simplify monetary policy operations. At present, New York Fed staff essentially estimates the banking system’s demand for excess reserves each day at the funds rate target and they supply that amount through open market operations. In the process, they must estimate a variety of “technical” influences on the reserves market — changes in Treasury balances, for example. The New York Fed staff generally intervenes only once each day, however, usually in the morning. Unanticipated disturbances to reserve supply or demand can occur after they have intervened, and these can drive the market federal funds rate away from the target. Although it is unclear whether there are significant welfare costs of intraday fed funds rate volatility, substantial resources are devoted to assembling data and estimating reserve factors.\n\nWith a sweep service in place paying interest at the target rate, monetary policy operations could in principle be substantially simplified by supplying, via open market purchases, more reserves than the banking system wishes to hold. No bank would lend overnight funds in the market at less than the rate on our sweep service. And a bank in need of borrowed funds could always find a willing lender at a risk-adjusted spread over the sweep rate. The market funds rate thus would not rise above the sweep rate, except to reflect borrower-specific risk. The New York Fed staff would merely need to provide an amount of reserves that will be sufficient to oversupply the system with reserves and meet daylight settlement needs. But they would not need to estimate daily reserves positions as precisely as they do now, because a “miss” would rarely affect the funds rate.\n\nIt’s easy to think of interesting questions about how one would implement an idea like this. For instance, because of some peculiar accounting rules, banks’ overnight reverse repurchase holdings “uses balance sheet” and could require costly additions to capital for participating banks. If so, then even a rate equal to the target rate would not necessarily fully eliminate the opportunity cost of excess reserves. This and other interesting questions merit careful further analysis. But the proposal demonstrates my theme that central payment credit should be understood whenever possible in the context of the broader set of monetary arrangements in place. This, by the way, is a point that is made very cleanly by the Freeman model.\n\nConclusion\n\nLet me conclude by emphasizing what I think are two key lessons from the theory of payments. First, understanding payments arrangements and the appropriate role of the central bank requires a clear understanding of private arrangements and private incentives in settings where the services (like payment clearing and settlement) involve multilateral benefits and shared costs. That is, models of payment behavior and analyses of payment policy should respect Baxter’s Dictum to evaluate effects on all parties to a payment arrangement. The Freeman Model, and other models derived from modern monetary theory are typically very diligent in this regard. Even so, it is hard in such models to identify imperfections that a central bank or other public entity is uniquely suited to resolve. As demonstrated by Green’s Theorem, pairing Baxter’s Dictum with a mechanism design approach makes clear the strong incentives that private agents have to find efficient arrangements, and this is the second lesson. The network nature of payments systems should not be taken to imply the existence of market failures when voluntary, multilateral arrangements are capable of incorporating all of the affected parties."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2006",
        "date": "February 14, 2006",
        "title": "Transition and Continuity at the Federal Reserve",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke February 14, 2006 at the West Virginia University's Acordia/Royal & SunAlliance Distinguished Lecture Series in Morgantown, Va.",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2006/lacker_speech_20060214",
        "content": "I would like to talk to you tonight about the evolution in the way the Federal Reserve goes about conducting monetary policy. As my title suggests, one theme is that a transition is taking place. Of course, the most striking transition at the Federal Reserve this year is the change in leadership. On January 31, Federal Reserve Board Chairman Alan Greenspan served his last day in office and chaired his last meeting of the Federal Open Market Committee. His successor, Ben Bernanke, took over the following day, and tomorrow morning, he delivers his first testimony to Congress as chairman.\n\nMuch has been written and said recently about this changing of the guard. It is quite natural in such circumstances for commentators to contrast an influential leader and his successor and to look for likely differences in philosophy and practice. But in my opinion, too much has been made of their differences. Thus, the second theme of my talk: continuity. In remarks upon the announcement of his nomination, Bernanke very deliberately emphasized the stability of monetary policy. He stated that his “first priority will be to maintain continuity with the policies and policy strategies established during the Greenspan years.” But I won’t ask you to take his word for it. Tonight, I hope to convince you that a careful student of the Federal Reserve should have good reason to believe that the practice of monetary policy will continue to evolve gradually. I will argue that a certain economic logic has influenced the way policy and practice evolved during the Greenspan years, and that that logic will continue to influence the evolution of policy during the Bernanke years. In particular, the stability of the public’s understanding of and expectations about the future conduct of monetary policy have been central to Chairman Greenspan’s success. As a consequence, the Federal Reserve has found it useful to steadily make the conduct of monetary policy more transparent. This logic is likely to continue to hold sway, and thus the Fed is likely to continue to emphasize credibility and enhanced transparency.\n\nLet me be clear, however. Understanding the economic logic of the evolution of monetary policy over the last several decades should take nothing away from the significant achievements of Alan Greenspan. He served as Fed chairman for more than 18-and-a-half years and his record during that time was exemplary. Under his leadership, the Federal Reserve brought inflation down to historically low levels, which contributed to a period of extended economic expansion interrupted by only two brief and mild recessions. Indeed, the term “The Great Moderation” has been given to the phenomenal improvement in macroeconomic performance during the period following the mid-1980s, just before Greenspan took over. In essence, he successfully completed the task begun by his predecessor, Paul Volcker, of re-establishing the expectation of price stability that had been lost in the inflationary decade of the 1970s.\n\nWe are fortunate to have in Ben Bernanke a new chairman who has experience in policymaking and has made important contributions to our understanding of monetary economics. He is well-versed in both the economic logic of monetary policy and the research devoted to dissecting the great monetary policy mistakes of the 1930s and 1970s. He is therefore eminently qualified to continue to lead Fed policymaking along the path laid out by his predecessors. And his public comments thus far, particularly in his confirmation hearings, suggest that we can expect the type of continuity in Fed policymaking that I’ll be talking about this evening.\n\nI should say at the outset that, as always, my remarks reflect my own views and not necessarily those of my colleagues in the Federal Reserve.\n\nMany observers, myself included, have argued that one of the hallmarks of the Greenspan legacy is adherence to a systematic approach to policymaking.1\n\n The value of a systematic approach to monetary policy goes beyond the usefulness of minimizing unexpected deviations from what the public anticipates. The purpose of monetary policy, after all, is to stabilize the value of money, and the value people place on money today depends critically on what value people expect money to have in the near future, which depends in turn on what value people expect money to have a bit further into the future, and so on. The future conduct of monetary policy is thus a fundamental determinant of the value of money. Therefore, the key to stabilizing the value of money is getting people to understand the systematic conduct of monetary policy.\n\n\n\nBut saying that Greenspan introduced systematic policymaking is not quite right — and not quite enough. After all, monetary policy during the inflationary 1970s was just as systematic. An econometrician could estimate a statistical relationship between macroeconomic variables and the Fed’s policy actions in the 1960s and 1970s. Indeed, many econometricians have estimated such relationships, and they tend to fit pretty well. But those relationships generally differ from what you get when you estimate relationships for the Volcker or Greenspan years. The question, then, is just what systematic relationship the Fed follows.\n\nIn the 1960s and 1970s monetary policy typically allowed inflation to rise noticeably during economic expansions. As the economy recovered from a recession and growth picked up, the Fed kept interest rates from rising as much as they should. In fact, at times, the Fed failed to raise nominal interest rates by as much as inflation was increasing. This caused real (that is, inflation-adjusted) interest rates to fall and, thus, provided further monetary stimulus at precisely the wrong time. The acceleration of inflation ultimately provoked a sharp tightening in policy, which often exacerbated or even caused an ensuing recession. Policymakers’ fear of even further deepening the slump led them to ease policy before inflation had fully subsided. So inflation was essentially ratcheting upward throughout the period before 1980.\n\nAn important part of the economic instability of the 1970s can be attributed to the fact that the public’s inflation expectations became untethered. For a long time before, indeed for centuries, inflation expectations had been anchored by commodity standards — that is, by arrangements that tied the value of money to the value of one or more precious metals like gold or silver. Under a commodity standard, inflation wasn’t eliminated, since changes in the supply and demand conditions for the commodity to which money was linked could change the value of money. But over the long run, the real value of a commodity like gold is determined by the cost of extraction, and this tended to be fairly stable. Thus, commodity standards provided people with confidence that movements in the value of money would not be persistent and that inflation would ultimately settle back down to low levels.\n\nThe 20th century saw a gradual but steady departure from the gold standard, culminating in the closing of the U.S. “gold window” in 1971. It is not surprising that expectational stability would have been lost around the same time. When inflation was observed to rise in the 1970s, the public saw no obvious mechanism in place for bringing it back down, and so higher inflation became built into people’s long-run expectations. The story of the Volcker-Greenspan era, then, is the story of how expectational stability was restored — the story of how the Fed regained the public’s confidence that it would and could keep inflation low and stable.\n\nThe emergence of persistent inflation expectations on the part of the public during the 1970s was one factor that contributed to important developments in the discipline of macroeconomics. Before the 1970s, the consensus framework for understanding the effects of monetary policy treated the public’s expectations about future inflation as a fixed parameter, unaffected by the actual current conduct of monetary policy. That framework contributed to the policy errors of that period by encouraging policymakers to discount the possibility that public expectation would shift over time in response to the actual conduct of policy.\n\nEconomists already had begun to think more carefully about how the public’s expectations are formed, and in the 1970s, they had begun to study models in which the public’s expectations were tied very tightly to how policy would actually be conducted. The experience of the 1970s confirmed the importance of this link by teaching us, essentially, that you can’t fool all the people all the time. People learn from what they see, and it was unreasonable to assume that people would continue to expect inflation to settle down to low levels when they kept seeing inflation continue to ratchet up. The models developed in the 1970s provided a compelling diagnosis of the deterioration in inflation we witnessed that decade: monetary policy had a systematic inflationary bias, and the public had come to understand that and behave accordingly. Those models also provided the Fed with a prescription: systematically adhere to, and convince the public we would systematically adhere to, a non-inflationary monetary policy.\n\nThat prescription is easier written than followed, however. Merely announcing an intention to bring inflation down is not sufficient. After all, the Federal Reserve had been publicly advocating lower inflation throughout the 1970s. We needed a way to convincingly demonstrate our commitment to bringing inflation down. The Fed began this process in October 1979, under Chairman Paul Volcker, by allowing interest rates to rise sharply and withstanding a deep recession while inflation ratcheted down. Thereafter, the Fed often had to raise the fed funds rate in response to signs of rising inflation, or in response to inflation scares — that is, signs from the bond market that inflation expectations were rising. Over time, however, inflation has stabilized at a low level and inflation scares have become much less frequent as the public has learned that the Fed would respond systematically in a way designed to keep inflation low and steady.\n\nThe conduct of monetary policy in the last two decades has brought us to a very favorable place. Inflation is low and stable, and the public appears to be fairly confident that inflation will remain persistently low and stable. This is the Greenspan legacy, and it is now our responsibility, under the leadership of the new chairman, to preserve that credibility.\n\nThe challenge of fulfilling that responsibility in the period ahead will be analytically demanding. For how does one conduct monetary policy when inflation is low and stable? Here we can draw on our understanding from a class of models that researchers have developed and studied in the last decade or two. While this research program is still in progress and important open questions remain, a few clear principles have emerged. First, holding interest rates steady until inflation or deflation pressures are actually visible is clearly inappropriate. Instead, policy should be conducted recognizing that real interest rates should be expected to fluctuate with economic conditions. A real interest rate is the relative price of current resources in terms of future resources. It represents the real amount of goods and services one must sacrifice in the future (in addition to the repayment of principal) to obtain real goods and services today. As I have emphasized elsewhere, real interest rates should be expected to fluctuate over time in response to variations in the relative pressure on current versus future resources.2\n\n When current resource demand is less than it will be in the near future — as was the case from 2001 through the beginning of 2004 — then real interest rates need to be low to reflect the relative lack of pressure on current resources. When that relative pressure on current resources rises, as has been happening over the last two years, then real interest rates need to rise. In such circumstances, if the Fed sets and keeps the funds rate too low, the inevitable result will be rising inflation.\n\n\n\nThe behavior of the Fed thus has evolved in a way that is consistent with the evolution of our understanding of the macroeconomy and of how inflation, growth, and interest rates interact over time. Alan Greenspan’s leadership was vital to this evolution. He understood the importance of stabilizing the public’s expectations regarding future monetary policy. He proved highly skilled at sensing the evolution of those expectations, and at influencing them through the practice of monetary policy. And his focus on the Fed’s credibility dovetailed with advances in economic research both inside and outside the Federal Reserve. It’s fitting, then, that leadership of the Fed should now pass to Ben Bernanke who, in addition to his previous experience on the FOMC, has made quite important contributions to research on monetary policy during his academic career.\n\nThe importance of expectations and how policy effectiveness hinges on expectations is one of the most significant contributions of macroeconomic research in recent decades. Given this importance, one might wonder whether a central bank can influence expectations through means other than its own rate-setting behavior. In particular, what role can or should communication by the Federal Reserve play in shaping the public’s expectations?\n\nBy itself, communication is not a particularly powerful tool for influencing people’s beliefs. To be effective, communication needs to be backed up by and consistent with the actual policy behavior. Otherwise, listeners tend to discount what the policymaker says in favor of how the policymaker behaves. Thus the popular aphorism: “Actions speak louder than words.” Words can matter, however, if the public believes that the policymaker will feel compelled to live up to them. A central banker who pledges to keep inflation low but then persistently lets it rise runs the risk of not being believed the next time around. This risk strongly discourages making empty promises. Thus, a clear central bank commitment to a policy objective can influence the public’s expectations.\n\nOne widely noticed feature of the Greenspan legacy is the dramatic increase in transparency during his tenure. This is especially apparent in the Fed’s communications regarding policy actions. Before 1994, the FOMC released only a difficult-to-interpret document called the “directive” regarding the supply of bank reserves to the market, and then only after the following meeting, when it had been superseded. Faced with this “radio silence,” an industry of Fed-watchers developed to try to infer policy decisions from market movements after a meeting.\n\nIn a series of steps beginning in 1994, the FOMC began to expand the amount of information released to the public immediately following a meeting. First, the intended target for the federal funds rate was released immediately following the meeting. This was then supplemented by a “balance of risks assessment,” which was often described as indicating the “tilt” in policy – that is, whether an increase or a decrease in the funds rate was relatively more likely at coming meetings. Over time, the statements gradually included more discussion of current and prospective economic conditions. Finally, in 2005, the FOMC began releasing the full minutes three weeks following the meeting, rather than after the subsequent meeting, as had been the practice.\n\nBack in 2003, the FOMC began issuing statements that sent fairly explicit signals about the likely path for the funds rate. Early in 2003, core inflation had drifted down to 1 percent. The statement released following the May meeting that year made reference to “an unwelcome further fall in inflation.” This was something of a watershed in Fed history – the first time in our modern experience that inflation threatened to fall too low. The language labeling a further fall in inflation as “unwelcome” conveyed the Committee’s intention to keep core inflation above 1 percent. At the next meeting in late June, the FOMC lowered the fed funds rate to 1 percent and repeated the “unwelcome” reference. After the August meeting, the statement said that “policy accommodation can be maintained for a considerable period.” This too was a watershed — explicit communication about the likely future path of the policy instrument. Since then the Committee has continued to communicate about the likely near-term policy path, using phrases like “can be patient” and “a pace that is likely to be measured.”\n\nThis series of moves toward greater and more timely communication form a natural progression. For decades, the Fed has articulated its desire for low and stable inflation. In the 1990s, the FOMC went beyond general goals and began disclosing current policy actions and their rationale, thus providing information on how past policy actions had been affected by current and prospective economic conditions. The balance of risks statement broke new ground by communicating, though somewhat elliptically, about the likely near-term policy direction. The forward-looking language used since 2003 has in turn provided richer insights into the policy actions that the Committee believes it would have to take to achieve its goals.\n\nI can understand why this progression took as long as it did. Starting from a status quo in which the Fed provided very little up-to-the-minute information on its policy, small initial steps were probably warranted. Too abrupt a change may have temporarily created uncertainty in the markets about the meaning of the information coming out of the Fed, although clearly, over time, market participants would learn how to interpret Fed statements. If that uncertainty fed into market volatility, even as a transitory matter while markets learned about the new format, then transparency skeptics might have been able to use that volatility as an argument against sharing information with the public.\n\nI think it is important to be clear about what the Committee communicates when it comments on the near-term policy direction. Central banks set interest rates in response to incoming economic data. What ultimately drives that data is the evolution of economic fundamentals — that is, the evolution of technologies and external factors like world commodity prices. So the current policy rate should be thought of as a function (though probably a fairly complicated one) of the fundamentals — that’s what systematic policy means. When the FOMC communicates about the likely future path of policy rates, as it did from mid-2003 on, it is communicating about two separate things at once — first, how fundamentals are likely to evolve, and second, how policy is likely to react to those fundamentals. I believe there are important roles for both, but that it is important to distinguish between the two.\n\nThe striking feature of the period since mid-2003 has been that the likely evolution of economic fundamentals and the likely policy reaction have combined to make the likely path of our policy instrument, the fed funds rate, relatively clear. Beginning in early 2004, for example, it was clear that short-term interest rates were going to have to rise steadily as the economy recovered and made the transition to a sustained growth path. Having now moved much closer to such a growth path, however, it may be much less common for the FOMC to find itself willing and able to forecast an extended string of rate changes.\n\nBut if the federal funds rate path becomes less predictable than it has been over the last 14 FOMC meetings, does that mean that the Committee must retreat to saying little beyond announcing its rate decisions when they are made? In my opinion, no. My sense is that there will still be room for forward-looking communications that entail more conditional statements about how policy is likely to react to evolving economic fundamentals, in contrast to the less conditional statements common since 2003. I opened my remarks tonight by noting the importance of systematic public expectations regarding monetary policy. Building better public understanding of how policy systematically responds to evolving economic conditions is the key to enhancing our credibility and improving the effectiveness of monetary policy.\n\nBeyond moving to more conditional forward-looking language, I think there is probably more that can be done to build systematic public understanding. An important component of that understanding is the public’s sense of our long-run intentions for inflation. Several FOMC members, myself included, have indicated the level or range of inflation that they would like to see prevail over the long run, but the Committee itself has not formally adopted such a goal or target for inflation. Providing quantitative guidance to the public about the Committee’s long-run inflation intentions would have the benefit of reducing uncertainty about future monetary policy, and more securely anchoring long-run inflation expectations.\n\nIn his nomination hearings last November, Chairman Bernanke acknowledged that he has supported the idea of a quantitative inflation objective in academic writings and in speeches as a member of the Board of Governors. But he assured the senators that, if confirmed, he would “take no precipitate steps” in this direction, and he indicated that the idea “requires further study … as well as extensive discussion and consultation.” He went on to say that he would act only if a “consensus” develops that doing so would further enhance our ability to achieve our mandated objectives. I have already expressed my own support for such a formal quantitative statement as a means of providing an anchor for long-term inflation expectations 3\n, and I look forward to further study and discussion of the issue under Chairman Bernanke’s leadership.\n\n\n\nI suspect that the next frontier in Fed communications will involve the framework we use for policy analysis. After we have had some practice at communicating our past actions and how we are likely to respond in the future, it will be time to communicate more about why we will respond the way we will respond. Admittedly, this will be a tougher nut to crack, if only because of the genuine scientific uncertainty that typically surrounds economic inferences. But to the extent that a reasonably firm consensus can be obtained on some basic principles, I believe that it would aid public understanding for us to find ways to communicate them. Public understanding of the economic reasoning underlying our policy choices would help prevent drawing the wrong lessons from history.\n\nFor example, the initial response of market participants to the spike in energy prices that followed Hurricane Katrina suggested that people considered it possible or even likely that the FOMC would pause in its sequence of rate hikes, and that we would be willing to tolerate an acceleration in broader measures of inflation in response to the energy price increases. In the popular media, this expectation was tied to the experience with energy price shocks in the 1970s. But as I’ve already emphasized, the systematic part of policy in the 1970s was very different from what it became in the Greenspan years. Such misunderstandings can create challenges for policymakers. If the public comes to expect rising inflation, based on an outdated view of how policy responds to an economic shock, then the task of preventing a rise in inflation is made more difficult.\n\nI commented earlier about the nature of monetary policy when inflation is low and stable. I emphasized that real, inflation-adjusted interest rates should be expected to fluctuate in such circumstances, even in the absence of visible fluctuations in inflation pressures. With inflation low and steady, changes in real interest rates require changes in the nominal overnight policy rate that the Fed directly controls. Communicating that fact will help the public understand that policy needs to respond to changing real economic conditions. Moreover, focusing on real interest rates draws attention to how and why policy must respond; real interest rates must fluctuate to accommodate changes in the relative pressure on current versus future resources. Widespread understanding of this would have aided the market response to Katrina; the storm impaired the supply of current resources relative to the future, and so, if anything real interest rates had to rise, not fall.\n\nTo sum up, then, Chairman Greenspan’s success was predicated on establishing credibility — that is, widespread public confidence that the future conduct of monetary policy would keep inflation low and stable. Building that confidence through actions alone was insufficient, and the Greenspan Fed began to expand communications, first about current policy actions, then about likely prospective actions. To maintain and build credibility, we are likely to continue to look for ways to enhance communications in the years ahead. Our efforts in this direction will be informed both by a rich history of experience with monetary policy and by a growing body of knowledge gained from viewing that history through the lens of economic logic.\n\nOn a personal note, I have known Ben Bernanke professionally since shortly after the publication of his influential 1983 paper on money and credit in the Great Depression. He is an outstanding monetary economist but also an imminently sensible monetary policy practitioner. I am looking forward with enthusiasm to serving in the Bernanke Fed.\n\nLooking back, I count it an extraordinary honor and privilege to have served under Chairman Greenspan since mid-2004."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2006",
        "date": "January 20, 2006",
        "title": "The Economic Outlook",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke January 20, 2006 at the Risk Management Association, Richmond Chapter, in Richmond, Va.",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2006/lacker_speech_20060120",
        "content": "It is a pleasure to be with you today to discuss the economic outlook for 2006 and beyond. It is a pleasure, in part, because the economic outlook is fairly encouraging. Growth is on a solid footing, despite this year’s run-up in energy prices and the disruptions of a devastating hurricane season. And after a brief pause this fall, employment is expanding again at a healthy pace, consumer spending continues to grow briskly, and business investment spending is robust. Granted, housing activity seems to be softening, and at least some potential price level pressures remain, so it may be too soon to break out the champagne. But inflation expectations remain contained, and we at the Fed are well-positioned to resist inflation pressures, should they emerge.\n\nSo all in all, it is quite a good outlook.\n\nIn my remarks today, I would like to review the economic outlook in a bit more detail, and then talk about monetary policy. As always, my remarks reflect my own views, and not necessarily those of my colleagues in the Federal Reserve.\nThe really striking feature of the current outlook is the extent to which economic activity in general and consumer spending in particular has rebounded from the shock of last year’s hurricane season. In the immediate aftermath of Hurricane Katrina, fears were widespread that consumers might pull back sharply on spending, both in response to sharply higher retail gasoline prices and out of a general sense of heightened anxiety about potential fallout from the storm damage.\n\nSurvey measures of consumer confidence, which plummeted in September, seemed to bolster this view. But the effect of the storms on consumer outlays have turned out to be far more limited than expected, exemplifying the oft-cited resilience of the U.S. economy. Apart from auto sales, which slid following expiration of the summer’s “employee discount” promotions, retail sales have held up fairly well and overall consumer spending has continued to advance. And on the whole, holiday spending came in stronger than many feared in the immediate aftermath of Hurricanes Katrina and Rita.\n\nI would argue that this episode illustrates quite well how consumption expenditures are governed predominantly by households’ assessment of their own future income prospects, rather than by any general economic nervousness, despite how they respond to telephone pollsters. With healthy income growth ahead and a reasonably strong overall job market, the outlook for consumer spending looks good.\n\nHousing market activity has been very strong over the last several years, and the historically low level of inflation-adjusted mortgage interest rates explains much of that strength. The fall in interest rates that began early in 2001 stimulated spending in interest-sensitive sectors like housing and durable goods and partially offset the then-emerging weakness in business investment spending. As the latter has recovered in the last two years, and real interest rates have had to rise as a consequence, a gradual “handoff” from housing investment has been expected.\n\nThat handoff has yet to occur; the ratio of business to residential investment outlays fell from around 2.75 in 2000 to about 1.75 last year, and has been fairly constant since then. Instead, the combination of low inflation-adjusted interest rates and sustained real income gains have continued to provide a strong stimulus to housing demand.\n\nIn recent months, we have received widespread anecdotal reports of what one informant of ours called “a return to normalcy” in several housing markets in our District. The multiple first-day bids and final sales at above-asking prices that were observed in some markets seem to have become less common. And in some markets, the amount of time a home stays on the market has returned to more typical levels.\n\nAt the same time, the aggregate measures of housing activity have so far shown only limited pull-back from their peaks and remain at historically high levels. The fact that housing market activity typically declines in the last few months of each year is making it difficult to collate quantitative and qualitative evidence and assess seasonally adjusted housing market trends. Still, mortgage rates are likely to stay somewhat above their recent lows in the coming year, so I would expect housing price appreciation to flatten out next year and aggregate residential investment to stop growing or perhaps even decline.\n\nThe fundamentals for business investment in equipment and software look quite sound. Business output is expanding steadily and real funding costs are relatively low, both because inflation-adjusted, risk-free rates have been low and because corporate risk spreads are relatively narrow.\n\nEvidently, there has been a sufficient flow of opportunities to deploy new capital profitably. Business investment in equipment and software has grown at over 11 percent in real terms since the first quarter of 2003, and it appears poised to grow at rates almost that strong next year. Capital formation, particularly investment in information and communications technology (ICT), played an instrumental role in the widely noted surge in productivity growth that took place in the late 1990s. The fundamental driving force was the sustained and rapid fall in the relative prices of these technologies.\n\nAlthough initial productivity growth figures for that period were revised downward in subsequent data releases, our best estimates now are that productivity accelerated significantly in the mid-1990s from the relatively stagnant pace of 1.5 percent seen over the previous 20 years to 2.6 percent over the second half of the 1990s.\n\nProductivity has grown at surprisingly strong rates since then — 3.4 percent since the end of 2000 — despite significantly lower rates of capital formation. Productivity growth in the first half of this decade thus must be mainly attributable to gains in what economists call “total factor productivity” — that is, output growth in excess of all input growth through reorganization of the use of those inputs. At the risk of oversimplification, one could say that firms increased productivity in the 1990s by providing workers with better technology, but in this decade by restructuring business processes to better exploit the technology they had.\n\nOne interpretation of these two episodes is that ICT investment outlays yield both an initial productivity gain (which our standard methods attribute to capital deepening) and then further productivity gains down the road as business processes are steadily optimized for the new infrastructure.\n\nOne implication of this perspective on recent productivity trends is that the current expansion in business investment is laying a foundation for future growth in total factor productivity, and thus provides at least some grounds for optimism that productivity growth might come in at 2.5 percent or higher, rather than the long-run trend rate of 2.25 percent.\n\nUnfortunately, empirical evidence on this is limited, and as always, forecasting productivity growth should be done with humility, given economists’ notably poor track record in this area.\n\nGains in labor productivity, whether due to capital deepening or improved business processes, ultimately pass through to real incomes. As a result, total real personal income has grown recently: over 2 percent per year since the rebound in employment in mid-2003, despite significant energy price increases. If productivity growth continues at or above trend, as seems likely, then we should see healthy growth in real income next year, anticipation of which should continue to support consumption growth in 2006.\n\nLabor markets have recovered from the recession of 2001. Although employment was stagnant for a time following the downturn, hiring picked up in 2003. Of course, Hurricane Katrina disrupted labor markets by forcing the displacement of close to a million people from the Gulf Coast region. That separated a substantial number of workers from their employers, and damaged a substantial portion of the capital stock in the affected areas. As a result, U.S. employment growth was noticeably depressed in September and October, although quantitative estimates of the storms’ effects are imprecise.\n\nPayroll expansion has averaged over 200,000 jobs per month since October, however, more than enough to keep up with the growth in working-age population.\n\nThe overall outlook therefore is for a healthy expansion next year. Real GDP should grow at about 3.5 percent. Household spending should grow at about the same rate in real terms. Business investment should expand substantially faster than overall output and residential investment should expand more slowly, perhaps even falling in real terms. And I expect employment to track the growth in the working-age population in 2006.\n\nThis is a fairly balanced picture, but naturally there is some uncertainty attached to it. Economic fundamentals could depart from their anticipated trajectories in any number of ways that could leave a mark on U.S. economic aggregates.\n\nFor example, spot oil prices — or other commodity prices for that matter — could well turn out either above or below the path embodied in futures prices. Many global commodity markets have been affected by the unanticipated surge in worldwide demand over the last several of years; those for which supply elasticities are low have experienced significant price run-ups. Commodity price surprises in either direction could alter aggregate supply conditions and either add or subtract from output growth.\n\nOn the demand side, there is some uncertainty regarding the rate at which housing activity is at all likely to cool in the coming year. Although I do not think that a sharp fall in housing investment is likely, a range of forecasts from flat to moderately declining seem reasonable.\n\nAnd while continued growth in the share of output devoted to business investment seems highly probable, it is difficult to foresee with any certainty the scale of investment that businesses will find profitable to undertake, so spending growth in this category could well deviate from expectations.\n\nIn contrast, growth in household spending is easier to forecast, because both economic theory and empirical evidence indicate that consumption growth is tied closely to income growth over time. The range of likely outcomes for real consumption growth is correspondingly more narrow.\n\nDifferences between how economic fundamentals are expected to unfold and how they actually unfold can have important implications for real interest rates and thus for monetary policy.\n\nAs I have emphasized elsewhere, a real interest rate is a relative price — the price of current resources relative to the future resources one either forgoes by borrowing or obtains by investing. Real interest rates need to respond to changes in the relative pressure on current versus future resources.\n\nUnpredicted movements in economic fundamentals, to the extent that they affect the relative pressure on current and future resources, thus will have implications for policy rates, even in situations in which inflation and inflation expectations are low and well-contained.\n\nCore inflation has been low and relatively steady in the last several years. The inflation measure that is widely preferred on methodological grounds, the price index for core personal consumption expenditures, has averaged 1.8 percent over the 12 months ending in November. That is within the 1-to-2 percent range that I and others have proposed as an announced target.\n\nEven before Katrina, overall inflation, including food and energy prices, was elevated due to the run-up in energy prices in the spring and summer. Hurricanes Katrina and Rita severely disrupted energy production in the Gulf and led to sharp increases in refining margins and prices for gasoline and natural gas. U.S. natural gas production and petroleum refining are still down 5 percent since Katrina, and crude oil production is down 10 percent.\n\nImmediately following Hurricane Katrina, as the magnitude of the effects on Gulf Coast energy production became clear, many observers came to fear that the resulting sharp increase in energy prices might lead to a broader increase in inflation, and perhaps even recessionary forces. These observers appeared to be reasoning by analogy to the 1970s, but I believe that analogy is seriously mistaken.\n\nInflation expectations were unanchored in the 1970s, the credibility of the Federal Reserve was low, and people expected the Fed to allow energy price shocks to feed through to overall inflation. The Fed often accommodated that expectation by preventing short-term real interest rates from rising. In fact, at times we kept nominal rates from rising as fast as inflation and thus provided further monetary stimulus. The Fed was then forced to raise rates dramatically to bring inflation back down, and in the process induced an economic contraction, exacerbating the real effects of the oil price shocks.\n\nThus, the proper lesson from the 1970s is not that energy price shocks induce major recessions or cause widespread inflation; it is that monetary policy that reacts to energy price shocks by accommodating the rise in inflation can induce major recessions.\n\nMonetary policy should to respond to energy shocks by remaining focused on price stability. That way, the economy can respond to energy price shocks the way it should — the relative price of energy increases, but core inflation remains anchored. In the immediate aftermath of Hurricanes Katrina and Rita, monetary policymakers naturally have focused on the risk that the attendant energy price increases would “pass through” to an acceleration in core inflation. While the lack of an upsurge in the core PCE inflation figures since September is somewhat encouraging, I think it is too soon to declare that “pass-through risk” is entirely behind us. This assessment is consistent with the statement released by the FOMC following its December meeting, which noted that, “…elevated energy prices have the potential to add to inflation pressures.”\n\nTo my mind, any energy price pass-through to core inflation that is more than marginal and transitory would be unwelcome.\n\nThus far, market participants appear to believe that core inflation will remain contained. Survey measures of expected inflation rose sharply in September when retail gasoline prices reached their peak, but have come back down since. Measures of expected inflation derived from market prices of inflation protected U.S. Treasury securities drifted up a bit this fall, but they too have returned to mid-summer levels.\n\nTo maintain credibility for price stability, it is essential that monetary policy should respond vigorously to any visible erosion in inflation expectations.\n\nMany of you may have noticed that in the statement released following the last FOMC meeting, the term “accommodation” was dropped, or, in the words of one of my colleagues, “given an honorable discharge.” Many observers are taking this as a sign that the Committee may be coming close to completing the current sequence of tightening moves that began in June of 2004.\n\nI discussed earlier that in an era of low and stable inflation, real interest rate movements will predominantly reflect the relative pressure on current versus future resources. Recessions, in modern industrialized economies, are associated with transitory declines in the demand for current goods and services. Since demand ultimately will recover, real interest rates need to fall in recessions to reflect the abundance of current relative to future resources. Thus, the FOMC engineered a reduction in real interest rates in 2001 that lasted until mid-2004, when a steady recovery in demand became evident. Since then, the economy has been on a transitional trajectory toward a path characterized by sustained and balanced expansion with relatively full utilization of resources. Along this transition, real interest rates have been rising toward a range consistent with the sustained growth path to which the economy has been headed.\n\nIt deserves emphasis, however, that sustained growth is not likely to be perfectly smooth and predictable. Unpredicted variations in economic fundamentals can and will affect economic conditions, even if they are not so large as to induce a recessionary break in growth.\n\nAnd as I emphasized earlier, if those variations have implications for the relative pressure on current versus future resources, they will have implications for real interest rates as well. The long expansions of the 1980s and 1990s were both cases in which interest rates fluctuated as the economy experienced sustained growth.\n\nThus, whenever the current sequence of tightening moves reaches completion, short-term interest rates should not be expected to remain constant for an extended period of time. Instead, they will likely move from time to time during the expansion ahead.\n\nPolicymakers will need to be alert for movements in economic fundamentals that shift the relative pressure on current versus future resources in ways that require changes in real interest rates, even if inflation pressures subside."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2006",
        "date": "January 18, 2006",
        "title": "The Economic Outlook",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke January 18, 2006 at the Townson University Economic Outlook Conference 2006 in Townson, Md. ",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2006/lacker_speech_20060118",
        "content": "It is a pleasure to be with you today to discuss the economic outlook for 2006 and beyond. It is a pleasure, in part, because the economic outlook is fairly encouraging. Growth is on a solid footing, despite this year’s run-up in energy prices and the disruptions of a devastating hurricane season. And after a brief pause this fall, employment is expanding again at a healthy pace, consumer spending continues to grow briskly, and business investment spending is robust. Granted, housing activity seems to be softening, and at least some potential price level pressures remain, so it may be too soon to break out the champagne. But inflation expectations remain contained, and we at the Fed are well-positioned to resist inflation pressures, should they emerge.\n\nSo all in all, it is quite a good outlook.\n\nIn my remarks today, I would like to review the economic outlook in a bit more detail, and then talk about monetary policy. As always, my remarks reflect my own views, and not necessarily those of my colleagues in the Federal Reserve.\nThe really striking feature of the current outlook is the extent to which economic activity in general and consumer spending in particular has rebounded from the shock of last year’s hurricane season. In the immediate aftermath of Hurricane Katrina, fears were widespread that consumers might pull back sharply on spending, both in response to sharply higher retail gasoline prices and out of a general sense of heightened anxiety about potential fallout from the storm damage.\n\nSurvey measures of consumer confidence, which plummeted in September, seemed to bolster this view. But the effect of the storms on consumer outlays have turned out to be far more limited than expected, exemplifying the oft-cited resilience of the U.S. economy. Apart from auto sales, which slid following expiration of the summer’s “employee discount” promotions, retail sales have held up fairly well and overall consumer spending has continued to advance. And on the whole, holiday spending came in stronger than many feared in the immediate aftermath of Hurricanes Katrina and Rita.\n\nI would argue that this episode illustrates quite well how consumption expenditures are governed predominantly by households’ assessment of their own future income prospects, rather than by any general economic nervousness, despite how they respond to telephone pollsters. With healthy income growth ahead and a reasonably strong overall job market, the outlook for consumer spending looks good.\n\nHousing market activity has been very strong over the last several years, and the historically low level of inflation-adjusted mortgage interest rates explains much of that strength. The fall in interest rates that began early in 2001 stimulated spending in interest-sensitive sectors like housing and durable goods and partially offset the then-emerging weakness in business investment spending. As the latter has recovered in the last two years, and real interest rates have had to rise as a consequence, a gradual “handoff” from housing investment has been expected.\n\nThat handoff has yet to occur; the ratio of business to residential investment outlays fell from around 2.75 in 2000 to about 1.75 last year, and has been fairly constant since then. Instead, the combination of low inflation-adjusted interest rates and sustained real income gains have continued to provide a strong stimulus to housing demand.\n\nIn recent months, we have received widespread anecdotal reports of what one informant of ours called “a return to normalcy” in several housing markets in our District. The multiple first-day bids and final sales at above-asking prices that were observed in some markets seem to have become less common. And in some markets, the amount of time a home stays on the market has returned to more typical levels.\n\nAt the same time, the aggregate measures of housing activity have so far shown only limited pull-back from their peaks and remain at historically high levels. The fact that housing market activity typically declines in the last few months of each year is making it difficult to collate quantitative and qualitative evidence and assess seasonally adjusted housing market trends. Still, mortgage rates are likely to stay somewhat above their recent lows in the coming year, so I would expect housing price appreciation to flatten out next year and aggregate residential investment to stop growing or perhaps even decline.\n\nThe fundamentals for business investment in equipment and software look quite sound. Business output is expanding steadily and real funding costs are relatively low, both because inflation-adjusted, risk-free rates have been low and because corporate risk spreads are relatively narrow.\n\nEvidently, there has been a sufficient flow of opportunities to deploy new capital profitably. Business investment in equipment and software has grown at over 11 percent in real terms since the first quarter of 2003, and it appears poised to grow at rates almost that strong next year. Capital formation, particularly investment in information and communications technology (ICT), played an instrumental role in the widely noted surge in productivity growth that took place in the late 1990s. The fundamental driving force was the sustained and rapid fall in the relative prices of these technologies.\n\nAlthough initial productivity growth figures for that period were revised downward in subsequent data releases, our best estimates now are that productivity accelerated significantly in the mid-1990s from the relatively stagnant pace of 1.5 percent seen over the previous 20 years to 2.6 percent over the second half of the 1990s.\n\nProductivity has grown at surprisingly strong rates since then — 3.4 percent since the end of 2000 — despite significantly lower rates of capital formation. Productivity growth in the first half of this decade thus must be mainly attributable to gains in what economists call “total factor productivity” — that is, output growth in excess of all input growth through reorganization of the use of those inputs. At the risk of oversimplification, one could say that firms increased productivity in the 1990s by providing workers with better technology, but in this decade by restructuring business processes to better exploit the technology they had.\n\nOne interpretation of these two episodes is that ICT investment outlays yield both an initial productivity gain (which our standard methods attribute to capital deepening) and then further productivity gains down the road as business processes are steadily optimized for the new infrastructure.\nOne implication of this perspective on recent productivity trends is that the current expansion in business investment is laying a foundation for future growth in total factor productivity, and thus provides at least some grounds for optimism that productivity growth might come in at 2.5 percent or higher, rather than the long-run trend rate of 2.25 percent.\n\nUnfortunately, empirical evidence on this is limited, and as always, forecasting productivity growth should be done with humility, given economists’ notably poor track record in this area.\n\nGains in labor productivity, whether due to capital deepening or improved business processes, ultimately pass through to real incomes. As a result, total real personal income has grown recently: over 2 percent per year since the rebound in employment in mid-2003, despite significant energy price increases. If productivity growth continues at or above trend, as seems likely, then we should see healthy growth in real income next year, anticipation of which should continue to support consumption growth in 2006.\n\nLabor markets have recovered from the recession of 2001. Although employment was stagnant for a time following the downturn, hiring picked up in 2003. Of course, Hurricane Katrina disrupted labor markets by forcing the displacement of close to a million people from the Gulf Coast region. That separated a substantial number of workers from their employers, and damaged a substantial portion of the capital stock in the affected areas. As a result, U.S. employment growth was noticeably depressed in September and October, although quantitative estimates of the storms’ effects are imprecise.\n\nPayroll expansion has averaged over 200,000 jobs per month since October, however, more than enough to keep up with the growth in working-age population.\n\nThe overall outlook therefore is for a healthy expansion next year. Real GDP should grow at about 3.5 percent. Household spending should grow at about the same rate in real terms. Business investment should expand substantially faster than overall output and residential investment should expand more slowly, perhaps even falling in real terms. And I expect employment to track the growth in the working-age population in 2006.\n\nThis is a fairly balanced picture, but naturally there is some uncertainty attached to it. Economic fundamentals could depart from their anticipated trajectories in any number of ways that could leave a mark on U.S. economic aggregates.\n\nFor example, spot oil prices — or other commodity prices for that matter — could well turn out either above or below the path embodied in futures prices. Many global commodity markets have been affected by the unanticipated surge in worldwide demand over the last several of years; those for which supply elasticities are low have experienced significant price run-ups. Commodity price surprises in either direction could alter aggregate supply conditions and either add or subtract from output growth.\n\nOn the demand side, there is some uncertainty regarding the rate at which housing activity is at all likely to cool in the coming year. Although I do not think that a sharp fall in housing investment is likely, a range of forecasts from flat to moderately declining seem reasonable.\n\nAnd while continued growth in the share of output devoted to business investment seems highly probable, it is difficult to foresee with any certainty the scale of investment that businesses will find profitable to undertake, so spending growth in this category could well deviate from expectations.\n\nIn contrast, growth in household spending is easier to forecast, because both economic theory and empirical evidence indicate that consumption growth is tied closely to income growth over time. The range of likely outcomes for real consumption growth is correspondingly more narrow.\n\nDifferences between how economic fundamentals are expected to unfold and how they actually unfold can have important implications for real interest rates and thus for monetary policy.\n\nAs I have emphasized elsewhere, a real interest rate is a relative price — the price of current resources relative to the future resources one either forgoes by borrowing or obtains by investing. Real interest rates need to respond to changes in the relative pressure on current versus future resources.\n\nUnpredicted movements in economic fundamentals, to the extent that they affect the relative pressure on current and future resources, thus will have implications for policy rates, even in situations in which inflation and inflation expectations are low and well-contained.\n\nCore inflation has been low and relatively steady in the last several years. The inflation measure that is widely preferred on methodological grounds, the price index for core personal consumption expenditures, has averaged 1.8 percent over the 12 months ending in November. That is within the 1-to-2 percent range that I and others have proposed as an announced target.\n\nEven before Katrina, overall inflation, including food and energy prices, was elevated due to the run-up in energy prices in the spring and summer. Hurricanes Katrina and Rita severely disrupted energy production in the Gulf and led to sharp increases in refining margins and prices for gasoline and natural gas. U.S. natural gas production and petroleum refining are still down 5 percent since Katrina, and crude oil production is down 10 percent.\n\nImmediately following Hurricane Katrina, as the magnitude of the effects on Gulf Coast energy production became clear, many observers came to fear that the resulting sharp increase in energy prices might lead to a broader increase in inflation, and perhaps even recessionary forces. These observers appeared to be reasoning by analogy to the 1970s, but I believe that analogy is seriously mistaken.\n\nInflation expectations were unanchored in the 1970s, the credibility of the Federal Reserve was low, and people expected the Fed to allow energy price shocks to feed through to overall inflation. The Fed often accommodated that expectation by preventing short-term real interest rates from rising. In fact, at times we kept nominal rates from rising as fast as inflation and thus provided further monetary stimulus. The Fed was then forced to raise rates dramatically to bring inflation back down, and in the process induced an economic contraction, exacerbating the real effects of the oil price shocks.\n\nThus, the proper lesson from the 1970s is not that energy price shocks induce major recessions or cause widespread inflation; it is that monetary policy that reacts to energy price shocks by accommodating the rise in inflation can induce major recessions.\n\nMonetary policy should to respond to energy shocks by remaining focused on price stability. That way, the economy can respond to energy price shocks the way it should — the relative price of energy increases, but core inflation remains anchored. In the immediate aftermath of Hurricanes Katrina and Rita, monetary policymakers naturally have focused on the risk that the attendant energy price increases would “pass through” to an acceleration in core inflation. While the lack of an upsurge in the core PCE inflation figures since September is somewhat encouraging, I think it is too soon to declare that “pass-through risk” is entirely behind us. This assessment is consistent with the statement released by the FOMC following its December meeting, which noted that, “…elevated energy prices have the potential to add to inflation pressures.”\n\nTo my mind, any energy price pass-through to core inflation that is more than marginal and transitory would be unwelcome.\n\nThus far, market participants appear to believe that core inflation will remain contained. Survey measures of expected inflation rose sharply in September when retail gasoline prices reached their peak, but have come back down since. Measures of expected inflation derived from market prices of inflation protected U.S. Treasury securities drifted up a bit this fall, but they too have returned to mid-summer levels.\n\nTo maintain credibility for price stability, it is essential that monetary policy should respond vigorously to any visible erosion in inflation expectations.\n\nMany of you may have noticed that in the statement released following the last FOMC meeting, the term “accommodation” was dropped, or, in the words of one of my colleagues, “given an honorable discharge.” Many observers are taking this as a sign that the Committee may be coming close to completing the current sequence of tightening moves that began in June of 2004.\n\nI discussed earlier that in an era of low and stable inflation, real interest rate movements will predominantly reflect the relative pressure on current versus future resources. Recessions, in modern industrialized economies, are associated with transitory declines in the demand for current goods and services. Since demand ultimately will recover, real interest rates need to fall in recessions to reflect the abundance of current relative to future resources. Thus, the FOMC engineered a reduction in real interest rates in 2001 that lasted until mid-2004, when a steady recovery in demand became evident. Since then, the economy has been on a transitional trajectory toward a path characterized by sustained and balanced expansion with relatively full utilization of resources. Along this transition, real interest rates have been rising toward a range consistent with the sustained growth path to which the economy has been headed.\n\nIt deserves emphasis, however, that sustained growth is not likely to be perfectly smooth and predictable. Unpredicted variations in economic fundamentals can and will affect economic conditions, even if they are not so large as to induce a recessionary break in growth.\n\nAnd as I emphasized earlier, if those variations have implications for the relative pressure on current versus future resources, they will have implications for real interest rates as well. The long expansions of the 1980s and 1990s were both cases in which interest rates fluctuated as the economy experienced sustained growth.\n\nThus, whenever the current sequence of tightening moves reaches completion, short-term interest rates should not be expected to remain constant for an extended period of time. Instead, they will likely move from time to time during the expansion ahead.\n\nPolicymakers will need to be alert for movements in economic fundamentals that shift the relative pressure on current versus future resources in ways that require changes in real interest rates, even if inflation pressures subside."
    }
]