[
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2005",
        "date": "December 22, 2005",
        "title": "The Economic Outlook",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke December 22, 2005 at the National Economists Club in Washington, D.C. ",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2005/lacker_speech_20051222",
        "content": "It is a pleasure to be with you today to discuss the economic outlook for 2006 and beyond. It is a pleasure, in part, because the economic outlook is fairly encouraging. Growth is on a solid footing, despite this years run-up in energy prices and the disruptions of a devastating hurricane season. After a brief pause this fall, employment has resumed expanding at a healthy pace, consumer spending continues to grow briskly, and business investment spending is robust. Granted, housing activity seems to be softening, and at least some potential price level pressures remain, so it may be too soon to break out the eggnog. But inflation expectations remain contained, and we at the Fed are well-positioned to resist inflation pressures, should they emerge. So all in all, it is quite a good outlook. In fact, in the spirit of the holiday season, I am tempted to say that I bring you tidings of comfort and joy, but I am afraid that might strike you as uncharacteristically exuberant for a central banker, so let me just say that tidings appear to be improving at a measured pace.\n\nIn my remarks today, I would like to review the economic outlook in a bit more detail, and then talk about monetary policy. As always, my remarks reflect my own views, and not necessarily those of my colleagues in the Federal Reserve.\n\nThe really striking feature of the current outlook is the extent to which economic activity in general and consumer spending in particular has rebounded from the shock of the hurricane season. In the immediate aftermath of Hurricane Katrina, fears were widespread that consumers might pull back sharply on spending, both in response to sharply higher retail gasoline prices and out of a general sense of heightened anxiety about potential fallout from the storm damage. Survey measures of consumer confidence, which plummeted in September, seemed to bolster this view. But the effect of the storms on consumer outlays have turned out to be far more limited than expected, exemplifying the oft-cited resilience of the U.S. economy. Apart from auto sales, which slid following expiration of the summers employee discount promotions, retail sales have held up well and overall consumer spending has continued to advance. And on the whole, holiday spending appears to be coming in stronger than many feared a month or two ago. I would argue that this episode illustrates quite well how consumption expenditures are governed predominantly by households assessment of their own future income prospects, rather than by any general economic nervousness, despite how they respond to telephone pollsters. With healthy income growth ahead and a reasonably strong overall job market, the outlook for consumer spending looks good.\n\nHousing market activity has been very strong over the last several years. The historically low level of inflation-adjusted mortgage interest rates explains much of that strength. The fall in interest rates that began early in 2001 stimulated spending in interest-sensitive sectors like housing and durable goods and partially offset the emerging weakness in business investment spending. As the latter has recovered in the last two years, and real interest rates have had to rise as a consequence, a gradual handoff from housing investment has been expected. That handoff has yet to occur; the ratio of business to residential investment outlays fell from around 2.75 in 2000 to about 1.75 last year, and has been fairly constant since then. Instead, the combination of low inflation-adjusted interest rates and sustained real income gains have continued to provide a strong stimulus to housing demand.\n\nIn recent months, we have received widespread anecdotal reports of what one informant of ours called a return to normalcy in several housing markets in our District. The multiple first-day bids and final sales at above asking prices that were observed in some markets seem to have become less common. And in some markets the amount of time a home stays on the market has returned to more typical levels. At the same time, the aggregate measures of housing activity have so far shown only limited pull-back from their peaks and remain at historically high levels. Still, mortgage rates are likely to stay somewhat above their recent lows in the coming year, so I would expect housing price appreciation to flatten out next year and aggregate residential investment to stop growing or perhaps even decline.\n\nThe fundamentals for business investment in equipment and software look quite sound. Business output is expanding steadily and real funding costs are relatively low, both because inflation-adjusted, risk-free rates have been low and because corporate risk spreads are relatively narrow. Evidently, there has been a sufficient flow of opportunities to deploy new capital profitably. Business investment in equipment and software has grown at over 11 percent in real terms since the first quarter of 2003, and it appears poised to grow at rates almost that strong next year.\n\nCapital formation, particularly investment in information and communications technology (ICT), played an instrumental role in the widely noted surge in productivity growth that took place in the late 1990s. The fundamental driving force was the sustained and rapid fall in the relative prices of these technologies. Although initial productivity growth figures for that period were revised downward in subsequent data releases, our best estimates now are that productivity accelerated significantly in the mid-1990s from the relatively stagnant pace of 1.5 percent seen over the previous 20 years to 2.6 percent.\n\nProductivity has grown at surprisingly strong rates since then 3.4 percent since the end of 2000 despite significantly lower rates of capital formation. Productivity growth in the first half of this decade thus must be mainly attributable to gains in what economists call total factor productivity that is, output growth in excess of all input growth through reorganization of the use of those inputs. At the risk of oversimplification, one could say that firms increased productivity in the 1990s by providing workers with better technology, but in this decade by restructuring business processes to better exploit the technology they had. One interpretation of these two episodes is that ICT investment outlays yield both an initial productivity gain (which our standard methods attribute to capital deepening) and then further productivity gains down the road as business processes are steadily optimized for the new infrastructure. One implication of this perspective on recent productivity trends is that the current expansion in business investment is laying a foundation for future growth in total factor productivity, and thus provides at least some grounds for optimism that productivity growth might come in at 2.5 percent or higher. Unfortunately, empirical evidence on this is limited, and as always, forecasting productivity growth should be done with humility, given economists notably poor track record in this area.\n\nGains in labor productivity, whether due to capital deepening or improved business processes, ultimately pass through to real incomes. As a result, total real personal income has grown recently: over 2 percent per year since the rebound in employment in mid-2003, despite significant energy price increases. If productivity growth continues at or above trend, as seems likely, then we should see healthy growth in real income next year, anticipation of which should continue to support consumption growth in 2006.\n\nLabor markets have recovered from the recession of 2001. Although employment was stagnant for a time following the downturn, hiring picked up in 2003. Of course, Hurricane Katrina disrupted labor markets by forcing the displacement of close to a million people from the Gulf Coast region. That separated a substantial number of workers from their employers, and damaged a substantial portion of the capital stock in the affected areas. As a result, U.S. employment growth was noticeably depressed in September and October, although quantitative estimates of the storms effects are imprecise. Payroll expansion resumed in November, however, and one would expect most of the gap to be made up over the next several months as reconstruction efforts get under way.\n\nThe overall outlook therefore is for a healthy expansion next year. Real GDP should grow at about 3.5 percent. Household spending should grow at about the same rate in real terms. Business investment should expand substantially faster than overall output and residential investment should expand more slowly, perhaps even falling in real terms. I expect employment to track the growth in the working age population.\n\nThis is a fairly balanced picture, but naturally there is some uncertainty attached to it. Economic fundamentals could depart from their anticipated trajectories in any number of ways that could leave a mark on U.S. economic aggregates. For example, spot oil prices or other commodity prices for that matter could well turn out either above or below the path embodied in futures prices. Many global commodity markets have been affected by the unanticipated surge in worldwide demand over the last several of years; those for which supply elasticities are low have experienced significant price run-ups. Commodity price surprises in either direction could alter aggregate supply conditions and either add or subtract from output growth.\n\nOn the demand side, there is some uncertainty regarding the rate at which housing activity is likely to cool in the coming year. Although I do not think that a sharp fall in housing investment is likely, a range of forecasts from flat to moderately declining seem reasonable. And while continued growth in the share of output devoted to business investment seems highly probable, it is difficult to foresee with any certainty the scale of investment that businesses will find profitable to undertake, so spending growth in this category could well deviate from expectations. In contrast, growth in household spending is easier to forecast, because both economic theory and empirical evidence indicate that consumption growth is tied closely to income growth over time. The range of likely outcomes for real consumption growth is correspondingly more narrow.\n\nDifferences between how economic fundamentals are expected to unfold and how they actually unfold can have important implications for real interest rates and thus for monetary policy. As I have emphasized elsewhere, a real interest rate is a relative price the price of current resources relative to the future resources one either forgoes by borrowing or obtains by investing.1\n\n Real interest rates need to respond to changes in the relative pressure on current versus future resources. Unpredicted movements in economic fundamentals, to the extent that they affect the relative pressure on current and future resources, thus will have implications for policy rates, even in situations in which inflation and inflation expectations are low and well-contained.\n\n\n\nCore inflation has been low and relatively steady in the last several years. The inflation measure that is widely preferred on methodological grounds, the price index for core personal consumption expenditures, has averaged 1.8 percent over the 12 months ending in October. That is within the 1-to-2 percent range that I and others have proposed as an announced target.2\n Although core PCE inflation on a year-over-year basis did drift above 2 percent for several months in late 2004 and early 2005 it went as high as 2.3 percent at one point it was only after the most recent Annual Revision to the National Income and Product Accounts that the series came in over 2 percent.\n\n\n\nEven before Katrina, overall inflation, including food and energy prices, was elevated due to the run-up in energy prices in the spring and summer. Hurricanes Katrina and Rita severely disrupted energy production in the Gulf and led to sharp increases in refining margins and prices for gasoline and natural gas. U.S. natural gas production and petroleum refining are still down 5 percent since Katrina, and crude oil production is down 10 percent.\n\nImmediately following Hurricane Katrina, as the magnitude of the effects on Gulf Coast energy production became clear, many observers came to fear that the resulting sharp increase in energy prices might lead to a broader increase in inflation, and perhaps even recessionary forces. These observers appeared to be reasoning by analogy to the 1970s, but I believe that analogy is mistaken. Inflation expectations were unanchored in the 1970s, the credibility of the Federal Reserve was low, and people expected the Fed to allow energy price shocks to feed through to overall inflation. The Fed often accommodated that expectation by preventing short-term real interest rates from rising. In fact, at times we kept nominal rates from rising as fast as inflation and thus provided further monetary stimulus. The Fed was then forced to raise rates dramatically to bring inflation back down, and in the process induced an economic contraction, exacerbating the real effects of the oil price shocks. Thus, the proper lesson from the 1970s is not that energy price shocks induce major recessions or cause widespread inflation; it is that monetary policy that reacts to energy price shocks by accommodating the rise in inflation can induce major recessions.\n\nMonetary policy should respond to energy shocks by remaining focused on price stability. That way, the economy can respond to energy price shocks the way it should the relative price of energy increases, but core inflation remains anchored. In the immediate aftermath of Hurricanes Katrina and Rita, monetary policymakers naturally have focused on the risk that the attendant energy price increases would pass through to an acceleration in core inflation. While the lack of an upsurge in the core PCE inflation figures for September and October is somewhat encouraging, I think it is too soon to declare that pass-through risk is entirely behind us. This assessment is consistent with the statement released by the FOMC following its meeting last week, which noted that: &elevated energy prices have the potential to add to inflation pressures. To my mind, any energy price pass-through to core inflation that is more than marginal and transitory would be unwelcome.\n\nThus far, market participants appear to believe that core inflation will remain contained. Survey measures of expected inflation rose sharply in September when retail gasoline prices reached their peak, but have come back down since. Measures of expected inflation derived from market prices of inflation-protected U.S. Treasury securities drifted up a bit this fall, but they too have returned to mid-summer levels. To maintain credibility for price stability, it is essential that monetary policy should respond vigorously to any visible erosion in inflation expectations.\n\nMany of you may have noticed that in the statement released following the last FOMC meeting, the term accommodation was dropped, or, in the words of one of my colleagues, given an honorable discharge. Many observers are taking this as a sign that the Committee may be coming close to completing the current sequence of tightening moves that began in June of 2004. I discussed earlier that in an era of low and stable inflation, real interest rate movements will predominantly reflect the relative pressure on current and future resources. Recessions, in modern industrialized economies, are associated with transitory declines in the demand for current goods and services. Since demand ultimately will recover, real interest rates need to fall to reflect the abundance of current relative to future resources. Thus, the FOMC engineered a reduction in real interest rates in 2001 that lasted until mid-2004, when a steady recovery in demand became evident. Since then, the economy has been on a transitional trajectory toward a path characterized by sustained and balanced expansion with relatively full utilization of resources. Along this transition, real interest rates have been rising toward a range consistent with the sustained growth path to which the economy has been headed.\n\nIt deserves emphasis, however, that sustained growth is not likely to be perfectly smooth and predictable. Unpredicted variations in economic fundamentals can and will affect economic conditions, even if they are not so large as to induce a recessionary break in growth. And as I emphasized earlier, if those variations have implications for the relative pressure on current versus future resources, they will have implications for real interest rates as well. The long expansions of the 1980s and 1990s were both cases in which interest rates fluctuated as the economy experienced sustained growth. Thus, whenever the current sequence of tightening moves reaches completion, short-term interest rates should not be expected to remain constant for an extended period of time. Instead, they will likely move from time to time during the expansion ahead. Policymakers will need to be alert for movements in economic fundamentals that shift the relative pressure on current versus future resources in ways that require changes in real interest rates, even if inflation pressures subside."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2005",
        "date": "December 21, 2005",
        "title": "The Economic Outlook",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke December 21, 2005 at the  Charlotte Chamber of Commerce Annual Economic Conference in Charlotte, N.C.",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2005/lacker_speech_20051221",
        "content": "It is a pleasure to be with you today to discuss the economic outlook for 2006 and beyond. It is a pleasure, in part, because the economic outlook is fairly encouraging. Growth is on a solid footing, despite this year’s run-up in energy prices and the disruptions of a devastating hurricane season. After a brief pause this fall, employment has resumed expanding at a healthy pace, consumer spending continues to grow briskly, and business investment spending is robust. Granted, housing activity seems to be softening, and at least some potential price level pressures remain, so it may be too soon to break out the eggnog. But inflation expectations remain contained, and we at the Fed are well-positioned to resist inflation pressures, should they emerge. So all in all, it is quite a good outlook. In fact, in the spirit of the holiday season, I am tempted to say that I bring you tidings of comfort and joy, but I am afraid that might strike you as uncharacteristically exuberant for a central banker, so let me just say that tidings appear to be improving at a measured pace.\n\nIn my remarks today, I would like to review the economic outlook in a bit more detail, and then talk about monetary policy. As always, my remarks reflect my own views, and not necessarily those of my colleagues in the Federal Reserve.\n\nThe really striking feature of the current outlook is the extent to which economic activity in general and consumer spending in particular has rebounded from the shock of the hurricane season. In the immediate aftermath of Hurricane Katrina, fears were widespread that consumers might pull back sharply on spending, both in response to sharply higher retail gasoline prices and out of a general sense of heightened anxiety about potential fallout from the storm damage. Survey measures of consumer confidence, which plummeted in September, seemed to bolster this view. But the effect of the storms on consumer outlays have turned out to be far more limited than expected, exemplifying the oft-cited resilience of the U.S. economy. Apart from auto sales, which slid following expiration of the summer’s “employee discount” promotions, retail sales have held up well and overall consumer spending has continued to advance. And on the whole, holiday spending appears to be coming in stronger than many feared a month or two ago. I would argue that this episode illustrates quite well how consumption expenditures are governed predominantly by households’ assessment of their own future income prospects, rather than by any general economic nervousness, despite how they respond to telephone pollsters. With healthy income growth ahead and a reasonably strong overall job market, the outlook for consumer spending looks good.\n\nHousing market activity has been very strong over the last several years. The historically low level of inflation-adjusted mortgage interest rates explains much of that strength. The fall in interest rates that began early in 2001 stimulated spending in interest-sensitive sectors like housing and durable goods and partially offset the emerging weakness in business investment spending. As the latter has recovered in the last two years, and real interest rates have had to rise as a consequence, a gradual “handoff” from housing investment has been expected. That handoff has yet to occur; the ratio of business to residential investment outlays fell from around 2.75 in 2000 to about 1.75 last year, and has been fairly constant since then. Instead, the combination of low inflation-adjusted interest rates and sustained real income gains have continued to provide a strong stimulus to housing demand.\n\nIn recent months, we have received widespread anecdotal reports of what one informant of ours called “a return to normalcy” in several housing markets in our District. The multiple first-day bids and final sales at above asking prices that were observed in some markets seem to have become less common. And in some markets the amount of time a home stays on the market has returned to more typical levels. At the same time, the aggregate measures of housing activity have so far shown only limited pull-back from their peaks and remain at historically high levels. Still, mortgage rates are likely to stay somewhat above their recent lows in the coming year, so I would expect housing price appreciation to flatten out next year and aggregate residential investment to stop growing or perhaps even decline.\n\nThe fundamentals for business investment in equipment and software look quite sound. Business output is expanding steadily and real funding costs are relatively low, both because inflation-adjusted, risk-free rates have been low and because corporate risk spreads are relatively narrow. Evidently, there has been a sufficient flow of opportunities to deploy new capital profitably. Business investment in equipment and software has grown at over 11 percent in real terms since the first quarter of 2003, and it appears poised to grow at rates almost that strong next year.\n\nCapital formation, particularly investment in information and communications technology (ICT), played an instrumental role in the widely noted surge in productivity growth that took place in the late 1990s. The fundamental driving force was the sustained and rapid fall in the relative prices of these technologies. Although initial productivity growth figures for that period were revised downward in subsequent data releases, our best estimates now are that productivity accelerated significantly in the mid-1990s from the relatively stagnant pace of 1.5 percent seen over the previous 20 years to 2.6 percent. Productivity has grown at surprisingly strong rates since then — 3.4 percent since the end of 2000 — despite significantly lower rates of capital formation.\n\nGains in labor productivity, whether due to capital deepening or improved business processes, ultimately pass through to real incomes. As a result, total real personal income has grown recently: over 2 percent per year since the rebound in employment in mid-2003, despite significant energy price increases. If productivity growth continues at or above trend, as seems likely, then we should see healthy growth in real income next year, anticipation of which should continue to support consumption growth in 2006.\n\nLabor markets have recovered from the recession of 2001. Although employment was stagnant for a time following the downturn, hiring picked up in 2003. Of course, Hurricane Katrina disrupted labor markets by forcing the displacement of close to a million people from the Gulf Coast region. That separated a substantial number of workers from their employers, and damaged a substantial portion of the capital stock in the affected areas. As a result, U.S. employment growth was noticeably depressed in September and October, although quantitative estimates of the storms’ effects are imprecise. Payroll expansion resumed in November, however, and one would expect most of the gap to be made up over the next several months as reconstruction efforts get under way.\n\nThe overall outlook therefore is for a healthy expansion next year. Real GDP should grow at about 3.5 percent. Household spending should grow at about the same rate in real terms. Business investment should expand substantially faster than overall output and residential investment should expand more slowly, perhaps even falling in real terms. I expect employment to track the growth in the working age population.\n\nThis is a fairly balanced picture, but naturally there is some uncertainty attached to it. Economic fundamentals could depart from their anticipated trajectories in any number of ways that could leave a mark on U.S. economic aggregates. For example, spot oil prices — or other commodity prices for that matter — could well turn out either above or below the path embodied in futures prices. Many global commodity markets have been affected by the unanticipated surge in worldwide demand over the last several of years; those for which supply elasticities are low have experienced significant price run-ups. Commodity price surprises in either direction could alter aggregate supply conditions and either add or subtract from output growth.\n\nOn the demand side, there is some uncertainty regarding the rate at which housing activity is likely to cool in the coming year. Although I do not think that a sharp fall in housing investment is likely, a range of forecasts from flat to moderately declining seem reasonable. And while continued growth in the share of output devoted to business investment seems highly probable, it is difficult to foresee with any certainty the scale of investment that businesses will find profitable to undertake, so spending growth in this category could well deviate from expectations. In contrast, growth in household spending is easier to forecast, because both economic theory and empirical evidence indicate that consumption growth is tied closely to income growth over time. The range of likely outcomes for real consumption growth is correspondingly more narrow.\n\nCore inflation has been low and relatively steady in the last several years. The inflation measure that is widely preferred on methodological grounds, the price index for core personal consumption expenditures, has averaged 1.8 percent over the 12 months ending in October. That is within the 1-to-2 percent range that I and others have proposed as an announced target.1\n\n\n\nEven before Katrina, overall inflation, including food and energy prices, was elevated due to the run-up in energy prices in the spring and summer. Hurricanes Katrina and Rita severely disrupted energy production in the Gulf and led to sharp increases in refining margins and prices for gasoline and natural gas. U.S. natural gas production and petroleum refining are still down 5 percent since Katrina, and crude oil production is down 10 percent.\n\nMonetary policy should respond to energy shocks by remaining focused on price stability. That way, the economy can respond to energy price shocks the way it should – the relative price of energy increases, but core inflation remains anchored. In the immediate aftermath of Hurricanes Katrina and Rita, monetary policymakers naturally have focused on the risk that the attendant energy price increases would “pass through” to an acceleration in core inflation. While the lack of an upsurge in the core PCE inflation figures for September and October is somewhat encouraging, I think it is too soon to declare that pass-through risk is entirely behind us. This assessment is consistent with the statement released by the FOMC following its meeting last week, which noted that: “…elevated energy prices have the potential to add to inflation pressures.” To my mind, any energy price pass-through to core inflation that is more than marginal and transitory would be unwelcome.\n\nThus far, market participants appear to believe that core inflation will remain contained. Survey measures of expected inflation rose sharply in September when retail gasoline prices reached their peak, but have come back down since. Measures of expected inflation derived from market prices of inflation-protected U.S. Treasury securities drifted up a bit this fall, but they too have returned to mid-summer levels. To maintain credibility for price stability, it is essential that monetary policy should respond vigorously to any visible erosion in inflation expectations."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2005",
        "date": "October 20, 2005",
        "title": "Interest Rate Policy After Greenspan",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke October 20, 2005 at the Winthrop University in Rock Hill, S.C.",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2005/lacker_speech_20051020",
        "content": "Early next year, we will experience an event that happens rarely in the Federal Reserve — the retirement of the Chairman of the Board of Governors. Alan Greenspan is just the fifth Fed Chairman in the modern era that began with the Treasury-Fed Accord in 1951, and his retirement provides us with an excellent opportunity both to look back at a period of extraordinary success in monetary policy-making and to look forward to the principles that might allow future policy to continue this success. I plan to do some of both today, but I may spend as much time looking back as looking forward, not because I’m particularly nostalgic for the 1990s, but because I think it’s important for us to understand the nature of our policy successes in order to draw the right lessons to guide our future thinking about policy. As always, the views expressed are my own, and do not necessarily represent the views of my colleagues in the Federal Reserve System.\n\nFirst and foremost, the success of monetary policy in the Greenspan era is evident in the behavior of inflation, the stability of which is our primary responsibility as a central bank. While under Greenspan’s predecessor, Chairman Paul Volcker, the Fed brought inflation down from double digit levels, the period since 1987 has seen inflation fall from an average of over four and a half percent per year in the late 1980s to about 2.5 percent in recent years. Perhaps just as importantly, inflation and inflation expectations have become more stable since then as well.\n\nDeclining inflation does not appear to have come at the cost of slower growth or high unemployment. The economy experienced two relatively mild recessions during that period, and they came on either end of the longest expansion in our nation’s history. Moreover, this has been a period during which there have been a number of real and financial shocks that might have been expected to derail economic growth. In fact, real economic growth appears to have been substantially less variable since the early 1980s, a development that several writers, including former Fed Governor Ben Bernanke have termed “The Great Moderation.”1\n\n\n\nOf course, not all of this bountiful good fortune is attributable to good monetary policy. For example, the 1990s saw a renewal of strong productivity growth, largely due to the emergence of significant new information processing and communications technologies, and fiscal policy was moved in a favorable direction that decade as well. But monetary policy has certainly played an important role, and Chairman Greenspan’s leadership of the Fed during this period has been widely praised for contributing to superior economic performance.\n\nTogether with the praise, a fair amount of recent commentary has focused on the reasons for this success. At the end of a policymaker’s term in office, it is natural to look back to appraise the conduct of policy during their tenure, and this task is considerably more pleasant when the results have been favorable.\n\nOne distinguishing characteristic of Fed policy under Chairman Greenspan that has been identified by some observers is “flexibility,” which they describe as a practical approach to policy that is not excessively tied to any one doctrine or any narrowly prescriptive approach to the conduct of policy.2\n In this view, the hallmark of monetary policy in the Greenspan years has been the careful analysis of the state of the economy, taking account of whatever is special about the current situation and then choosing an action appropriate for that situation. These observers see the flexibility of the Greenspan Fed as contrasting with adherence to a monetary policy “rule,” or with adoption of a numerical inflation target for performance.\n\n\n\nThis observation calls to mind the economics literature on “rules vs. discretion” in policy-making, a line of research recognized by the award of last year’s Nobel Prize in economics to Professors Finn Kydland and Edward Prescott.3\n In the late 1970s they demonstrated that a central bank that sets policy on a discretionary basis each meeting, focusing solely on current and prospective economic conditions, generally will not deliver the best possible policy. In particular, they showed that a central bank taking such a discretionary approach will be tempted at times to ease policy to boost employment and output, despite the risk of higher inflation. The anticipation that policymakers will behave this way in the future will drive up current inflation. The general problem is that the behavior of market participants today depends crucially on how they expect the central bank to set policy in the future. This feature makes monetary policy conceptually different from, say, driving a car, since the current behavior of the car doesn’t depend on what it expects the driver to do in the future.\n\n\n\nKydland and Prescott showed that as a result of this feature of the economy the policymakers would do better if they could “commit” to a pattern of behavior that avoids the temptation to ease policy at the expense of inflation. That is, by choosing now how they will conduct policy in the future, and convincing market participants that they will do so, policymakers can improve on the results of choosing policy on a period-by-period basis. To put it more concretely, a central bank can achieve better outcomes today by convincing markets that they will avoid inflationary temptations in the future. This is why central banks have come to focus so heavily on inflation expectations, and to react strongly when those expectations seem in danger of becoming unstable.\n\nTo achieve superior outcomes, however, the central bank’s promise has to be believable, that is to say “credible.” One way to do so is for the central bank to explicitly commit to a formula that determines the target level of the federal funds rate as a simple arithmetic function of a few macroeconomic variables, such as inflation and unemployment. The now famous (at least in central banking circles) “Taylor Rule” — which makes the policy interest rate a linear function of an output gap and the deviation of inflation from a target — is one such arithmetic formula. But the benefits of policy credibility can be achieved without a mechanical formula, as long as the central bank adheres to a consistent, predictable pattern of behavior that the public understands. The term “rules,” in the sense used by Kydland and Prescott, is best understood in this broader sense as a consistent and widely understood pattern of policymaking. Arithmetic rules are one way to achieve that, but not the only way. What is essential is a consistent pattern of behavior that the public understands and believes actually will describe the central bank’s future behavior.\n\nClearly, if the central bank wants the public to continue to believe that it will stick to a pattern of behavior in the future, it must actually follow through with that behavior as events unfold. And this idea of following through is key to the true distinction between a discretionary policymaker and a rule-like policymaker. Having worked to guide the public’s expectations about future policy, a rule-like policymaker sees actions that would disappoint those expectations as undesirable and to be avoided. That is, a rule-like policy maker seeks to preserve its reputation, as reflected in the public’s expectations. In contrast, a discretionary policymaker focuses solely on current and future economic conditions, and ignores the previous expectations of market participants concerning the policymaker’s current behavior.\n\nTo identify discretionary policy setting in the Kydland and Prescott sense as the hallmark of the Federal Reserve under Chairman Greenspan is to seriously misconstrue the historical record, in my opinion. It is true that Greenspan has voiced doubts about the desirability of conducting policy according to “the prescriptions of a formal policy rule.”4\n But he was clearly referring to the arithmetic rules of the type I described earlier, which represent only one of many representations of commitment. The Federal Reserve has worked hard over the years to shape the public’s expectations regarding the conduct of monetary policy. Central to those efforts has been the pursuit of what many call credibility — that is, a reputation for pursuing low and stable inflation. To my mind, building monetary policy credibility has been the true hallmark of the Federal Reserve under Chairman Greenspan’s leadership.\n\n\n\nThe Fed’s credibility has been built through a number of channels during the Chairman’s tenure. First, of course, has been the actual behavior of inflation — having brought inflation down to a low and steady rate over the last two and a half decades, people expect us to keep it there. Equally important, the Fed has responded forcefully whenever signs emerge that the public’s faith in our commitment may be slipping. In the famous episode of 1994, for example, interest rates on long-term bonds indicated that inflation expectations were rising.5\n The Fed responded preemptively by raising the federal funds rate target in seven steps from 3 percent to 6 percent, even though inflation itself had not yet begun to rise. A discretionary policymaker would have been less likely to raise rates preemptively.\n\n\n\nCommunication is another important tool in building and maintaining credibility. In the early 1990s, the Fed’s Monetary Policy Reports to Congress and public statements by Chairman Greenspan and other Fed officials repeatedly emphasized the importance of reducing inflation and keeping it low. More broadly, during Greenspan’s tenure the Fed has become far more transparent about its policy actions, first by announcing federal funds rate decisions immediately, beginning in 1994, and then by gradually expanding the substantive content of the statement accompanying those announcements. Recent statements have provided information regarding likely actions at future meetings. Together with public statements by FOMC participants, these moves toward greater transparency serve to enhance the public’s understanding of how the Fed is likely to respond to economic conditions as they unfold over time — in other words, to help the public form expectations consistent with our future behavior.\n\nNow, for a speech with the words “after Greenspan” in the title, I’ve done a lot of talking about “during Greenspan.” So let me turn now to talk about the future. At the outset, I noted the Fed’s success at bringing inflation down during Chairman Greenspan’s tenure, to the point where we can be said to have achieved “price stability” — a situation in which core inflation and inflation expectations are low and stable. I want to spend the rest of my time discussing some of the consequences of price stability for monetary policy. To facilitate that discussion here today, I would like to ask you to suppose that the Fed continues its recent success in maintaining stable inflation expectations on the part of the public. I believe we will be successful, and I have, in a speech earlier this year, expressed my belief that adopting an explicit numerical inflation target would be helpful in this regard.6\n But for the purposes of our discussion here today I want to take as a premise that inflation and expected inflation remain low and steady and ask, how should we conduct interest rate policy in such a world?\n\n\n\nFirst, let me remind you that any interest rate, whether it’s the overnight interbank rate (the so-called federal funds rate) that the FOMC sets, or the yield on a long-term Treasury bond, has three parts. One is simply compensation for expected changes in the purchasing power of money — expected inflation, in other words. A second part is a premium to compensate lenders for inflation risk. The remainder is the “real interest rate,” essentially the inflation-adjusted rate of return stated in terms of real resources over time.\n\nIf the public is convinced that the central bank will not allow inflation to move persistently outside of some low target range, then expected inflation will not move around a lot and the inflation compensation that financial markets build into longer-term interest rates will not fluctuate much either.7\n Moreover, inflation risk premiums will not vary much either. In the past, the Fed has often had to raise the funds rate in response to rising inflation. At times, the Fed has raised rates preemptively when rising long-term interest rates indicated rising inflation expectations — as in 1994 — even though inflation itself was stable. But neither of these triggers would occur in a world where inflation and expected inflation remained low and stable. Does this mean that the Fed would never have to change interest rates if inflation was fully stabilized? The answer is an emphatic, “No.”\n\n\n\nThe reason stems from the fact that with stable expected inflation, nominal interest rates move one-for-one with real interest rates, and from the principle that real interest rates need to fluctuate in a healthy, well-functioning economy. A real interest rate, as I stated earlier, is a rate of return expressed in units of real resources. It represents the real amount of goods and services one must sacrifice in the future (in addition to the repayment of principal) to obtain real goods and services today. A real rate thus represents a relative price — the price of current resources relative to future resources.\n\nIn a market economy, relative prices will generally fluctuate in response to shifts in demand and supply. For example, when the demand for crude oil grows relative to the supply, the price of oil needs to increase relative to the prices of other goods and services in order to reflect the increased relative scarcity of oil. Similarly, the relative price of current and future resources should fluctuate in response to shocks that affect the demand and supply of current resources, relative to future resources.\n\nLet me illustrate with an example lifted from today’s headlines. Two successive hurricanes have caused devastation and heartbreaking losses to the central Gulf Coast region in the last two months. In economic terms, a natural disaster that destroys or damages residential and business capital represents a temporary disruption to productive capacity. Fewer goods and services are available for current consumption and investment. Setting aside the energy price increases, which I will discuss in a moment, history shows that our economy rebounds pretty strongly from this sort of event. After several quarters of rebuilding, our productive capacity should be back to about where it would have been otherwise.\n\nA disaster like this thus makes current resources scarcer relative to future resources. In addition, the heightened demand for reconstruction resources places further strain on current capacity. For both reasons, one would expect real interest rates on this account to be, if anything, higher than otherwise in the short run, in order to reflect the scarcity of current resources relative to future resources and help adjust demand to the disaster-induced reduction in the current capacity.\n\nThe only caveat to this prediction is the possibility of an offsetting reduction in demand. But what would cause such a demand effect? A catastrophic event can certainly affect the public’s mood, as captured, for instance, by consumer confidence surveys. But consumers and producers also can be expected to understand, from the history of such events, that the disturbance to economic activity is likely to be relatively short-lived. With that understanding, and the prospects for rising output and income in the not-too-distant future, there’s little reason to expect a significant reduction in the current demand for resources by households and firms.\n\nTo take another example from today’s headlines, and one to which I’ve already referred, oil price increases can be expected to have implications for real interest rates. Many economists are fond of noting that an oil price increase acts like a tax on the production and consumption of oil-related products. Since energy is an important input to most production, oil price increases can also be thought of as adverse productivity shocks; as energy prices rise, and producers cut back on their use of energy resources, the productivity of other inputs — labor and capital — will decline. If the increase is expected to be temporary, its effects are analogous to a disaster-induced reduction in productive capacity, making current production more costly relative to future production. An increase in real interest rates is needed in this case, again to reflect the relative scarcity of current and future resources.\n\nEnergy prices figured prominently in the economic events of the 1970s, when sharp oil price increases were associated with rising inflation and subsequent recessions. But care is needed to avoid drawing the wrong lessons. Inflation was high and variable in the 1970s, inflation expectations were untethered, and the credibility of monetary policy was quite low. Oil price increases engendered expectations of increased inflation across a broad range of goods. The Fed accommodated the pickup in inflation by failing to increase nominal interest rates by as much as the increase in inflation. The result was falling real interest rates and further monetary stimulus. The Fed ultimately tightened policy in an effort to combat accelerating inflation, thereby inducing economic contraction. The proper lesson from the 1970s is not that energy price shocks induce major recessions; it is that monetary policy that reacts to energy price shocks by accommodating the rise in inflation and then subsequently has to fight inflation can induce major recessions. Thus, sharp energy price increases are not, by themselves, reasons to ease policy. The proper central bank response to energy price shocks is to remain focused on maintaining price stability.\n\nProductivity trends seldom make the headlines, but sustained changes in productivity growth rates have figured prominently in recent macroeconomic history and can have important consequences for interest rate policy. The U.S. economy experienced a productivity slow-down from the mid 1970s through the early 1990s. Growth in output per hour went from about 2.5 percent per year before 1974 down to about 1.5 percent. In the mid-1990s, productivity growth accelerated back to about 2.5 percent. Most observers link the increase in productivity growth in the late 1990s to advances in information and communications technology. When a sustained increase in productivity comes to be widely recognized by households and firms, the effect is to increase the demand for current resources relative to supply. Because gains in labor productivity ultimately show through to real income, households experience a productivity surge as a pickup in real income growth and will tend to extrapolate brighter real income prospects into the future. Higher corporate profits raise equity values as well, further boosting consumers’ real wealth. Households will attempt to spend some of those anticipated income gains in the present. On the business side, the pickup in productivity growth usually implies stronger returns to installing productive capacity, providing a boost to business investment spending. If real interest rates do not change, a step-up in productivity growth would raise current demand by more than current supply. Thus, real interest rates have to rise. Forces like these put upward pressure on interest rates in the late 1990s.\n\nAll three of my examples thus far have required real interest rates to rise. For an example in which real interest rates must fall, one can run the productivity pickup in reverse: a sustained fall in productivity growth should lead to lower real interest rates, everything else equal. Another example of conditions that could indicate declining real interest rates is an independent fall in investment spending. This is different from my other examples because a change in investment spending reduces the demand for current resources rather than the supply. For example, the investment boom of the late 1990s came to an abrupt stop around 2000, especially in the telecommunications industry, where it become clear that growth in the installed capital stock was outstripping growth in demand for the industry’s services. The slow-down in capital spending in some sectors amounted to a reduction in demand for current resources relative to supply — which warranted a reduction in real interest rates. The FOMC facilitated this decline by cutting the federal funds rate sharply in 2001 from 6.5 percent to 2 percent.\n\nBy considering a world of perfectly complete credibility, I hope I have convinced you that there is more to monetary policy than responding to inflation scares or “emerging inflation pressures.” Real shocks that alter the relative balance of current and future resource utilization will require appropriate adjustments to real interest rates over time. In a world of stable inflation expectations, the responsibility for making such adjustments falls to the central bank.\n\nIt’s not too hard to find examples in the past when the Fed’s response to shocks has been different from what I’ve described here as appropriate. But it’s important to remember that no past experience perfectly matches the hypothetical world I’m describing in which inflation expectations have been perfectly stabilized. So if you think the Fed is likely to build on its recent success and maintain low inflation and inflation expectations — and I do — then past history, drawn from times when credibility was lacking, will be a poor guide to future economic performance.\n\nSo, what will interest rate policy look like in an after-Greenspan world in which the Fed continues to build on its recent success at convincing the public of its commitment to price stability? Policy will less often be reacting to fluctuations in actual or expected inflation, and will more often be realigning real interest rates in response to changing economic fundamentals. The Fed will have to constantly monitor the state of the economy, understand the shocks that are affecting the economy’s growth, and form an assessment of the appropriate implications for real interest rates. In other words, not that different from recent Fed policy.\n\nIt’s important to remember that, even as understanding of economics and the economy continues to improve, models will never be perfect. Assessments of economic conditions and the associated forecasts will always be subject to substantial uncertainty. This may be one reason why rule-like policymaking may never take the form of the simple arithmetic formulas that are so handy for research purposes. Rather, both our understanding of the economy and our assessment of appropriate policy actions are likely to continue to evolve and be influenced by emerging data and trends.\n\nI should be clear, however, that moving toward such robust credibility will take continual vigilance. Credibility regarding our general intentions is often less in question than how we will respond to various contingencies. Markets may have a firm idea that the Fed favors low inflation, but may not be sure how we will react in every conceivable circumstance. Somewhat unique contingencies may arise in which market participants harbor some uncertainty about whether the Fed would be willing to tolerate a sustained increase in inflation; financial market responses following Hurricane Katrina are a case in point. This means that the Fed’s credibility, while quite strong now, might never be entirely unassailable. To preserve and build on the credibility we already enjoy, we will need to continue to respond to changing economic conditions in a way that confirms our commitment to low inflation. Key to this will be helping the public understand that we intend to respond to future conditions in a way that keeps inflation low and stable.\n\nI have argued that the conduct of monetary policy under Chairman Alan Greenspan is best characterized as “rule-like.” One possible pitfall of rule-like behavior that relies on the central bank’s desire to preserve its reputation is that reputations can be associated with individual leadership, as opposed to the institution itself. Is the Federal Reserve in danger of losing its hard-won reputation with the upcoming change in leadership? As you might expect, I don’t think so. I anticipate a stable transition with no sharp departure in the actual conduct of policy, and this stability should quickly become apparent to the public.\n\nThis observation in no way diminishes my assessment of the accomplishments of Chairman Greenspan. My confidence in the institutional continuity in the conduct of monetary policy rests in large part on what we have learned from Chairman Greenspan over the last 18 years about the theory and practice of monetary policy. The lessons of the Greenspan era — lessons having to do with expectations and the importance of consistent behavior — are now widely understood in the central banking and academic worlds. Academics may debate whether Greenspan-era policy is an example of discretion or rules, but no one in that debate really argues that the Fed’s recent success proves the virtue of pure, unconstrained discretion. That discretion must be tempered by constraints linked to expectations, reputation and commitments is a lesson that I think has been widely learned, and that’s what gives me great confidence in the continuity of monetary policy.\n\nReferences\n\nBernanke, Ben. \"The Great Moderation.” Speech to Eastern Economic Association, Washington, D.C., February 20, 2004.\n\nBernanke, Ben. \"'Constrained Discretion' and Monetary Policy.\" Speech to the Money Marketeers of New York University, N.Y., February 3, 2003.\n\nBlinder, Alan and Ricardo Reis. “Understanding the Greenspan Standard.” Speech at the Federal Reserve Bank of Kansas City Symposium. Jackson Hole, Wyo. August 26, 2005.\n\nGreenspan, Alan. “Monetary Policy Under Uncertainty.” Speech at the Federal Reserve Bank of Kansas City Symposium. Jackson Hole, Wyo. August 29, 2003.\n\nGoodfriend, Marvin. “The Phases of U.S. Monetary Policy: 1987-2001.” Federal Reserve Bank of Richmond Economic Quarterly 88 (Fall 2002): 1-17.\n\nLacker, Jeffrey M. “Inflation Targeting and the Conduct of Monetary Policy.” Speech at the University of Richmond, Richmond, Va., March 1, 2005."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2005",
        "date": "June 20, 2005",
        "title": "Retail Financial Innovation",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke June 20, 2005 at the Virginia Bankers Association in Hot Springs, Va.",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2005/lacker_speech_20050620",
        "content": "Over the last two decades, we have witnessed what can arguably be called a revolution in retail consumer finance. Perhaps the hallmark of this revolution has been the dramatic expansion of unsecured lending through the proliferation of credit cards. This growth has not been limited to unsecured credit, but also includes mortgage and home equity lending. One of my themes this morning will be that these trends are the result of a wave of innovation, largely related to information technology, that has brought widespread change to financial services and other industries. At the same time, as retail credit extension has grown we have also seen a significant expansion of regulations pertaining to the extension of such credit, and a growing concern in some quarters that American households have lost control of their finances to a dizzying array of new products and options. My second theme concerns the relationship between these two broad developments. I will argue that there is a natural tendency for credit expansions like the one we've seen to lead to calls for new regulation. My hope is that understanding this relationship will better equip us to assess current conditions in retail credit markets, including legislative and regulatory proposals, and to think clearly about the industry's future direction.\n\nI should say at the outset that I will not be speaking from the perspective you might expect from a banking agency official. A regulatory official's usual approach on an occasion like this, especially at this point in the credit cycle, is to warn industry executives about the \"unique risks\" embedded in emerging banking practice, and to encourage them to make better efforts in various risk management nooks and compliance crannies. Instead, I will speak from the perspective of a hypothetical outsider observing the joint evolution of both the banking business and regulatory practice. In this, I am being true to my roots as an academic economist. But I also believe that such a perspective is essential for understanding the long-run evolution of the banking industry, which has been both shaped by the regulatory environment and has in turn shaped that environment. In other words, it has been a two-way street; regulations obviously have influenced banking practice, but developments in retail banking have themselves been a major influence on regulatory trends. This perspective implies that, perhaps to an even greater extent than usual, the views expressed are my own and not necessarily those of my bank regulatory colleagues.\n\nThe Technology-Driven Expansion in Retail Credit\n\nThe expansion of consumer credit in the United States over the last decade and a half has been truly astonishing. The expansion occurred across a number of product lines. Probably most prominent has been the expansion of credit card lending. Home mortgage lending, including home equity credit, has also seen robust growth. An especially prominent feature of the secular expansion of consumer credit has been the growth in lending to lower-income consumers, many of whom had in the past been unable to obtain credit on as favorable terms from the financial sector. Indeed, many had been unable to obtain credit except from fringe lenders such as pawnbrokers or through informal arrangements with friends and family.\n\nFor the most part, this expansion was driven by advances in information and communications technologies, which reduced the cost of gathering, processing and retaining consumer account information. At a basic level, this dramatically increased the productivity of the back-office functions associated with all phases of the banking business. Payments processing and the associated book-keeping tasks became much cheaper. The result was a general decline in the intermediation costs that lenders ultimately must recover on top of their funding costs. Competition forced lenders to pass on these cost savings to borrowers in the form of lower lending rates. More consumers could afford to borrow, and the market expanded.1\n\n\n\nPerhaps the most profound effect of new technologies, and the effect most relevant to retail lending, was the automation of underwriting. Lower communication and data storage costs led to a proliferation of large electronic databases of consumer information, and allowed the collation and integration of such information from multiple sources. Specialized intermediaries such as credit bureaus took advantage of economies of scale in the assembly of such information. Lenders were able to access a far more comprehensive array of information about any given consumer. And reliable information is essential to effective underwriting. The key was the creation of databases on a large enough number of consumers to allow reliable statistical inference regarding their behavior.\n\nLower computing costs also allowed the development of new software to exploit these extensive databases by automating a broad array of underwriting decisions. Credit scoring is one such technique. [Mester, 1997] Quantitative analysis of past repayment behavior by a large number of consumers can be used to create a simple statistic – a credit score – that summarizes any given consumer's creditworthiness. Calculating a credit score is a parsimonious way to assess the likelihood of repayment by a consumer with a given set of characteristics and credit history.\n\nThe new technologies did not change the fundamental nature of underwriting, however; they just made it more effective. Underwriting is about making distinctions between people in order to decide whether to lend, and if so, how much to charge. When lenders were limited to eyeballing paper loan applications and old-fashioned credit reports, they could only sort potential borrowers into a few broad categories. New technologies allowed lenders to bring more and more consumer-specific information to bear on the lending and pricing decision, and thus make finer distinctions between consumers. The result was that the lending decision and loan terms could be more closely tailored to individual borrowers. For example, credit cards used to be available at fairly high interest rates for those who could obtain them. Automated underwriting allowed credit card lenders to offer lower interest rates to more creditworthy customers, and to pluck out the creditworthy from among the group of customers that formerly were unable to qualify for credit.\nAdvances in information technology also contributed to the development and growth of loan securitization. Automated underwriting simplified the analysis necessary to create securities of unambiguous quality from a portfolio of loans to numerous heterogeneous borrowers. By allowing investors other than the originator to hold loans, securitization dispersed risks more widely and reduced the spreads associated with consumer lending. And again, reduced lending rates expanded the market.\n\nSomewhat surprisingly, perhaps, the development of quantitative methods for pricing options also contributed to the expansion of retail credit markets. While at first blush these innovations seem only tangentially related to retail credit, in fact they were fairly important to the growth of securitization. Accurately valuing puts and calls required combining the massive computing power that came online in the late 1980s with the models that finance theorists pioneered in the mid-1970s. These techniques opened the door for banks and other financial intermediaries to properly price the put feature inherent in a long-term mortgage, and the option value of loan commitments such as home equity lines of credit and credit cards. Such techniques were essential to the liquidity of markets for securitized loans, which again helped reduce the cost of borrowing and thus expand retail credit markets.\n\nThe technology-driven expansion of retail credit is evident in the growth of household debt, but it also is evident in the proliferation of entirely new credit products. For example, high loan-to-value ratio mortgages, including zero-down-payment mortgages, would not have been economically feasible without the new, more effective underwriting techniques. Likewise, home equity lines of credit became more widely available in part because of the lower cost of accurately assessing creditworthiness.\n\nThe Benefits of Expanded Retail Credit\n\nThe expansion of retail credit has brought distinct benefits to American consumers. At a fundamental level, the purpose of credit is to allow people to choose a spending pattern that is smoother over time than their income stream. Broadly speaking, households face two types of income variation. One type stems from the fact that people's income usually rises at first then falls over their lifetimes. As a result, individuals typically borrow more when they are young and their incomes are low, and repay borrowings as they advance in their careers. Doing so allows them to spread the benefits of their peak earning years over a greater portion of their lives. This life cycle pattern of household financial behavior also allows households to purchase homes, cars, and other \"big-ticket\" durables without first accumulating the savings that would be necessary without access to credit. Accordingly, the increased availability of low-down-payment mortgages has improved the ability of young households to make the initial transition from renting to homeownership. This has been of particular help for low-income households that might not otherwise have had the wealth to get into a home.\n\nThe other type of variation in income that a household faces is that associated with employment disruptions or other shocks to earning ability. Funding such shocks entirely out of current resources can require large changes in current living standards or else large savings buffers. And adjusting one's consumption habits in response to every change in income can be costly in its own right. By borrowing against future resources, adjustments can be spread out over time, ultimately resulting in less sacrifice in well-being. This enhances consumers' ability to smooth over temporary financial shocks – and this includes shocks like unexpected, uninsured health costs as well as changes in employment or income.\n\nFinally, research on entrepreneurship indicates that many small startup businesses make significant use of consumer credit products such as credit cards and home equity lines of credit. The expansion of retail credit thus brings with it the added benefit of encouraging entrepreneurial innovation.\n\nIn short, credit is an important tool in household risk management and in the process of building wealth for households all across the income distribution. And the evidence suggests that the expansion of credit over the last two decades has indeed yielded positive net benefits for American consumers.\n\nThe Consumer Credit Backlash\n\nNew technologies, then, have led to dramatic increases in the productivity of the retail credit industry that have greatly benefited many consumers. Borrowing costs have been lowered for many households. New products have given consumers a much wider array of choices in retail credit. And access to credit has been opened up for a vast array of consumers who formerly could not have qualified. Consumers can borrow in new ways, and a new set of consumers can borrow.\n\nThe usual presumption about an industry experiencing technological progress is that more choice and more access are good things and that competition spreads those benefits broadly to the industry's customers. But despite the evident benefits, the expansion of credit and the growth of new credit products have not been universally welcomed. The popular media regularly recount horror stories of unsuspecting consumers who find themselves in dire straits following an encounter with the retail credit industry. Consumer advocates have publicized accounts of abusive practices by lenders and have charged regulators with lax enforcement of existing consumer protections. It appears that a significant constituency now favors tighter legislative and regulatory constraints on retail credit providers of all types, from banks to payday lenders.\n\nThis consumer credit backlash has contributed to the adoption or proposal of several legislative or regulatory changes in recent years. In North Carolina, path-breaking legislation was aimed at curbing predatory lending by limiting certain practices in the subprime market, and several other states have adopted or are considering such laws. At the national level, the data that lenders are required to submit under the Home Mortgage Disclosure Act now must include information on interest rates if they exceed a certain spread over funding costs. Some advocates have recently proposed expanding credit card disclosure requirements to include, for example, the time it would take to repay the bill while just making the minimum payments - and there are discussions of imposing strict rules for minimum payments on credit card debt. Payday lending – short-term loans typically secured by post-dated checks – has come under fire as an encouragement to irresponsible borrowing. And in the last few months some critics have expressed anxiety about the popularity of interest-only mortgage loans. Federal banking agencies have recently issued guidance on underwriting mortgage and home equity products.\n\nThis broad regulatory movement is aimed at rectifying the perceived failings of consumer credit markets and generally reining in retail lenders. It has arisen in direct response to the strong expansion in retail credit. This cycle of credit expansion and regulatory reaction has occurred before. The 1920s saw an explosion of installment lending for the purchase of emerging consumer durables like automobiles and electric refrigerators. Many states responded with regulations in the 1930s, mainly dealing with disclosure. [Olney, 1991] Earlier still, in response to growing concerns about abusive and deceptive practices among non-bank lenders extending small short-term credits (resembling payday loans), the Russell Sage Foundation began in 1916 to promote \"Uniform Small Loan Laws.\" [Carruthers, Guinnane and Lee, 2005] Ultimately passed by most states, these laws created a class of lenders who, in exchange for being explicitly permitted to lend at rates in excess of usury ceilings, agreed to certain practices, most notable of which was a standardized, single-number disclosure of interest costs. This pattern of credit expansion and political response in the United States may derive in part from a long-standing strain of populist aversion to financial institutions.\n\nThis type of regulatory countercurrent should be expected as part of a significant credit expansion like the one we have seen, quite apart from whether or not any specific proposal is advisable on public policy grounds. Credit extension is at times associated with outcomes people view negatively: delinquency, default, or bankruptcy. Fraudulent transactions are another class of adverse outcomes that occur in credit markets, as in other consumer markets. A credit expansion naturally brings with it an increase in the incidence of such \"bad\" outcomes. As credit becomes more widely available there is an inevitable increase in the number of delinquencies, defaults and bankruptcies. Moreover, the new borrowers being drawn in to credit markets are likely to be, on average, less financially savvy and more vulnerable to the unscrupulous as they struggle to learn about unfamiliar credit products. The popular focus on cases of fraud and financial distress often drives the politics of consumer finance. These compelling stories are powerful motivators for attempts to regulate or restrain new practices.\n\nProposed restraints often are intended to protect consumers from their own poor judgment, which makes them vulnerable to abusive lending practices. In fact, in every historical episode of expanding credit, the desire to regulate has been accompanied by a popular belief that growing debt is a sign of decaying values and thrift. One historian has referred to this persistent belief as the \"myth of lost economic virtue.\" [Calder, 1999] Belief in this myth can easily hide from view the positive economic role of credit in household financial management and the benefits that come from expanded access to credit.\n\nSome Implications for Credit Policy\n\nIndividual policy proposals should be evaluated on their own merits in order to disentangle the winners and losers. While in general we would expect that regulations ought to adapt to changing credit market practices, there is a very real danger here of throwing the baby out with the bathwater. As I argued above, the expansion of retail credit has been tremendously beneficial for U.S. consumers. We need to keep in mind that most measures designed to protect consumers from bad credit market outcomes also raise lending costs and can prevent them from obtaining credit in the first place. Such measures confront an inherent trade-off between preventing adverse effects for some and limiting the availability of credit to others for whom it would be beneficial. Sound judgment about such trade-offs requires quantitative understanding of the relative fractions in each respective group – that is, the likelihood of good and bad outcomes. This strikes me as the only way to ground policy in reality rather than fall victim to what I have referred to elsewhere as policymaking-by-anecdote. [Lacker, 2005]\n\nHaving said that, policymakers should be alert for opportunities to reduce adverse outcomes without harming credit availability – in other words, to improve the terms of the trade-off between credit availability and borrower protection. For example, efforts to improve the detection and prevention of fraudulent consumer financial transactions could enhance credit market efficiency. Improved disclosure can strengthen consumers' understanding of financial products and increase the odds of consumers getting the product that is best for them. Designing disclosure requirements is a delicate task, however. Too much disclosure can overload consumers and impede rather than enhance understanding. And an overly prescriptive approach can hinder banks' ability to craft informative and reader-friendly communications, something many institutions have been willing and able to do with the help of communications specialists.\n\nBut while better law enforcement and improved disclosure may enhance credit market performance, both economic reasoning and abundant practical experience have demonstrated that constraints on allowable interest rates are counterproductive, and generally reduce consumer well-being. Similarly, prohibitions of particular lending practices reduce the financial management options available to some households, and without reliable empirical evidence on the relative frequency of legitimate and abusive uses it is very difficult to determine the net costs or benefits of a prohibition. So while some policies that carefully target truly abusive practices are warranted, the broader risk is of a regulatory overreaction that stifles much of the benefit of the technology-driven expansion in consumer credit.\n\nIn summary then, the technology-driven expansion of retail credit has been very beneficial to U.S. consumers. And yet such dramatic improvements in credit availability are accompanied by predictable increases in the incidence of delinquencies and abuse, as new products proliferate and new borrowers are drawn into the market. The spread of adverse outcomes inevitably triggers calls for new regulatory constraints on lenders. But while truly abusive practices certainly deserve regulatory attention, policy measures that impede the functioning of credit markets need to be approached cautiously to avoid an overreaction that stymies much of the benefit of the innovations in retail credit practices.\n\nBut aside from regulatory intervention, what can we do to improve retail credit markets? It is clear that all those involved in the retail credit industry – policymakers, bankers, and industry critics alike – have an interest in increasing consumers' understanding of financial products and the management of their own financial affairs. Broad efforts to improve financial literacy may not add to a lender's near-term bottom line, but financial institutions, both individually and as a whole, depend critically on their customers' trust. And trust is built on understanding. As bankers, I am sure you recognize that you have an interest in your customers' ability to understand your products and to recognize when a scam artist's offer is too good to be true. You also have a wider interest in consumers' ability to choose and use financial products in a manner that is consistent with their long-run self-interest.\n\nAs the value of financial literacy has become apparent, and as its importance has grown along with the growing complexity of consumer finance options, the Federal Reserve System and the Richmond Fed have made a commitment to improve our understanding of how people learn to be good financial decision makers and to lend our support to efforts to raise the general level of financial awareness. Such efforts can, I believe, yield double benefits. They certainly contribute to improving the performance of credit markets. But even beyond that, an electorate that has a broad appreciation of the efficiency of credit markets will also have an easier time sorting out when any particular policy proposal is truly in their interests. In the long run, this is the path to better banking policy. After all, we in the United States have arguably one of the most efficient retail credit markets in the world. Let's not kill the goose that lays golden eggs.\n\nReferences:\n\nCalder, Lendol. Financing the American Dream: A Cultural History of Consumer Credit. Princeton, N.J.: Princeton University Press, 1999.\n\nCarruthers, Bruce G., Timothy W. Guinnane and Yoonseok Lee. \"Bringing 'Honest Capital' to Poor Borrowers: The Passage of the Uniform Small Loan Law, 1907-1930.\" Center Discussion Paper No. 971, Economic Growth Center, Yale University, May 2009.\n\nLacker, Jeffrey M. \"Opening Remarks at the Federal Reserve System's Fourth Community Affairs Research Conference.\" Washington, D.C., April 7, 2005.\n\nMester, Loretta J. \"What's the Point of Credit Scoring?\" Federal Reserve Bank of Philadelphia Business Review (September/October 1997): 3-16.\n\nOlney, Martha L. Buy Now Pay Later: Advertising, Credit and Consumer Durables in the 1920s. Chapel Hill, N.C.: University of North Carolina Press, 1991."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2005",
        "date": "June 14, 2005",
        "title": "Retail Financial Innovation",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke June 14, 2005 at the 109th Annual Convention of the North Carolina Bankers Association in Kiawah, S.C. ",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2005/lacker_speech_20050614",
        "content": "Over the last two decades, we have witnessed what can arguably be called a revolution in retail consumer finance. Perhaps the hallmark of this revolution has been the dramatic expansion of unsecured lending through the proliferation of credit cards. This growth has not been limited to unsecured credit, but also includes mortgage and home equity lending. One of my themes this morning will be that these trends are the result of a wave of innovation, largely related to information technology, that has brought widespread change to financial services and other industries. At the same time, as retail credit extension has grown we have also seen a significant expansion of regulations pertaining to the extension of such credit, and a growing concern in some quarters that American households have lost control of their finances to a dizzying array of new products and options. My second theme concerns the relationship between these two broad developments. I will argue that there is a natural tendency for credit expansions like the one we've seen to lead to calls for new regulation. My hope is that understanding this relationship will better equip us to assess current conditions in retail credit markets, including legislative and regulatory proposals, and to think clearly about the industry's future direction.\n\nI should say at the outset that I will not be speaking from the perspective you might expect from a banking agency official. A regulatory official's usual approach on an occasion like this, especially at this point in the credit cycle, is to warn industry executives about the \"unique risks\" embedded in emerging banking practice, and to encourage them to make better efforts in various risk management nooks and compliance crannies. Instead, I will speak from the perspective of a hypothetical outsider observing the joint evolution of both the banking business and regulatory practice. In this, I am being true to my roots as an academic economist. But I also believe that such a perspective is essential for understanding the long-run evolution of the banking industry, which has been both shaped by the regulatory environment and has in turn shaped that environment. In other words, it has been a two-way street; regulations obviously have influenced banking practice, but developments in retail banking have themselves been a major influence on regulatory trends. This perspective implies that, perhaps to an even greater extent than usual, the views expressed are my own and not necessarily those of my bank regulatory colleagues.\n\nThe Technology-Driven Expansion in Retail Credit\n\nThe expansion of consumer credit in the United States over the last decade and a half has been truly astonishing. The expansion occurred across a number of product lines. Probably most prominent has been the expansion of credit card lending. Home mortgage lending, including home equity credit, has also seen robust growth. An especially prominent feature of the secular expansion of consumer credit has been the growth in lending to lower-income consumers, many of whom had in the past been unable to obtain credit on as favorable terms from the financial sector. Indeed, many had been unable to obtain credit except from fringe lenders such as pawnbrokers or through informal arrangements with friends and family.\n\nFor the most part, this expansion was driven by advances in information and communications technologies, which reduced the cost of gathering, processing and retaining consumer account information. At a basic level, this dramatically increased the productivity of the back-office functions associated with all phases of the banking business. Payments processing and the associated book-keeping tasks became much cheaper. The result was a general decline in the intermediation costs that lenders ultimately must recover on top of their funding costs. Competition forced lenders to pass on these cost savings to borrowers in the form of lower lending rates. More consumers could afford to borrow, and the market expanded.1\n\n\n\nPerhaps the most profound effect of new technologies, and the effect most relevant to retail lending, was the automation of underwriting. Lower communication and data storage costs led to a proliferation of large electronic databases of consumer information, and allowed the collation and integration of such information from multiple sources. Specialized intermediaries such as credit bureaus took advantage of economies of scale in the assembly of such information. Lenders were able to access a far more comprehensive array of information about any given consumer. And reliable information is essential to effective underwriting. The key was the creation of databases on a large enough number of consumers to allow reliable statistical inference regarding their behavior.\n\nLower computing costs also allowed the development of new software to exploit these extensive databases by automating a broad array of underwriting decisions. Credit scoring is one such technique. [Mester, 1997] Quantitative analysis of past repayment behavior by a large number of consumers can be used to create a simple statistic – a credit score – that summarizes any given consumer's creditworthiness. Calculating a credit score is a parsimonious way to assess the likelihood of repayment by a consumer with a given set of characteristics and credit history.\n\nThe new technologies did not change the fundamental nature of underwriting, however; they just made it more effective. Underwriting is about making distinctions between people in order to decide whether to lend, and if so, how much to charge. When lenders were limited to eyeballing paper loan applications and old-fashioned credit reports, they could only sort potential borrowers into a few broad categories. New technologies allowed lenders to bring more and more consumer-specific information to bear on the lending and pricing decision, and thus make finer distinctions between consumers. The result was that the lending decision and loan terms could be more closely tailored to individual borrowers. For example, credit cards used to be available at fairly high interest rates for those who could obtain them. Automated underwriting allowed credit card lenders to offer lower interest rates to more creditworthy customers, and to pluck out the creditworthy from among the group of customers that formerly were unable to qualify for credit.\n\nAdvances in information technology also contributed to the development and growth of loan securitization. Automated underwriting simplified the analysis necessary to create securities of unambiguous quality from a portfolio of loans to numerous heterogeneous borrowers. By allowing investors other than the originator to hold loans, securitization dispersed risks more widely and reduced the spreads associated with consumer lending. And again, reduced lending rates expanded the market.\n\nSomewhat surprisingly, perhaps, the development of quantitative methods for pricing options also contributed to the expansion of retail credit markets. While at first blush these innovations seem only tangentially related to retail credit, in fact they were fairly important to the growth of securitization. Accurately valuing puts and calls required combining the massive computing power that came online in the late 1980s with the models that finance theorists pioneered in the mid-1970s. These techniques opened the door for banks and other financial intermediaries to properly price the put feature inherent in a long-term mortgage, and the option value of loan commitments such as home equity lines of credit and credit cards. Such techniques were essential to the liquidity of markets for securitized loans, which again helped reduce the cost of borrowing and thus expand retail credit markets.\n\nThe technology-driven expansion of retail credit is evident in the growth of household debt, but it also is evident in the proliferation of entirely new credit products. For example, high loan-to-value ratio mortgages, including zero-down-payment mortgages, would not have been economically feasible without the new, more effective underwriting techniques. Likewise, home equity lines of credit became more widely available in part because of the lower cost of accurately assessing creditworthiness.\n\nThe Benefits of Expanded Retail Credit\n\nThe expansion of retail credit has brought distinct benefits to American consumers. At a fundamental level, the purpose of credit is to allow people to choose a spending pattern that is smoother over time than their income stream. Broadly speaking, households face two types of income variation. One type stems from the fact that people's income usually rises at first then falls over their lifetimes. As a result, individuals typically borrow more when they are young and their incomes are low, and repay borrowings as they advance in their careers. Doing so allows them to spread the benefits of their peak earning years over a greater portion of their lives. This life cycle pattern of household financial behavior also allows households to purchase homes, cars, and other \"big-ticket\" durables without first accumulating the savings that would be necessary without access to credit. Accordingly, the increased availability of low-down-payment mortgages has improved the ability of young households to make the initial transition from renting to homeownership. This has been of particular help for low-income households that might not otherwise have had the wealth to get into a home.\n\nThe other type of variation in income that a household faces is that associated with employment disruptions or other shocks to earning ability. Funding such shocks entirely out of current resources can require large changes in current living standards or else large savings buffers. And adjusting one's consumption habits in response to every change in income can be costly in its own right. By borrowing against future resources, adjustments can be spread out over time, ultimately resulting in less sacrifice in well-being. This enhances consumers' ability to smooth over temporary financial shocks – and this includes shocks like unexpected, uninsured health costs as well as changes in employment or income.\n\nFinally, research on entrepreneurship indicates that many small startup businesses make significant use of consumer credit products such as credit cards and home equity lines of credit. The expansion of retail credit thus brings with it the added benefit of encouraging entrepreneurial innovation.\n\nIn short, credit is an important tool in household risk management and in the process of building wealth for households all across the income distribution. And the evidence suggests that the expansion of credit over the last two decades has indeed yielded positive net benefits for American consumers.\n\nThe Consumer Credit Backlash\n\nNew technologies, then, have led to dramatic increases in the productivity of the retail credit industry that have greatly benefited many consumers. Borrowing costs have been lowered for many households. New products have given consumers a much wider array of choices in retail credit. And access to credit has been opened up for a vast array of consumers who formerly could not have qualified. Consumers can borrow in new ways, and a new set of consumers can borrow.\n\nThe usual presumption about an industry experiencing technological progress is that more choice and more access are good things and that competition spreads those benefits broadly to the industry's customers. But despite the evident benefits, the expansion of credit and the growth of new credit products have not been universally welcomed. The popular media regularly recount horror stories of unsuspecting consumers who find themselves in dire straits following an encounter with the retail credit industry. Consumer advocates have publicized accounts of abusive practices by lenders and have charged regulators with lax enforcement of existing consumer protections. It appears that a significant constituency now favors tighter legislative and regulatory constraints on retail credit providers of all types, from banks to payday lenders.\n\nThis consumer credit backlash has contributed to the adoption or proposal of several legislative or regulatory changes in recent years. In North Carolina, path-breaking legislation was aimed at curbing predatory lending by limiting certain practices in the subprime market, and several other states have adopted or are considering such laws. At the national level, the data that lenders are required to submit under the Home Mortgage Disclosure Act now must include information on interest rates if they exceed a certain spread over funding costs. Some advocates have recently proposed expanding credit card disclosure requirements to include, for example, the time it would take to repay the bill while just making the minimum payments - and there are discussions of imposing strict rules for minimum payments on credit card debt. Payday lending – short-term loans typically secured by post-dated checks – has come under fire as an encouragement to irresponsible borrowing. And in the last few months some critics have expressed anxiety about the popularity of interest-only mortgage loans. Federal banking agencies have recently issued guidance on underwriting mortgage and home equity products.\n\nThis broad regulatory movement is aimed at rectifying the perceived failings of consumer credit markets and generally reining in retail lenders. It has arisen in direct response to the strong expansion in retail credit. This cycle of credit expansion and regulatory reaction has occurred before. The 1920s saw an explosion of installment lending for the purchase of emerging consumer durables like automobiles and electric refrigerators. Many states responded with regulations in the 1930s, mainly dealing with disclosure. [Olney, 1991] Earlier still, in response to growing concerns about abusive and deceptive practices among non-bank lenders extending small short-term credits (resembling payday loans), the Russell Sage Foundation began in 1916 to promote \"Uniform Small Loan Laws.\" [Carruthers, Guinnane and Lee, 2005] Ultimately passed by most states, these laws created a class of lenders who, in exchange for being explicitly permitted to lend at rates in excess of usury ceilings, agreed to certain practices, most notable of which was a standardized, single-number disclosure of interest costs. This pattern of credit expansion and political response in the United States may derive in part from a long-standing strain of populist aversion to financial institutions.\n\nThis type of regulatory countercurrent should be expected as part of a significant credit expansion like the one we have seen, quite apart from whether or not any specific proposal is advisable on public policy grounds. Credit extension is at times associated with outcomes people view negatively: delinquency, default, or bankruptcy. Fraudulent transactions are another class of adverse outcomes that occur in credit markets, as in other consumer markets. A credit expansion naturally brings with it an increase in the incidence of such \"bad\" outcomes. As credit becomes more widely available there is an inevitable increase in the number of delinquencies, defaults and bankruptcies. Moreover, the new borrowers being drawn in to credit markets are likely to be, on average, less financially savvy and more vulnerable to the unscrupulous as they struggle to learn about unfamiliar credit products. The popular focus on cases of fraud and financial distress often drives the politics of consumer finance. These compelling stories are powerful motivators for attempts to regulate or restrain new practices.\n\nProposed restraints often are intended to protect consumers from their own poor judgment, which makes them vulnerable to abusive lending practices. In fact, in every historical episode of expanding credit, the desire to regulate has been accompanied by a popular belief that growing debt is a sign of decaying values and thrift. One historian has referred to this persistent belief as the \"myth of lost economic virtue.\" [Calder, 1999] Belief in this myth can easily hide from view the positive economic role of credit in household financial management and the benefits that come from expanded access to credit.\n\nSome Implications for Credit Policy\n\nIndividual policy proposals should be evaluated on their own merits in order to disentangle the winners and losers. While in general we would expect that regulations ought to adapt to changing credit market practices, there is a very real danger here of throwing the baby out with the bathwater. As I argued above, the expansion of retail credit has been tremendously beneficial for U.S. consumers. We need to keep in mind that most measures designed to protect consumers from bad credit market outcomes also raise lending costs and can prevent them from obtaining credit in the first place. Such measures confront an inherent trade-off between preventing adverse effects for some and limiting the availability of credit to others for whom it would be beneficial. Sound judgment about such trade-offs requires quantitative understanding of the relative fractions in each respective group – that is, the likelihood of good and bad outcomes. This strikes me as the only way to ground policy in reality rather than fall victim to what I have referred to elsewhere as policymaking-by-anecdote. [Lacker, 2005]\n\nHaving said that, policymakers should be alert for opportunities to reduce adverse outcomes without harming credit availability – in other words, to improve the terms of the trade-off between credit availability and borrower protection. For example, efforts to improve the detection and prevention of fraudulent consumer financial transactions could enhance credit market efficiency. Improved disclosure can strengthen consumers' understanding of financial products and increase the odds of consumers getting the product that is best for them. Designing disclosure requirements is a delicate task, however. Too much disclosure can overload consumers and impede rather than enhance understanding. And an overly prescriptive approach can hinder banks' ability to craft informative and reader-friendly communications, something many institutions have been willing and able to do with the help of communications specialists.\n\nBut while better law enforcement and improved disclosure may enhance credit market performance, both economic reasoning and abundant practical experience have demonstrated that constraints on allowable interest rates are counterproductive, and generally reduce consumer well-being. Similarly, prohibitions of particular lending practices reduce the financial management options available to some households, and without reliable empirical evidence on the relative frequency of legitimate and abusive uses it is very difficult to determine the net costs or benefits of a prohibition. So while some policies that carefully target truly abusive practices are warranted, the broader risk is of a regulatory overreaction that stifles much of the benefit of the technology-driven expansion in consumer credit.\n\nIn summary then, the technology-driven expansion of retail credit has been very beneficial to U.S. consumers. And yet such dramatic improvements in credit availability are accompanied by predictable increases in the incidence of delinquencies and abuse, as new products proliferate and new borrowers are drawn into the market. The spread of adverse outcomes inevitably triggers calls for new regulatory constraints on lenders. But while truly abusive practices certainly deserve regulatory attention, policy measures that impede the functioning of credit markets need to be approached cautiously to avoid an overreaction that stymies much of the benefit of the innovations in retail credit practices.\n\nBut aside from regulatory intervention, what can we do to improve retail credit markets? It is clear that all those involved in the retail credit industry – policymakers, bankers, and industry critics alike – have an interest in increasing consumers' understanding of financial products and the management of their own financial affairs. Broad efforts to improve financial literacy may not add to a lender's near-term bottom line, but financial institutions, both individually and as a whole, depend critically on their customers' trust. And trust is built on understanding. As bankers, I am sure you recognize that you have an interest in your customers' ability to understand your products and to recognize when a scam artist's offer is too good to be true. You also have a wider interest in consumers' ability to choose and use financial products in a manner that is consistent with their long-run self-interest.\n\nAs the value of financial literacy has become apparent, and as its importance has grown along with the growing complexity of consumer finance options, the Federal Reserve System and the Richmond Fed have made a commitment to improve our understanding of how people learn to be good financial decision makers and to lend our support to efforts to raise the general level of financial awareness. Such efforts can, I believe, yield double benefits. They certainly contribute to improving the performance of credit markets. But even beyond that, an electorate that has a broad appreciation of the efficiency of credit markets will also have an easier time sorting out when any particular policy proposal is truly in their interests. In the long run, this is the path to better banking policy. After all, we in the United States have arguably one of the most efficient retail credit markets in the world. Let's not kill the goose that lays golden eggs.\n\nReferences:\n\nCalder, Lendol. Financing the American Dream: A Cultural History of Consumer Credit. Princeton, N.J.: Princeton University Press, 1999.\n\nCarruthers, Bruce G., Timothy W. Guinnane and Yoonseok Lee. \"Bringing 'Honest Capital' to Poor Borrowers: The Passage of the Uniform Small Loan Law, 1907-1930.\" Center Discussion Paper No. 971, Economic Growth Center, Yale University, May 2009.\n\nLacker, Jeffrey M. \"Opening Remarks at the Federal Reserve System's Fourth Community Affairs Research Conference.\" Washington, D.C., April 7, 2005.\n\nMester, Loretta J. \"What's the Point of Credit Scoring?\" Federal Reserve Bank of Philadelphia Business Review (September/October 1997): 3-16.\n\nOlney, Martha L. Buy Now Pay Later: Advertising, Credit and Consumer Durables in the 1920s. Chapel Hill, N.C.: University of North Carolina Press, 1991."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2005",
        "date": "May 20, 2005",
        "title": "Payment Economics and the Role of the Central Banks",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke May 20, 2005 at the Bank of England Payments Conference in London, England.",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2005/lacker_speech_20050520",
        "content": "I would like to start by commending the organizers of this conference for their goal of \"making payments mainstream.\" If I could do a bit of wordsmithing, I would add the word \"economics\" after the word \"payment,\" because a distinct and coherent field of payment economics appears to be emerging, and it deserves some attention, especially among central bank policymakers (Lacker and Weinberg, 2003). In my remarks today, I will say a few words about payment economics, and then discuss the role of the Central Bank in the payment system and implications of that role for several current issues. As always, the views expressed do not necessarily represent the views of my colleagues in the Federal Reserve System.\n\nAt the core of payment economics are systems of exchange financed by private and/or public liabilities and the institutions that facilitate the clearing and settlement of these instruments. In other words, payment economics can be defined as the study of the mechanics of exchange. It is based on the core insight of monetary economics that the instruments that people use to pay for goods and services serve to communicate reliably (that is, in an incentive-compatible way) about the buyer's past actions (Townsend, 1989, and Kocherlakota, 1998). Payment economics extends banking theory to encompass the role of banks as private issuers of payment instruments, and reflects the observation that virtually all institutions usually thought of as banks are significantly involved in payments. Indeed, the defining feature of banks appears to be their issuance of payment instrument liabilities, as opposed to their role as balance sheet intermediaries between savers and borrowers. Banks, from this perspective, are specialized institutions for facilitating the transmission and recording of relevant payment information, and the industrial organization of the banking system therefore affects the characteristics of the monetary system. Payment economics thus lies at the intersection of monetary and banking economics with industrial organization.\n\nThe fact that payment instruments and specialized institutions are at the core of the economics of payment arrangements has important methodological implications. It means that the choice of payment instrument and the structure of the institutional arrangements that support them should be viewed as endogenous. This defines an approach known as mechanism design, the cornerstone of modern monetary theory. Under this approach, payment instruments are seen as messages that embody contingent contracts, and one can model the information and risk allocation characteristics of alternative payment arrangements in a way that takes into account the limitations imposed by real world payment technologies – for example, the costliness and falsifiability of communication, verification, and authentication. For example, the mid-1980s presumption that paper checks were socially inefficient because of their higher processing costs ignored other, apparently consumer-relevant, characteristics of checks.\n\nThe central role of communication in payments instruments and institutions has implications for the organization of payments activities. Communication technologies invariably are characterized by such features as economies of scale, common costs, and joint production. These features often take the form of network effects, in which much of the benefits and costs of network activities are shared among multiple participants. Private organizations that deal effectively with such characteristics can be described as clubs, in which terms of membership are just as important as the pricing and terms of service provision in inducing efficient participation. There is a tradition in Industrial Organization of questioning the extent to which competition ensures efficient performance in markets with these characteristics. This hinges on the extent to which markets are contestable, as has been emphasized by Ed Green and Dick Todd in their essay for the Minneapolis Fed's 2000 Annual Report (Green and Todd, 2001).\n\nThe organizers posed for this panel a challenging set of questions that at their core concern the role of central banks in payments. I would like to sketch out a tentative view that seems consistent with the emerging lessons of payment economics. It is not the only possible view one could take, but it strikes me as compelling, and until a better one comes along, I view it as a logical benchmark model.\nThis view is built on two core ideas. First, central banks have more or less nationalized the clearinghouses at the \"apex\" of the payment system. One can debate whether this was efficiency enhancing, as Goodhart (1988) argues, or whether it arose instead to re-allocate the costs and benefits of clearing and settlement activities. Clearinghouse activities appear to have aspects of club goods, as I noted earlier, and for club goods there is often a range of allocations consistent with efficiency – that is, with Pareto optimality. Central bank intervention sometimes alters the distribution of net benefits among payment system participants. For example, the Fed's entry into check clearing seems to have been less about efficiency improvements than it was about shifting the costs of clearing checks drawn on country banks. In any event, legal restrictions nowadays more or less compel many banks to settle at least some transactions through the transfer of central bank account balances.\n\nThe second core idea is that many, if not most, of the private sector institutions that are the major players in the payment systems benefit from a substantial public sector safety net. In many cases explicit deposit insurance provides the most visible government support. But in addition, significant support is provided in conjunction with central bank payment operations. Central banks generally supply credit, both intraday and overnight, to key payment system participants. (The Swiss, until recently, were notable exceptions.) Moreover, there is a widespread perception among private payment system participants that central bank credit will be made available, perhaps even overnight, to facilitate the resolution of operational problems or other settlement disruptions. As Marvin Goodfriend and I have emphasized in a joint paper, this constitutes a backstop line of credit provided by the central bank (Goodfriend and Lacker, 1999). Indeed, operational protocols and the routine provision of daylight credit in some cases leave the central bank with no other choice but to lend. For example, in the case of the disruption at the Bank of New York in November of 1985, the extension of overnight credit was a fait accompli (Lacker, 2004).\n\nTaking the terms on which central banks clear, settle, and lend as given, the usual presumption is that competitive pressures will drive private sector institutions toward second-best efficiency. Underpriced access to central bank credit will of course distort private sector choices. Absent countermeasures, banks will take excessive risks and central bank credit will be overused, a distortion often referred to as moral hazard. It is in the nature of lines of credit, however, that they are underpriced at the point in time at which they are utilized. Credit lines provide guaranteed access to funds at a prespecified rate that does not vary with the borrower's ex post creditworthiness. Thus borrowers essentially obtain insurance against adverse shocks to their creditworthiness. Private line of credit lenders are generally compensated for this insurance provision through up-front fees. Other features of typical credit lines act to constrain moral hazard. Lenders limit the extent of their liability through loan covenants that let them deny credit if certain financial conditions are not satisfied. In addition, lenders generally monitor borrower financial conditions on a regular basis, and often reserve the right to audit borrowers.\n\nThe potential for moral hazard due to a public sector safety net, and in particular the provision of central bank credit in connection with payment operations, is to my mind the central rationale for central bank oversight of payment system participants. Such oversight should be aimed at measuring and efficiently constraining private risk taking that could affect the extension of central bank credit or the provision of public sector support. Much central bank payment system supervisory activity obviously fits this description well. Having said that, it is my sense that central banks have not come close to fully offsetting the safety net's moral hazard distortion, although I would be hard pressed to document that claim, except to note the extent to which access to central bank settlement seems to be highly prized by financial institutions.\n\nThis description of central bank payment activities implies a minimal service provision role – basically just offering clearing accounts that are used to settle interbank obligations. And, this role is a byproduct of having de facto monopolized interbank settlement. In this, I find Green and Todd (2001) persuasive when they argue that the rationale for more extensive central bank service provision depends on the extent to which there are economies of scope between additional activities and the basic clearing account function. A focus on payment systems as communications mechanisms suggests the importance for this question of the relative effectiveness of alternative configurations of communications architectures, and potential economies in verifying messages and safeguarding information. My sense, however, is that there are far less by way of economies of scope than would be needed to justify, on economic efficiency grounds, the current scale of Federal Reserve service provision, particularly in clearing \"retail\" payments such as checks and ACH. In fact, I have argued elsewhere that the evidence suggests that the Fed's role in clearing retail payments rests on altering the allocation of clearing costs that would result from purely private provision. The imminent transition away from paper check clearing makes the Green and Todd question a live issue right now in the U.S.\n\nNotice that I have made no use of the notion of \"market failure.\" My own working hypothesis is that market failures are largely absent from the payment system. After all, participants in any given payment arrangement are all linked by voluntary contractual relationships. Thus I find it hard to see how an externality, in the classic sense, could possibly arise. (The only genuine payment system externality I know of occurred when the Federal Reserve incinerated worn paper currency, thus polluting the air.) Note that the lack of an observed market does not mean market failure. For example, large banks don't clear checks for rural banks in the U.S. Surely this is due to the terms on which the Fed provides the same service. After all, there was an active market before we did it. But as I argued earlier, we don't need a market failure to motivate central bank supervision of private payment system activities. To me, our policy interest is amply motivated by the presence of a substantial public sector safety net to payment system participants, and the central bank's role in providing and setting the terms and conditions of important elements of that safety net.\n\nThe perspective I have just outlined implies that the terms and operational conditions on which central banks extend daylight and overnight credit are of central importance. Years ago, when many aspects of current arrangements were put in place, operational considerations made it costly to implement systems that did not automatically extend daylight central bank credit in one form or another. New technologies may have significantly altered this cost-benefit trade-off, and in my opinion a re-examination of daylight credit policies is in order. For example, many banks monitor and control the extension of daylight credit to many of their corporate customers, and indeed, supervisors expect them to. It would be ironic for central bank risk management to lag behind private sector practices in this regard.\n\nA focus on central bank credit also makes clear that paying interest on reserves is more important than is commonly appreciated. The prohibition of interest on central bank deposits, as in the U.S., greatly enhances the demand for daylight credit, in the sense that larger overnight balances act as substitutes for daylight overdrafts. As a result, limitations on central bank credit extension would be less costly if reserves earned interest. More broadly, it seems plausible that a huge fraction of settlement activity originates in transactions whose main purpose is to allow entities to evade interest prohibitions, and thus to some extent are socially wasteful.\n\nThe relation of central bank credit to the broader public safety net has implications that are sometimes overlooked. For example, the collateralization of central bank credit extension may reduce risks to the central bank, but it can increase risk to the deposit insurance fund. Therefore, the central bank ought to consider more than just its own balance sheet risk in making lending decisions. This is especially important because, as the lender of last resort, the central bank can often force an institution's closure by refusing credit.\n\nAnd finally, notwithstanding several seemingly strong policy pronouncements of mine this afternoon, I do not believe that our understanding of the economics of intraday credit is at this point sufficient to provide quantitative guidance on the optimal pricing of daylight credit, even apart from moral hazard considerations. In that light, I regard conferences such as this devoted to the advance of payment economics among the most noble uses of central bank resources.\n\nReferences\n\nGoodfriend, Marvin, and Jeffrey M. Lacker. \"Limited Commitment and Central Bank Lending.\" Federal Reserve Bank of Richmond Economic Quarterly 85 (Fall 1999): 1-27.\n\nGoodhart, Charles. The Evolution of Central Banks. Cambridge, MA.: The MIT Press, 1988.\n\nGreen, Edward J., and Richard M. Todd. \"Thoughts on the Fed's Role in the Payments System.\" Federal Reserve Bank of Minneapolis Quarterly Review 25 (Winter 2001): 12-27.\n\nKocherlakota, Narayana R. \"Money: What's the Question and Why Should We Care About the Answer?\" American Economic Review 92 (May 2002): 58-61.\n\nLacker, Jeffrey M. \"Payment System Disruptions and the Federal Reserve Following September 11, 2001.\" Journal of Monetary Economics 51 (July 2004): 935-65.\n\nLacker, Jeffrey M., and John A. Weinberg. \"Payment Economics: Studying the Mechanics of Exchange.\" Journal of Monetary Economics 50 (March 2003): 381-87.\n\nTownsend, Robert M. \"Currency and Credit in a Private Information Economy.\" Journal of Political Economy 97 (December 1989): 1323-44."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2005",
        "date": "April 07, 2005",
        "title": "Community Affairs Research Conference - Opening Remarks",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke April 7, 2005 at the Capital Hilton Hotel in Washington, D.C.",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2005/lacker_speech_20050407",
        "content": "On behalf of the Federal Reserve System, it is my pleasure to welcome you to the fourth Community Affairs Research Conference. I believe this year’s conference has brought together a set of papers and participants that will make interesting and useful contributions to our knowledge on matters of consumer finance and community development. These are areas in which innovations in lending practices and supporting information technologies have led to a substantial expansion of credit to U.S. households. I think its fair to say that these markets are in a state of flux, making it particularly difficult to assess the costs and benefits of private sector practices and public policies. Against this backdrop, research like that represented at this conference is all the more valuable.\n\nOver the last six years these meetings have provided a valuable venue for presenting and discussing research relevant to improving our understanding of how markets in general, and credit markets in particular actually work, especially for people and households in the lower portion of the income distribution. While many of the headlines we see about the economy and about economic policy deal with the overall level of income and employment, it’s clear that market outcomes for individual households can vary widely. I see the community affairs research program as being squarely aimed at understanding this variation in outcomes across households and how it is affected by public policies. Indeed, this purpose is reflected in the title and subtitle for this year’s conference — “Promises and Pitfalls: As Consumer Finance Options Multiply, Who is Being Served and at What Cost?” This research program has already taught us some valuable lessons. In particular, we have seen the emergence of a substantial body of research that aims at measuring the effects of community development policy on the behavior of market participants and on economic outcomes in targeted communities.\n\nIn the first area, much of the focus has been on the effects of the Community Reinvestment Act on bank lending behavior. While it is always difficult to attribute such changes to a particular policy when there may be many causes, the evidence so far suggests that the CRA has had the expected effect of increasing lending by banks and thrifts to low and moderate income households, especially since the late 1990s and especially for large banks, for whose acquisition strategies can be affected by CRA ratings.\n\nAt the same time, many studies have examined the effects of such lending on low and moderate income neighborhoods. They generally find faster growth in average home ownership and average home values in these areas, which may be at least partly attributable to the increased lending encouraged by the CRA and other initiatives. These results suggest that targeted intervention by government or private community development organizations can significantly alter the development path of the target locations.\n\nIt appears to me, however, that as valuable as this research has been, much of it has been of only limited usefulness as a guide to public policy. The goal of community development lending is to facilitate the ability of low and moderate income households to make use of financial markets and instruments for their own benefit. In other words, policy is about the well-being of people, not neighborhoods. Research that focuses on neighborhood-wide outcomes — even important outcomes like home ownership or property values — in some sense misses the mark. By focusing on neighborhoods rather than people, they are unable to provide conclusive answers to the most fundamental questions dealing with the economic well-being of households. A measured increase in ownership and home prices in a neighborhood, for example, cannot, by itself, tell us who has or has not gained from these changes. Its unlikely, for instance, that people’s location decisions are invariant with respect to the policy intervention under consideration.\n\nSimilarly, studies of the behavior of the providers of credit, while important for understanding the impact of regulatory intervention, are limited in their ability to shed light on the impact of such credit on borrowing households. The benefits that households derive from financial market transactions relate to building wealth and smoothing consumption. And people obtain credit from an array of market and non-market sources. Focusing on lenders rather than people, leaves us unable to say much about the evolution of people’s well-being.\n\nAs a corollary, while the HMDA data reported by lending institutions may be useful in screening for potentially discriminatory lending, further refinements of the HMDA reporting regime are unlikely to yield information that is of much use in making inferences about the well-being of the people we are trying to help. It would be more productive, in my view, to devote resources to collecting longitudinal data, analogous to the PSID. We ought to be building datasets containing detailed and comprehensive records of financial transactions as well as income and spending for a panel of households over the course of many years. So if I were to try to identify the main challenges in community development research, one would be to find ways of improving our understanding of how changes in financial institutions, instruments and regulation have affected the behavior and welfare of individual households in the population of interest. This is not an easy task. One obvious reason for the greater focus on lenders and neighborhoods is data availability. But to the extent to which our research program is driven by policy questions, I would hope that our future research efforts are guided more by a focus on people. And this year’s conference program heads in the right direction — a number of papers seek to examine how developments in consumer finance markets, which have been changing rapidly in recent years, are affecting consumers.\n\nGiven a focus on people and economic well-being, another challenge for research is to take care in how economic well-being is assessed. One of the primary functions of financial markets is to allocate risk. In consumer finance, borrowing and saving are important tools in households’ efforts to prepare for and respond to risks associated with loss of income, illness, or other shocks. The effectiveness of financial market participation as an aid to household risk management can only be fairly assessed on an ex ante basis — that is, from the perspective of before the financial transaction has been initiated. After the fact, a defaulting borrower may not appear to have been made better off by taking out a loan. But for economic research, and economic policy as well, the more relevant question is whether a change in credit market conditions makes the average borrower better off ex ante: “at the time the loan is taken out.” That is, how do changes in institutions or government regulation affect the price and terms on which households can obtain credit, and how does that affect their ability to respond to shocks, smooth consumption, and build wealth? This emphasis on an ex ante perspective is not just a good guide for research. It is also, I believe, an important principle for how we think about public policy with regard to consumer finance. And this point relates directly to the “pitfalls” referenced in our conference’s title. I think the most significant pitfall is that the broad expansion of credit and proliferation of lending practices that we’ve seen in the last couple of decades has made evaluation of credit options more difficult, both for the consumer and for the policy analyst.\n\nPopular discussions of lending practices often take a decidedly ex post perspective, revolving around the consequences of particularly bad outcomes. This amounts to policy by anecdote. An ex ante approach would ask instead whether a particular practice expands the menu of borrowing options in a way that is useful to households in pursuing their economic goals. Similarly, it seems to me that both sides of the recent political and media debate about the bankruptcy bill were largely divorced from the essential economics of bankruptcy: namely, the role that it plays in facilitating household risk management. Bringing an ex ante perspective to bear on credit market policy questions is, I think, an area where research can be especially valuable.\n\nBy the way, this ex ante perspective underscores the importance of financial literacy. As the array of financial market options available to households continues to expand, their ability to assess those options and make choices that are good for them will increasingly rely on them being well-informed. In other words, the community affairs research program is complementary to the Federal Reserve’s interest in building consumer understanding of financial market opportunities and risks.\n\nThe future of community affairs research is promising in my view, despite the pitfalls I’ve noted here. The fact that there remain gaps in our knowledge I see not as a failure of research to date but as a measure of the opportunities for further productive work. The fact that popular policy discussions in this area are so prone to error also points out the social value of good solid research.\n\nAs a Federal Reserve economist, a director of research, and now as a Reserve Bank president and chairman of the Committee on Research, Public Information and Community Affairs, I have gained a deep respect for the activities of our Community Affairs offices and for their mission of promoting community development and fair and impartial access to credit. And as an economist, I think that one of the best contributions we can make to this mission is by supporting research into the fundamental forces that determine the well-being of the people who make up the communities in which we have a particular interest. Credit markets clearly play an important role in this process, and as we advance our understanding of this role, we will further our community affairs mission and become better as financial regulators. As a result, I will watch the work discussed at this and future conferences with great interest."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2005",
        "date": "March 01, 2005",
        "title": "Inflation Targeting and the Conduct of Monetary Policy",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke March 1, 2005 at the University of Richmond in Richmond, Va.",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2005/lacker_speech_20050301",
        "content": "I am pleased to be here with you today to talk about inflation targeting and the conduct of monetary policy. I would like to thank Dr. Coughlan, Dean Newman and the University of Richmond for giving me an opportunity at this time to express my views on this important subject. The timing is right for two reasons. It has been just a little more than half a year since I became president of the Federal Reserve Bank of Richmond. As bank president, I participate regularly in the Federal Open Market Committee, the body that makes monetary policy in the United States. And I welcome this as an opportunity to make the first comprehensive presentation of my views on the conduct of monetary policy. As I will explain in detail, I believe that the adoption and announcement of an explicit, numerical, long-run inflation target by the Fed would enhance the effectiveness of monetary policy.\n\nThe second reason the time is right to discuss inflation targeting is that the Federal Open Market Committee, at its most recent meeting on Feb. 1 and 2, considered at length the pros and cons of formulating and announcing an explicit, numerical inflation objective. I should note that I am not divulging any great secrets here; the minutes of that meeting were made public on Feb. 23, according to the newly accelerated release schedule that began with the January meeting this year. Preparing for that extensive discussion helped me to clarify my own views on inflation targeting and related issues, and I would like to share my thinking on these matters with you this afternoon.\n\nBefore I do, however, several stipulations are in order. The first is the usual disclaimer that, as will become clear later on, the views I express do not necessarily represent the views of my colleagues around the System. The second is that nothing in my prepared text should be construed as implicit commentary on current economic conditions or imminent interest rate decisions. I will be discussing monetary policy principles, not this year’s tactics.\n\nThe third stipulation has to do with terminology. Some economists draw a sharp distinction between an inflation objective and an inflation target. An objective, in their usage, refers to a simple announcement, while targeting involves an accompanying institutional apparatus, including formal requirements to submit reports to a legislative body. In some contexts, these distinctions are important. But for the issue before us in the United States, the distinction is less important since, as I will argue later on, an announced objective will inevitably draw the Fed into commenting on where inflation stands relative to the objective. And besides, the Fed is already required to report to Congress twice a year about inflation and the economy. For today’s discussion, therefore, I will use the terms “target” and “objective” interchangeably. Moreover, I will use the terms to refer to an objective that is explicit, long-run, and numerical.\n\nMy thinking on this subject is rooted in the rich history of contributions to the theory and practice of monetary policy at the Federal Reserve Bank of Richmond. My predecessor, Al Broaddus, and his predecessor, Bob Black, were tireless inflation fighters in the 1970s and early 1980s when high and variable inflation was a serious problem. It was in those days that Bob and Al, who was then Bob’s research director, earned their wings as inflation hawks by the strong stand they took against inflation both inside and outside of the Federal Reserve. Their practical work was buttressed by the research writings of Marvin Goodfriend, a long-time Richmond Fed monetary economist, and his colleagues in the Richmond research department.\n\nAl exercised his hawk wings as Richmond Fed president and a voting member of the FOMC in 1994, when he led the fight for preemptive action against inflation in that year. That was when Al brought the idea of inflation targeting to the FOMC, suggesting that some form of inflation objective might be helpful in securing the Fed’s credibility for low inflation. Chairman Greenspan invited Al, together with Janet Yellen, currently the president of the Federal Reserve Bank of San Francisco and then a member of the Federal Reserve Board, to lead a discussion on inflation targeting at the January 1995 meeting.\n\nWhat happened in 1994 to precipitate the Committee’s initial interest in inflation targeting is a good example of the experience that informs my own view of the subject. Hence, I think it is useful to review that story in a little detail here.\n\nTo set the stage, the Fed had brought the inflation rate down from over 10 percent in the early 1980s to around 3 percent by the mid-1990s; yet 1994 was a year of heightened risk of rising inflation. There was an inflation scare in the bond market that took the 30-year bond rate from below 6 percent in October 1993 to a peak of over 8 percent in November 1994. That nearly 2.5 percentage point increase in the bond rate indicated the Fed’s credibility for low inflation was far from secure.\n\nThe Fed fought the challenge to its credibility by raising the federal funds rate — our monetary policy instrument — in seven steps from 3 percent to 6 percent between February 1994 and February 1995. Incidentally, starting with its February policy action that year, the Fed, for the first time in its history, began to announce every federal funds rate target change immediately after the FOMC meeting; and the country watched and debated each increase in the funds rate. With this series of actions, the Fed held the line on inflation at 3 percent, marking only the second time in its history (the first was in 1983-4) that the Fed successfully preempted a cyclical rise in inflation. In spite of the policy tightening, real GDP grew by around 4 percent in 1994, up from around 2.5 percent in 1993, and the unemployment rate actually fell from around 6.5 percent to 5.5 percent from January to December 1994. The bond rate returned to around 6 percent by January 1996, and one began to hear talk of the “death of inflation.”\n\nFrom this experience I draw three conclusions for monetary policy. First, a well-timed preemptive increase in the federal funds rate is nothing to be feared. In 1994, it was necessary to take the real federal funds rate — the nominal rate adjusted for expected inflation — from around zero up to around 3 percent in order to avert the potential build-up of inflationary pressures. And yet real growth picked up and the unemployment rate trended down.\n\nSecond, to keep inflation well-anchored, the Fed must be prepared to move the federal funds rate around over the business cycle even though inflation remains stable. There is a simple but underappreciated principle at work here: real, inflation-adjusted interest rates must vary over time with shifts in economic fundamentals, even if inflation is perfectly constant. Since our policy instrument is a nominal interest rate, it has to vary over time as well, even without noticeable deviations in inflation or inflation expectations.\n\nThird, the anchoring of inflation expectations achieved by preemptive policy in 1994 has produced enormous benefits for monetary policy. The bond market arguably has not exhibited a major inflation scare since 1994 — not during the boom in the late 1990s and not during the period of very low federal funds rates in the last few years. The successful stabilization of inflation expectations has been the cornerstone for effective monetary policy ever since. The Fed’s credibility for low inflation allowed it to act aggressively against the recession in 2001 and after the Sept. 11 terrorist attacks. The federal funds rate was dropped in 11 steps from 6.5 percent at the beginning of the year to 1.75 percent in December, without a substantial rise in inflation expectations. Subsequently, credibility against inflation enabled the Fed to fight potential deflation by lowering the funds rate down to 1 percent between June 2003 and June 2004. In short, low and stable inflation expectations have enhanced the ability of monetary policy to react flexibly to both positive and negative shocks since the mid-1990s.\n\nThis was in sharp contrast to the 1970s and early 1980s when the failure to stabilize inflation expectations subjected the economy to severe inflation scares that at times forced the Fed to respond with aggressive interest rate policy actions. When inflation accelerated, interest rates had to rise just to keep real interest rates from falling — in other words, to keep the stance of monetary policy unaltered. Further rate increases were required in order to raise real interest rates and reduce inflation. The aggressive rate increases needed to contain inflation and inflation expectations put the economy at risk of recession and at times actually precipitated a recession, or prolonged a recession already in progress.\n\nI think it is fair to say that the experiences and lessons I just outlined are now widely appreciated by central bankers and monetary economists alike, and account for the fact that the Federal Reserve has made low inflation and the stabilization of inflation expectations a priority as never before in our history.\n\nMy reading of the recent monetary history, especially the 1994 policy actions and subsequent developments, leads me to favor the adoption of an inflation target. Why would that enhance the effectiveness of monetary policy? I will organize my discussion around the passage from the latest FOMC minutes that reports on the Committee’s discussion of the issue, because it represents a succinct summary of the viewpoints that have been articulated both within the Committee and among economists at large.\n\nThe minutes start by reporting that “meeting participants uniformly agreed that price stability provided the best environment for maximizing sustainable economic growth in the long run…” In other words, the debate about inflation targeting is not about whether actual inflation should be low and stable. The question is whether the Fed should announce an explicit numerical objective.\n\nThe minutes cite three benefits of an explicit price-stability objective: (1) its usefulness as an anchor for long-term inflation expectations, (2) its power to enhance the clarity of Committee deliberations, and (3) its usefulness as a communication tool. I agree wholeheartedly with the first point in light of the critical importance of tying down inflation expectations as I discussed earlier. As much as one can debate the usefulness of allowing short-run fluctuations in realized inflation, I see no utility in tolerating unnecessary fluctuations in long-run expectations of inflation.\n\nI also agree completely with the second point about enhancing the clarity of deliberations, because when it comes to internal policy analysis and discussion, coherence demands that FOMC participants implicitly agree on a long-run numerical objective for inflation. Accountability in a democratic society then argues for making available to the public the numerical objective upon which our internal discussions of monetary policy are based.\n\nFinally, I believe that the Fed’s experience in May and June 2003 indicates that references to inflationary or deflationary risks in the policy statements we now release after every meeting cannot reliably substitute for an explicit inflation target. The statement issued following the May 2003 FOMC meeting asserted that a fall in inflation — then about 1 percent — would be “unwelcome.” This came as a something of a surprise to markets and caused a sharp reaction in long-term rates. If an inflation target range had been in place in 2003 with a lower bound of 1 percent, the public could have inferred the Fed’s growing concern about disinflation as the inflation rate drifted down toward that bound. Expected future federal funds rates and longer-term interest rates would have moved lower continuously, with less chance of overshooting or undershooting the Fed’s likely policy path.\n\nIf the May 2003 statement is interpreted as the revelation of the lower bound of an inflation target range, then half of an inflation target range has been announced. And if revealing a dislike of inflation below 1 percent was useful in May 2003, is it not likely that revealing an upper bound will prove useful in some future circumstance?\n\nIn short, I strongly support each of the three reasons given in the minutes in favor of an explicit long-run numerical objective for inflation. The minutes also cite three drawbacks to the adoption of an explicit price-stability objective. I would like to comment on these, because they also are widely mentioned outside the Fed in discussions of inflation targeting. The first is that an inflation target might appear to be inconsistent with the Committee’s so-called “dual mandate” of fostering maximum employment as well as price stability. On the contrary, for the reasons I gave earlier, I think both experience and economic theory strongly suggest that the best contribution monetary policy can make to promoting employment and growth is by tying down inflation and inflation expectations. That is, in the long run, employment and growth are maximized by keeping inflation low and stable. Moreover, there is widespread agreement among central bankers and monetary economists that although, over the long run, it is feasible for the central bank to control inflation, long-run growth and employment are predominantly determined by forces independent of monetary policy. So it makes little sense for the central bank to adopt a long-run objective for growth or employment.\n\nI would like to digress here for a moment to say a few words about this idea that the Federal Reserve has a “dual mandate.” If you go back and look at the direction Congress gave us — it appears in Section 2A of the Federal Reserve Act and was most recently revised in 1977 — you find that they actually gave us three mandates: “maximum employment, stable prices, and moderate long-term interest rates.” Nobody mentions the third mandate, moderate long-term interest rates, and for good reason. It is widely understood that the best contribution monetary policy can make to keeping long-term interest rates low is by keeping expected inflation low, because this minimizes the inflation premium built into nominal long-term rates. This is true despite the fact that keeping inflation low sometimes requires pushing short-term rates up, which sometimes raises long-term rates for a time by raising expected near-term real rates. Thus, there can be said to be a short-run trade-off between keeping inflation low and keeping long-term interest rates down, even though in the long-term there is no such trade-off.\n\nAnalytically, this is quite analogous to the relationship between our inflation and employment mandates. In the long run, low inflation is best for employment growth, but keeping inflation down can require actions that might reduce employment growth in the short run. Admittedly, employment is a real quantity while interest rates are prices, so people might have a different sort of interest in employment than they do in long-term interest rates. Presumably, this motivates elevating the rhetorical status of employment over long-term interest rates by speaking of a “dual mandate.” But that rhetoric should not obscure the economics of the relationship between the two or the fundamental primacy of our price stability goal.\n\nThe second point critics make is that the adoption of an inflation target might inappropriately bias or constrain policy at times. An inflation target would be aimed at anchoring expected inflation in the long-run. That will be successful over time only if Fed actions keep actual inflation around its long-run target. These critics argue that while we want the public to believe inflation will remain well-anchored in the future, when the future finally rolls around, we might want the flexibility to pursue policies that are inconsistent with the earlier promise implied by an inflation target. In other words, we might find the commitment implied by our announced inflation target constraining.\n\nI would argue that this is a flexibility the Fed should be happy to do without. In the process of establishing the credibility of our commitment to price stability, we have already given up the flexibility to let expected inflation get out of control. That is what a commitment is — a pledge to forego future flexibility. An announced inflation objective is meant to guide policy actions over the long run. It would not hinder the kinds of policy actions undertaken these days to stabilize employment and output in the short run. And as I discussed earlier, there is evidence that anchoring inflation expectations more securely with an explicit long-run target would actually increase the flexibility of monetary policy to react to shocks in the short run.\n\nIf the Fed adopts an explicit inflation target, we would inevitably feel compelled to explain and work to unwind any substantial short-run departures from our long-run target. More to the point, however, it would force the Fed to respond when measures of expected inflation move much outside of the target range. There are no circumstances, I would submit, in which expected inflation should be outside of a narrow band around the target for very long. This is the narrow scope of flexibility that’s at stake with an inflation target, and it is hard to see what good it would do to retain it.\n\nFinally, a third factor critics mention is that with inflation expectations well-contained over recent years, the benefits of announcing a specific inflation objective might be small in any case. In response, I would point out that no credible observer believes there is any reason for inflation to be persistently higher or lower than it is today. The benefits might not be large, but I fail to see the case for encouraging fluctuations in the credibility of the Fed’s commitment to price stability. In short, I disagree with each of these three arguments against an inflation objective.\n\nBefore concluding, I would like to say a few words about how establishing an inflation objective would work in practice. First, I think it would be best if the target were stated in terms of one of the consumer price indexes, because the public is most familiar with such measures. In addition, economic theory tells us that monetary policy should stabilize the value of money relative to the goods and services that go into household consumption, as opposed to other conceivable baskets of goods and services. There are several consumer price indexes to choose from. In terms of the Consumer Price Index (CPI), I would want a 2 percent midpoint for the target range. The CPI has some well-known methodological flaws as a measure of purchasing power, however. Economists prefer the Price Index for Personal Consumption Expenditures from the National Income and Product Accounts — the so-called PCE price index. In terms of that index I would want a 1.5 percent midpoint for the target range.\n\nI should say something about what reasoning led me to these values — 2 percent for the CPI or 1.5 percent for the PCE index. First, 1 percent appears to be a good target for actual inflation. One factor in selecting a target is that interest rates ultimately build in compensation for expected inflation, so higher target inflation ultimately means higher interest rates. Minimizing inflation therefore helps minimize the inefficiencies that arise from the incentive to substitute away from assets like currency or bank reserves that do not bear interest. Minimizing inflation also reduces the distortions that arise in sectors where prices are sticky. A 1 percent target is preferable to zero or some negative number, however, because of the value of building in a cushion against the possibility that interest rates bump up against the zero lower bound. Real interest rates are what matters for monetary policy, but it is nominal interest rates — which are just real rates plus expected inflation — that cannot be driven below zero. Thus, an inflation rate a bit above zero gives us a bit more leeway to lower real interest rates to prevent inflation from falling below target.\n\nFinally, given a target of 1 percent for actual inflation, we need to take into account known measurement biases in our price indices. Our best current research indicates that the CPI overstates actual inflation by about 1 percentage point, on average, and the PCE price index overstates actual inflation by about a half of a percentage point. Thus, I prefer 2 percent for the CPI and 1.5 percent for the PCE.\n\nI have a preference for targeting a measure of core inflation — in other words excluding food and energy — because it would be sufficient to anchor overall inflation over time, but it would give us the latitude to allow relative energy and food prices to fluctuate in the short run without necessarily requiring an immediate monetary policy response. Not coincidentally, the core PCE price index is the one favored by Fed staff and policymakers for internal analysis and discussions, and thus its use as a target would enhance monetary policy transparency.\n\nI favor a range around our target with a width of 1 percentage point, rather than a simple point target. As I noted earlier, an announced target range would inevitably draw the Fed into discussing inflation in relation to the range. If inflation moved outside the range we would feel compelled, I believe, to acknowledge that fact and to state how inflation will be brought back within the range. A range rather than a point target would give the Fed a reasonable “safe harbor” within which we would not be pressed to explain fluctuations in inflation. The narrowness of a 1-percentage-point range, however, would discipline us to explain any substantial deviations of inflation from the target.\n\nI would regard the Fed’s announced inflation objective as a long-run range, and hence I would expect the Fed to revise its numerical inflation objective relatively rarely, mainly for improvements in measurement.\n\nHow the Fed communicates about an inflation target is important. In some countries, the adoption of inflation targeting has involved explicit action by the legislature or the administration. In contrast, I believe that the Federal Reserve can legitimately describe inflation targeting as a natural incremental step in the evolution of our policy operations, a step we take because we believe it will improve the ability of monetary policy to stabilize employment, growth, and inflation by enhancing the effectiveness of short-run communications and tying down inflation expectations. We also should emphasize, I believe, that that our announced inflation objective is meant to guide monetary policy over the long run, and that it should not prevent the Fed from taking the kinds of policy actions it takes today to stabilize employment and output in the short run.\n\nAdmittedly, monetary policy has been working reasonably well of late. In particular, the Fed already makes low inflation a priority, and inflation expectations have been low and reasonably stable. So why take the additional step of announcing an explicit inflation objective? At the risk of some repetition, let me summarize the argument I have advanced here. First, the enormous costs of failing to maintain price stability are now well understood, and there is no reason for inflation to be much higher or lower on average than it is today. Second, by announcing an inflation objective, the Fed would not be surrendering any flexibility that it has not given up already, or should not be happy to give up. We would be reducing extraneous fluctuations in expected inflation. And third, since there are no circumstances in which the Fed would like to see inflation much higher or lower than it is today, announcing an explicit long-run inflation objective is mainly an incremental step in the direction of transparency. Ambiguity about the Fed’s long-run inflation intentions has outlived its usefulness.\n\nIn addition to the operational benefits mentioned above, greater transparency is important because it defuses the idea that secrecy has any role to play in the policy process, and it opens the door to even greater transparency and a broader comprehension of short-run policymaking on the part of the public. And the understanding and support of our citizens is ultimately the only way to secure good monetary policy in the future."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2005",
        "date": "January 18, 2005",
        "title": "Technology and Labor Markets",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke January 18, 2005 at the Guilford College, University of North Carolina -- Greensboro, FNB Southeast in Greensboro, N.C.",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2005/lacker_speech_20050118",
        "content": "Labor market developments receive a considerable amount of media coverage. Over the last two years there have been countless stories about the “jobless recovery.” Employment growth following the 2001 recession has been slow compared to previous business cycles, slower in fact than any other recovery since WWII. In fact, only in 2004 did the economy get to something we might consider a normal pace of employment growth for a period of expansion. Many observers see technology as the culprit in sluggish employment growth. By raising productivity, we are told, technology weakens the demand for labor, and allows firms to meet growing demand without adding workers. In the last year or so, there also have been widespread stories about the growing number of jobs lost to imports or outsourcing. Of course, this movement of jobs overseas has been facilitated by technological advances in communication and information processing, so there is a sense in which one can describe these jobs as being lost to technology as well.\n\nNorth Carolina and the Triad region in particular have experienced labor market difficulties that have been in some ways more pronounced than at the national level. Employment losses locally were greater in percentage terms than national losses during the 2001 recession, and payrolls recovered faster in North Carolina than in the U.S. as a whole. State-wide employment growth in 2004 was similar to the national numbers, but the Triad continued to lag. Both the state and the region have a higher-than-average percentage of their workforce in manufacturing, which is in the midst of a long-term decline in employment. This is particularly notable in the Triad where the effects of layoffs in textiles and furniture are still being felt.\n\nIn addition to concerns about employment — the quantity of labor — there has been some anxiety as well about wages — the price of labor. Recent media coverage has focused on the extent to which new jobs do not pay as well as jobs that are being lost. The broad theme of most of this coverage, I believe, has been a sense that the benefits of the economic expansion are not being widely shared, and a general disappointment in the labor market outcomes being experienced by many workers.\n\nToday I hope to provide you with a different perspective than the typical media coverage, a perspective that is grounded in economic research. In particular, I’d like to discuss U.S. labor markets from the point of view of the relationship between technological change and labor market outcomes. Certainly technological change has been occurring at a breathtaking pace in the last decade, and it wouldn’t surprise you to learn that economists believe that technological change has had significant macroeconomic effects. But economists also believe that technological change has had significant effects within labor markets, and that those changes have been going on for several decades. In other words, the effects go back to well before the Internet revolution.\n\nI want to start by reviewing some basic facts about U.S. jobs market. One of the most prominent is that the structure of wages in the U.S. economy has become more unequal over the last half-century. One popular measure of wage inequality is the “90-10 ratio.” This ratio compares the wages of workers at the 90th and 10th percentiles. In 1970, workers at the 90th percentile earned a little more than three times what those at the 10th percentile earned. By 2000, this multiple had risen to five. This increase in inequality has been especially rapid since 1980. This doesn’t necessarily mean that “the rich are getting richer and the poor are getting poorer,” but it does mean that in terms of economic well-being, those at the top of the pay scale outpaced those at the bottom. There are many other measures of wage inequality, but all tell pretty much the same story over recent decades.\n\nThe growth of inequality in the latter part of the 20th century represents a reversal of trends from the first half of the century, when inequality was generally declining. So growing inequality is not an unavoidable feature of capitalist growth. The historical record shows that at times inequality widens and at times it narrows.\n\nEconomists usually presume a worker’s compensation is tied closely to that worker’s productivity — in other words, the value contributed to their employer’s production process. For example, it’s natural to suspect that higher-skilled workers are in general more productive and therefore paid more than those with lower skills, and this turns out to be true. Economists use the term “skill premium” to refer to the gap between the wages of workers at differing skill levels.\n\nCan changes in the skill premium explain the increase in wage inequality over the last half-century? One broad measure of skill is a worker’s educational attainment. And by this measure the skill premium has been rising. In 1970, the average wage of a male college graduate, employed full-time, was about one-third higher than that of a high school grad. By 2000, the average college grad was earning more than twice as much as a high school grad. The education premium for women also grew over this period, from about zero to about 50 percent. Another indicator of the relative value of worker productivity is the premium paid to more experienced workers. This “experience premium” has also increased in recent decades, but primarily among workers with less education.\n\nDo these two productivity measures — the education premium and the experience premium — explain the increase in inequality since the 1970s? It turns out that these factors, together with various demographic characteristics, account for only about 40 percent of the rise in inequality. The remainder of the increase in inequality (more than half of the total) is in “residual inequality,” the part not explained by any measurable worker characteristics. That is, if one groups workers with given education and demographic characteristics, the within-group variation in wages has increased.\n\nLet’s sum up these labor market observations, then. First, wage inequality has steadily increased since the 1970s. Second, almost half of the increase is due to a rising “skill-premium” — the relative value of skilled and unskilled labor (as measured by education and experience). Third, the rest of the increase in inequality is due to greater variation in wages across workers with given education and demographic characteristics (“residual inequality”).\n\nWhat’s behind these trends in the relative wages of different types of labor? The economist’s typical, if somewhat hackneyed response is, “It’s all supply and demand.” But there’s truth in this response. According to the logic of supply and demand, a rising relative value of skilled labor must mean a reduction in relative supply, an increase in relative demand, or both.\n\nTaking the supply side first, has the supply of skilled labor fallen in recent decades relative to the supply of unskilled labor? No: The percent of the workforce with a college education rose steadily throughout the second half of the 20th century, reaching about 30 percent in 2000. That is, the relative supply of skilled workers has been increasing. By itself, this would imply a falling skill premium. The skill premium did indeed fall some in the 1970s, when the college-educated population was growing particularly rapidly. But other than that decade, the relative supply of skilled workers was rising at the same time the skill premium was rising.\n\nThus, we need to look to demand to understand the changing skill premium. The demand for labor, like that for other inputs of production, depends on the demand for final goods and on the technology used to turn inputs into final goods. Most economists think production technology (as opposed to shifting composition of demand) has been the main character of the story. We have obviously been living through a period of rapid technological innovation that has brought about dramatic changes in the way a wide range of goods and services are produced. And this is not a new phenomenon. Early in the 20th century, the spread of electrification and innovations in communications had profound effects on production processes.\n\nBut not all technical advances have the same effect on production processes, and in particular on the relative importance of different types of labor. Sometimes new technology favors unskilled labor. For example, the introduction of assembly-line techniques in manufacturing in the first part of the 20th century allowed the production of complicated machinery like automobiles to be broken into a series of simpler steps. The result was that some goods that had previously been produced in small shops by skilled workmen could now be mass-produced in factories employing unskilled workers. As I noted earlier, inequality was generally falling early in the 20th century. And it turns out that the skill premium was falling as well, suggesting that new technologies in that era enhanced the value of unskilled workers by more than they enhanced the value of skilled workers.\n\nIn other cases, technology has the opposite effect of favoring the use of skilled labor. For example, advances in manufacturing later in the 20th century, such as the introduction of computer-controlled machine tools, have often meant fewer workers on the factory floor. The remaining workers needed a higher level of skill to operate the increasingly sophisticated equipment. Technological advances have had similar effects on the workplace in a wide array of industries. Consider the division of labor between architects and draftsmen. Before the advent of computer-aided design (or “CAD”), a draftsman would create and revise plans under the guidance of an architect. With CAD, however, the architect can easily generate and manipulate plans on the computer, resulting in the employment of fewer draftsmen per architect and boosting the productivity of the overall design process.\n\nIn these examples, the new technology improves the productivity of skilled workers relative to unskilled workers. Economists call such technologies “skill-biased,” and they refer to the introduction of such technologies as “skill-biased technical change.” Research evidence indicates that the rising skill premium in the late 20th century has been driven by skill-biased technological change. Why should technological change be systematically different now than it was 100 years ago? Many observers have pointed to the information technology revolution as a prime example of skill-biased technical change in the late 20th century. Computers, after all, are good at doing (or at controlling other machines that do) certain types of tasks — tasks that can be described by a “program,” which is just a set of rules. These are tasks that were previously more likely to be performed by less-skilled workers. In sum, IT-related skill-biased technological change appears to be an important part of the explanation for rising wage inequality.\n\nI want to turn now to another way in which technology affects labor markets. Technological change can be disruptive. New products and new ways of producing arrive, and skills that were tied to the old ways of doing things lose value, sometimes dramatically. This is what Schumpeter famously called the “perennial gale of creative destruction.” In this environment, some unskilled workers are doubly unlucky. First, skill-biased technological change can lower the relative demand for unskilled labor, reducing their wages relative to skilled workers. As I argued above, this is what happened in the late 20th century. Second, because less-skilled workers have less education, the skills they do have tend to be based on the specific experience they’ve accumulated. In other words, their skills may tend to be closely related to their particular job and their particular industry. When those skills do not transfer well to other sectors, these workers are more vulnerable to long-term earnings losses should their industries suffer declines.\n\nThe magnitude of creative destruction in job markets is huge, both in good times and bad. For example, during the last expansion, from 1992 to 2000, gross job creation averaged about 17 million per year, and gross job destruction averaged about 15 million per year. Net job growth, the difference between the two, was just about 2 million per year.\n\nJobs are continually being created and destroyed in a healthy, growing economy. In fact, that process is critical to our rising standards of living. Without shifting workers to expanding, more productive industries, we would not be able to take full advantage of technological advances, and real income growth would be stifled. Thus, the fundamental economic forces driving the increase in wage inequality are the same forces raising our standard of living.\n\nThis process is not without dislocations, however. Workers who are displaced from declining or less productive industries and occupations can suffer significant wage losses. But the reduction in the costs of production raise the real value of existing income flows. As overall real incomes rise, so does the overall demand for goods and services, which in turn induces firms to pull in workers who have been displaced from other declining industries.\n\nMost times, the economy manages to shift workers between industries fairly smoothly. In a typical recession, however, gross job destruction goes up, gross job creation goes down, and employment falls, on net. In the ensuing recovery, gross destruction and creation rates return to more normal levels, and net job growth resumes. Recessions, then, are times at which the shift of workers out of some industries increases, while the pace at which they are absorbed in other industries declines.\n\nIn the current recovery from the recession of 2001, gross destruction has returned to normal, so the typical bulge in job destruction is over. Job creation, however, has been slower than usual to pick up. Firms have been able to expand output without adding as many workers as would have been typical in the past. Thus, we have what has been called a “jobless recovery” — net employment growth has been relatively subdued coming out of this business cycle trough. Arithmetically this can only happen with an increase in productivity, so in this sense the jobless recovery reflects strong productivity growth. But this is consistent with widespread anecdotal reports that in recent years businesses have been focusing on extracting efficiencies from the capital infrastructure installed during the investment boom of the late 1990s.\n\nSo far, I’ve focused on the effects of technological change on labor markets and relative wages. But the last quarter of the 20th century was also a period of expanding international trade, as many countries lowered barriers to imports and as transportation and other trade-related transactions costs fell. Imports as a share of GDP grew from about five percent in 1970 to nearly 15 percent in 2002. Could it be that growing imports, especially from less developed countries, contributed to the growing disparity of wages between skilled and unskilled workers? This is what standard theory would predict, given the abundance of unskilled workers abroad. But how big is the effect of trade compared to the effect of technology? Here the estimates vary, but even researchers who see a significant impact of trade estimate that import growth can account for only about a quarter of the growth of the college premium. That still leaves technology as the dominant force behind recent relative wage trends.\n\nWhile import growth does not appear to have been the driving force in labor market developments for the U.S. economy as a whole, it’s certainly the case that some industries and some regions have been affected quite significantly by trade. Here in the Triad region, traditional manufacturing industries like furniture and textiles have seen particularly large growth in import competition and have also experienced painful declines in employment. Many observers have predicted large additional job losses in the wake of the expiration at the beginning of this year of the long-standing system of quotas on textiles and apparel imports. But even in the case of these industries, where import substitution is so visible, once you control for the effects of long-term technological trends, trade appears less important as a determinant of labor market conditions. This suggests that going forward, the same technological trends I’ve been talking about today are likely to be at least as important as developments in international trade in determining the direction of employment and wages in textiles and apparel manufacturing.\n\nIn any case, whether trade or technology is the major force, the message is the same. The world has become a much more challenging place for workers with lower skills and workers whose skills are largely specific to their job or industry. The weight of empirical evidence strongly suggests that erecting barriers to trade would ultimately prove a counterproductive response to this phenomenon. There is a fundamental congruence between the effects of trade and technology on labor market outcomes. Both can displace workers and force them to make the transition to other sectors. But both ultimately elevate standards of living. Few seriously propose impeding technological progress for the sake of jobs. We should be equally hesitant to impede trade.\n\nThe perspective on labor markets that I’ve described for you suggests that education and training are the keys to job market experiences. The more useful skills people can acquire, either before they enter the workforce or later in life, the better. But this perspective says a bit more as well. It suggests that adaptability will be increasingly important in the years ahead. A worker can no longer count on an initial occupation to maintain its relative position over time. They are quite likely to have to change jobs and industries over the course of their lifetime. Generalized skills — the type that are applicable in a wide range of settings and that enhance a worker’s ability to learn new jobs later in life — are key.\n\nBeyond emphasizing the importance of education, is there a role for the public sector to play in helping people adjust to the dislocations brought about by technology or trade? In a world of rapid technological change, job loss can imply long-term unemployment and/or persistently lower wages on re-employment. Such dislocations are often unanticipated — the typical 50-year-old textile worker could be forgiven for placing little probability on today’s global textile market conditions when they entered the industry over 30 years ago. Households generally attempt to insulate themselves against fluctuations they might foresee in their earnings or expenses — whether by saving and borrowing to maintain their consumption as income varies or by obtaining insurance against some of the shocks that can upset household finances. In an ideal world, these mechanisms would work well enough to leave little room for improvements via public sector programs. The difficulty households can have in practice in insuring themselves against large and persistent income shocks is part of what gives rise to government unemployment insurance programs. Beyond regular unemployment insurance, some displaced workers have access to additional support in the form of “trade adjustment assistance” — programs that provide income or training support to workers displaced by imports.\n\nI have two observations to offer on such programs. The first is the economist’s obligatory reminder that such unemployment insurance programs have strong and well-documented incentive effects. The recipient’s incentive to search for new employment is reduced, and thus unemployment spells tend to be noticeably longer. In designing such programs, we need to balance our desire to assist those displaced by economic change against the cost of inefficient use of labor resources.\n\nSecond, trade adjustment assistance treats one part of the population of displaced workers more generously than the rest. I have argued that trade-related job losses are fundamentally no different from job losses arising out of the ongoing turbulence of technological change. It’s hard to see why one set of transitioning workers should be singled out for favorable treatment, except perhaps to reduce political opposition to trade liberalization. But if the goal is to alleviate the sense of anxiety workers feel about their earnings prospects, then trade adjustment assistance may be too limited. Any expansion of assistance, however, would also expand the costs resulting from adverse incentive effects.\n\nMuch of what I’ve said today relates to a very basic truth — that in a dynamic, growing economy, people are affected differently by shifting market forces. Any fundamental change creates some winners and some losers and forces some people into difficult transitions. But these fundamental changes are at the very heart of what drives the broad, sustained advance in standards of living. Understanding the causes and consequences of economic change are vital to creating a broadly accepted belief in the benefits of technological innovation, unrestricted trade, and the other drivers of economic progress."
    },
    {
        "speaker": "Jeffrey M. Lacker",
        "year": "2005",
        "date": "January 03, 2005",
        "title": "The Economic Outlook",
        "summary": "Richmond Fed President Jeffrey M. Lacker spoke January 3, 2005 at  a meeting of the North Carolina Citizens for Business and Industry and North Carolina Bankers Association in Research Triangle Park, N.C.",
        "href": "https://www.richmondfed.org/press_room/speeches/jeffrey_m_lacker/2005/lacker_speech_20050103",
        "content": "I am delighted to be here today to discuss the economic outlook for 2005 and beyond. But before I do, it would be helpful to review briefly the course of economic activity over the past year or so. The usual disclaimer applies, however: the views expressed are my own, and are not necessarily those of others in the Federal Reserve System.\n\nLooking back, 2004 was the year that the economic recovery from the recession of 2001 finally set down firm roots. Real output grew four percent over the year ending in the third quarter. This is the same rate at which output grew over the course of 2003, so from that point of view, 2004 might not look much better. But this superficial similarity masks the significant improvements in the strength of the recovery that have taken place this past year.\n\nBusiness investment has made rapid gains over the last year and a half. In dollar terms, capital goods spending is on a solidly upward trend. And adjusted for price changes, spending has been even stronger, given the continuing secular decline in equipment prices. Third quarter investment in equipment and software, for example, was up nearly thirteen percent in real terms over the previous year. Anecdotal reports of managers’ hesitance to commit to investment outlays notwithstanding, the data indicate that opportunities to profitably deploy new capital goods continue to emerge.\n\nHousehold spending has remained on track throughout the recession and recovery. Despite job market weakness dating back to 2001, consumers apparently have been anticipating reasonably healthy future job prospects. Looking through the fairly substantial month-to-month choppiness, trend growth in consumer spending has risen from around two and one half percent in 2001 to around four percent, consistent with a steadily firming labor market over that time. Related, residential investment has been driven to all-time highs by historically low real interest rates, and this category has continued to surprise on the upside this past year.\n\nThe key improvement in 2004 has been the long-awaited pickup in net job growth. As is well known, employment in this recovery has lagged behind the pace of other post-war U.S. recoveries, but in the spring the pace of new hiring finally accelerated. November employment now stands two million, or 1.6 percent, ahead of a year ago, an average gain of 171 thousand per month. This comfortably exceeds the working age population growth rate of just under one percent, and thus notable progress has been made toward absorbing the overhang of those willing to work.\n\nThe path of labor productivity — the amount of real output per hour of worker input — is essential to understanding labor market developments over the past year, and indeed, over the past decade. I’ll return to this topic later in my remarks, but for now I’ll just note that the sluggish pace of job gains prior to this year has been a reflection of the truly astonishing extent to which firms have been able to expand real output without needing to add workers. This phenomenon appears to be drawing to a close, as firms more recently have been forced to make net hires to sustain output gains.\n\nInflation has remained steady this past year, and, just as importantly, inflation expectations have been contained. Inflation rates firmed at the beginning of the year, in part due to the surge in commodity and energy prices. This left a discernable effect on near-term expectations of inflation, as measured, for example, by spreads between nominal and inflation-indexed Treasury yields. The inflation compensation built into five-year Treasury spreads rose initially in the spring, settled back a bit over the summer, but then drifted up again. But expectations at longer horizons have remained steady, and a variety of survey measures have been relatively stable.\n\nFOMC policymakers responded to the firming inflation outlook by signaling and then in June initiating a series of increases in the target federal funds rate. Such a policy shift was inevitable, in the sense that the strengthening recovery was going to require real interest rates to rise, whether or not inflation accelerated. The key point here is that real short-term interest rates have to vary in response to shifts in economic fundamentals, even in the absence of noticeable fluctuations in inflation.\n\nLooking forward to 2005, it seems reasonable to project a continuation of growth along a quite similar trajectory. Consumer spending, fueled by expectations of sustained income growth, should continue to expand at something near the strong pace we have been seeing.\n\nBusiness investment spending might well show a temporary slowdown in the first quarter after the expiration of the tax incentives, but should resume expanding at a robust pace shortly thereafter, reflecting assessments that substantial opportunities remain to enhance efficiency by installing new capital goods, particularly IT and communications equipment.\n\nOutput growth next year should also be helped by a reduction in the drag from net exports. Although exports will be dampened somewhat by moderating growth among our major trading partners, the recent fall in the dollar ought to support export growth and contain imports as well. On the other hand, the declining fiscal policy impetus and the likely downward trend in housing starts will both detract from output growth. On balance, GDP growth seems most likely to lie in the three and one half to four percent range next year, barring a large unforeseen economic shock.\n\nThe outlook for productivity is pivotal to next year’s economic prospects, and it is especially hard to project at the present time. The middle of the 1990s saw a significant acceleration in productivity growth, from about one and one half percent per year to about two and three quarters percent. This acceleration was marked by “capital deepening,” which refers to the process of enhancing output per worker by increasing the amount of capital per worker.1\n\n\n\nCapital formation fell dramatically in the recession of 2001, of course, and didn’t begin recovering until about a year and a half ago. Surprisingly, however, productivity growth accelerated even further as the recession began, and has averaged about four and one quarter percent since the first quarter of 2001. Productivity growth over this period has not been driven by capital deepening. Instead, the dominant force has been what economists call “total factor productivity” (TFP), meaning that growth in output per worker has been due to increases in the amount of output produced by given quantities of capital and labor. In other words, firms lifted output per worker by becoming more efficient at using their existing labor and capital resources, rather than by applying more capital per worker.\n\nWhile this conclusion — that recent productivity growth represents mostly TFP rather than capital deepening — is an accounting necessity, given the slowdown in employment growth and investment while output recovered, it also accords well with widespread accounts of the recent evolution in business strategies. The late 1990s saw a dramatic run up in the deployment of new communications and computing technologies (capital deepening, in other words), along with stunning gains in the productivity of the industries producing these technologies. When limits were reached in the extent to which further additions to capital could be profitably absorbed, investment decelerated sharply and the recession began. With many firms taking a pause in investment and with demand growth slowing, managerial attention turned toward devising ways of reducing costs within the context of firms’ existing capital infrastructures (TFP growth, in other words).\n\nProductivity optimists see the recent surge in productivity growth as evidence of the extent to which fruitful efficiency gains remain to be discovered and exploited by firms, and they expect productivity growth to come in significantly above the long term average rate of around two and one quarter percent, perhaps closer to three percent.\n\nAn alternative view sees capital deepening and TFP gains as to some extent substitutes. Business managers face a trade-off, in this view, between planning and adding capital infrastructure, and devoting efforts to reorganizing business processes to use existing infrastructure and resources more effectively. If so, then TFP gains are likely to decelerate as investment continues to climb, which may to some extent offset productivity gains from the current growth in capital deepening.\n\nUnfortunately, we do not now have in hand the analysis necessary to distinguish between these two views, or to pin down their likely quantitative implications. As a result, there is significant uncertainty about whether productivity gains at elevated rates are likely to continue, or whether they will revert toward the longer run trend rate of around two and one quarter percent.\n\nThe importance of productivity growth for the economic outlook stems from its role as a link between aggregate demand and labor market conditions, and the consequent pressures on real interest rates. Slower productivity growth, for any given rate of demand growth, means more rapid employment growth and less downward pressure on wages, and could therefore necessitate a steeper-than-otherwise path for real interest rates.\n\nMy discussion of the outlook for 2005 has placed productivity trends rather than inflation at center stage, because it strikes me that greater uncertainty surrounds productivity over the coming year. This is not to say that inflation is unimportant or unaffected by swings in near-term productivity prospects of the sort I have been discussing. On the contrary, keeping inflation and inflation expectations well-contained is a central bank’s primary responsibility. But as a general principle — that is to say, looking beyond the current tightening cycle — real interest rates must fluctuate as economic fundamentals change, even in the absence of visible perturbations in inflation prospects.\n\nAs I noted earlier, inflation expectations have been relatively stable. For 2005, I expect inflation to come in between one and two percent, as measured by the PCE price index, and I expect inflation expectations to remain contained. The sequence of policy tightenings initiated in June seemed to check the rise in inflation expectations that emerged earlier in the year. Moreover, monetary policy is capable of preventing oil price increases, or changes in the foreign exchange value of the dollar for that matter, from showing through to the underlying inflation rate.\n\nIn summary, we are well-positioned for fairly healthy economic conditions next year. Real growth, led by healthy capital formation and solid consumer outlays, should boost employment more rapidly than the working age population grows, and inflation should remain well-contained. A central banker couldn’t ask for much more than that."
    }
]