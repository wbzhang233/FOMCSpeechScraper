[
    {
        "title": "U.S. Regulations and Approaches to Cryptocurrencies",
        "date": "Dec 12, 2019",
        "speaker": "Michael Held",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/hel191212",
        "content": "Thank you, Diego, for “volunteering” me to speak about digital currencies—a field in which I count myself as very much a trainee, not an expert. Today I will focus on the U.S. regulatory landscape for digital currencies, in particular on digital currencies issued by private organizations that are intended to be used like money. As always, the views I express are my own, not necessarily those of the Federal Reserve Bank of New York or the Federal Reserve System.1\n\nPolicy makers and regulators in the United States, to date, have not developed an overarching framework for regulating private digital currencies. The field has been seen as too new for a comprehensive regulatory response. To be sure, the digital nature of new private currencies will raise challenges to which policy makers must respond. In my view, however, we spend so much time wrestling with the novelty of digital currencies that we forget that private currency is nothing new. The theme of my talk today is accordingly best encapsulated by a quote that is attributed—perhaps wrongly—to Mark Twain: “History may not repeat itself, but it does rhyme.”\n\nThe Past Is Not Dead. It Isn’t Even Past\n\nSo, let’s take a little walk through the history of privately-issued currency.\n\nWe begin in Michigan in 1837, when the state legislature passed the first “free” banking law in the United States.2 Upon commencing business, free banks could issue banknotes—that is, private currencies—that were redeemable in specie—gold or silver.3 These banknotes were transferable debt backed by the general creditworthiness of the bank that issued them, plus assets like bonds and mortgages on real estate, and for a brief time, personal guarantees.4\n\nThe statute permitted bank organizers to establish a bank by filing an application with the local county treasurer and county clerk. They did not need approval from the state banking commissioner. (At the time, the United States had no federal banking supervisor.5 Indeed, the “free banking” era generally begins with Congress’s failure to recommission the Second Bank of the United States before its charter expired in 1836, and ends with the passage of the National Bank Act in 1863.) The result, predictably, was chaotic. The state banking commissioner was unsure of how many banks had even been established.6 Some banks in Michigan were established with the intent to issue banknotes but without the intent to ever redeem them.7 By 1839 almost the entire system had collapsed.\n\nAfter closing down one bank in Michigan, the commissioner found shards of window glass, lead, and nails where he should have found gold and silver coin.8\n\nSome of these free banks became known as “wildcat” banks. They set up offices in remote areas—where only the wildcats roamed—making it difficult to redeem notes for specie.9 In truth, the fate of free banks was not all bad. Free banks in New York apparently fared pretty well.10 But the experiences of Michigan and some other states demonstrated the risks in dealing with lightly regulated issuers of private currencies of that time. Banknotes of that era were consistently transferred at a discount,11 state-to-state requirements to value the assets that backed them varied,12 and banks were engaged speculative activities: building hotels, roads, railroads, and canals.13 I’m told they also engaged in banking.\n\nBy the 1860s Congress was fed up with the free banking system. In an address to the United States Senate in 1863, Senator John Sherman of Ohio decried:\n\n“We had every diversity of the bank system in this country that has been devised by the wit of man, and all these banks had the power to issue paper money. With this multiplicity of banks . . . it was impossible to have a uniform national currency, for its value was constantly affected by their issues. There was no common regulator; they were dependent on different systems. . . . There was no check or control over these banks. There was a want of harmony and concert among them.”14\n\nThe National Bank Act authorized the creation of national banks that would issue national bank notes. It was a push towards harmony in private currency, a pressing need for a wartime government in the throes of a civil war. Unlike wildcat banknotes, national bank notes were backed universally by federal government bonds and deposits of other money, such as U.S. government issued “greenback,” in each case held by the United States Treasury, and they circulated at parity with other forms of money.15 The National Bank Act also introduced the division between banking and commerce in the United States, which may have erased some of the more-speculative bank activities of the wildcat era.16\n\nFederal intervention and tighter regulation meant that national banks and their notes were both safer and more fungible than their wildcat predecessors, and the United States’ transition into national bank notes, coupled with new, unfavorable tax treatment for state bank notes, meant the demise of wildcat banknotes.17 Decades later, the establishment of the Federal Reserve System, a central bank currency, and a push for banks to engage in deposit-taking activities meant the overall demise of private currency in the United States for decades.18\n\nThe Child Is Father to the Man\n\nThis is, admittedly, a fairly crude history of money and banking in the United States, but you see where I’m going with this. It is hard not to see certain parallels between today’s digital currencies and the bank notes issued during the wildcat era. Digital currencies are developing in in the absence of a comprehensive regulatory system, and there are countless digital currencies, with countless idiosyncrasies, and countless entities of varying quality that can issue them. According to one source, there are currently nearly 3000 digital currencies but I doubt anyone really knows the true number.19\n\nWhile we have not seen issuers of digital currencies hold shards of glass, lead, and nails in reserve, we have seen digital currencies with no backing by design—Bitcoin being the prime example—digital currencies backed by other volatile digital currencies—for those who put a premium on safety—and digital currencies styled after Donald Trump, Vladimir Putin, and Kanye West—though Mr. West’s lawyers made sure that Coinye never really got off the ground.\n\nThe analogy between the current era and the wildcat era isn’t perfect, of course. History rhymes after all; it doesn’t repeat.\n\nIn contrast to the wildcat era, various federal regulators have demonstrated a willingness to apply existing regulatory tools to digital currencies. But, policy makers and regulators are also still monitoring developments in digital currencies, and we have not fully formulated a cohesive policy response. Whether we like it or not, however, we are beginning to confront at least two questions similar to those that policymakers addressed in the waning days of the wildcat era:\n\nAs I’ve already suggested, current U.S. law does not have direct answers to these questions. Digital currencies can fall into a variety of categories: commodities or securities or other instruments. How a digital currency is classified will depend partly on the functional use of the digital currency and partly on the priorities of various lawmakers and regulatory agencies that have the authority to regulate it.\n\nThe Financial Crimes Enforcement Network (FinCEN), the primary U.S. authority responsible for implementing U.S. anti-money laundering laws, was one of first authorities to issue guidance in the United States on digital currencies. FinCEN clarified in 2013 that transmitters and exchangers of digital currency are money service businesses under FinCEN regulations, if the digital currency acts as a substitute for real currency or has an equivalent value in real currency.20 Accordingly, any person that engages in the business of transferring or exchanging digital currency is subject to FinCEN requirements to establish an anti-money laundering program, which includes recordkeeping, reporting, and customer identification and verification requirements.21 FinCEN’s early entrance into the field sought to address the fact that digital currencies, like other forms of payment, may be used for nefarious purposes.\n\nFast forward to 2017. Jay Clayton, the Chairman of the Securities and Exchange Commission (SEC), the federal agency that regulates securities, released a statement on digital currencies and initial coin offerings (ICOs). In Chairman Clayton’s view, “tokens and offerings that incorporate features and marketing efforts that emphasize the potential for profits based on the entrepreneurial or managerial efforts of others continue to contain the hallmarks of a security under U.S. law.”22 In 2018, SEC staff published guidance to clarify that exchanges, investment vehicles, investment advisers, and dealers doing business in digital currencies that are digital asset securities would be subjected to securities laws.23 The guidance makes clear that digital currencies that function as securities will be subject to U.S. securities law, but where we draw this line may not always be clear.\n\nProducts that blur the boundaries between money and securities are not new. Take, for instance, certificates of deposit. In 1982, the United States Supreme Court analyzed whether a certificate of deposit was a security for purposes of certain U.S. anti-fraud laws. The Court concluded that it was not. In doing so, the Court recognized that the most important distinction between a certificate of deposit issued by a bank—money—and other long-term debt was the comprehensive regulation of the banking system.24 The Court’s conclusion was a sensible one more than a century after the “wildcat” banking era: Lawmakers could not have intended to treat money in the banking system like it was something else because there was no need to do so.\n\nAt least for now, the case does not appear to be as strong for digital currencies. Without a regulatory framework designed for digital currencies, regulators in United States are reacting to abuses instead of proactively addressing risks. As an example, the Commodity Futures Trading Commission (CFTC), the agency in the United States that regulates futures and derivatives contracts, used its enforcement authority over fraudulent purchases or sales of commodities to stop an alleged digital currency scheme.25 Organizers of the scheme claimed that the digital currency in question—My Big Coin—was backed by gold and actively traded, but the CFTC concluded that neither was true, that payouts to customers were funded with money fraudulently obtained from other victims of the scheme, and that the organizers misappropriated approximately $6 million in customer assets.26\n\nMy Big Coin appears to have been a modern wildcat currency, but the scheme was detected only because the CFTC could regulate it as a commodity. We are fortunate that, today, the CFTC has this market-based authority to combat fraud in U.S. commodities markets that was not present in the wildcat era.\n\nMeanwhile, banks, which are the focus of our federal regulatory efforts for holding and transferring money, have largely not engaged in holding, transferring, or <gasp> issuing digital currencies, and federal banking regulators in the United States have remained largely silent on the permissibility of these activities.27 The Office of the Comptroller of the Currency (OCC) sought to carve out a special-purpose charter—its so-called “fintech charter”—presumably to bring some of these activities under the OCC’s regulatory umbrella, but it has since been mired in litigation over issuing these special-purpose charters with states in the U.S. that wish to license these activities under local law.28\n\nSeparately, the Bureau of Consumer Financial Protection, which has authority to write rules governing certain consumer accounts and payment products, determined it would further analyze digital currencies and did not try to bring them into its purview in a recent amendment to its Regulation E.29\n\nTo be sure, states within the U.S. have begun to adopt laws and regulations for digital currency businesses. New York and Wyoming, to cite two examples, have adopted digital currency-specific regimes. There is, however, little coordination in the approach at the state level,30 which highlights that a state-by-state approach risks both inconsistent treatment and the possibility of a “race to the bottom” on licensing standards that could be reminiscent of the wildcat era. The challenges states will face in crafting sensible rules are only amplified with respect to digital currencies, which tend to operate without borders.\n\nIf you were a worrier like me, would you worry that we are putting off alternative approaches until new problems exhaust old solutions? Would you worry about how effectively we will respond if a digital currency achieves critical mass? Governor Quarles highlighted this latter concern in his recent remarks at the European Banking Federation.31 He noted that stable coins do not pose risk to financial stability today but that they bring with them the potential for scale that could pose regulatory challenges from a financial stability standpoint in the future.\n\nBut knowing that regulators in the U.S. are already dealing with concerns over fraud and the prospect of other forms of misconduct related to digital currencies, and acknowledging Governors Quarles’ concerns about financial stability, would it behoove us to start identifying areas of focus for crafting a regulatory response that is more proactive?\n\nPablo Hernández de Cos, Chairman of the Basel Committee on Banking Supervision (BCBS) and Governor of the Bank of Spain, remarked in November that digital currencies “do not reliably perform the standard functions of money and are unsafe to rely on as a medium of exchange or store of value.”32 That sounds like a problem.\n\nWe have dealt with a somewhat similar problem before. Under the National Bank Act, the United States chose to reform its currency system by adopting a new regulatory regime for national banks—the would-be issuers of stable private currencies (that is, national bank notes)—and by crafting requirements for these issuers to backstop their currencies with safer government-issued bonds. These steps served to stabilize private currency issuers and their currencies, but there were other benefits of the approach that are easy to overlook: we disincentivized less desirable products—wildcat notes (that is, by imposing taxes on them); certain key participants in the system—specifically the custodian banks and trust companies—were already regulated institutions; bank notes had a certain level of transparency built in—a holder’s right to receive payment and from whom was literally written on the face of the note; and for all intents and purposes, federal law and regulation were ubiquitous.\n\nWe have also developed comprehensive regulatory schemes targeted at other widely-used financial products as they have evolved, including recent work to regulate swap transactions. If digital currencies reach critical mass, we may want to draw on lessons from these efforts and consider whether tools for regulating systemically important activities are fit for purpose. The designation process under the Dodd-Frank Act for systemically important financial institutions may be instructive here, regardless of whether it is directly applicable to digital currencies. In this process, the Financial Stability Oversight Council (FSOC) has the authority to designate for enhanced regulation and supervision certain financial institutions, financial market infrastructures, and payments, clearing, and settlement (PCS) activities, to the extent that FSOC determines the organization or activity to be systemically important. Though the authority in the Dodd-Frank Act is limited in some ways—it applies only to certain systemically important activities and the designation process requires coordination across regulatory agencies and related administrative processes—the scheme is flexible in other ways because it contemplates an activities-based approach to regulation and oversight in addition to a prudential approach for certain institutions.33 To be crystal clear, I am not saying that any digital currencies are currently ripe for consideration under the FSOC designation process; just that a similarly flexible though not necessarily identical process involving coordination and collaboration across multiple regulators may be warranted.\n\nThe Path Forward\n\nIn applying lessons learned from the past to the modern era, one path forward might be to consider the following questions in determining whether a broader regulatory scheme is sensible:\n\nThis is not easy stuff. Some might look at this list, and want a faster, ready-made solution. I can hear them now: “Why not just put this activity in the banking system and borrow a more robust regulatory structure?” I think we should consider that, but bank regulation may not be fit for purpose. History rhymes; it doesn’t repeat—that’s the last time I will use that line. So, at the same time we consider what banks will or should be going forward, we also should consider some more fundamental questions: (i) What is the most practical approach? (ii) What approach gives us the right balance of functional and risk-based regulation? (iii) What approach is the most neutral to technological change? (iv) What approach provides the clarity to the greatest extent possible (recognizing that every model will have its faults)? (v) What approach will last to and through the next cycle of innovation?\n\nI mentioned the SEC earlier. The SEC analyzes digital currencies using a test that the Supreme Court formulated almost 90 years ago—that is mind boggling, but for what the SEC does, it is also still very relevant—this is something worth striving for.\n\nYou might be surprised that I don’t have the answer to what the next 90 years of digital currency regulation will look like, but I hope to have the opportunity to explore this issue with some of you. Thank you."
    },
    {
        "title": "No Man Is an Island",
        "date": "Nov 14, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil191114",
        "content": "“No man is an island entire of itself,” wrote the poet John Donne in 1624. Almost four centuries later, his words still ring true. And they are particularly salient when it comes to the financial fortunes of the global economy.\n\nToday I’m going to explore the commonalities and interconnectedness of the global economy, and what that means for the outlook in the United States.\n\nBut before I delve into these ideas, I want to say what a pleasure it is to be back at the San Francisco Fed, among so many friendly faces. The Asia Economic Policy Conference is always a fascinating day, and I’d like to thank Mary Daly for the invitation to speak, and Zheng Liu and Mark Spiegel for putting together an outstanding program.\n\nI knew I was back in San Francisco when this morning I spent 20 minutes in line, waiting to spend five dollars on a cup of coffee. This is one thing for which inflation appears to be alive and well.\n\nBefore I explore issues like inflation in any greater depth, I should give the standard Fed disclaimer that the views I express today are mine alone and do not necessarily reflect those of the Federal Open Market Committee or others in the Federal Reserve System.\n\nThe U.S. Economy\n\nIf monetary policy represents a set of scales, on one side we have a strong U.S. economy with unemployment near a 50-year low, and on the other, a more challenging and uncertain landscape.\n\nGDP looks to grow around 2 percent this year, and unemployment has stayed at or near historically low levels, boosted by solid job growth. At the same time, the Federal Open Market Committee (FOMC) has moved to cut rates three times. What’s going on?\n\nThere are three trends I’ll explore today, which all demonstrate the commonalities and the interconnectedness of the global economy.\n\nThe first is the low neutral rate of interest, or r-star, the second is slowing global growth, and the third is rising geopolitical tensions. All are critically important to the long-term prospects of the U.S. economy, and each tells an important story about the conditions we’re experiencing today.\n\nLow R-star\n\nThose of you who know me won’t be surprised that I’m starting with low r-star! The neutral rate of interest—the rate that prevails when monetary policy is neither restrictive nor accommodative—has been an area of study, and a personal passion, for the past two decades.\n\nI could talk about r-star at great length, but you may be relieved to hear that for the purpose of this speech, there’s just one point I want to make: r-star has fallen, not only in the U.S., but around the world.\n\nFundamental shifts have been occurring in the two main drivers of r-star over the past two decades: demographics and productivity growth. Lower fertility rates and aging populations, as well as slower productivity growth, have pushed down r-star by as much as two percentage points in a number of developed nations.\n\nThese structural shifts are having fundamental effects on real interest rates, growth, and inflation. A number of advanced economies have inflation below desired levels and longer-term interest rates that are well below where they were a decade ago.\n\nThere are, of course, a few outliers, but the commonalities between the world’s developed economies are becoming more and more apparent.\n\nThere’s no doubt that the U.S. is experiencing first-hand the effects of the structural shifts in demographics and productivity growth. But the fact that these changes are a common experience for so many developed economies creates challenges to our economic prospects at home.\n\nSlowing Global Growth\n\nWhich brings me to the second trend I highlighted: slowing global growth. This story—of low interest rates, low inflation, and slow growth—is showing up in the data.\n\nThe latest numbers from the International Monetary Fund (IMF) follow the recent trend of steady downward revisions of growth estimates. Their forecast for global growth for 2019 is now 3 percent, which is the lowest since 2008–09, and it’s not hard to think of scenarios in which growth slows even further.1\n\nIn the old days, central bankers used to worry about inflationary pressures being too great. Old habits die hard, and I’m always on the lookout for signs that inflation is rising too fast. But in the current climate it’s inflation that’s too low that’s of greater concern.\n\nYou see, my five-dollar cup of coffee masks an inconvenient truth: inflation in the United States is below target and has been for some time.\n\nBeyond sluggish inflation, we’re starting to see signs of slower global growth playing out in the U.S. data. While consumer spending is holding up, other parts of the economy—including manufacturing, exports, and business investment—have seen growth stall or experienced outright declines.\n\nGeopolitical Tensions\n\nThe third and final issue I want to discuss—the rising geopolitical tensions around the globe—further complicates an already complex picture.\n\nWhen it comes to the economic outlook, the word of the day—and probably the year—is “uncertainty.”\n\nTrade negotiations, civil unrest, and tensions in a number of corners of the world are driving many businesses to take a wait-and-see approach to investment, thus putting a further dampener on growth prospects.\n\nIt’s notable that later in the same poem I used to open my speech, John Donne writes, “if a clod be washed away by the sea, Europe is the less.”\n\nThe ongoing uncertainty around Brexit and its eventual outcome is a cause of concern not just for the UK economy, but many of its European trading partners. It’s impossible to predict what the spillover effects will be when Brexit finally reaches a conclusion.\n\nWe face similar uncertainty on our own shores regarding trade tensions. I frequently hear from my business contacts that they’re holding off on making investments until the landscape is clearer.\n\nAnd the ongoing uncertainty in both of these situations, rather than the day to day ups and downs, has become a key driver of business decisions.\n\nTrade negotiations and Brexit are top of mind when it comes to uncertainty. But geopolitical risks and political upheaval are affecting regions across the globe.\n\nIt’s striking that in almost every corner of the world geopolitical tensions are threatening to put the brakes on growth. Even if the outcomes of current negotiations are ultimately positive—be they between the U.S. and China, or the UK and the European Union (EU)—the uncertainty created by current events is no doubt having a lasting effect on the economic conditions we’re experiencing today.\n\nFurthermore, the interconnectedness of our economies means that literally, no man is an island. If one economy starts to struggle, the spillover effects onto others can take hold rapidly.\n\nThe common thread of geopolitical tensions is that while they’re difficult to predict, they’re also impossible to ignore. On a backdrop of low r-star and slowing global growth, they drive businesses to be more cautious, more vigilant, and more conservative in their approach.\n\nData Dependent, Not Data-Point Dependent\n\nThe long-term factors driving low r-star and slowing global growth, as well as the more near-term geopolitical tensions, are afflictions from which we all suffer.\n\nSo what’s the solution? Where do we as policymakers, researchers, and regulators go from here?\n\nAt press conferences I’m often asked: if growth slows by X percent, or unemployment increases by Y percent, will that mean that the FOMC is going to do Z at its next meeting?\n\nIf the Fed had a motto, it would probably be “data dependent.” When asked about this, it’s important to clarify that while I’m data dependent, I’m not data-point dependent.\n\nIt’s not about being guided by one single data point—GDP or job growth—but by all of the data and information that tell us where the economy is, and where the economy is heading. Importantly, it’s not about looking at things merely as they are—it’s about trying to read how the economy is likely to evolve over the next couple of years, and what risks lie ahead, based on a broad range of evidence.\n\nMonetary policy is a long game. The policy decisions we’ve made this year will continue to have a ripple effect years into the future. As policymakers, we need to take in how far-reaching those ripples are going to be.\n\nMonetary Policy\n\nThe adjustments to monetary policy we made this year were designed to balance maintaining a strong U.S. economy with slowing global growth, and provide insurance against ongoing and potential future risks.\n\nAnd that’s what they’ve done. The economy is in a good place, and monetary policy is as well. My forecast is for moderate GDP growth, the labor market remaining strong, and inflation moving back to our symmetric 2 percent target. Of course, things can change. Data dependency remains our motto, and if there were a material change to this outlook, we would adjust monetary policy in support of our goals of maximum employment and price stability.\n\nConclusion\n\nI’ll conclude with this. John Donne’s words are perhaps more relevant in 2019 than they were in 1624. No man is an island, and no economy can insulate itself—for better or worse—from the fortunes of its neighbors.\n\nAt the same time, all economies are having a common experience as a result of longer-term structural changes.\n\nShifts in demographics and productivity growth point clearly in the direction of low neutral interest rates and slowing growth. We’re seeing the reality of these changes in the form of sluggish inflation and interest rates much lower than in previous decades.\n\nCentral banks and policymakers around the world need to prepare for economies that are more connected than ever before.\n\nI’ll now take your questions.\n\nAddendum\n\n'No Man is an Island'\nNo man is an island entire of itself; every man\nis a piece of the continent, a part of the main;\nif a clod be washed away by the sea, Europe\nis the less, as well as if a promontory were, as\nwell as any manner of thy friends or of thine\nown were; any man's death diminishes me,\nbecause I am involved in mankind.\nAnd therefore never send to know for whom\nthe bell tolls; it tolls for thee.\n\nJohn Donne"
    },
    {
        "title": "The New York Fed’s Work on Financial Institution Culture",
        "date": "Nov 14, 2019",
        "speaker": "James P. Bergin",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/ber191114",
        "content": "Thank you for the opportunity to speak to you today. I will discuss why the culture of the financial industry is important to the New York Fed, the nature of the problem we are trying to solve, and some of our various initiatives in this area. Please note that my comments today are my own and do not necessarily represent the views of the Federal Reserve Bank of New York or the Federal Reserve System.1\n\nWhy Organizational Culture Matters\n\nMy perspective on culture has been shaped by my experience at the New York Fed. I currently serve as Deputy General Counsel, but have had the opportunity to work in other non-legal roles over time as well. The New York Fed is an interesting place to work, because we wear a number of different hats. Among other things, we implement monetary policy for the Federal Open Market Committee; we supervise financial institutions as part of the broader Federal Reserve System; and we run a major wholesale payments system. In all of these missions, we need to work on and through the financial system to promote the well-being of the real economy. So it matters greatly to us that the financial system is operating in a way that is safe, fair and transparent, and that serves to intermediate effectively between borrowers and savers to the benefit of all, not just a narrow few.\n\nUnfortunately, we have not always seen that. I started at the Fed in 2005, in the years before the financial crisis. Since that time, we have seen numerous misconduct scandals plague the financial markets in which we operate and upon which we rely. In 2013, misconduct in foreign exchange markets, following closely on the heels of similar misbehavior in reference rate markets, moved our former President, Bill Dudley, to remark upon the “deep-seated cultural and ethical failures at many large financial institutions.”2 He set us upon a work program to address financial institution culture. This emphasis has continued and evolved under our current President, John Williams. And, it has been taken up by many other authorities around the world.\n\nMisconduct by financial institutions can cause out-sized damage. The most important harm of misconduct is on the consumers, counterparties or competitors directly affected, but the ripple effects can extend far beyond. Misconduct can affect the stability of the firms that engage in it. There is the immediate, pecuniary effect of sanctions on the firm’s balance sheet but, just as importantly, there is the damage to an individual firm’s reputation—a loss of trust by consumers, investors, regulators, and the general public.3\n\nBeyond the individual firm, there is a broader cost to the markets themselves if the public loses confidence in their fundamental fairness. We are seeing right now an enormous and global public and private effort to move off of the use of LIBOR as a reference rate. This costly effort resulted, in part, from exploitation of flaws in the design of that crucial rate by unethical traders for their own gain. More broadly, when a scandal of this dimension happens, it calls into question whether the financial system is doing the job the public expects. The resulting lack of trust can impede the ability of the financial system to perform its intermediation function, and this in turn affects the well-being of the real economy. For example, turning back to reference rates—they affect almost everyone, whether you need credit to buy a house or a car. When those rates are manipulated, it has an effect outside of the financial markets themselves.\n\nThe Nature of the Culture Problem\n\nI think this recognition—that misconduct always has a tail that hits the real economy—has led to the New York Fed working to improve financial institution culture. That said, I think people also have recognized that this is a type of problem that differs from some others that we tackle at the central bank. Two differences are worth discussing.\n\nFirst, and very importantly, organizational culture is an issue that the institutions themselves need to solve. Legislators and regulators set the rules for financial firms to follow. These set the legal boundaries of behavior. Regulators and enforcement agencies can, should, and do penalize firms when they cross those boundaries, as we have seen time and again over the past several decades, including in the foreign exchange and reference rate contexts. But regulators are not in a position to prescribe one model of organizational culture that will lead firms to avoid crossing these legal boundaries in the first place. That said, I will also note that authorities in other countries have used regulation innovatively to improve conduct standards, short of prescribing a model culture. I understand, for example, that the Senior Managers and Certification Regime here in the United Kingdom combines enforceable conduct rules with enhanced vetting and accountability requirements for directors and senior officers.\n\nPractically speaking, though, there will always be a need for individuals to exercise ethical judgment when interpreting the rules that apply to them, regardless of how detailed the regulation is which applies to a particular market. It is an organization’s and an industry’s culture which sets the norms that guide that judgment, and that is where firms and the industry at large need to step up. Jay Clayton, the Chair of the U.S. Securities and Exchange Commission called this “filling in the gaps” in a visit to the Fed last year. He explained that even if the law permits a range of actions, that range is not the end of the road for ethical decision-making. Some choices, even if possibly permissible, may be wrong, and may be more likely to get you and your firm in front of a governmental authority—and not in a good way. I am a member of the legal profession, and a proud one, but the law only has so much imagination—it cannot cater for every possible situation that an individual working in financial services might find themselves in. There is a need to fill in the gaps between the outer boundary of what is allowed and what is wise from a firm, system, and ethical perspective.\n\nWhere a decision about what to do or not do can cause significant harm, those on the front lines need a touchstone to guide their choices.4 These gaps are not a failure of regulation. They are a natural consequence of the complexity of many financial services markets, which are international, multi-faceted and constantly evolving. Even for markets that are structurally less complex, the behavioral norms that underpin them can be complex because (at least until the robots take over) it is ultimately imperfect human beings who deliver financial services. That can be a strength when ethical judgment is exercised responsibly, and a weakness when not.\n\nIt is that complexity that is the second aspect of why culture is such tricky terrain. At this conference last year, my colleague, Kevin Stiroh, the head of our Supervision Group, explained that culture reform is a \"complex\" issue—meaning it is marked by the interconnectedness of a large number of factors, constant evolution, feedback loops, “unknown unknowns,” and unpredictable outcomes.5 But you do not throw up your hands because culture is somehow unknowable and squishy, or because people will always be people, and hard to understand and control. That is not the approach that we take to other important problems, and it would be odd if it was adopted here. You need be focused, determined and persistent in order to wrestle the problem to the ground. But with a complex problem, there may not be one linear path to resolution. Instead, you may need to work several margins at once, embracing a number of approaches and tools, and you must be prepared for your strategy to evolve over time. Diverse perspectives, professional training, and life experience are also critical to challenging assumptions and pointing out uncomfortable truths.\n\nNew York Fed’s Culture Initiative\n\nThese two aspects of the organizational culture issue—that it is not an area where the official sector can simply dictate everything, and that it is, by nature, a “complex” problem—have informed the approach that the New York Fed has taken to it. At the New York Fed, we believe that the official sector has a vital role to play in helping industry to achieve positive change via the raising of behavioral standards. Our President, John Williams, describes this role using three principles—“connect,” “convene,” and “catalyze.”6 Connecting people with the latest data and research gives them the insights and tools they need to make change. “Convening” means bringing together stakeholders to share ideas, perspectives, and approaches. “Catalyzing” is about using the New York Fed’s unique position to create change.\n\nI’ll start by describing our efforts to “convene” and “connect.” The New York Fed has hosted five major culture conferences7 which proved to be valuable fora to exchange ideas, challenges and approaches. We first focused on recognizing that the industry has a culture problem, exploring drivers and outcomes of that problem, followed by a workshop in which representatives of the major firms provided an update on progress and challenges in different elements of culture change—performance management, incentives, accountability frameworks, and leadership. Over the years, we have increased the diversity of voices that are represented—because a complex problem needs complex solutions. Philosophers, economists, psychologists, industry leaders, government officials and many others have contributed valuable perspectives to the conversation.\n\nIn addition, we regularly host a Supervisors Roundtable for Governance Effectiveness. This is a group of around 20 supervisory agencies from 15 jurisdictions around the world who share emerging and evolving practices for supervisors to use in the assessment of culture and behavior at firms. We are always keen to engage with and learn from our international counterparts, and we watch with interest the various initiatives that are in train such the FCA’s Culture Sprints, which aim to empower mid-level managers to find innovative solutions for culture transformation.8\n\nOne of our newer initiatives, that we are quite excited by, is the Education and Industry Forum on Financial Services Culture. This forum grew out of meetings that we convened with business schools and industry leaders to talk about how the business schools could educate the ethical judgment of future bankers. The aim of the “EIF,” as it’s known among my colleagues, is to provide regular opportunities for senior representatives from business schools and financial institutions to identify challenges with regard to culture and conduct, and identify possible areas of collaboration to address them.\n\nWe are currently working on a series of case studies of ethical dilemmas faced by junior employees, which would become an integrated part of business school curricula, undergraduate economics courses, recruiting, and employee training. The goal is for these case studies to help to raise awareness of, and hone, ethical judgment in a practical context, working collaboratively with some of the key communities (business schools and employers) that individuals starting out in the financial sector are part of. Our aim is to reach the next generation of leaders in finance.\n\nFilling in the Gaps\n\nFinally, we have been heavily involved in the last several years in promoting the development of industry best practices and codes. The point here is to “fill in the gaps”—to provide touchstones around which individuals at firms can orient their behavior when there is not a specific law telling you to stop or to go, but when there is some market integrity interest potentially at stake. The goal is to convene industry in such a way that they can raise the bar for their own behavior, both individually and organizationally.\n\nOur efforts have focused particularly on those markets that we operate in as a central bank. For example, we sponsor a Treasury Market Practices Group9, comprised of a diverse set of participants in the U.S. Treasury market. Over time, the TMPG has articulated recommendations on a variety of matters, such as how to manage large positions with care, and how to handle confidential information appropriately. This group has remained active and engaged over time, continuing to make sure they are articulating appropriate standards of behavior in various corners of the market.\n\nWorking closely with our international counterparts, we have also been heavily involved with the development and launch of the FX Global Code. This was a significant effort set in motion by central bank governors around the world in the wake of the foreign exchange rate manipulation scandals. Facilitated and guided by central bank staff, the Code represented a cooperative effort by participants in foreign exchange markets around the world to articulate and raise standards of behavior. It represents a single global standard of good practice in this market that is crucial to the real economy, and has been endorsed by the foreign exchange committees in 17 jurisdictions across the world.\n\nImportantly, from my perspective as a lawyer in the United States, these practice codes are not law; they have not been set as a minimum standard of behavior using governmental authority. This is a feature and not a bug. I fully believe in the power and importance of law, regulation and robust enforcement; but these practice codes represent another tool in the toolkit. They are an opportunity for market participants to articulate and embrace better standards of behavior for themselves. They can also provide an anchor for compliance and risk officers who can play a vital role in raising conduct standards, and a benchmark for testing whether those standards are being achieved.\n\nThose firms and market participants that are serious about seeking to reduce misconduct risk can face a coordination problem, as short-term competitive pressures can make it difficult for individual institutions to work together to make long-term investments in cultural capital if other firms are not also making the same investments. Such coordination failures can prevent private actors from achieving a common objective, even if it is in their collective best interest—and again, if they are actually committed to change.10 Our facilitation of practice code efforts is a mechanism toward overcoming that coordination problem. While the realization of that opportunity depends upon the will of market participants to engage and implement change, the official sector can play a role in catalyzing that change.\n\nConclusion\n\nTo conclude, I would like to reiterate a message from John Williams, the current President of the New York Fed. In a recent speech, he stated that while the journey toward reforming culture in the financial services industry is a long one, we are optimistic that we can and we will make progress.11 With that in mind, I look forward to further innovation, adaptation and iteration of the various tools that we have to tackle this very complex issue.\n\nSlide Presentation"
    },
    {
        "title": "Emerging Issues for Risk Managers",
        "date": "Nov 7, 2019",
        "speaker": "Kevin J. Stiroh",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/sti191107",
        "content": "Introduction\n\nGood morning and welcome to the GARP Global Risk Forum. The Federal Reserve Bank of New York has been collaborating with GARP on this forum for a decade and it is always a pleasure to participate. This discussion provides real-time insight into critical issues at the center of the practice of risk management.\n\nA quick scan of today’s agenda highlights a number of topics of obvious importance—operational resilience, cyclical aspects of risk management, technology and big data. We are all thinking about these issues, and I look forward to your insights.\n\nI’d like to briefly discuss two topics where I expect risk managers will increase their focus in coming years—climate change and the cultural aspects of digital transformation. Both issues reflect potentially transformative trends for the economy and the financial system, and I believe they will take an increasing share of your bandwidth over the next decade. In both cases, the relevant questions from a supervisors’ perspective are: what new risks are introduced? How do firms manage them? And how do supervisors view them?\n\nBefore proceeding, I will emphasize that I am speaking for myself and these views do not necessarily reflect those of the Federal Reserve Bank of New York or the Federal Reserve System.\n\nClimate\n\nThe U.S. economy has experienced more than $500 billion in direct losses over the last five years due to climate and weather-related events.1 In addition, climate change has significant consequences for the U.S. economy and financial sector through slowing productivity growth, asset revaluations and sectoral reallocations of business activity.2\n\nFor these climate-related risks, a new lexicon has emerged. Physical risk is the potential for losses as climate-related changes disrupt business operations, destroy capital and interrupt economic activity. Transition risk is the potential for losses resulting from a shift toward a lower-carbon economy as policy, consumer sentiment and technological innovations impact the value of certain assets and liabilities. These effects will be felt across business sectors and asset classes, and on the strategies, operations and balance sheets of financial firms.\n\nRisk managers must be aware of these risks and develop the tools needed to identify, monitor and manage them appropriately. Physical and transition risks may manifest through traditional risk stripes such as credit, market, operational, legal and reputational risk, but today’s tools are not necessarily well-suited for the unique challenges associated with climate change. Let me mention three of these challenges.\n\nFirst, climate change is a long-term issue where actions today are likely to have an impact over many decades. This exceeds the typical life span of a bank exposure, as well as the typical control and planning horizon of a financial institution. Risk management tools, models and scenarios are not designed to capture the long-term nature of climate-related risks.3 Nonetheless, real impacts are already being felt and we must develop the tools to assess and manage them.\n\nSecond, climate change is complex—the impacts are uncertain, non-linear and hard-to-predict. Trusted credit models, for example, may prove less useful if the historical patterns of the frequency and severity of weather-related shocks become a poor guide to the distribution of future losses. Asset prices may move sharply, and unpredictably, as consensus views change and future impacts are priced today. Moreover, transition risks may stem from second-order, indirect effects like the impact on supply chains, sectoral shifts or worker productivity, which are inherently difficult to model and predict.\n\nThird, we face significant data gaps. Climate change is a global phenomenon, but risks should be assessed locally, which requires new data such as asset-specific, geo-spatial data. Borrower-specific analysis requires data, for example, on asset utilization and climate effects for particular industries at particular locations. Such granular data is currently limited and requires significant resources to acquire and process, but greater transparency about the physical location of assets will allow institutions to better assess exposures to at-risk borrowers or to specific geographic risks.\n\nAs supervisors, we can consider climate-related risks in terms of both microprudential and macroprudential objectives. To be clear, in my view, supervisors should take a risk management perspective, not a social engineering one. It is beyond our mandate to advocate or provide incentives for a particular transition path. Rather, supervisors should focus on the risks that emerge along the path decided by the public at large and their elected governments. Supervisors can use our tools to ensure financial institutions are prepared for and resilient to all types of relevant risks, including climate-related events.\n\nTo do this, we must continue to invest in research to better understand the economic and financial impact of climate-related events. For example, on Friday, the Federal Reserve Bank of San Francisco will host a conference on the economics of climate change. We can also continue to follow closely and learn from the approaches taken by other central banks, including the use of scenario analysis, stress testing and supervisory expectations to encourage institutions to factor climate risk into their risk management practices. Finally, we can seek to strengthen the data and modeling capabilities we need to assess climate-related risks as part of our forward-looking, data-driven supervision.\n\nGiven the far-reaching impact of climate risk on the structure of the economy and the financial sector, I am confident that risk managers will respond appropriately and firms will continue to build resilience to all risks. GARP, for example, is exploring this and has recently surveyed its members to better understand professional needs for “sustainability, climate risk, and ESG integration.”4 I am interested to hear from all of you about how your institutions are developing tools and practices to integrate climate risk into your risk management frameworks.\n\nInnovation and Culture\n\nIt is no surprise to this audience that technological innovation, big data, and advanced analytics are transforming financial services. We can debate where we are on the maturity curve and what innovations are most likely to be transformative, but the direction of travel seems clear. I expect the afternoon session on “Technology, Big Data, and the Practice of Risk Management” will address core issues such as new approaches for model risk management in a world of widespread artificial intelligence; the inherent challenge of “explainability” and potential bias in algorithms; the strategic risks from new entrants; and the impact on human capital strategies.\n\nI want to highlight a complementary, but different issue and focus on how firms manage these changes. More broadly, what type of corporate culture is needed to support a successful digital transformation?\n\nOver the past five years, the New York Fed has emphasized the importance of a healthy culture to reduce misconduct risk. In addition, I think there is likely a link between a firm’s culture and its ability to adapt to a changing environment, whether it is technological innovation, demographic change, new business models, or climate change. Firms need a culture that allows change and is resilient to these trends.\n\nIn discussions of the impact of technology on financial services, for example, one extreme view is that new fintech entrants—either smaller start-ups or big-tech firms—will leverage technology and a customer-centric approach to disrupt the industry and outcompete stagnant incumbents, fundamentally transforming the industrial organization of financial services. An alternative, equally extreme view, is that incumbent financial firms with established customer bases, scale, vast quantities of data, and experience working in a regulated environment will “disrupt themselves” and maintain their central position in finance. The most likely outcome, as is typically the case, is somewhere in the middle, but change seems inevitable.\n\nIf all firms face a strategic imperative to innovate and adapt, a relevant question is what types of firm-specific factors will determine success? Clearly, things like business model, technological expertise, and resources will matter. I think that firm culture will also matter.5 And by culture, I mean the values and shared norms within a group that influence individual behavior, collective decision-making and outcomes.\n\nAs firms innovate and transform how financial services are provided, I expect we’ll see the value of leadership from senior managers and boards as they outline the strategic case for change and exhibit the critical behaviors that promote it. I expect we’ll see the value of innovative environments where employees speak up, both raising new ideas and questioning assumptions. I expect we’ll see the value of cognitive diversity as firms tackle complex problems like a digital transformation from multiple perspectives. I expect we’ll see the value of agility that facilitates product innovation, reprioritization, constructive partnerships, or the successful integration of financial market specialists and technologists. Finally, I expect we’ll see the value of well-crafted incentives where employees are rewarded based on both “what” they do and “how” they do it.\n\nMore broadly, I think firms with high levels of “cultural capital” will be more likely to successfully implement a digital transformation in a sustainable, prudent and responsible manner.6 This is a central question for risk managers and supervisors because the evidence suggests that most large-scale transformations fail.7 Just like firms, investors and supervisors consider execution risk when assessing bank mergers, I expect more attention will focus on culture and the change management risks associated with innovation and digital transformation.\n\nIn my view, the official sector can facilitate this transition. The 2018 guidance on responsible innovation related to anti-money laundering and terrorist financing is a good example.8 Supervisors can also press on the governance and change management framework to promote effective internal oversight. From our perspective, supervisors also need to be nimble and develop the necessary skills to assess innovation as firms transform.\n\nWhile a firm’s culture is not directly observable, I believe this will prove to be a critical part of any change initiative. Technological progress, innovation, and new models of competition are changing the financial industry and there will undoubtedly be winners and losers. Understanding all of the factors that drive successful transformations will be a critical insight for both risk managers and supervisors.\n\nConclusions\n\nThe financial sector is large, complex and constantly evolving due to a wide range of technological, business, and macro factors. Your job, and ours, is to be vigilant and open-minded in our thinking, constantly questioning our traditional assumptions and scanning the horizon to understand how the risk landscape is changing.\n\nThe topics covered at today’s conference are some of the most important risks that the industry faces. Climate change and digital transformation are two powerful trends with the potential to fundamentally change financial services. We all need to be forward-looking and proactive in order to promote our ultimate goal of a safe and sound financial system that supports the sustainable provision of financial services and economic growth.\n\nThank you for your attention."
    },
    {
        "title": "The Gold Beneath Our Feet",
        "date": "Nov 6, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil191106",
        "content": "When plans for the New York Fed’s headquarters here at 33 Liberty Street were developed in 1921, the building committee compiled 33 pages of requirements for potential architects.1 I can assure you that our appreciation for bureaucracy has endured the test of time!\n\nJoking aside, the focus of many of those requirements was specifications for the building’s gold vault. Beneath our feet lies a triple-tiered vault system with 122 separate compartments for storing gold bars. For those of you who are Harry Potter fans, it’s as close to Gringotts Bank as you’ll find anywhere in the world—although for security reasons I am not allowed to confirm or deny that we have a dragon down there.\n\nThe whole building was constructed around the vault. Engineers had to excavate 117 feet down to the bedrock for a foundation that could support the weight of the construction.\n\nWhy am going into so much depth about the gold? There is a reason!\n\nBut before I explain, I should give the standard Fed disclaimer that the views I express today are mine alone and do not necessarily reflect those of the Federal Open Market Committee or others in the Federal Reserve System.\n\nWhen the building was planned and constructed during the early part of the twentieth century, the gold was seen as a real vulnerability. It was conceivable that someone would try to rob the Bank and steal the gold.\n\nThe only time that’s happened was in the Bruce Willis movie Die Hard 3, which, to be clear, was only a movie!\n\nA lot of that owes to the skill of the architects of the building, and the law enforcement who protect it. But the other reason is that the risks we face today in 2019 look very different to the ones we were planning for in 1921.\n\nFor a thief today, it’s easier to use a laptop to buy some stolen credit card details online and commit fraud than it is to plan a Hollywood-style bank heist.\n\nAnd that’s what I’m going to say a few words about—the changing nature of risk—before handing over to this evening’s keynote speaker, John Dugan, Chair of Citigroup.\n\nIn previous decades it was easier to identify risk and what to do about it. You could see it, measure it, and fortify against it. But, as the world has changed, so have the risks that demand our attention and our resources. The new risks are harder to predict and inherently ambiguous: Steel and concrete can do little to protect us.\n\nOur reliance on technology means we need to think differently about cyber risk. The importance of innovation and intellectual property means we need to look for new approaches to talent and culture. And the threat posed by climate change means we need to give serious consideration to the resilience of our physical and digital infrastructure.\n\nThe New York Fed has the appearance of an historic institution, but behind the stone façade we’re grappling with these issues in the same way that you are. In our economic research, our supervision of financial institutions, the payments systems we operate, in short – in all that we do.\n\nTalent, technology, and innovation challenge us every day. They create incredible opportunity, but they also test us by exposing new vulnerabilities.\n\nI know that over the next couple of days there will be lots of valuable insights shared by experts about how to handle the challenges posed by big data, cryptocurrency, and climate change.\n\nBut first, to kick off the conference, it’s my pleasure to introduce John Dugan, Chair of Citigroup. Citigroup has operations in over 150 countries, with about 200 million customer accounts worldwide.2 As head of a global employer with complex, international operations, John is no stranger to the changing nature of risk.\n\nJohn became Chair of Citigroup at the start of this year. Prior to that he was chair of the Financial Institutions Group at the law firm Covington & Burling LLP, where he spent much of his career. He has also held a number of public sector roles, including Comptroller of the Currency during a critical time for our financial system and economy, from 2005-2010.\n\nI’m looking forward to his insights and I know they will be the catalyst for some rich discussion.\n\nPlease join me in welcoming John Dugan."
    },
    {
        "title": "Thrive in Any Environment: Strengthening Resilience Through Risk Management",
        "date": "Nov 6, 2019",
        "speaker": "Joshua Rosenberg",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/ros191106",
        "content": "Good morning and thank you for the opportunity to speak to you today. Let me begin by saying that the views I express are my own and do not necessarily represent those of the Federal Reserve Bank of New York or the Federal Reserve System.\n\nThe ability to thrive in any environment, in both normal and chaotic conditions, is an organizational imperative. We can strengthen organizational resilience—“the ability to prepare and plan for, absorb, recover from, and more successfully adapt to adverse events”—through risk management.1 That’s because the tools of risk management help us make plans and take action today in the face of an uncertain tomorrow. In fact, we can deliver even more value to our organizations by defining resilience as a goal of risk management.\n\nToday, I will talk about some of the challenges to achieving resilience and how risk management can be part of the solution. While there are many aspects of resilience, I’m going to choose two challenges and their connection to risk management: organizational silos and the complexity of control.\n\nOrganizational Silos\n\nThe first challenge is organizational silos.2 Threats do not respect organizational boundaries. Some, like cyber threats, actually exploit them. Hurricanes don’t care about your organizational chart, and cyber attackers would actually like a copy of it to use it against you. Since threats aren’t siloed, our defenses can’t be siloed either.\n\nOne of the insights of enterprise risk management is that good outcomes rely on integration across organizational boundaries and types of risk. That’s why it’s not just risk management, its enterprise risk management.3 We can use approaches from enterprise risk management to create bridges across silos.\n\nTo start, enterprise risk management focuses on an organization’s objectives and the critical processes that support them. Critical processes span organizational units in a connected chain of activities, starting with inputs delivered by suppliers, to value-added transformations within the organization, to the final delivery to customers. So, to strengthen resilience, we identify the risks to process outcomes and then design processes and controls to address those risks.\n\nSince silos weaken resilience, you may want to look in your organization at some of the hotspots where enterprise-wide coordination is essential. I’d start with incident response. Are you ready for complex threats that require teamwork across organizational units? Another challenging area is external dependency management. Do you have a coordinated and coherent approach to understand and manage vendor exposures across the full spectrum of risks, including information security, data privacy, compliance, business continuity, and credit?\n\nRisk silos are as problematic as business-line silos.4 While the specialization of risk disciplines has benefits, common negative side effects are insufficient communication and coordination, followed by information gaps and ineffective decision making.\n\nTake the case of a vendor that provides critical services and is on the brink of bankruptcy. Are credit risk analysts, operational risk and business continuity professionals, and the business affected working together to share information, insights, and potential responses? Or, are silos preventing information from getting to the right people at the right time to prepare and act?\n\nRisk silos can result in siloed decisions, and looking at risks one by one is incoherent. We could drive operational risk to zero by having no operations (I guess we can all go home early), but then strategic risk rises to infinity. Since most meaningful decisions involve balancing risks, to get to the right decision, we need a risk picture that covers all relevant risk types and engages all relevant risk experts.\n\nSimilarly, real world vulnerabilities do not have to conform themselves to a particular risk type. Think about weaknesses in identity and access management. This is a driver for risks ranging from cyber, to compliance, to operations, and more. This is also a type of risk that can’t be mitigated solely through technical controls put in place by information security experts; that is, a siloed risk response is insufficient. So, stronger collaboration and coordination across risk disciplines is one solution to the negative effects of risk silos.\n\nAnother specific pain point created by risk silos is the flourishing, and perhaps overgrowth, of risk assessments. Most organizations have many risk reviews: often a different review for each type of risk. Some focus on assets, others focus on organizational units, and yet others look at specific parts of processes. The result can be a set of assessments with potentially inconsistent coverage and disconnected recommendations.\n\nSo, an appealing alternative to counter that trend and bridge siloed assessments is to integrate risk assessments. A unified approach can bring together risk and business professionals to understand and manage the risks to critical processes. The idea is to create a common picture of gaps and then design coordinated action plans that can target improvements in resilience.\n\nLet’s focus on business continuity to see how an integrated risk approach can strengthen resilience.5 Business continuity programs have traditionally been designed to restore critical physical or technology infrastructures that can be disrupted by threats like extreme weather or loss of power. So, a business continuity plan might involve arranging for a backup data center if the primary data center goes down due to a hurricane. Within an organization, different business areas may develop their own plans that are focused on their specific business needs and priorities.\n\nAn integrated business continuity plan is one that is designed to protect critical organizational processes and their essential assets from the full range of relevant threats. An integrated approach begins with organizational objectives. This bridges a gap between what good looks like to an individual business and what it looks like to the organization as a whole. Among other things, an integrated approach analyzes and plans for the impacts of disruptions on suppliers and customers from an operational as well as reputational perspective.\n\nIntegration broadens the focus from the availability of infrastructure (e.g., the servers are up and the facilities are accessible) to successful organizational outcomes (e.g., the organization is able to deliver products and services). In the current threat environment, achieving resilience is as much about protecting the integrity and confidentiality of information as it is about maintaining availability. An integrated plan delivers coordinated and coherent responses to incidents, including ones of significant scale, complexity, and surprise that can overwhelm normal control systems.6\n\nThe Complexity of Control\n\nThe second resiliency challenge is the complexity of control. To improve resilience, we seek to control outcomes, and risk management has a lot to say about control. A simple story about control goes like this: when we are setting up a business process, we look for sources of problems and start adding controls. Mainly, we’ll create controls to prevent problems.\n\nBut, recognizing that there will be times when prevention doesn’t work, we also put in place controls to detect problems and then correct them. Once the process is up and running, if problems come up that we didn’t expect, we add additional controls to prevent those problems from happening again.\n\nI wanted to go through some examples of how this doesn’t always work well in practice. From that, we’ll see that a systems perspective can explain why our attempts at control sometimes fail and what we can do about it.7\n\nA key insight from systems thinking is that controls are part of a system. Controls interact with other controls and with the production process. One implication of connectedness is that changes to any part of the system can spillover and affect other parts. Controls that work well separately may not work well together.\n\nThat’s one reason why controls created in isolation and added incrementally can create unexpected results. It’s a weakness of control systems that have grown through an ad-hoc approach of “see a problem, add a control.”\n\nLet’s walk through a scenario where controls don’t behave as expected or intended.\n\nOn Sunday, you run a business resumption exercise to test the ability of your primary data center to fail over to your backup data center. The test appears to be a success, and you have some useful lessons learned to make the failover process run better next time. But, when you come to work on Monday morning, something is wrong. It turns out the test corrupted customer account data. Your call center is overwhelmed with questions and complaints, because your billing system sent a blast email with overdue payment notices to all of your customers.\n\nIn a simple world, controls solve problems, they don’t create them. In the real world, the control and production systems are linked, so the failed execution of a control can, in and of itself, disrupt operations. In other words, controls can cause harm.8\n\nOn Tuesday morning, a hurricane damages your main office and floods the backup generator that powers the emergency communications system. Because the emergency communication system is offline, employees do not know they should report to the backup work location, so operations are out for the day.\n\nIn a simple world, controls are protection, they don’t need protection. In the real world, both the production and control system can be damaged and degraded. So, the control system itself must be designed to operate under stress, and its performance must be monitored.\n\nIt’s been a tough week so far, but you’ve made it to Wednesday. At noon, your information security officer tells you that a security scan has flagged a possible cyber breach. You cut off all network connections, send non-essential employees home, and wait until the end of the day until diagnostics are complete. The news at the end of the day is good: it turns out that the alert was a false positive. But, the organization lost half a day of productive activity.\n\nIn a simple world, controls detect problems immediately, diagnose with perfect accuracy, and result in corrective actions that are instantly and fully effective. In the real world, detection takes time, diagnosis is not always accurate, solutions take time to implement, and they don’t always work. That said, we must make decisions—including ones about whether and how to execute controls—based on the information available at the time.\n\nControl design and execution take place in a world in which there is uncertainty about whether there is a problem, what the problem is, what to do about it, when to do something, and what the effects of what we do will be. We can make better decisions when we understand and manage that uncertainty.\n\nThere is also uncertainty about threats. We will never be able to envision the characteristics of all future causes of disruption. Even when we plan effectively, we will be surprised again. And, like any other system, the control system will exhibit performance variability. For these reasons, we can’t rely solely on prevention. Resilience requires the ability to withstand and recover when threats make it through preventive barriers, so a strong control system must incorporate preventive, detective, and corrective controls.\n\nPerhaps because we are overoptimistic about prevention, detective and corrective controls sometimes get second-class billing. For many threats, time is not on our side; the impact grows as the detection time increases. That’s the case with “dwell time” for cyber breaches, which is the amount of time attackers are on your network before you detect them.9 It’s important to decide how long you are willing to wait to find out that you have a problem, and then use that as a requirement to design detective controls.\n\nUnderinvesting in corrective controls is problematic too. For example, how would you feel after a ransomware attack, if you find out that your backed-up data is intact but it will take months and millions of dollars to restore? Or, if your first corrective action is the wrong solution and makes the problem worse? It takes time and resources to develop the tools, procedures, and skills to accurately diagnose causes, contain the immediate threat, and implement a long-term solution. Will you be satisfied with the performance of your corrective controls when you need them?\n\nThere’s also a bias towards automated controls that is not always well considered. Both manual and automated controls have a place in a control system. While machines are better than people at repetitive tasks with clear decision criteria, people are better than machines (for now) at tasks that are ambiguous and require judgment and flexibility. When you automate controls, are you preserving the knowledge and skills of staff who need to understand the system well enough to solve problems when automation fails?\n\nSo, we’ve seen how controls are part of a dynamic system in an environment of uncertainty.  We can’t know whether our controls are sufficient unless we look at the behavior of the control system as a whole. And, we can’t achieve our desired outcomes unless we design for them.10\n\nConclusions\n\nToday, I’ve highlighted two challenges to resilience: organizational silos and the complexity of control. And, I’ve proposed a set of solutions based on risk management tools: a focus on organizational objectives, end-to-end management of critical processes, integrated risk management, and a systems approach to control. These all share the common characteristic of being integrative.\n\nResilient components do not necessarily add up to a resilient whole. So, organizational resilience is an enterprise goal that is achieved through coherent and coordinated enterprise solutions.\n\nA resilient organization is able to make better plans, decisions, and actions that deliver desired outcomes over a range of conditions. To strengthen resilience, we seek to tame the problematic present and constructively move forward into the hazy future. The tools of risk management can help get us there."
    },
    {
        "title": "Money Market Developments: Views from the Desk",
        "date": "Nov 4, 2019",
        "speaker": "Lorie K. Logan",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/log191104",
        "content": "Welcome to the 2019 Annual Primary Dealer Meeting. Each of your firms plays an important role as a trading counterparty in the implementation of monetary policy—a core responsibility of the Federal Reserve Bank of New York as the operating arm for the Federal Open Market Committee (FOMC). This is an area that has generated increased operational activity over the past year. These annual meetings provide us an opportunity to strengthen our relationship and communicate the New York Fed’s expectations of its counterparties.\n\nOver the course of the year, there have been important developments in the Federal Reserve’s framework for implementing monetary policy, and the markets in which we operate have continued to evolve. I’d like to focus today on what the New York Fed’s Open Market Trading Desk (the Desk) is currently doing to implement the FOMC’s ample reserves regime, particularly over the next several months. In mid-October, the FOMC directed the Desk to maintain over time ample reserve balances at or above the level that prevailed in early September, and we recently began Treasury bill purchases to achieve that objective. Since it will take some time to accumulate securities holdings, we also continue to conduct temporary repurchase agreement (repo) operations in order to ensure that the supply of reserves remains ample and to mitigate the risk of money market pressures that could adversely affect policy implementation. Although money market conditions have calmed since the mid-September volatility, the Federal Reserve is carefully monitoring reserves conditions and money market developments, and the Desk will continue to use its tools and adjust operational plans as needed to maintain the federal funds rate in the FOMC’s target range.\n\nIn addition to discussing open market operations today, I would also like to focus briefly on another key development in money markets—the transition away from LIBOR. I will highlight today’s announcement about the New York Fed’s proposal to produce averages for the Secured Overnight Financing Rate (commonly known as the SOFR).\n\nBefore I begin, I will remind you that the views presented here are mine alone and do not necessarily reflect those of the New York Fed or the Federal Reserve System.1\n\nOpen Market Operations in an Ample Reserves Regime\n\nI’d like to start with some context about how the FOMC planned its transition to an ample reserves operating regime for monetary policy implementation.\n\nIn January, the FOMC announced its intention to continue implementing monetary policy using an ample reserves operating regime. This means that the supply of reserves is sufficient to ensure that control over the federal funds rate and other short-term interest rates is exercised primarily through the Federal Reserve’s administered rates, and active management of the supply of reserves is not required.2 This last clause is an important feature of the framework and means that, over time, the level of reserves will be large enough to absorb short-term changes in factors affecting the supply of reserves without the need for frequent, sizeable interventions to offset these changes (Figure 1).\n\nIn March, the FOMC indicated that the longer-run level of reserves will be a level consistent with “efficient and effective” implementation of monetary policy.3 To support a smooth transition to the longer-run level of reserves, the FOMC also stated its intention to slow the pace of the decline in reserves—which had been coming down as a result of normalizing the balance sheet—over subsequent quarters, first by slowing and then by stopping the reduction in the Federal Reserve’s aggregate securities holdings. The end of balance sheet runoff occurred in August, and the FOMC continued to monitor a wide range of information to reach a judgement about the level of reserves most consistent with efficient and effective policy implementation in an ample reserves regime.4\n\nIn mid-September, upward pressure emerged in funding markets as corporate tax payments and the settlement of Treasury securities increased demand for securities financing and sharply reduced reserves in the system. Consistent with the directive from the FOMC, the Desk responded with temporary open market operations to foster conditions that would maintain the federal funds rate within the target range. In mid-October, the FOMC directed the Desk to maintain over time reserve balances at or above the level that prevailed in early September in order to ensure that the supply of reserves remains ample.5 The FOMC judged this level more supportive of smooth market functioning and effective control over the federal funds rate, and we have seen money market pressures stabilize.\n\nIt’s worth noting that the quantity of reserves needed to maintain an ample reserves framework is subject to uncertainty and may change over time. Banks’ demand for reserves is not static. It shifts in response to changing financial conditions and will evolve through time as banks adjust their business models and respond to changes in the economic and regulatory environment. In addition, the level of reserves needed to maintain an ample reserves regime is more than the sum of individual banks’ reserves demand, particularly when there are frictions that result in inefficient redistribution of reserves. The measurement of these factors is complex, and the FOMC will continue to monitor a broad range of information to assess the level of reserves consistent with efficient and effective implementation of policy.\n\nI’d like to turn now to how the Desk will supply reserves to carry out the FOMC’s directive to maintain reserves over time “at or above” the level that prevailed in early September. This directive requires provision of a supply of reserves that can sufficiently absorb normal fluctuations in non-reserve liabilities, so shifts in these balances do not bring reserves below desired levels and potentially compromise effective interest rate control.\n\nThe Desk has been directed to purchase Treasury bills at least into the second quarter of next year to achieve the FOMC’s objective, and we are currently executing those reserve management purchases at the initial pace of $60 billion per month.6,7 I’d like to share some thoughts on the composition and sizing of those purchases.\n\nFirst, why Treasury bills?\n\nAs I noted earlier, the goal of the reserve management purchases is to maintain an ample supply of reserves to support effective and efficient monetary policy implementation.\n\nBecause Treasury bills are short-dated instruments, purchases of these securities can help maintain the supply of reserves while limiting the impact on financial conditions. Moreover, bills are liquid instruments with deep markets, and the Federal Reserve doesn’t currently hold many of them. The Federal Reserve traditionally held a relatively large portfolio of Treasury bills (in addition to holdings across the coupon curve) in its System Open Market Account (SOMA), but sold or redeemed that bill portfolio during the Global Financial Crisis and in the Maturity Extension Program. While the FOMC has not yet made decisions about the long-run composition of its domestic securities portfolio, current bill holdings in the SOMA are low by any measure.8 Treasury bills represent less than 3 percent of the SOMA’s Treasury portfolio, compared to 15 percent of the outstanding Treasury universe (Figure 2). From a portfolio composition perspective, these purchases will incrementally add bills back to the portfolio as the FOMC continues to consider its desired long-run maturity composition.\n\nSecond, how do we approach the design of our purchases?\n\nAt present, three elements are driving the overall size of the reserve management purchases:\n\nIn practice, this will mean looking out over several months and sizing reserve management purchases to offset expected reserve declines. For example, reserve balances tend to decline notably in late April as the TGA grows with the collection of individual taxes, and the Desk’s planned purchases over the next several months will help to offset the anticipated decline in reserves associated with those flows.\n\nBesides estimating the total amount of purchases, the Desk also needs to determine the pace at which it will execute those purchases. The amount and timing of the operations take into account market functioning considerations and allow for some adjustments around periods with sharp expected declines in reserves or liquidity conditions.\n\nSo far, reserve management purchase operations have proceeded smoothly. The Desk has purchased $52.5 billion of the initial $60 billion in monthly purchases, through seven purchase operations of roughly $7.5 billion each. To date, those operations have generated strong offer-to-cover ratios, pricing that has been favorable relative to market and theoretical prices, and broad participation and awards from your firms. Moreover, the Treasury bill market is accustomed to shifting amounts of securities available for purchase due to seasonal dynamics related to TGA inflows and outflows and the debt ceiling, and we’ve seen market prices adjust as we expected given the flow of our planned purchases. With roughly $2.4 trillion of Treasury bills outstanding and daily turnover of around $95 billion, the bill market has substantial capacity to support our activity, and we expect to be able to maintain the current pace of Treasury bill purchases for some time.10\n\nThat said, the Desk is prepared to adjust the pace and other parameters of the reserve management purchases as necessary to maintain an ample supply of reserves and based on money market conditions. We’ll be closely monitoring the attractiveness of propositions and indicators of market functioning to assess whether operational adjustments are appropriate.\n\nIn the meantime, because it will take some time for the bill purchases to sustain the supply of reserves at or above levels prevailing in early September, the FOMC has also instructed the Desk to continue offering term and overnight repo operations at least through January. Repo operations foster conditions to maintain the federal funds rate within the target range through two channels. First, they supply reserves in support of the FOMC’s operating regime, helping to ensure that reserves are maintained over time in amounts at or above the early September levels, even in the face of sharp increases in non-reserve liabilities.11 Second, they provide funding in repo markets to damp repo market pressure that could otherwise pass-through to the federal funds market and adversely affect policy implementation.\n\nThe repo operations have successfully offset supply changes and money market pressures associated with big anticipated drops in reserves, such as around mid-month and month-end settlements. In doing so, they have been effective at restoring calm in money markets and maintaining control over the federal funds rate. Overnight and term money market rates have moderated, on average, relative to IOER, and the effective federal funds rate has stayed well within the FOMC’s target range. Participation in the repo operations has been robust and the transmission to broader money markets has been good.\n\nOn October 23, the Desk announced an increase in the amount offered in overnight repo operations from at least $75 billion to at least $120 billion. We also increased the amount offered in the remaining term repo operations spanning the October month-end from at least $35 billion per operation to $45 billion, raising the total amount of term repos offered over this period to up to $180 billion. This increased capacity was supportive to money markets, and the federal funds rate was stable over October's month-end.\n\nAs we head into year-end, the Desk will continue to make adjustments as needed to mitigate the risk of money market pressures that could adversely affect policy implementation. It is good to see firms starting to address year-end funding needs, and we understand that, as firms manage their balance sheets more carefully in December, rates can be somewhat more variable and dealers may adjust their approach to our operations. We will monitor conditions closely and will be prepared to adapt open market operations as needed to meet the directives from the FOMC.\n\nAverages of SOFR Rates and the Transition from LIBOR\n\nBefore I wrap up, I’d like to say a few words about reference rates. As I noted to this group last year, the New York Fed is committed to producing robust and resilient reference rates and to supporting the transition away from LIBOR.12 New York Fed President John Williams has emphasized this issue, noting that while progress has been made, there is still much work to do, and with no guarantee of LIBOR’s existence beyond the end of 2021, the clock is ticking.13\n\nThe New York Fed helps advance these objectives as the administrator and producer of the SOFR, which was selected by the Alternative Reference Rates Committee (ARRC) as its preferred alternative to U.S. dollar LIBOR, and also, in coordination with the Federal Reserve Board, through support of the ARRC. The SOFR, which launched in April 2018, is a measure of the general cost of borrowing cash overnight collateralized by Treasury securities in the repo market—a rate that broadly reflects how financial institutions fund themselves. Importantly, it reflects underlying transaction volumes averaging more than $1 trillion per day over the past year, and its production is compliant with IOSCO’s Principles for Financial Benchmarks. Over the past year-and-a-half, SOFR futures markets have demonstrated promising growth in notional volumes traded and liquidity, even if volumes remain low compared to both federal funds and Eurodollar futures. Additionally, there has been meaningful issuance of SOFR-linked cash products like floating rate notes.\n\nA next step in supporting broader adoption of the SOFR as a LIBOR alternative will be to meet the need expressed by market participants—particularly financial and corporate issuers of securities and loans—for a set of SOFR averages that can be used in a wide range of financial products.14 To this end, today, the New York Fed released a solicitation for public comment on a plan to publish daily, starting in the first half of 2020, a set of three compounded averages of the SOFR, as well as a daily SOFR index that would allow users to calculate compounded averages over custom time periods.15\n\nAn expansion to the suite of reference rates we produce is an important next step that further demonstrates the Federal Reserve’s commitment to support the usefulness of adopting the SOFR. I also urge you to commit to this critical endeavor. The comment period for the public solicitation will close on December 4, and I encourage you to review the proposal and provide feedback, including about the tenors and compounding methodology for the new products. While the response to preliminary socialization of the program parameters has been positive, we recognize the importance of getting these averages right. Market participant feedback was critical as the New York Fed began producing Treasury repo reference rates to support the transition, and your continued input will help ensure that the rates we produce best meet the needs of the financial system. More generally, every market participant needs to understand their LIBOR exposures and use all available tools to transition to robust alternative rates, such as the SOFR, as they prepare for a world without LIBOR.\n\nConclusion\n\nIn sum, the Desk continues to operationalize the FOMC’s ample reserves regime. We are doing this by conducting Treasury bill purchases to maintain a supply of reserves consistent with the FOMC’s directive to maintain reserves at or above early-September levels. Also, since it will take some time to accumulate securities holdings to achieve that objective, the Desk is continuing to provide reserves temporarily through overnight and term repo operations. We are monitoring market conditions closely, have tools to effectively and efficiently support monetary policy implementation, and will make adjustments as needed. Both in the execution of open market operations and support for the transition from LIBOR, it is a very active time on the Desk, and our partnership with each of your institutions remains important to the smooth implementation of this work.\n\nFigures"
    },
    {
        "title": "Money Markets and the Federal Funds Rate: The Path Forward",
        "date": "Oct 17, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil191017",
        "content": "Good afternoon, everyone. When I give a closing keynote I try to bear in mind that you’ve been listening for hours to complex and technical material. I wish I could say that I’m here to offer some light relief, but, alas, no.\n\nThe topic I’m going to address this afternoon is both complex and technical. It’s the recent turmoil in the repo markets and the Fed’s actions to address it.\n\nI’m going to start by laying out the Fed’s operating framework for carrying out monetary policy. This will take me into aspects of the “plumbing” of the financial system, which may seem like an esoteric topic to many, but it’s one of critical importance, so please bear with me.\n\nI will then go over the money market turmoil from a month ago that led the New York Fed to conduct open market operations to stabilize short-term interest rates. I’ll conclude by describing the path forward as laid out in the Fed announcements this past Friday.1\n\nBefore I go any further, I should give the standard Fed disclaimer that the views I express today are mine alone and do not necessarily reflect those of the Federal Open Market Committee or others in the Federal Reserve System.\n\nMonetary Policy, Interest Rates, and Ample Reserves\n\nAs a prelude, it’s worthwhile to step back and remember how this all connects to the Fed’s overarching monetary policy goals.\n\nThe Federal Reserve has two goals set by Congress: maximum employment and price stability. The main way we achieve these goals is by controlling the federal funds rate, the rate at which banks lend each other money overnight in the form of unsecured loans. This rate in turn affects overall financial conditions and thereby the wider economy.\n\nAt each of its meetings, the Federal Open Market Committee (FOMC) decides on the target range for the federal funds rate, which currently stands at 1-3/4 to 2 percent.2\n\nBut how does this work—that is, what keeps the federal funds rate within the target range? The Fed has an operational framework in place designed to keep interest rates where the FOMC wants them. We call this an “ample reserves” regime.\n\nBack in January of this year, the FOMC communicated its intention to continue to implement policy according to this regime.3\n\nThe key benefit of this approach is that it’s a simple, effective way of controlling the federal funds rate and thereby influencing other short-term interest rates.\n\nThe ample reserves framework has three elements designed to maintain the federal funds rate within the target range.\n\nThe first, not surprisingly, is supplying an amount of reserves that banks hold at the Federal Reserve that is “ample.” By that, we mean that the supply of reserves is adequate to efficiently and effectively implement monetary policy. Here, “efficient” means that the level of reserves is not excessive, relative to what’s needed to be effective. And “effective” means that typical temporary movements in the demand or supply of reserves don’t cause large changes in the federal funds rate without active management of reserves. I’ll come back to the challenge of knowing what constitutes “ample” later.\n\nThe second element consists of the interest rates that the Fed itself sets and that influence the federal funds rate. These “administered” rates, as they are known, include the interest rate paid to banks on their reserve balances and the overnight reverse repo rate that the Fed pays to a wider set of money market participants on a similar, risk-free overnight investment.\n\nThese first two elements—an ample quantity of reserves and administered interest rates—are meant to do the lion’s share of work to keep the federal funds rate within the target range.\n\nBut there may be relatively infrequent situations when these two elements are not enough to keep the federal funds rate within the target range. Therefore, the third element is the directive from the FOMC for the New York Fed’s Open Market Trading Desk (the Desk) to conduct open market operations as needed to keep the federal funds rate within the target range. For example, through repurchase agreements of Treasury and agency securities, the Fed can temporarily increase the amount of reserves in the system.\n\nTo support the economy in the aftermath of the financial crisis, the FOMC increased its long-term securities holdings dramatically. This led to a similarly large increase in reserves, which peaked at about $2.8 trillion in 2014. Subsequently, growth in non-reserve liabilities such as currency and reductions in the Fed’s securities holdings caused reserve levels to decline.\n\nAs the level of reserves has come down, we stepped up our monitoring of the effect of reserve levels on interest rates. The level of reserves consistent with “ample” is inherently highly uncertain, so we have been actively looking for signs that reserves might be growing scarce. Along with in-depth analysis of conditions in money markets, we conduct surveys of banks and other outreach to market participants to understand the factors that influence the demand for reserves.4\n\nTo smooth the transition to a level of reserves consistent with the framework, the FOMC announced a slowing of the pace of the reduction in securities holdings at its March meeting this year and stopped it outright at the July meeting.5\n\nBased on a variety of metrics and information, the supply of reserves appeared ample during the summer and into early September. The funds rate traded well within the target range and money markets functioned well.\n\nRecent Developments in Money Markets\n\nWith the stage set, I’ll turn to market conditions over the past month and the Fed’s actions.\n\nConditions in money markets became highly volatile about a month ago. Secured and unsecured rates moved higher and became more disparate on Monday, September 16, and this intensified the following day.\n\nWe had expected a number of factors—including corporate tax payments and the settlement of newly issued Treasury securities—to put some upward pressure on short-term rates. However, the size of the reaction in repo rates, the spillover to the federal funds market, and the emergence of strains in market functioning were outside of recent experience.\n\nIn response to the simultaneous increase and widening dispersion in repo rates and the federal funds rate, on the morning of Tuesday, September 17, the Fed conducted the first non-test repo operations in many years, followed by daily operations. These actions had the desired effect of reducing strains in markets, narrowing the dispersion of rates, and fostering conditions in money markets to keep the federal funds rate within the target range. To ensure that the federal funds rate remained within the target range going forward, we announced daily overnight and regular term repo operations, which continue to this day.6\n\nThe Path Forward\n\nOur open market operations have succeeded at keeping the federal funds rate within the target range and have stabilized conditions in short-term funding markets.\n\nAt the same time, recent experience has provided important lessons for the successful operation of the ample reserves framework.\n\nA confluence of events contributed to the volatility in money markets a month ago. But one telling observation is that when increases in the Fed’s non-reserve liabilities caused the level of reserves to fall well below those prevailing during summer and early September, strains in money markets emerged. And when the prior level of reserves was quickly restored through temporary open market operations, normal interest rates and market functioning returned.\n\nIn light of these events, we have learned that the ample reserves framework has worked smoothly with a level of reserves at least as large as we saw during summer and into early September. Although temporary open market operations are doing the trick for the time being, anticipated increases in non-reserve liabilities would cause reserves to decline in coming months without further actions.\n\nBased on these considerations, last Friday the FOMC announced that the Fed will be purchasing U.S. Treasury bills at least into the second quarter of next year.7 Specifically, the Desk announced an initial monthly pace of purchases of $60 billion. These permanent purchases will, over time, bring the underlying level of reserves—by which I mean absent temporary open market operations—to a level consistent with the ample reserves framework on a sustained basis.\n\nIn concert with these purchases, the FOMC announced that the Desk will continue temporary overnight and term open market operations at least through January of next year.8 This combination of permanent Treasury bill purchases and ongoing temporary open market operations is designed to provide effective control of the federal funds rate over the next several months.\n\nI should emphasize that all of these actions are aimed at the implementation of monetary policy and do not in any way represent a change in the stance of monetary policy. The goal is to make sure that the federal funds rate stays within the target range set by the FOMC.\n\nAs we move forward, we will continue to learn about demand for reserves and other Federal Reserve liabilities and market functioning, and may adjust the specifics of the plan as appropriate.\n\nWhat Does All of This Mean for SOFR?\n\nThat’s a lot on monetary policy, interest rates, reserves, and repos.\n\nBut before I close, I’d like to emphasize an important point about a particularly important repo benchmark rate, and that’s the Secured Overnight Financing Rate, or SOFR. SOFR is the reference rate that the Alternative Reference Rates Committee (ARRC) has selected as the preferred replacement for LIBOR.\n\nAt the same time that we saw turmoil in the repo market, we saw a temporary spike in SOFR. As market participants make preparations to transition away from LIBOR, they’re understandably watching SOFR very closely.\n\nThere are a few things I’d like to highlight with respect to SOFR. First, a temporary spike is not surprising, given that SOFR reflects rates on real-world transactions. Second, the very fact that we saw a spike in SOFR is an indication of how representative of its underlying market it is. It’s based on actual transactions, rather than judgment (like LIBOR), which is part of what makes SOFR so robust. Third and final, is that in the vast majority of use cases, the relevant metric for SOFR is an average over time. Focusing on overnight SOFR isn’t particularly useful in this context, as financial contracts will generally refer to an average of SOFR over many weeks or months.\n\nMy message to you is: Don’t let last month’s temporary spike in SOFR, or hope for the creation of some other replacement reference rate, become an excuse for delaying your transition away from LIBOR. I’ve said it before and I’ll say it again: like death and taxes, the end of LIBOR is unavoidable, and we must do all that it takes to prepare for a LIBOR-less future.9 And the existence of LIBOR is only guaranteed for another 807 days.10\n\nConclusion\n\nI’ve talked through a lot of important issues this afternoon, and I greatly appreciate your attention.\n\nThe events I’ve described are an apt reminder of the importance of well-functioning financial markets and the vital role the Federal Reserve plays in providing liquidity. They are also a demonstration of our ability to act swiftly and effectively to execute our monetary policy implementation goals and keep the federal funds rate within in the target range.\n\nThank you."
    },
    {
        "title": "Welcome Remarks to US-MENA Private Sector Dialogue Conference",
        "date": "Oct 17, 2019",
        "speaker": "Michael Held",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/hel191017",
        "content": "Hello everyone – I’m Michael Held, an Executive Vice President and the General Counsel at the Federal Reserve Bank of New York.1\n\nI would like to welcome you to the New York Fed and to our latest US-MENA (Middle East North Africa) Private Sector Dialogue Conference entitled “The Challenge & Opportunities for MENA Banks – Understanding and Meeting US Regulatory Expectations and the Expectations of Their Correspondents.” The New York Fed is honored to host this event with the International Monetary Fund and the Union of Arab Banks (UAB), with support from the Department of the Treasury.\n\nThe New York Fed first partnered with the Treasury and the Union of Arab Banks in December 2006, when we hosted the initial Private Sector Dialogue Conference between U.S. authorities, MENA banks, and their regulators. Our most recent US-MENA Private Sector Dialogue Conference was two years ago in this spot and its success set the stage for today’s event.\n\nI am happy to continue this critical dialogue. I am a major supporter of collaboration among banks, regulators, and law enforcement in the effort to fight terrorist financing and financial crimes. I hope to keep today’s productive dialogue going in the years to come.\n\nThe New York Fed is very pleased to provide the venue for this year’s event focusing on anti-money laundering (AML) and other issues related to the prevention of financial crime. These issues affect the dynamics of correspondent relationships around the world. This conference plays a vital role in enhancing coordination between U.S. banks and banks headquartered in the Middle East, and encouraging well-controlled correspondent banking relationships. In doing so, this conference promotes U.S. dollar liquidity in this critical region of the world.\n\nThe New York Fed fully supports the US-MENA partnership to improve communication, transparency, and understanding regarding expectations between respondents and their correspondents, so as to minimize the unnecessary de-risking of accounts. Everyone here will agree that there is no benefit to chasing money from the banking sector to illicit third-parties, such as unlicensed hawalas. These transactions stymie transparency in the financial sector. Hopefully, with patience, better understanding of each other’s regulatory environment, and focused attention and resources, banks represented at today’s conference can enhance and expand correspondent relationships that are effective, efficient, and well controlled.\n\nThe New York Fed’s AML efforts started in earnest over 30 years ago. For more than 25 years, we have been working with and supporting foreign regulators, and U.S. law enforcement on parallel enforcement matters. The New York Fed’s enforcement attorneys work with our colleagues at the Board of Governors in Washington, D.C., and play an important role in financial regulatory investigations. We also help build strong coordination between the Fed, domestic and foreign regulators, and our partners in law enforcement. Many of the Fed’s enforcement actions have involved high-profile sanctions evasion cases and AML system deficiencies.\n\nBut we don’t just want to punish bad activity. It is important to promote increased transparency and compliance with U.S. regulations and Financial Action Task Force (FATF) goals. My colleagues and I are committed to helping financial institutions stay out of regulatory trouble by hosting high-quality AML training like today’s conference. Everyone will be able take away a few new ideas, and the contacts you develop here may help enhance a current correspondent relationship, or even lead to new relationships.\n\nWe have an excellent agenda, featuring true experts in the field. I am especially happy to welcome our esteemed Treasury colleague, Mr. Paul Ahern, the Acting Principal Deputy Assistant Secretary for the Office of Terrorist Financing and Financial Crimes. He will be joined by his colleague, and our old friend, Mr. Jamal El-Hindi, the Deputy Director of the Financial Crimes Enforcement Network (FinCEN), as well as other officials from the Treasury department.\n\nI am also very honored to welcome governors from many central banks, including Egypt, Lebanon, Mauritania, Oman, Palestine, Qatar, Tunisa, and Yemen, in addition to other senior representatives who have traveled from the Middle East. We also have two representatives from the IMF—Stéphane Roudet and Arz El-Murr—who are our co-hosts today.\n\nFinally, I would like to thank Wissam Fattouh, the Secretary General of the UAB, Mounir Al-Akhal, the Deputy Director for International Affairs, and Muhammad Baasiri, the Chairman for this event, and a long time representative from the Central Bank of Lebanon. They have had the primary responsibilities for making this event a reality.\n\nNow I have the great pleasure and honor of introducing Dr. Joseph Torbey, Chairman of the World Union of Arab Banks. Dr. Torbey has led that group, which aims to establish a network of communication and solid relations among Arab bankers and finance professionals, since 2006. In addition, he has served as Chairman and General Manager of the Credit Libanais since 1988, and as Chairman of the Executive Committee of the UAB since May 2007. He also chaired the Board of UAB for two consecutive mandates from April 2001-2007. Dr. Torbey is also the former Chairman of the Association of Banks in Lebanon, a position he held for many mandates (2001-2005, 2009-2013, and 2015-2019).\n\nDr. Torbey earned a Ph.D. in law from the University of Lyon (France) and has followed extensive studies in public administration and business taxation at the University of Southern California, Los Angeles.\n\nPlease join me in welcoming Dr. Torbey."
    },
    {
        "title": "LIBOR: The Clock Is Ticking",
        "date": "Sep 23, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil190923",
        "content": "Good morning everyone and welcome to the New York Fed. It’s a pleasure to be speaking at the fifth annual U.S. Treasury Market Conference.\n\nThis conference is particularly valuable because it brings together market participants and five public sector bodies—the New York Fed, the Board of Governors, the Treasury, the SEC, and the CFTC—to discuss the U.S. Treasury market.\n\nThe Treasury market is arguably the most important market in the world, and today is a unique opportunity to discuss some of the big topics in front of us.  Increasing transparency, preparing for future risks, and taking advantage of innovation in this market are vital areas of discussion.\n\nThis morning, I’ll begin with some observations on recent volatility in money markets and the Fed’s approach to support stability in this critical part of the financial system as we carry out the FOMC’s monetary policy decisions.\n\nI’ll then pivot to a different but related topic: the transition away from LIBOR. I think I can get away with it because this conference’s five sponsoring agencies, which first came together following the “flash rally” in 2014, also all play a vital role in facilitating the industry’s move to more robust reference rates. Crucially, they are ex officio members of the Alternative Reference Rates Committee (ARRC), the private sector group convened to guide the transition away from LIBOR. And of course the Secured Overnight Financing Rate (SOFR), the ARRC’s selected replacement for U.S. dollar LIBOR, reflects transactions that finance U.S. Treasury securities.\n\nBefore I go any further, I need to give the standard Fed disclaimer that the views I express today are mine alone and do not necessarily reflect those of the Federal Open Market Committee or others in the Federal Reserve System.\n\nMarket Conditions\n\nI’ll start with recent developments in money markets. Conditions in funding markets became highly volatile early last week, with secured lending rates moving higher on Monday and rising sharply on Tuesday. We had expected a number of factors—including quarterly corporate tax payments and the settlement of Treasury auctions—to put upward pressure on funding rates. However, the size of the reaction in repo rates, the spillover to unsecured markets such as federal funds, and the emergence of strains in market functioning were outside of recent experience. For example, in April and June of this year, sizable payment flows led to relatively muted movements in secured lending rates and small movements in unsecured rates.\n\nAs had been done ahead of prior dates with anticipated significant funding pressures, on Friday, September 13, the Markets team at the New York Fed, and others in the Federal Reserve System put in place a plan for increased monitoring of money markets. This included more frequent reporting on transactions, data, and information from market participants on Monday.    \n\nAlthough conditions remained calm on that Friday, the simultaneous increase and widening dispersion in repo rates and, importantly, federal funds rates on Monday indicated that markets were not effectively distributing liquidity across the system.  The New York Fed’s Markets team built a timeline and implementation plan in case the Open Market Desk at the New York Fed (Desk) would need to operate on Tuesday. Early Tuesday morning, it became clear that this situation had persisted and had the potential to become more acute.\n\nIn response to these developments, and in keeping with the FOMC’s standing directive to the Desk to conduct open market operations as necessary to keep the federal funds rate within the target range, on Tuesday morning the Desk announced and conducted a repo operation. Although such operations were common before the financial crisis, this was the first non-test repo operation in many years. This was followed by repo operations each day since.\n\nThese actions had the desired effect of reducing strains in markets, narrowing the dispersion of rates, and lowering secured and unsecured rates to more normal levels relative to other benchmarks. \n\nThe experience of the past week drew attention to the possibility of the development of strains in funding markets around the upcoming end of the quarter.  This past Friday we announced a plan of continued daily overnight repo operations through October 10, accompanied by three two-week term repo operations that span the month end.1 The goal of these actions is the same as for recent open market operations: foster conditions in money markets to keep the federal funds rate within the target range.\n\nThis episode reminds us all of the importance of having well- functioning markets and the vital role that the Federal Reserve plays in supplying liquidity to the system when markets are under stress. We were prepared for such an event, acted quickly and appropriately, and our actions were successful. Friday’s announcement on open market operations to address potential quarter-end funding pressures on interest rates followed this same approach: quickly diagnose the problem, develop the right action plan, and execute that plan.\n\nAt the same time, it is equally important that we examine these recent market dynamics and their implications for the liquidity needs in relation to the overall amount of reserves held at the Federal Reserve. We will continue to monitor and analyze developments closely. As Federal Reserve Chair Powell stated in his most recent press conference, the FOMC will assess the implications for the appropriate level of reserves and time to resume organic growth of the Federal Reserve’s balance sheet consistent with the successful execution of the FOMC’s ample reserves framework.2\n\nThat’s enough on recent events for now. I’ll turn to another very important issue, reference rate reform.\n\nLIBOR Won’t Last Forever\n\nSome say only two things in life are guaranteed: death and taxes. But I say there are actually three: death, taxes, and the end of LIBOR.\n\nEveryone in the financial services industry needs to be aware that the date when the existence of LIBOR can no longer be guaranteed is fast approaching.\n\nI titled remarks I gave on this subject earlier in the year “901 Days” to remind everyone just how little time both the public sector and market participants have to prepare for a world without LIBOR.3\n\nWe are now 831 days away from that world, and while some institutions are making good progress, others are sticking their metaphorical heads in the sand, hoping the issue will go away.\n\nEven more concerning are those wearing rose-tinted glasses, getting nostalgic about LIBOR and hoping for an extension to the deadline or a reincarnation of the rate. I cannot emphasize enough that the clock is ticking and everyone needs to get their firms ready for January 1, 2022.\n\nA look at the numbers, and recent history, reveals the need for urgency. There are $200 trillion of financial contracts referencing U.S. dollar LIBOR.4 But the volume of actual transactions that term LIBOR is based on is very small. When individual banks make LIBOR submissions they are largely based on judgment, not real transactions. This makes the rate vulnerable to manipulation, and banks are increasingly reluctant to provide submissions, adding yet further risk around using the rate.\n\nThe U.K. Financial Conduct Authority (FCA) has reached an agreement with banks to keep submitting rates until the end of 2021, but beyond that date the existence of LIBOR is not guaranteed.5  The FCA has made it clear that it expects at least some banks to leave the LIBOR panels soon after 2021, making LIBOR even less representative than it is now, and that it would need to judge whether LIBOR was still representative at that stage.6\n\nHeadway Is Being Made\n\nWhile the clock is inevitably ticking, progress is being made.  \n\nIn 2017, the ARRC selected SOFR as its preferred alternative to U.S. dollar LIBOR.7\n\nSince April 2018, the New York Fed has produced SOFR every business day. It’s based on an important underlying market with much higher volumes than LIBOR, and is compliant with the Principles for Financial Benchmarks set forth by the International Organization of Securities Commissions (IOSCO).8\n\nThe International Swaps and Derivatives Association (ISDA) has led really important work on the development of contingencies for some derivatives products for the scenario where LIBOR ceases to exist.9\n\nA key question is what will happen if the FCA finds LIBOR to no longer be representative. Two large central counterparties have already indicated that they would expect to move the LIBOR trades they clear to SOFR if this occurred. The Financial Stability Board’s Official Sector Steering Group (OSSG) has indicated its support for ISDA to include such a trigger to avoid fragmentation between cleared and uncleared derivatives.10\n\nDerivatives contracts account for 95 percent of the exposure to U.S. dollar LIBOR, so universal changes to these contracts would be a significant leap forward. If the market signs up to the ISDA protocol when it’s published, it will be a considerable milestone and will go a long way toward reducing risks to firms, markets, and the financial system.   \n\nIn addition, the ARRC has released four sets of recommended fallback language for different types of cash products and has published potential paths forward for adjustable-rate mortgages (ARMs).11 While mortgages are a small part of the overall exposure, it’s vitally important that consumers understand what the changes being made mean in real terms.\n\nLast week, the Federal Reserve Board, the FDIC, the OCC, and other agencies released a proposal on adjusting margin requirements for covered swap entities that, among other things, are meant to remove unintended hurdles to signing the ISDA protocol and moving away from LIBOR.12  The Financial Accounting Standards Board has similarly released an exposure draft with proposals to smooth the transition from an accounting perspective.13  Finally, earlier this year, the SEC staff issued a statement highlighting risks and disclosures for market participants to evaluate proactively as they transition away from LIBOR.14\n\nThe Challenges Ahead\n\nProgress has been made, but there’s still much to do.\n\nSOFR is sometimes criticized for the lack of a term rate, but my message is don’t wait for a term rate to get your house in order. Don’t use that as an excuse to halt the vital work of understanding where your exposure to LIBOR lies and how to prepare your business. The OSSG has produced a document explaining how market participants can use overnight risk-free rates, as opposed to waiting for term rates.15 In addition, the New York Fed is preparing average SOFR rates and a SOFR index, with the goal of publishing them daily by the middle of next year.    \n\nContracts that reference U.S. dollar LIBOR continue to be written, which only serves to increase the level of systemic risk. In rare cases where for some reason LIBOR must be referenced, robust fallback language that accounts for a future without LIBOR needs to be included. I also strongly encourage market participants to address legacy LIBOR-linked contracts. There’s no one-size-fits-all approach for closing out or converting existing LIBOR positions so market participants need to get ahead of this issue.\n\nWe know this is difficult and complex work. At the New York Fed, we’re working to identify where we have potential exposure to LIBOR and other non-IOSCO-compliant reference rates. We have teams examining monetary policy operations, foreign reserve management activities, and trading agreements with counterparties.\n\nWe want to hear your voice so we can incorporate your feedback and facilitate the development of the tools you need to ensure an orderly transition.  Engage with the official sector about how LIBOR affects your firm and your challenges in this transition. And note that just last week, the ARRC published a checklist designed to support market participants with the adoption of SOFR.16\n\nImplementation will be complex: financial contracts need to be scrutinized, operations need to be evaluated, and technology needs to be updated. The work involves numerous jurisdictions and multiple asset classes, and will require changes from how business is conducted to how systems are built. These things take time, and time is running out.\n\nConclusion\n\nThe LIBOR transition is a serious issue the industry needs to address. Both the official sector and market participants have made enormous progress, but we need to see much broader and consistent preparation for the transition.\n\nIf your firm is one of those hoping the problem will go away, or feeling nostalgic and counting on an extension to the deadline, take this message back: The clock is ticking, LIBOR’s days are numbered, and we all need to play our part in preparing the industry for January 1, 2022.\n\nThank you, and I hope you enjoy today’s conference."
    },
    {
        "title": "We're Only Human: Culture and Change Management",
        "date": "Sep 5, 2019",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/hen190905",
        "content": "Introduction\n\nGood morning, ladies and gentlemen. It’s a pleasure to be here at the Westminster Business Forum policy conference. I look forward to sharing with you the approach the New York Fed has taken toward the reform of culture and conduct in financial services. Some of our work runs parallel to what was discussed this morning: fostering greater accountability within firms and by their leaders. Other parts may be less well known: for example, our emphasis on the education and training of the next generation of the financial services workforce. I will also discuss where I see our efforts focusing in the future, including the importance of better understanding the process by which individuals and organizational cultures change. But before I go any further, I should make clear that the views I express today are mine alone, and do not necessarily reflect those of the Federal Reserve Bank of New York or the Federal Reserve System.\n\nNew York Fed’s Culture Initiative\n\nThe New York Fed’s Culture Initiative has stemmed from the continuing \"evidence of deep-seated cultural and ethical failures\" within the financial services industry.1 As this conference attests, progress has certainly been made in creating greater awareness of the problem. Some firms and regulators have made significant headway in implementing culture reform measures. We must also acknowledge, however, that major misconduct continues in financial services across the globe. In the past two years alone, headline scandals have involved money laundering, bribery, international sanctions evasion, and consumer abuse. Not surprisingly, public trust in the industry remains extremely low.\n\nConcerns about misconduct in financial services, the cultural norms within firms, and the sustainability of the industry over the long term are germane to the mission of a central bank such as the Federal Reserve. As discussed in a New York Fed whitepaper on the topic,2 misconduct can affect the stability of firms and the financial system broadly. It can seriously damage an individual firm’s reputation among consumers, investors, regulators, and the general public. Customer attrition and legal penalties can, over time, impair a firm’s balance sheet and its resiliency. And widespread misconduct may cause market participants to lose confidence in the financial sector as a whole. We should also never lose sight of the moral harms that misconduct entails. Fraud is just the beginning. Violations of anti-money laundering laws and trade sanctions, for example, facilitate the activities of terrorists, drug cartels, human traffickers, and rogue nations.\n\nAims and Areas of Activity\n\nAccordingly, from the start, the aims of the New York Fed’s Culture Initiative have been threefold:\n\nThese are long-term goals. They have to be, given the ever-shifting nature of organizational culture and the constantly evolving external environment. There is no silver bullet, no one-time \"fix\" for culture. Experience teaches, however, that with sustained effort, significant progress can be made over time.\n\nTo advance our aims, our work to date has focused in four areas: awareness, research, supervision, and education.\n\nAwareness has involved convening industry and the official sector to discuss the ongoing need for reform and to provide updates on progress and new approaches. We do this every year to encourage firms to actively manage their cultures, beyond maintaining traditional compliance programs. Actively managing a culture requires a conscious, multi-pronged approach that aligns behavior with a firm's purpose and strategy. The effort must be embraced by management and fully incorporated into the firm's activities related to leadership and communications; governance structures and accountability; financial and non-financial incentives; employee lifecycle management (recruiting, on-boarding, training, performance management, off-boarding); and measurement and assessment of culture as evidenced through behavior.\n\nRegarding research, as I mentioned we have published a white paper on misconduct risk, and periodically engage with academics through informal consultations and by hosting academic workshops. We are currently planning a large academic conference for the coming year.\n\nWe have encouraged attention, collaboration, and collective learning among bank supervisors through convening the Supervisors Roundtable for Governance Effectiveness. This group of 20 supervisory agencies from 15 jurisdictions around the world share emerging and evolving practices for supervisors to use in the assessment of culture and behavior at firms.\n\nAnd, most recently, we have launched the Education and Industry Forum of Financial Services Culture.3 This group brings together business school deans and professors and senior representatives from financial services firms to improve the training of the next generation of the financial sector workforce. The Forum looks to promote the recognition of the complex ethical components of financial decisions and the development of future leaders, both while in school and as they progress through the ranks at firms. The Forum’s first work product will be a series of case studies that present such business and ethics dilemmas. The goal is for these case studies to become an integrated part of business school curricula, undergraduate economics courses, recruiting, and employee training.\n\nAdditional Lenses to View Culture\n\nIn each of these areas of activity—awareness, research, supervision and education—we have maintained our fundamental concentration on the problem of misconduct. Significantly reducing the scale and frequency of misconduct is central to our mission as a supervisor and a central bank. As our understanding of misconduct has grown through our various efforts, we have also become increasingly cognizant of the many elements that influence organizations in developing healthy work cultures. In an effort to build a more nuanced appreciation of these factors, we have tried over the past year to approach culture using three distinct lenses:\n\nEach figured prominently in our most recent culture conference in June.\n\nLessons from behavioral science are, perhaps, the most influential. For example, many of you may be familiar with the book by Daniel Kahneman, Thinking, Fast and Slow. Kahneman divides human decision-making into two systems. \"System 1\" uses associative, subconscious memory to make quick judgments. \"System 2\" employs reason in a slower and more methodical way. The two ways of thinking are sometimes nicknamed \"the gut\" and \"the head.\" While we may believe that most of our actions result from System 2 deliberation, in fact we mostly rely on System 1 reactions. In terms of ethics, this suggests that individuals usually default to automatic decision-making. What affects that quick, automatic thinking? Millennia of human evolution and a lifetime of learning, for certain. But group norms also shape our automatic, associative judgments. Organizations need to recognize phenomena such as these when devising ways to influence the behavior of their employees and improve their cultures.\n\nTechnological change holds both great promise and risks for the development of healthy cultures. New technology enables monitoring of employee and customer behavior as never before. This has its upsides, which may include early detection of misconduct. For example, some data scientists maintain that network analysis of communication patterns among employees may identify or even predict problem areas within an organization. At the same time, we know that bias can infect automated outcomes. Algorithms and analysis are only as reliable as those who write code and interpret data. In all of this, there is the danger of regarding people as objects of measurement, rather than as beings of inherent worth and irreducible complexity. We will likely have to deal with technological advances that both support trustworthy human connection and exacerbate social distancing.\n\nThe composition and dynamics of the workforce in financial services have also been evolving. One development is the entrance of a new generation of young workers, many of whom may exhibit different styles of communication and distinct motivations. At the risk of generalizing, younger workers are digital natives, less interested in hierarchy, seek continuous feedback, and are eager for their work to have an immediate impact. Moreover, many of the newer entrants to the workforce do not want to stay at one firm for more than a few years. All of these factors present challenges to the development and maintenance of a coherent organizational culture. They probably also introduce greater diversity of thought, perspective, and experience to the financial services sector than ever before. These demographic shifts may necessitate new ways of achieving cultural goals, including considering how employees are most effectively incentivized and engaged.\n\nUnderstanding the Change Process\n\nMy colleagues and I plan to continue to employ these three lenses—insights from non-financial disciplines, attention to technology developments, and awareness of workforce changes—in our Culture Initiative work in the coming year.\n\nI expect we will also emphasize a fourth lens, one that focuses more closely on the human processes within change management.\n\nMuch of this morning’s discussion rightly centered on the importance of building an environment of psychological safety in organizations. I would like to take this discussion a step further. In addition to psychological safety, we also need to take greater account of the on-going processes of individual and group change.\n\nIt is estimated that upwards of 70 percent of organizational culture change efforts ultimately fail. That’s a sobering number for those of us who advocate for industry culture reform. What is at the root of this failure? No doubt there are many causes, but it seems to me that one of the main reasons is the failure to recognize that while change can sometimes occur in an instant, it takes most people’s perceptions and feelings a much longer time to adjust and move ahead.\n\nIn addition, and perhaps because they are by nature forward-leaning, many leaders tend to see the desired end state of change—that is, the goal or the future—much more clearly than their followers. As a result, leaders may not always appreciate how much time or support is needed for the stages of learning that are necessary for real change to take hold across an organization.\n\nThe author William Bridges has addressed these dilemmas, and has proposed a well-regarded theory that describes organizational transition as a three-phase process.4 I won’t go into great detail this morning about Bridges’ theory, but his basic ideas are straightforward enough to describe in my remaining time.\n\nThe first phase of organizational transition is called \"Ending, Losing, and Letting Go.\" In this phase, leaders help employees deal with their tangible and intangible losses stemming from change, and mentally prepare to move on.\n\nThe second phase is called \"The Neutral Zone.\" This involves helping people move through the change period, making the most of confusion and uncertainty by encouraging employees to experiment and innovate.\n\nThe final phase of transition is \"The New Beginning.\" In this period, leaders help others form a new identity and discover a renewed sense of purpose that make changes lasting and effective.\n\nThe second phase—The Neutral Zone—is perhaps the most unrecognized and most important step in organizational transition. According to Bridges, critical psychological realignments and re-patterning take place in this Zone. It is an uncomfortable place. Most people try to get out of it as quickly as possible, either by rushing ahead toward the future or retreating into the past. Bridges counsels, however, that spending meaningful time in The Neutral Zone is crucial. This is where the creative potential of transition is fully realized and true transformation takes place. Bridges’ further insights about The Neutral Zone—for example, the need to properly mourn the past before the future can be embraced—also ring true to me, both on an individual and group level.\n\nConclusion\n\nAs I noted earlier, significant strides have been made in establishing the importance of culture in firms and the financial industry as a whole. Many of the key levers that help create healthy cultures have also been identified. But more work needs to be done to understand how these various levers of change function.\n\nFor the New York Fed's Culture Initiative, increasing the likelihood of successful culture reform may well depend on developing greater practical insight into the winding, sometimes fitful, and almost always long-term processes of individual and organizational transformation. By viewing culture reform through the human change process, rather than solely through the end changes we wish to see occur, we may become aware of previously unseen obstacles and discover fresh paths to get to our sought-after destination. In closing, I would like to reiterate a message from John Williams, the current President of the New York Fed. In a recent speech,5 he stated that while the journey toward reforming culture in the financial services industry is a long one, we are optimistic that we can and we will make progress. Thank you."
    },
    {
        "title": "Monetary Policy and the Economic Outlook",
        "date": "Sep 4, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil190904",
        "content": "Thank you for the opportunity to speak this morning. In a speech Chair Powell gave last month, he said, “Low inflation seems to be the problem of this era, not high inflation.”1 Persistently low inflation is just one of the reasons the theme of this conference is very timely.\n\nOur Dual Mandate Goals\n\nAnd inflation is one of the topics I’m going to address today. The Fed has two goals set by Congress: maximum employment and price stability. These are also known as the dual mandate. When I talk about price stability, our objective is to keep the inflation rate near our symmetric 2 percent goal,2 so understanding inflation data and how it fits into the broader economic picture is critically important.\n\nThis morning I’m going to set the scene for that broader economic picture and explain how changes in the landscape influenced the rate adjustment the Federal Open Market Committee (FOMC) made in July. I’ll then discuss the challenges on the horizon and how they feed into my thinking on monetary policy.\n\nThis is as good a time as any to remind everyone that the views I express today are mine alone and do not necessarily reflect those of the Federal Open Market Committee or others in the Federal Reserve System.\n\nThe Economic Outlook\n\nIf you do an online search about recent economic developments, the word “uncertainty” will come up a lot. And it’s true that uncertainty, both at home and abroad, is playing an important role in my thinking about the economic outlook and monetary policy, as I’ll discuss shortly.\n\nBut when you take a step back and look at where things stand today relative to our goals, the economy is in a favorable place. The current expansion is the longest on record. The economy continues to grow at a moderate pace, as seen in the latest GDP numbers. And this growth has supported month after month of job gains and a pickup in wage growth in the past couple of years.\n\nFurthermore, the overall unemployment rate is near the lowest it has been in 50 years. And the benefits of the strong labor market have been broad across the population: Unemployment rates for African-Americans and Hispanics, while still higher than the overall rate, are near their all-time lows. These low rates aren’t just a fluke of how unemployment is measured in the official statistics—the broadest measure of underemployment, called U-6, is at its lowest level since 2000. These are all very positive signs when it comes to assessing our maximum employment goal.\n\nThe economy is in a good place, but not without risk and uncertainty (there, I said it!). Persistently low inflation is a key area of my attention, with the core PCE inflation rate—which strips out volatile food and energy prices—running at 1.6 percent, nearly half a percentage point below our 2 percent longer-run target.\n\nOn its own, inflation somewhat below our longer-run goal would not be such a big deal, especially with our economy strong. But the broader context is important. Ongoing disinflationary pressures from abroad, and the risk that inflation expectations in the U.S. may have drifted down after many years of inflation running below 2 percent, form an important part of this picture.\n\nIf we look beyond the headline GDP figure, which remains good, there are more mixed signals coming from different sectors. Robust consumer spending is balanced by signs of slowing business investment. We’ve also seen a decline in exports and weakening manufacturing data, reflecting slowing global growth and uncertainty related to trade and geopolitical risks.\n\nI am carefully monitoring this nuanced picture and remain vigilant to act as appropriate to support continuing growth, a strong labor market, and a sustained return to 2 percent inflation.\n\nMonetary Policy Adjustments\n\nThis brings me to our decision to lower the federal funds rate in July and how that fits into recent history. Cast your minds back to a year ago, when the Fed was continuing along the path of normalizing monetary policy. This context provides useful information for how and why our stance has evolved over time.\n\nDuring the summer of 2018, the economy was growing above trend as businesses rode the wave of fiscal stimulus. The unemployment rate had fallen below 4 percent, a figure not seen since the peak of the tech boom in 2000. The global outlook was still quite positive, and inflation was close to our 2 percent goal. Against that backdrop, very gradually bringing interest rates back to more normal levels was appropriate in order to keep the economy on a sustainable path of growth.\n\nBut the economic outlook and the uncertainty around that outlook have evolved considerably since then. While there’s not been a dramatic change seen in the overall numbers yet, the more detailed picture that emerged by summer of this year pointed to an outlook of slowing growth and inflation falling short of our goal. This in turn argued for a somewhat more accommodative monetary policy stance.\n\nThe combination of muted inflation, slowing global growth, and uncertainties related to trade and other global developments led the FOMC to adjust the federal funds rate down by a quarter of a percentage point and conclude the run-off of the Fed’s balance sheet two months earlier than previously indicated.\n\nOur policy actions and communications have contributed to an easing of financial conditions that should help sustain the expansion and get us closer to our dual mandate goals.\n\nChallenges on the Horizon\n\nWhen I think about the road ahead, my number one goal is to keep the expansion on track. This is how we will keep unemployment low and bring inflation back to 2 percent. I will be watching closely at how both domestic and broader, global developments unfold.\n\nAt present, slowing global growth and geopolitical uncertainty pose particular challenges. In its latest World Economic Outlook report, the International Monetary Fund downgraded its projection for global growth and highlighted downside risks.3\n\nSlower global growth reduces demand for our exports and puts a dampener on both U.S. inflation and growth prospects. And these aren’t just projections for the future—we’re seeing manifestations of slowing growth around the world now. Germany, the UK, and China are all experiencing slowdowns, and the euro area is of particular concern. In response, the European Central Bank and several other central banks have either adjusted, or indicated they will adjust their monetary policy stance to support their economies.\n\nThese concerns about global growth are compounded by broader geopolitical uncertainty. The risk of a no-deal Brexit for the UK is looming large, the political situation in Italy remains fraught, political tensions continue in Hong Kong, and Argentina’s economy is coping with a debt crisis.\n\nOn our own shores, concerns around trade policy with China are adding to an uncertain picture. My contacts in the business community have said this is making them more cautious about investment. The effects of this angst are already showing up in the investment numbers.\n\nAn additional fly in the ointment, if you will, is the recently released downward revision for GDP growth covering last year and an announced estimate of a sizable downward revision to payroll employment. One implication of these revisions is that the economy’s underlying momentum was already somewhat less robust than previously thought, even before recent developments pointed to a less rosy outlook.\n\nThe Path Forward\n\nSo what will the response from the Fed be to all of these developments? Our role is to navigate a complex and at times ambiguous outlook to keep the economy growing and strong.\n\nAs events unfold and conditions evolve, it’s critical we understand which developments are mere bumps in the road and those that will affect the outlook in a material way over the longer term. That means maintaining a data-dependent approach that takes into account the risks and uncertainty that are weighing on the economy.\n\nI will continue to consult everything from very granular data about individuals’ experiences of employment conditions, through to the macro projections for global growth. We need to consider all of the information available and be flexible in our response.\n\nConclusion\n\nLow inflation is indeed the problem of this era. The current outlook of moderate growth, low unemployment, but stubbornly low inflation is a reflection of the broader economic picture—the July rate adjustment an appropriate response to ease financial conditions and support the economy.\n\nHow the U.S. outlook evolves in the future is fundamentally tied to the fortunes of the economies around the world. As I look ahead, I’m keeping a keen eye on all the data, both domestic and global, and the implications for our economy. With an uncertain outlook, vigilance and flexibility are essential for achieving our dual mandate goals of maximum employment and price stability. Persistently low inflation, heightened uncertainty, and global cross-currents make this a particularly challenging time for monetary policy, and my laser focus is on doing the best we can to support a strong economy and achieve our 2 percent inflation goal.\n\nThank you."
    },
    {
        "title": "Living Life Near the ZLB",
        "date": "Jul 18, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil190718",
        "content": "My wife is a professor of nursing, and she says one of the best things you can do for your children is to get them vaccinated. It’s better to deal with the short-term pain of a shot than to take the risk that they’ll contract a disease later on.\n\nI think about monetary policy near the zero lower bound—or ZLB for short—in much the same way. It’s better to take preventative measures than to wait for disaster to unfold.\n\nToday I’m going to talk about three things: first, the heightened relevance of the ZLB for monetary policy; second, the strategies designed to mitigate the effects of the ZLB in an economic downturn; and finally, why these strategies are so important for achieving our monetary policy goals.\n\nI’ve already referred to monetary policy several times. So, before I go any further, I need to give the standard Fed disclaimer that the views I express today are mine alone and do not necessarily reflect those of the Federal Open Market Committee or others in the Federal Reserve System.\n\nThe Global Economic Context\n\nOver the past quarter century, a great deal of research has gone into understanding the causes and consequences of the zero lower bound.1 The achievement of low inflation in the United States—which, all else equal, implied lower interest rates—coupled with Japan’s long period of deflation and near-zero interest rates, reignited concern about the ZLB back in the 1990s. And the experience of the global financial crisis and its aftermath—with many advanced economies facing, or in some cases, crossing the zero lower bound—has moved this from a mostly theoretical exercise to a very practical one.\n\nAn added impetus to this research has been the growing evidence that the neutral rate of interest rate has fallen significantly. I, working with various coauthors, have devoted a significant chunk of my academic career to studying r-star, or the long-run neutral rate of interest, and its implications for monetary policy.\n\nOur current estimates of r-star in the United States are around half a percent. That’s actually now lower than at any time before the Great Recession.2 We’ve seen similar declines in r-star in other advanced economies, including in Japan and the euro area.\n\nAs shown in Figure 1, the weighted average of estimates of r-star for Canada, the euro area, Japan, the United Kingdom, and the United States is now also half a percent, nearly 2 percentage points below where it stood at the turn of this century.\n\nThese very low neutral rates are a result of long-term structural factors slowing growth. They’re driven by demographic changes and slow productivity growth, which are unlikely to reverse any time soon.\n\nLow r-star implies that many central banks will be grappling with the challenges of life near the ZLB, which is why it’s so critical to consider how the ZLB alters strategies related to monetary policy.\n\nMonetary Policy Near the ZLB\n\nAnd that’s exactly the question we looked at around 20 years ago. In a paper and subsequent FOMC briefing in January 2002, my colleague Dave Reifschneider and I evaluated the effects of the ZLB on the macro economy and examined alternative monetary policy strategies to mitigate the effects of the ZLB.3\n\nThis work highlighted a number of conclusions based on model simulations.4 In particular, monetary policy can mitigate the effects of the ZLB in several ways:\n\nThe first: don’t keep your powder dry—that is, move more quickly to add monetary stimulus than you otherwise might. When the ZLB is nowhere in view, one can afford to move slowly and take a “wait and see” approach to gain additional clarity about potentially adverse economic developments. But not when interest rates are in the vicinity of the ZLB. In that case, you want to do the opposite, and vaccinate against further ills. When you only have so much stimulus at your disposal, it pays to act quickly to lower rates at the first sign of economic distress.\n\nThis brings me to my second conclusion, which is to keep interest rates lower for longer. The expectation of lower interest rates in the future lowers yields on bonds and thereby fosters more favorable financial conditions overall. This will allow the stimulus to pick up steam, support economic growth over the medium term, and allow inflation to rise.\n\nThese first two conclusions featured in my paper with Dave Reifschneider from 2000. In that paper, these ideas were described as modifications of an otherwise standard Taylor rule for monetary policy. The “forward-looking adjustment” to the Taylor rule illustrated the idea of moving in advance of a downturn to provide timely stimulus. The “backward-looking adjustment” illustrated the idea of keeping interest rates lower for longer.\n\nFinally, policies that promise temporarily higher inflation following ZLB episodes can help generate a faster recovery and better sustain price stability over the longer run.5 In model simulations, these “make-up” strategies can mitigate nearly all of the adverse effects of the ZLB.\n\nOne particular benefit is that such strategies can be highly effective at keeping inflation near the target on average, thereby anchoring inflation expectations at the desired level.\n\nThe Gravitational Pull of the ZLB\n\nBut why are these measures so important close to the ZLB?\n\nThe Fed has two goals: maximum employment and price stability. Our goal for prices—as I’m sure you’re aware—is two percent. The short answer is, in order to achieve our goals.\n\nThe long answer centers on inflation and inflation expectations. I’m going to give you the long answer!\n\nIn the last decade it’s proven particularly challenging to keep inflation at our two percent goal. Japan and the euro area have faced similar challenges in terms of sustaining inflation at their goals.\n\nOne reason it’s been hard to sustain inflation at the target rate is that persistently low inflation due to policy being constrained by the ZLB can feed into inflation expectations. If inflation gets stuck too low—below the 2 percent goal—people may start to expect it to stay that way, creating a feedback loop, pushing inflation further down over the longer term.\n\nThe lower average level of inflation translates into a lower level of interest rates cuts available during a downturn, making it even harder for policymakers to achieve their goals.6\n\nIn 1980, when core PCE inflation reached a peak of 10 percent, it was hard to conceive of a world where inflation that’s too low would be the preoccupation of central banks.\n\nBut the 10 years since the Great Recession demonstrate just how much low r-star has changed the economic environment. Unprecedented monetary stimulus in advanced economies supported economic recoveries and significant declines in unemployment, with unemployment rates today near, or in many cases well below, those seen before the crisis.7 Bold monetary policy actions surely played an important role in averting far greater catastrophes and aided economic recovery.\n\nDespite these actions and this success at bringing unemployment to low levels, inflation rates have persistently been below central banks’ goals. The fact that inflation has been running below target in most advanced countries suggests that this challenge is not due to factors specific to a single country. Instead, there are more systemic factors at play, with the root cause being a combination of low neutral interest rates and the lower bound on interest rates.\n\nInvestors are increasingly viewing these low inflation readings not as an aberration, but rather a new normal. This is evidenced by a broad-based decline in market-based measures of longer-run inflation expectations since the mid-2000s, with the UK providing the exception, as shown in Table 1.\n\nThis downward slippage of longer-run inflation expectations, if it persists, implies that in the future central banks start further from their inflation goals and have even less room to maneuver, making the problem of low r-star and the ZLB even more difficult.\n\nConclusion\n\nThe ZLB has been a topic of study for nearly two decades. But recent history and the outlook for the longer-term future, make it more relevant than ever.\n\nLow inflation expectations, very low r-star, and slower growth all point to a challenging world where policymakers need to make the best use of the tools at their disposal to achieve their goals of strong economies and price stability.\n\nThe key lessons from this research hold today and in the future. First, take swift action when faced with adverse economic conditions. Second, keep interest rates lower for longer. And third, adapt monetary policy strategies to succeed in the context of low r-star and the ZLB.\n\nThese actions, taken together, should vaccinate the economy and protect it from the more insidious disease of too low inflation.\n\nThank you.\n\nPresentation"
    },
    {
        "title": "901 Days",
        "date": "Jul 15, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil190715",
        "content": "Thank you for the kind introduction and the opportunity to speak this morning. It’s a particular pleasure to be sharing the stage with Andrew Bailey, who has played such an important role in leading efforts on establishing robust references rates to replace the London Interbank Offered Rate (LIBOR). This critical undertaking traverses markets and transcends jurisdictions, making international cooperation and coordination essential for success.\n\nBefore I go any further, I need to give the standard Fed disclaimer that the views I express today are mine alone and do not necessarily reflect those of the Federal Open Market Committee or others in the Federal Reserve System.\n\nWhere Have We Come From?\n\nToday I’m going to talk about the progress we’ve made, and the path that lies ahead. But first, given that 12 years have passed since LIBOR first became an acute area of concern, it’s important to remember why replacing it is so critical.\n\nLIBOR is based on submissions from individual banks. The volume of actual transactions that term LIBOR is based on is very small—totaling around $500 million on a typical day. To most people, $500 million may sound like a lot, but given that $200 trillion of financial contracts reference U.S. dollar LIBOR, it’s really a drop in the ocean.1\n\nAs a result, submissions are largely based on judgment, as opposed to real numbers. When the LIBOR scandal erupted, it became clear that there had been fraud and collusion, both within and across financial services firms, in the pursuit of profit.\n\nThe reliance on expert judgment, rather than actual data, makes the rate too vulnerable to manipulation. It’s readily apparent to all those in the industry—both firms and regulators—that LIBOR is fundamentally broken.\n\nConsequently, banks are increasingly reluctant to provide LIBOR submissions, adding to the level of risk around using the rate. The U.K. Financial Conduct Authority (FCA) has reached an agreement with banks to keep submitting rates through the end of 2021, but in 2022 the existence of LIBOR will no longer be guaranteed. In other words, LIBOR’s survival is assured for only another 901 days.\n\nThose in the market know the full scale of the risks this creates for the financial system. LIBOR isn’t just the rate that banks use to borrow from one another—it’s also the underlying rate on numerous derivatives and loan contracts. The $200 trillion figure I mentioned earlier is the approximate total exposure to U.S. dollar LIBOR. That’s about 10 times U.S. gross domestic product.\n\nNone of what I am telling you is new, but it’s worth reminding ourselves just how significant reference rates are to the functioning of financial markets, both in the United States and around the world. There’s broad recognition that exposure to LIBOR is a leading risk to financial stability.2 We need to treat it as such.\n\nProgress Has Been Made\n\nAnd, to date, significant progress has been made, moving us toward a safe and sound reference rates regime. What’s more, there’s been notable momentum in recent weeks.\n\nBut first, let’s take a look back at some of the more significant accomplishments.\n\nBack in 2014, the New York Fed and the Board of Governors convened the Alternative Reference Rates Committee (ARRC), which is made up of market participants. In an important milestone, the ARRC selected the Secured Overnight Financing Rate (SOFR) as its preferred alternative to U.S. dollar LIBOR in 2017.\n\nSince April 2018, the New York Fed has produced SOFR every day. It is based on much higher volumes than LIBOR, and compliant with the Principles for Financial Benchmarks set forth by the International Organization of Securities Commissions (IOSCO).3 SOFR is based on transactions—not judgment—making it much more robust.\n\nAll of this points to the good work that can happen when the private and official sectors come together.\n\nNow, there has been some criticism leveled at SOFR, most notably the lack of a term rate, and not enough liquidity in the market.\n\nBut liquidity has begun to develop in derivatives and cash markets. And, earlier this year Federal Reserve Board economists published research demonstrating how forward-looking term rates can be derived from SOFR futures and swaps markets.\n\nHowever, we are still some time off from a point at which a robust, IOSCO-compliant term rate can be created, and use of such a term rate should be limited to certain segments of the loan market and to fallbacks for new contracts. I want to emphasize that the industry must not wait for a SOFR term rate to transition away from LIBOR.\n\nIn my view, the biggest challenge isn’t liquidity or the creation of a term rate, it’s a willingness on the part of the market to stop using LIBOR.\n\nWe need a mindset shift where firms realize that every new U.S. dollar LIBOR contract written digs a deeper hole that will be harder to climb out of.\n\nIf companies are going to use LIBOR, they need to start including robust fallback language in the contract, so that if LIBOR ceases to exist, chaos does not ensue.\n\nThis is an area of recent progress I mentioned earlier. The International Swaps and Derivatives Association (ISDA) has led great work on the development of contingencies for some derivatives products for the scenario where U.S. dollar LIBOR ceases to exist. The public consultation on fallback language for some non-U.S. dollar derivatives contracts took place last year. The corresponding consultation for U.S. dollar and some other non-U.S. dollar derivatives contracts closed just last week, and I look forward to hearing the results and next steps.\n\nUniversal changes to derivatives contracts will take out about 95 percent of the exposure—the $200 trillion number. If the market signs up to the ISDA protocol when it’s published early next year, it will be a huge step in the right direction.\n\nIn addition, the ARRC has released four sets of recommended fallback language for different types of cash products. This fallback language clearly stipulates trigger events, replacement rates, and spread adjustments, and the triggers are close to those that ISDA has consulted on for derivatives.\n\nAnd on Friday, the Securities and Exchange Commission (SEC) staff issued a statement highlighting risks and disclosures for market participants to evaluate proactively as they transition away from LIBOR.\n\nWhile derivatives and institutional cash markets make up the bulk of LIBOR-based contracts, consumer products are a critical area where the industry needs to focus. Ensuring a fair transition away from LIBOR is going to be particularly challenging, and it’s important consumers understand what the changes being made mean in real terms.\n\nLast week the ARRC published potential paths forward for adjustable-rate mortgages (ARMs). These include a set of guiding principles for ARMs and other consumer products, draft fallback language for new ARMs referencing U.S. dollar LIBOR, and a framework for use of SOFR in new ARMs.\n\nMuch progress has been made, but there is still much to do. The ARRC transition plan is well underway, but we need to see certain developments in the U.S. financial industry for a smooth transition.\n\nMilestones for the future include deeper liquidity in SOFR derivatives markets, greater issuance of SOFR-linked cash products, a reduction in the issuance of LIBOR-linked cash products, and the closing out of legacy positions on cash products, to name a few.\n\nThese all require the partnership and perseverance of the market.\n\n2022 Is Just Around the Corner\n\n2022 feels like it’s a long way away, but believe it or not 901 days can disappear, almost in an instant.\n\nAnd I don’t always sense urgency among market participants on this issue. Tellingly, contracts referencing U.S. dollar LIBOR, without robust fallback language, continue to be written.\n\nMy message: don’t wait for term rates to get your house in order. Engage with this issue now and understand what it means for your operations. Recognize where your exposure lies and deal with the contracts that mature after 2021 that lack robust fallback language.\n\nThis is a problem you have the opportunity to get ahead of now. Don’t wait until January 1, 2022 to manage your business’ transition away from LIBOR, because it’s going to be too late.\n\nConclusion\n\nI’ll conclude with this: Reference rates are a complex issue, with numerous countries, a wide range of public and private entities, and trillions of dollars involved. Progress has been made, and the way forward is clear. But we are now at a critical point in the timeline. The very complexity of this issue is why the industry cannot afford to wait any longer. The clock is ticking, and there are 901 days left."
    },
    {
        "title": "A Tale of Many Economies",
        "date": "Jul 11, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil190711",
        "content": "Thank you for that kind introduction and for having me to speak today.\n\nI’m an economist, so I naturally have a fascination with data, statistics, and what we can learn from them. But the reason I find economics so interesting isn’t the numbers or the charts—it’s how they shape the everyday lives of people. And that’s the reason running a Federal Reserve Bank is such a privilege. The work we do—our public mission—plays a big role in helping people get jobs, take out mortgages, or grow their businesses.\n\nToday I’m going to talk a bit about that work and the health of the economy—both at the national level and for this region, the Federal Reserve’s Second District.\n\nBefore I go any further I need to give the standard Fed disclaimer that the views I express today are mine alone and do not necessarily reflect those of the Federal Open Market Committee or others in the Federal Reserve System.\n\nI’ve just referred to the Federal Open Market Committee and the Federal Reserve System, but I know it’s not always entirely clear to people what they are, or what they do. The Federal Reserve has two monetary policy goals set by Congress: keeping prices stable, and ensuring there are as many jobs in the U.S. as possible.\n\nUnderpinning both of these goals is a strong economy, and we use monetary policy to support that. A major part of monetary policy is setting interest rates, and we do this at Federal Open Market Committee—or FOMC—meetings in Washington, D.C.\n\nThe Federal Open Market Committee consists of the presidents of the 12 Federal Reserve Banks and the Board of Governors, who sit down as a group every six weeks to decide what interest rate will ensure sustainable growth. We don’t want the economy to grow so fast that it’s unsustainable, or so slowly that it discourages jobs and investment. Just like Goldilocks and her porridge, we don’t want it too hot or too cold—we want it just right.\n\nThe Macro Outlook\n\nWe make those decisions by examining a wide range of data and information. Given that our goals are price stability and maximum employment, we tend to focus a lot on data such as the inflation rate, employment, unemployment, and GDP growth. But, that’s not all—we also collect and analyze enormous amounts of other information to help us assess the state of the economy and the economic outlook.1\n\nSo, what does all that information tell us about the current economic outlook?\n\nAfter surging ahead last year, the U.S. economy appears now to be growing at a more moderate pace. I expect GDP growth to be around 2-1/4 percent this year, moderately above my estimate of the long-run sustainable growth rate for the economy.\n\nIn my past speeches, I often mentioned the fact that we’re nearing the longest expansion on record. Well, as of this month, I can finally say that we’ve reached that milestone. The economy has been growing for 121 months—a little over 10 years.\n\nWhile this is undoubtedly good news, the headline data mask a more nuanced economic picture. Consumer spending has been an important driver of growth, and the most recent readings have been very positive. However, other signs point to slowing growth. In particular, the latest indicators suggest that business fixed investment has softened and that manufacturing production is in decline. And the outlook for growth outside the United States has dimmed, which will weigh on demand for U.S. products.\n\nThis mixed picture is mirrored in the employment data. On one hand, the unemployment rate, which stands at 3.7 percent, is near the lowest we’ve seen in 50 years. On the other, job growth has slowed this year relative to last year’s pace.\n\nFinally, turning to prices, the latest data show that underlying inflation is 1.6 percent, below our 2 percent goal. The major challenge with inflation that’s persistently lower than 2 percent is that low inflation feeds into inflation expectations. If inflation stays too low, people will start to expect it to stay that way, creating a vicious cycle, pushing inflation further down over the longer term, and making it harder to achieve our goals through monetary policy.\n\nThe Second District\n\nAll of these numbers tell the story of the U.S. economy at the macro level, but they’re not always reflected in the real experiences of Americans across the country. I referred at the beginning of this speech to the Second District—which includes New York State, parts of New Jersey and Connecticut, Puerto Rico, and the U.S. Virgin Islands—and is the area the New York Fed is responsible for. I spent yesterday and today meeting with district business leaders, community organizers, and elected officials, hearing about their work, their successes, and their challenges.\n\nPeople’s experiences are influenced by the macro level, but their local economy plays an equal, and often more important role in shaping their economic opportunity.\n\nOne aspect of the Fed that we don’t talk about enough is the work our regional economists and our outreach teams do, trying to understand what’s going on at the local level.\n\nWhile the U.S. economy has been growing for the last 10 years, analysis by New York Fed economists shows that not everyone is feeling the benefits equally.\n\nGrowth is concentrated in the largest cities like New York and San Francisco, and those who benefit the most are those who already have high incomes.2 A large part of this is because current economic conditions favor highly skilled workers who tend to flock to cities. These big metropolitan areas are successful, but they also suffer from some of the starkest wage inequality in the country.\n\nBy contrast, upstate New York is less unequal, but the disappearance of manufacturing jobs has held back growth. More equal wage growth is only good news if people have jobs. But many have found themselves in the position of leaving the area in which they grew up to look for work.\n\nThe Albany area has bucked this trend. The mix of colleges and universities specializing in innovative subjects like nanotechnology, and high-tech businesses, alongside its position as a state capital, has created a real economic success story. I know there are a lot of students in the room, and choosing SUNY was a wise move. Investing in an education that equips you for the future will pay off over the long term.\n\nThe Tale of Many Economies\n\nSo what can the Fed do about this complex picture—the tale of many economies I’ve talked about today?\n\nMonetary policy is an important tool, but it alone cannot address all the economic issues that we face. The policies we enact at the FOMC are vitally important for sustaining growth at the national level, but they can’t determine everything that happens at the local level.\n\nThis is where our community development work comes in. Through our research and outreach, we put data and analysis into the hands of community leaders to give them tools to strengthen their local economies. We have programs that use our convening power to bring together stakeholders to share knowledge—our workforce development program is a key example. And finally, we use our research to catalyze action and investment. Tools such as our resource guidebooks serve as a starting point for investors.\n\nConclusion\n\nI started by talking about our goals—keeping prices stable and ensuring maximum employment. These things are essential for a strong and prosperous economy. And while the current picture is complex, the economy is in a good place.\n\nBut not every community has benefited from the growth of the last decade to the same degree. At the New York Fed, we are committed to understanding what drives growth, economic inequality, and what creates opportunity. Those are the insights that drive my fascination with economics and my commitment to the Federal Reserve and its public mission. Those are the insights from the data we need to leverage, so that all parts of the U.S. economy can reach their full potential."
    },
    {
        "title": "Diversity and Inclusion: We Are Not Where We Need to Be",
        "date": "Jun 25, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil190625",
        "content": "Good morning. I’m an economist, so I’m going to start with some numbers.\n\nThe unemployment rate for lesbian, gay, bisexual, and transgender people is nearly double the national average.1 And the unemployment rate specifically for transgender people is three times the national rate.2 As President of a Federal Reserve Bank, I am focused on two vital economic goals: maximum employment and stable prices for the U.S. economy. These statistics paint a clear picture of why conferences like this one are so important: We are not where we need to be.\n\nBefore I go any further, I need to give the standard Fed disclaimer that the views I express today are mine alone and do not necessarily reflect those of the Federal Open Market Committee or others in the Federal Reserve System.\n\nLet me say, as an ally, that it’s a genuine honor to be invited to open today’s Forum.\n\nThis Pride Month marks 50 years since the Stonewall uprising. We’ve come a long way since the 1960s, but we still have far to go. It was also a great honor to be able to fly the Pride flag outside the Federal Reserve Bank of New York for the first time this June. But, given that it is 50 years since Stonewall, it’s clear we have more catching up to do.\n\nAnd this is where we owe OPEN Finance a great debt of gratitude. OPEN Finance gives us the tools we need to assess how we’re doing, and where we need to make progress.\n\nWith the support of OPEN Finance, and the dedication of the Open Network at the New York Fed, we’ve introduced a number of new policies and measures that drive our efforts to make the Bank the open and inclusive employer we aspire to be. I’d personally like to thank David Ignell and Gil Carroll, presidents of our Open Network, for their work moving us forward.\n\nThese efforts are important because they send a message to LGBT employees and the organization as a whole about our values and culture.\n\nI’d like to share one specific example. We recently created the option to add gender pronouns to our official e-mail signatures. This is a part of an organization-wide effort to raise awareness around the use of pronouns. On its own, it may seem like a small change, but it sends a loud message that part of respecting one another is using the correct pronouns. This effort has been well received, and a big part of that is because it’s created spaces for conversations that just weren’t happening before.\n\nBut policies can only go so far.\n\nIn fact, we can have world-class policies, but if we don’t have a culture where people feel comfortable bringing their whole selves to work, rules alone won’t get us to where we need to be.\n\nThis is true of every element of diversity and inclusion. In a culture where taking extended leave damages your career prospects, a generous parental leave policy won’t make up for that. In a culture where people don’t feel that their voices are welcome, few will speak up. In a culture that fails to embrace differences, we won’t reap the benefits of diversity.\n\nThe data are clear. Nearly half of LGBT people are closeted at work, and the top reason LGBT workers don’t report negative comments they hear about LGBT employees is they don’t believe that anything will be done about it.3\n\nThis gets to the heart of why the “inclusion” piece of diversity and inclusion is essential, and a personal priority. Obviously, there’s a moral imperative for diversity, and numerous studies have shown the benefits to both productivity and the bottom line.4\n\nBut for me, it’s my experiences as both an academic and a policymaker that have made inclusion so important. Economics is a profession that has a glaring diversity issue. As economists, we have not done enough to create a diverse profession or an inclusive culture.\n\nWhen I’m sitting in a room and see a bunch of people who look and sound the same, I know it’s a problem. When everyone has an identical view, it’s easy to assume it’s because that view is the right one. But for me it raises questions: Who isn’t in the room? Who’s missing, and why aren’t they here?\n\nI’m always on the hunt for different perspectives on how to interpret the data, new takes on old problems, and innovative approaches to tackle the challenges that lie ahead. If I have a bunch of cookie-cutter theories and explanations that look like my own, I know it’s highly probable we’ve not been challenging ourselves enough.\n\nMost CEOs will tell you that that their people are their greatest asset. I know that to be true. That’s another reason why an inclusive culture is something that’s so important, and needs to be embedded across an organization. It’s not enough for diversity to live in HR documents and policies. We need to create an environment where everyone has the potential to flourish; to bring their best, most innovative, and creative selves to work; and to do the best work of their careers.\n\nSo how do we make progress? We need to have honest and open conversations every day. Not just about work performance and ideas, but on everything. We need to make sure that everyone—regardless of their background, their age, the color of their skin, their sexual orientation, or their gender identity—feels their perspectives are heard and valued.\n\nCreating an inclusive culture has no end date. Like so many aspects of culture and values, it’s not a project with a deadline, where we can say, “OK, we’re done now.” Instead, we need to be constantly reassessing our goals, refining what we do, and how we do it.\n\nWe need to be vigilant, attuned to change, and open to new ideas. Because it’s one thing to create an open culture, but a much harder challenge to keep it that way.\n\nI’m reminded of a quote by Brené Brown, which I believe sums this up: “To not have the conversation because of discomfort is the definition of privilege…”\n\nWhen we step back, when the dialogue—however uncomfortable it may be—stops, or is never initiated in the first place, is when there’s a problem. It’s the conversations that aren’t spoken that erode trust, alienate people, and take us back.\n\nI’d like to conclude where I began. I spend a lot of time examining data, and it’s a source of constant disappointment and frustration that LGBT people, women, and people of color continue to have less economic opportunity than their straight, white, male counterparts.\n\nCreating equality of opportunity at the national level is a challenge the United States continues to struggle with. But it’s both the right thing to do and the only way we’ll reach our full economic potential.\n\nCreating a more diverse—and equally important—a more inclusive Federal Reserve is a key priority. We have come a long way, but there’s no doubt we still have far to go. And this is a journey without a final destination. When someone tells me “we’re done,” is when I know we have the furthest to go. It’s also a journey that is so much easier when we do not walk alone. Partners like OPEN Finance are crucial to our progress.\n\nOnce again, I’d like to thank you for inviting me to open today’s forum. I know it’s going to be a productive and insightful day, and one full of the conversations we all need to be having more frequently."
    },
    {
        "title": "If We Fail to Prepare, We Prepare to Fail",
        "date": "Jun 6, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil190606",
        "content": "Note: These remarks are based on those delivered at the 9th High-Level Conference on the International Monetary System on May 14, 2019.\n\nIntroduction\n\nThank you for that kind introduction and for the opportunity to speak again at the Council on Foreign Relations. I’m going to keep my remarks brief so that there’s plenty of time for Steve to grill me on interest rates, followed by some thoughtful questions from the audience.\n\nWith the mention of interest rates, it’s time I remind everyone that the views I express today are mine alone and do not necessarily reflect those of the Federal Open Market Committee or others in the Federal Reserve System.\n\nJohn Maynard Keynes is credited with saying, “When the facts change, I change my mind. What do you do, sir?” The economic era we’ve experienced for the last 11 years has been defined by the Great Recession. There’s the world before, and the world after, the subprime mortgage crisis, the collapse of Lehman Brothers, and the ensuing financial crisis.\n\nThe Road to Recovery\n\nGovernments and central banks have spent the intervening years working to get their economies growing, create jobs, and put regulation in place to prevent another such catastrophe.\n\nCentral banks instituted huge asset purchase programs and brought short-term interest rates to near zero—in some cases below zero—to stimulate economic recovery.\n\nThe good news is that, for the most part, monetary policy did its job. Advanced economies have seen steady growth and significant declines in unemployment. But the recovery has been slow. And despite low unemployment, inflation rates have been running persistently below central banks’ goals. The Federal Reserve, like many central banks, has a goal of keeping inflation at 2 percent.\n\nIn the pre-2008 era, inflation was a major concern for the public and central banks alike. And, while I will always be vigilant about inflation that’s too high, inflation that’s too low is now a more pressing problem.\n\nThe experience of a slow recovery and persistently low inflation is a symptom of deeper problems afflicting advanced economies. Two changes, unrelated to the crisis, have been taking place, causing a shift in economic conditions and quietly shaping the trajectories of advanced economies.\n\nThese are the “the facts” that have changed. Today I’m going to discuss them in more detail, and explore our options for dealing with the challenges they present. \n\n“The Facts”\n\nShifts in demographics and productivity growth have fundamentally altered the economic environment in which we operate.\n\nStarting with demographics, two changes have taken place: People are generally living longer, and population growth is slowing. Dwindling birth rates are bringing population growth to a standstill. In OECD countries, population growth averaged over 1 percent back in the 1950s and 1960s, but it is now running at half a percent per year, and is expected to gradually fall before dipping into negative figures by 2070.1\n\nSlowing birth rates mean fewer consumers and fewer workers. People planning on longer retirements will tend to save more. And while this is undoubtedly good news in many respects, an abundance in the supply of savings also has a big effect on the global economic picture.\n\nWhen it comes to productivity, there has also been a stark step-down in growth. Rapid changes in the kinds of technology we use in our daily lives—robotic vacuum cleaners, self-driving cars, and facial recognition technology in our phones—may make this seem counterintuitive or plain wrong.2\n\nBut the data are clear.\n\nIn OECD economies, growth in labor productivity—the amount produced per worker hour—has averaged a little over 1 percent per year since 2005, about half the pace seen over the prior decade.\n\nFalling R-star\n\nSo what does all this mean for the future?\n\nTwo things: First, slower population and productivity growth translate directly into slower trend economic growth. Second, an abundance of savings, and a decline in demand for savings resulting from slower trend growth, together lead to lower interest rates. All of these factors combined have contributed to dramatic declines in the longer-term neutral rate of interest, or r-star.\n\nWhile a central bank like the Fed sets short-term interest rates, r-star is a result of longer-term economic factors. It’s the rate expected to prevail when interest rates are neither giving the economy a boost, nor slowing it down. That’s why it’s called the “neutral” rate.\n\nThe evidence of a sizable decline in r-star across economies is compelling. The weighted average of estimates for five major economic areas—Canada, the euro area, Japan, the United Kingdom, and the United States—has declined to half a percent.3 That’s about 2 percentage points below the average natural rate that prevailed in the two decades before the financial crisis. A striking aspect of these estimates is that they show no signs of moving back to previously normal levels, even though many advanced economies have fully recovered from the crisis.\n\nThis poses significant challenges for monetary policy. When interest rates are low, central banks don’t have much room to maneuver to deal with a crisis. They will only be able to cut interest rates by a small amount before they hit zero—or as economists call it, the “zero lower bound.” Of course, central banks can, and have, used negative rates to stimulate growth, but they bring with them a separate set of challenges.\n\nThe result is that future recoveries will be slow, and slow growth is usually characterized by low inflation. Persistently low inflation creates a vicious circle, where expectations of low inflation drag down current inflation. If inflation falls, central banks will have even less room to maneuver when faced with a slowdown.4\n\nA Fresh Approach\n\nAll this begs the question: what can we do to prepare ourselves and our economies for the next crisis? There’s no single silver bullet that can solve these challenges. But changes to monetary, fiscal, and other economic policies could all help us become more resilient. \n\nStarting with monetary policy, central banks should reassess their strategies, goals, and the tools they use to achieve them.5 This might include things like reassessing how we achieve our 2 percent goal.\n\nOutside of monetary policy, there are a number of ways fiscal policy can support an economy. One is to strengthen the “automatic stabilizers,” which, in a downturn, reduce taxes without legislation to stimulate growth. Another is to use regulation to enhance financial stability. \n\nFinally, fiscal and other economic policies can attack the sources of slow growth and low r-star directly. Demographics are not an area where most economists, or policy makers, want to meddle—and I’m certainly not going to stand here and say people should stop living so long!\n\nBut there are things we can do to improve long-run growth. These include raising public and private investment in human and physical capital, infrastructure, and science and technology, as well as removing barriers to participation in the labor force and the economy more broadly.\n\nIt’s impossible to predict the future. But we can deal with the present. The economic environment in which we operate today is a direct consequence of the demographic and productivity shifts taking place. Low neutral interest rates are very real, and they’re here to stay.\n\nConclusion\n\nThe facts have changed, and so it is time to change our minds also. It is often said that change is hard. But, experience teaches us that it is better to prepare for the future than wait too long. Ultimately, failure to prepare often means preparation for failure."
    },
    {
        "title": "Banking Culture: The Path Ahead",
        "date": "Jun 4, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil190604",
        "content": "Good morning, everyone. I’d like to begin by welcoming you to the Federal Reserve Bank of New York and to the fifth conference on culture in financial services that we’ve hosted.\n\nBefore I launch into my remarks, it’s time I remind everyone that the views I express today are mine alone and do not necessarily reflect those of the Federal Open Market Committee or others in the Federal Reserve System.\n\nWhen I took on the role as President of the New York Fed last year it so happened that my first day coincided with this conference. In my remarks that day, I emphasized that culture is a long-term project, a constantly evolving one, which is never done.\n\nThe trouble with long-term projects with no distinct finish line is that they can lack a sense of urgency. The pressure to make real progress is often missing, especially when there are so many other priorities competing for our time.\n\nHeadlines in the financial news illustrate the lack of progress. In the year since our last conference, stories of money laundering and fraud have been an all-too-frequent feature. Culture is at the heart of many of these issues, and addressing the root causes of misconduct must remain a high priority for the industry and regulators.\n\nThe Supervisory Approach\n\nGiven this imperative for change, the question is, what can we do to improve culture in financial services?\n\nCulture is like many of the risk management issues challenging businesses. Cyber threats, digital transformation, and climate change are all facts of life that companies face today, and will be facing 10 years from now.\n\nBut I don’t hear any CEOs saying that securing their servers is a project that can wait until 2029. Nor does anyone think that there are “once and done” fixes for these problems. Indeed, one important lesson from cybersecurity is that this is not purely a technology problem, but one that requires a fundamental shift in mindset and culture over the long term.\n\nSo the first thing is the industry, the regulatory agencies, and the banks need to take culture seriously, now.\n\nA major and essential part of this is that there must be meaningful consequences for firms and people when things go wrong. We have learned that simply levying large fines on companies is not enough to create lasting change. More is needed.\n\nRegulators must hold management and boards of directors to account. The legal element of supervision will always remain paramount. And a scandal often spurs change. But if leaders wait until things have gone wrong to think about culture, they’re leaving it too late.\n\nWhat Do We Do to Get to a “Good” Culture?\n\nThe second thing is we need to understand what contributes to a good culture, and what allows a bad one to flourish. What fosters an organizational culture where people feel empowered to speak up? And what leads to a work environment where employees feel emboldened to commit fraud?\n\nThat’s one of the things I’m most looking forward to in today’s conference. I can’t wait to hear the insights from such a variety of voices. Looking beyond our own narrow frame of reference to other fields of study, like organizational psychology, will help us understand how to achieve the progress that’s so desperately needed.\n\nBehavioral science really gets to the root of how human nature interacts with an organization’s processes. Everything from how we set goals to how we motivate our staff to achieve those goals has an effect on culture, for better or for worse.\n\nThis is where the rubber hits the road for most organizations, in the financial services industry, and beyond.\n\nWe’re also going to hear from experts on the role technology plays, including the ethics of data handling, and what artificial intelligence means for relationships with customers.\n\nAnd I’m particularly looking forward to hearing about how diversity, something that’s a strategic priority for me and this Bank, affects cultural norms. This includes diversity not only in terms of race and gender, but diversity of thought and experience, which also affects an organization’s culture and decision-making.\n\nAs an academic at heart, I have a self-confessed bias toward research. But in this case, there’s real value in understanding the full breadth and depth of these issues.\n\nOne of the most important reasons is that we can debunk some of the myths that have acted as a security blanket to the industry. Too often, when things go wrong, business leaders blame a single “bad apple” as the cause of the problem.\n\nBut focusing on an individual bad actor can obscure a culture in which people feel that misconduct in the pursuit of profit is tolerated, or even condoned. The “bad apple” theory acts as an excuse for not doing the hard work of cultural reform. In this process of blaming bad apples, leaders fail to look at what’s happening to the rest of the barrel. What kind of environment has let the rot set in?\n\nUnderstanding the full complexity of what forms a culture is important, but our unique position within the financial system means the Federal Reserve Bank of New York must do more.\n\nOur Role in Achieving Progress\n\nThree principles—“connect,” “convene,” and “catalyze”—guide my thinking in this area.\n\nConnecting people with the latest data and research—and sharing experiences—gives them the insights and tools they need to make change. Conferences like this one are critical—as is our online research hub, which acts as a clearing house for the latest information and initiatives from a wide range of organizations.\n\nBut, that’s not enough. In this regard, we recently announced the Education and Industry Forum on Financial Services Culture to support these efforts.\n\nThe Forum, which kicked off in April, brings together business schools and industry representatives. Participants will research and design case studies for business school students. This will mean that students planning to enter the industry will have the tools to handle the ethical challenges that can arise in real-life work situations.\n\n“Convene” means bringing together stakeholders to share best practices. And that’s where our Supervisors Roundtable comes in. The Roundtable brings together supervisors from around the world to share their jurisdictions’ approach and what they’ve learned from it. Participants have developed a shared set of tools for supervisors that can be leveraged in cultural assessments.\n\nFinally, let me turn to this principle of “catalyze,” and how we can use our position in the heart of the Financial District, to create change.\n\nAs well as talking the talk—we’re walking the walk. This year, for the first time, this Bank is taking the Banking Standards Board (BSB) Assessment.\n\nThe BSB Assessment measures the extent to which nine characteristics are demonstrated by an organization, and it includes things like honesty, accountability, and respect. The assessment started in the UK, and the Irish Banking Culture Board now takes a similar approach.\n\nI would love to see firms in the United States adopt an industrywide survey that can measure culture and benchmark banks against their peers. Data gathering is the first step of understanding the nature of organizational culture.\n\nThese three concepts—“connect,” “convene,” and “catalyze”—will continue to be our guiding principles on the issue of culture as we build on our current work and explore new initiatives.\n\nLike addressing cyber risks and climate change, the journey toward reforming culture in the banking and financial services industry is a long one. And there’s no single action that a regulator or a bank can take to make it happen. Getting it right is going to require the will and expertise of regulators, financial services firms, academics, and practitioners. And while changing organizational culture is a long-term, complex, and challenging task, I’m optimistic that we can and we will make progress. And the Federal Reserve Bank of New York is committed to being a partner on that journey.\n\nThank you."
    },
    {
        "title": "The Research-Policy Nexus: ZLB, JMCB and FOMC",
        "date": "May 31, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil190531",
        "content": "John Maynard Keynes quipped, “Practical men, who believe themselves to be quite exempt from any intellectual influences, are usually the slaves of some defunct economist.” I wonder whether Keynes would see this as a feature rather than a bug in light of his enduring influence on the profession more than 70 years after his death.\n\nThe theme of my talk today is that during my quarter century as a researcher and policymaker at the Fed, we have not been overly beholden to defunct economists. Quite the contrary. A quick scan of FOMC memos, briefings, and meeting minutes provides ample evidence that a wide range of economic research—new and old, conventional and outside the box—regularly enters into our debates and influences our decisions. This represents a rich two-way dialogue between researchers and policymakers, with both sides probing for new answers to old questions.\n\nI could list a long catalog of examples of the synergistic relationship between research and policymaking at central banks. Instead, I will focus my comments today on one specific topic: the zero lower bound on interest rates, or ZLB for short. This topic has obvious importance to central bank practice past, present, and future. In addition, the Journal of Money, Credit and Banking (JMCB), the 50th anniversary of which we are recognizing today, played a very important role in shaping how we think about the ZLB. Today I’d like to highlight how this work has influenced my own thinking, as a researcher and policymaker.\n\nBefore I proceed any further, I should give the usual disclaimer that everything I say today reflects my own views and not necessarily those of the FOMC or anyone else in the Federal Reserve System.\n\nThe Zero Lower Bound Emerges\nAlthough the ZLB was not a new idea by any means, three circumstances—two empirical, one theoretical—came together to reignite interest in this topic by researchers and policymakers in the 1990s.1\n\nThe first was the achievement of low inflation in the United States, which, all else equal, implied lower interest rates. The second was Japan’s lengthening period of deflation and near-zero interest rates.2 The third was renewed curiosity about the theory of monetary policy as decision rules for interest rates, following the publication of John Taylor’s “Discretion versus Policy Rules in Practice.”3\n\nAlthough that paper did not mention the ZLB, it fundamentally transformed how many economists, including one impressionable Stanford grad student, thought about monetary policy. Indeed, the very logic of the Taylor rule—that interest rates systematically move up and down with changes in economic conditions—forced macroeconomists to come to terms with the presence of the ZLB. Put simply, if you run macroeconomic model simulations with a Taylor rule, you will eventually encounter the ZLB and face the decision of what, if anything, to do about it.\n\nThis tension is evident in research papers on monetary policy rules from this era, in which authors either sidestepped the ZLB or ignored it altogether. In fact, I’ll plead guilty as charged when it comes to my own research during this period. At the same time, a small cadre of researchers at the Fed and in academia started to take the ZLB head-on and explore its effects in macro models.4\n\nResearch on the ZLB quickly made its way to policymakers. The ZLB was analyzed in a note to the FOMC in the summer of 1998 and discussed at the subsequent FOMC meeting in July.5 That discussion led to interest at the Fed in having a research conference to provide a broad set of views and perspectives on the ZLB.\n\nThis convergence in interest between researchers and policymakers resulted in a Federal Reserve conference on “Monetary Policy in a Low-Inflation Environment” held in Woodstock, Vermont, in October 1999. The conference proceedings were subsequently published in a special issue of the Journal of Money, Credit and Banking in November 2000. The goal of the conference was to bring together academic and Fed researchers and policymakers to study the implications of low inflation and the ZLB from a variety of perspectives.6\n\nLooking back, what stands out the most from this conference was the richness and diversity of thought brought to a topic that had lain mostly dormant for many decades. There were papers on the historical experience with very low inflation and interest rates, and on financial market functioning in such an environment. The conference concluded with an international policymaker panel.\n\nWhat is striking rereading these papers today is how all authors took for granted that the ZLB was a real problem that needed addressing, and how they approached it in novel and different ways. This despite the fact that the economy was in the midst of a productivity-driven boom, the longest expansion on record, and the prevailing federal funds rate was 5.25 percent. These papers and the ensuing discussions and debates planted seeds that would germinate in research and policymaking. Central banks across the globe ultimately used variations of nearly all of the tools discussed at the conference. Indeed, the topics of interest rate policies, targeting asset prices, asset purchases, and negative interest rates are among the most prominent discussions in monetary economics today.\n\nImportantly, this research made its way straight to policymakers within a few years—a quick turnaround by central bank standards! In January 2002, Marvin, Dave, and I reprised our roles from the conference in briefings to the FOMC.10 And in June 2003, Vince Reinhart briefed the FOMC on alternative monetary policy approaches—including asset purchases and forward guidance—to mitigate the effects of the ZLB.11 This was followed by a flurry of further research by academics and central bank economists that delved deeper into these topics.12\n\nPutting Research into Practice\nThis research had important practical lessons for policymakers in the United States and abroad following the global financial crisis. One lesson was that short-term rates should be cut aggressively when deflation or a severe downturn threatens.13 That is, do not “keep your powder dry.” A second lesson is that short-term rates should be kept “lower for longer” as the economy recovers.14 A third is that large-scale asset purchases (LSAPs) can complement conventional policy actions by making financial conditions more favorable for growth even when short rates are constrained by the ZLB.15\n\nArmed with these insights, policymakers were prepared and able to act quickly and decisively when the financial crisis hit. A great deal was learned about the ZLB and unconventional policy tools along the way.16 Indeed, another conference organized by the Boston Fed was held in October 2010 to assess lessons learned, and the proceedings of that conference were published in a special issue of the JMCB in February 2012. For me, the most interesting aspect of that conference was that participants took the ZLB as a defining feature of the landscape: what was once viewed as primarily theoretical had become grounded in practice and experience.\n\nThe ZLB in 2019\nThis brings me to today. The Federal Reserve has embarked on a review of its monetary policy strategic framework. As was the case 20 years ago, the question is how to best achieve our monetary policy goals in the context of low inflation and the ZLB. Indeed, the policy questions we are grappling with today are exactly the ones laid out in Woodstock. The one fresh wrinkle is that with estimates of the neutral real interest rate much lower than those that prevailed 20 years ago, the ZLB is likely to be an even more powerful force than was imagined in 1999.17\n\nIn approaching this question today, we should take lessons from the conference held in Woodstock 20 years ago: Bring together leading researchers and policymakers, draw out the best and most creative thinking, and take a long-run view of how we can navigate the environment before us. I am happy to say that this process is already in train, with the Fed’s framework review incorporating perspectives from a wide range of stakeholders, culminating in an academic conference next week in Chicago. \n\nConclusion\nIn closing, I want to add a personal thanks to Ken West for all he has done to promote outstanding research that is both innovative and relevant to the real-world problems that central banks face. And my best wishes for the JMCB, which I hope may continue its illustrious record for the next 50 years. As a researcher and policymaker, the one thing I am certain about is that fostering an open and active dialogue between researchers and policymakers is the best way for us to succeed in our work.\n\nThank you."
    },
    {
        "title": "When the Facts Change…",
        "date": "May 14, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil190514",
        "content": "Introduction\n\nJohn Maynard Keynes is credited with saying, “When the facts change, I change my mind. What do you do, sir?” Over the past decade, policymakers across the globe have been focused on getting their economies and financial systems back in order. But at the same time, a sea change in supply-side realities has taken place. These changes have redefined what is achievable, and challenged conventional wisdom on monetary policy.\n\nShifting demographic trends and a slowdown in productivity are driving slower trend growth and historically low levels of real interest rates across the globe. This new set of facts requires us to rethink what we once knew, reassess how to best foster strong and stable economies, and ready ourselves for the future.\n\nWith that start, it’s time I remind everyone that the views I express today are mine alone and do not necessarily reflect those of the Federal Open Market Committee or others in the Federal Reserve System.\n\nThe Road to Recovery\n\nFollowing the global financial crisis, central banks took bold and decisive actions to right their economies and bring them back to health. Short-term interest rates were brought to near zero—in some cases below zero—and held there for many years (Figure 1). The combination of low short-term rates and asset purchase programs has driven yields on long-term sovereign debt to historically low levels (Figure 2). In large parts of Europe and Japan, short- and long-term yields remain near zero, a full decade after the onset of the financial crisis.\n\nEconomic developments since this unprecedented monetary stimulus provide valuable insights into the efficacy and limitations of monetary policy. On the positive side of the ledger, advanced economies have seen steady growth and significant declines in unemployment, with rates today near, or in many cases well below, those seen before the crisis (Figure 3). Bold monetary policy actions surely played an important role in averting far greater catastrophes and aided economic recovery.\n\nHowever, this success was achieved only after a disappointingly slow recovery and many years of economic hardship. And, despite the improvement in the real side of economies, inflation rates have persistently been below central banks’ goals (Figure 4). The fact that inflation has been running below target in most advanced countries—the United Kingdom being the most prominent outlier—suggests that this challenge is not due to factors specific to a single country. Instead, there are more systemic factors at play.\n\nInvestors view these low inflation readings not as an aberration, but rather a new normal. This is evidenced by a broad-based decline in market-based measures of longer-run inflation expectations since the mid-2000s, with the UK again providing the exception (Figure 5).\n\nThis experience of slow recovery and persistently low inflation is a symptom of a deeper problem afflicting advanced economies—the root cause of which is a combination of low neutral interest rates and the lower bound on interest rates, a topic to which I now turn.\n\nWhat’s Past Is Prologue\n\nUnderlying these events has been a sea change in the supply side of economies, owing to fundamental shifts in demographics and productivity growth.\n\nTwo demographic trends are evident: People are generally living longer and population growth is slowing. Overall life expectancy in member countries of the Organisation for Economic Co-operation and Development (OECD) has increased from about 65 years in the 1950s to nearly 80 years today, and is projected to exceed 90 years by the end of this century (Figure 6).1 Despite this increase in longevity, dwindling birth rates are bringing population growth to a standstill. In OECD countries, population growth averaged over 1 percent back in the 1950s and 1960s, but is now running at half a percent per year, and is expected to gradually fall before dipping into negative figures by 2070 (Figure 6).2\n\nWhen it comes to productivity, the changes are no less dramatic. In OECD economies, growth in labor productivity—the amount produced per worker hour—has averaged a little over 1 percent per year since 2005, about half the pace seen over the prior decade (Figure 7)*. A major factor driving the slowdown is a dramatic decline in the growth rate of “total factor productivity,” or TFP, a measure of innovation and technological change.3\n\nAs Bob Gordon has taught us, U.S. productivity growth tends to cycle between periods of high growth in decades following fundamental breakthroughs in science and technology, and relatively fallow periods of steady, incremental improvements. All the evidence indicates that we are in one of the latter periods of “normal” productivity growth of 1 - 1.5 percent per year, and that things will likely stay that way until the next scientific or technological revolution.4\n\nR-star Descends\n\nThese global shifts in demographics and productivity have two important implications for the future of our economies and for monetary policy. First, slower population and productivity growth translate directly into slower trend economic growth. Second, these trends have contributed to dramatic declines in the longer-term normal or “neutral” real rate of interest, or r-star.\n\nSlower trend growth reduces the demand for investment, while longer life expectancy tends to increase household saving.5 This combination of lower demand for and higher supply of savings, along with other factors, has pushed down r-star. With open capital markets, these global changes in supply and demand affect r-star globally.\n\nThe evidence of a sizable decline in r-star across economies is compelling. The weighted average of estimates for five major economic areas—Canada, the euro area, Japan, the United Kingdom, and the United States—has declined to half a percent (Figure 8).6 That’s 2 percentage points below the average natural rate that prevailed in the two decades before the financial crisis. A striking aspect of these estimates is that they show no signs of moving back to previously normal levels, even though economies have recovered from the crisis. Given the demographic waves and sustained productivity growth slowdown around the world, I see no reason to expect r-star to revert to higher levels in the foreseeable future.\n\nThe global decline in r-star will continue to pose significant challenges for monetary policy. Given the limited policy space for interest rate cuts in future downturns, recoveries will be slow and inflation below target. The limitation in the ability of central banks to offset downturns results in an adverse feedback loop, whereby expectations of low future inflation drag down current inflation and further reduce available policy space.7\n\nNew Facts, New Approaches\n\nPolicymakers around the globe need to prepare for the challenges of navigating the new realities of slow global growth and low r-star. This necessitates new thinking and approaches to monetary, fiscal, and other economic policies.\n\nStarting with monetary policy, central banks should revisit and reassess their policy frameworks, strategies, and toolkits, to maximize efficacy in a low r-star world. The Bank of Canada already regularly does this, and the Federal Reserve is currently undergoing a review of its framework and strategy.8 Absent such changes, central banks will be severely challenged to achieve stable economies and well-anchored inflation expectations.\n\nOutside of monetary policy, there are a number of avenues by which fiscal authorities can enhance the resilience of economies to negative shocks. One is to strengthen the “automatic stabilizers” that provide a boost to the economy during a downturn. A second is to align debt management decisions more with monetary policy. For example, during a downturn, the fiscal authority could choose to shorten the duration of debt issuance to reinforce the effects of central bank quantitative easing. Third, regulatory and supervisory policies that support the resilience of the financial system can limit the economic effects of negative shocks.\n\nFinally, fiscal and other economic policies can attack directly the sources of slow growth and low r-star. This includes raising public and private investment in human and physical capital, infrastructure, science and technology, and policies aimed at removing barriers to participation in the labor force and the economy more broadly.\n\nConclusion\n\nThe facts have changed, and so it is time to change our minds also. It is often said that change is hard. But, experience teaches us that it is better to prepare for the future than wait too long. Ultimately, failure to prepare often means preparation for failure.\n\nFigures\n\n* Correction (May 23, 2019): An earlier published version of these remarks referred to an incorrect time frame for recent growth in labor productivity. Growth in labor productivity has averaged a little over 1 percent per year since 2005. The sentence in question has been corrected."
    },
    {
        "title": "The Joys of Spring",
        "date": "May 10, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil190510",
        "content": "It’s an honor to be kicking off Bronx Week, and I’d like to thank Borough President Ruben Diaz, Jr., for the kind invitation. I first visited the Bronx—and first met Ruben—back in November, when I also spent time with students discussing their plans for the future. I was particularly impressed by their talent, energy, and enthusiasm, which is a true reflection of this vibrant and proud community.\n\nToday I’m going to share some brief remarks on the national and global economy. I’ll then discuss what that means for monetary policy today and going forward. For me, the most interesting part of this event is hearing and responding to your questions, so please, don’t be shy when I open the floor for discussion.\n\nI’ve warned you that monetary policy is on the agenda, so before I go any further, I have to give the usual Fed disclaimer that everything I say reflects my own views and not necessarily those of the Federal Open Market Committee or anyone else in the Federal Reserve System.\n\nThe U.S. Economy\n\nWith that out of the way, let me start by saying the U.S. economy is in a very good place. Late last year, as the long, dark winter closed in, there were considerable jitters in financial markets, with some observers fretting that the economy may be on the verge of a hard landing. Indeed, there was a lot of talk about the possibility of a recession looming.\n\nI am happy to report that the economy has emerged from the New York freeze. The sun is out, the tulips and the cherry blossom are now in full bloom and the picture of the economy is similarly rosy. In the first four months of the year, the U.S. economy added over 800,000 jobs, and the unemployment rate is now 3.6 percent—the lowest level in nearly 50 years.\n\nThe latest GDP numbers indicate that the economy remained in high gear through the first quarter, with growth clocking in at an impressive 3.2 percent annual rate.\n\nDespite this very strong economy, we are not seeing any signs of inflationary pressures. In fact, the latest inflation data have come in lower than expected, with core inflation over the past 12 months dipping to 1.6 percent. Now, most people don’t seem that bothered by inflation that is a touch too low, but at the Federal Reserve we have set a long-run inflation goal of 2 percent. When inflation runs persistently above or below that goal, we take that very seriously.\n\nSo far, the recent downward movement appears mostly to reflect normal volatility in inflation statistics. For example, if you look at my preferred measure of underlying inflation, called trimmed-mean inflation—which removes the most volatile price movements—it’s been running at 2 percent. This measure of inflation hasn’t shown any signs of trending up or down. That said, I will be watching developments closely for signs of a persistent shift away from our 2 percent long-run goal.\n\nAs I mentioned, the economy started the year with a lot of momentum. But there are reasons to believe that the surge in growth won’t persist through the remainder of the year. Taking into account of all the indicators we follow, I anticipate a step-down of economic growth, from the 3 percent growth recorded last year to a still-solid pace of about 2-1/4 percent in 2019.\n\nThis is good news. It’s above the economy’s underlying potential, but not unsustainably so. Healthy economic growth should in turn fuel solid job gains, higher wages for workers, and some further declines in unemployment. The strong economy and labor market should also support inflation returning to 2 percent, the Federal Reserve’s long-run goal.\n\nThe Global Picture\n\nIn addition to the positive data we’ve been getting for the U.S., we’ve also seen encouraging signs that risks from global economic and financial markets are abating. Last year there were worries that the global growth slowdown centered in China and Europe would spill over onto our shores. This was one factor contributing to a more downbeat assessment of U.S. growth and the stock market sell-off.\n\nSince then, we’ve seen more positive readings on China’s economy, in part due to steps the authorities have taken to reignite growth. And financial markets have definitely rebounded from their winter doldrums.\n\nStill, we’re not entirely out of the woods regarding risks from a global growth slowdown. Signs of weakness in Japan, South Korea, and some regions of Europe continue, so I am particularly vigilant about monitoring the international data. Trade tensions and ongoing uncertainty regarding Brexit continue to present downside risks.\n\nMonetary Policy\n\nWhat does this mean for monetary policy? At last week’s meeting, the Federal Open Market Committee decided to leave the target range for the federal funds rate unchanged, between 2-1/4 and 2-1/2 percent. We also reiterated our patient approach to assessing the need to adjust interest rates in the future.\n\nIn a nutshell: The economy remains on a path of healthy growth, with a very strong labor market and without the emergence of inflationary pressures. The current setting of policy positions us well to keep it that way.\n\nAlthough my view that we are in the right place in terms of monetary policy has not changed of late, the reasons for it have evolved in line with the economic and financial developments I sketched out earlier in my remarks. The strength of recent data on economic activity, the rebound of growth in China, and the reversal in the tightening of financial markets all imply that near-term risks to growth have receded somewhat. This has increased my confidence that the economy remains on track for moderate growth going forward. At the same time, recent price data reaffirm that inflationary pressures remain muted.\n\nConclusion\n\nLooking ahead, although the baseline forecast I laid out looks very good, the situation can change. We need to remain vigilant in responding to evolving economic conditions and the outlook, with our eye on sustaining the expansion, keeping the labor market strong, and fostering price stability.\n\nThank you. I look forward to your questions."
    },
    {
        "title": "Observations on Implementing Monetary Policy in an Ample-Reserves Regime",
        "date": "Apr 17, 2019",
        "speaker": "Lorie K. Logan",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/log190417",
        "content": "Thank you for the kind introduction. While preparing these remarks, I learned that the Money Marketeers organization was founded by Dr. Marcus Nadler, a gifted educator who challenged market participants to deepen their understanding of the forces that move markets, and that it was born out of a popular lecture series he regularly held at NYU. This spirit of continual learning is a core value at the Federal Reserve, and that has been particularly important in recent years as the Fed has been operating in a new monetary policy implementation regime.1\n\nI last had the pleasure of speaking about monetary policy implementation at this forum two years ago. At that time, the Federal Open Market Committee (FOMC) had yet to make a decision on what the long run operating regime would look like. Substantial progress has been made since then. My intention tonight is to share some thoughts on what the FOMC’s recent decisions could mean for the manner in which the New York Fed’s Open Market Trading Desk (the Desk) implements policy. In particular, I would like to discuss the Federal Reserve’s ongoing process for learning about the banking system’s demand for reserves and money market dynamics more broadly. I will also outline how the Desk could supply reserves through open market operations as the Federal Reserve transitions from implementing policy in an ample-reserves regime driven by the asset side of the balance sheet to one driven by demand for liabilities.\n\nI will bring an operational perspective to these topics, given my position as head of the Desk and Deputy Manager of the Federal Reserve System Open Market Account. I should also say now that the views I express tonight are my own and do not necessarily reflect those of the Federal Reserve Bank of New York or the Federal Reserve System.\n\nAn Operational Framework for an Ample-Reserves Regime\n\nLet me begin by reviewing the FOMC’s recent decisions about the operating regime. In January, the FOMC communicated its intention to continue implementing policy in a regime in which an ample supply of reserves ensures that control over the federal funds rate and other short-term interest rates is exercised primarily through the Federal Reserve’s administered rates and in which the active management of the supply of reserves is not required.2 This type of regime is often referred to as a “floor system” because the administered rates—including IOER (the rate of interest paid on excess reserves) and the ON RRP rate (the offered rate at the overnight reverse repurchase agreement facility)—place a floor under the rates at which banks and other market participants will lend.3 I’ve illustrated this framework in the stylized graphs showing the relationship between the interest rate and banks’ demand for reserves in Figure 1. Before the crisis, the Desk operated along the steep part of the demand curve to achieve its directive from the FOMC, whereas in an ample-reserves regime, like the one we’ve used in recent years, we operate along the flatter part of the curve.4\n\nOver the past decade, the Federal Reserve has learned a great deal about operating in the current regime, and during recent deliberations about the long-run implementation framework the FOMC highlighted several key benefits of the current regime as supporting its decision.5\n\nFirst, the ample-reserves regime has been very effective at controlling short-term interest rates because the Federal Reserve’s administered rates provide strong incentives in money markets. As shown in Figure 2, overnight interest rates moved in line with changes in these administered rates, even as the FOMC lifted rates from near zero in late 2015 to over 2 percent more recently. During this same period, there were sweeping changes in banks’ liquidity management practices and in money market fund regulation that transformed the structure of money markets, and the floor system proved resilient to those changes.6 Looking forward, this regime should provide stable interest rate control if unexpected situations arise in which large amounts of liquidity need to be added to relieve stress in the financial system or large-scale asset purchases are needed to provide macroeconomic stimulus because short-term rates are at their effective lower bound.7\n\nSecond, from an operational perspective, an ample-reserves regime achieves this effectiveness in a simple and efficient manner. For example, it reduces the need for active management of reserve supply because shocks to the supply of or demand for reserves can be absorbed without the need for sizable daily interventions by the central bank. In the U.S., the ability to accommodate variability in non-reserve liabilities is particularly important because those liabilities—such as the Treasury General Account (TGA), Foreign Repo Pool, and balances held by Designated Financial Market Utilities—provide benefits for the economy and the global financial system.8\n\nAlthough reserves will continue to be ample going forward, the level will be significantly lower than has prevailed in recent years. In March, the FOMC issued an updated “Balance Sheet Normalization Principles and Plans” in which it stated that the longer-run level of reserves will be a level consistent with “efficient and effective” implementation of monetary policy.9 To support a smooth transition to the longer-run level of reserves, the FOMC also stated that the reduction in the Federal Reserve’s securities holdings will slow this spring and then stop in September. When balance sheet runoff stops in September, the FOMC expects the level of reserves to be somewhat higher than necessary for efficient and effective monetary policy implementation. If so, the total balance sheet will be held constant for a time, and trend increases in non-reserve liabilities, such as those shown in Figure 3, will gradually reduce the average level of reserves until it reaches a level consistent with efficient and effective implementation of monetary policy.\n\nAs Chair Powell noted in his January press conference, the level of reserves consistent with efficient and effective implementation of monetary policy will be principally determined by financial institutions’ demand for reserves, and an additional buffer so that interest rate control continues to be exercised through administered rates.10 The stylized framework I showed earlier in Figure 1 illustrates that banks demand different levels of reserves depending on the prevailing levels of interest rates. However, for purposes of the discussion today, I will define the banking system’s demand for reserves as a level that is just to the right of the steeper portion of the demand curve, representing the lowest quantity of reserves that banks demand in aggregate at rates near IOER. This corresponds to the minimum level of reserves needed to continue operating in a floor system, and supplying reserves at or above this level would be consistent with a floor system.\n\nIn practice, assessing banks’ demand for reserves will be a continual process because the factors determining that demand are complex and may change over time. Federal Reserve staff survey banks, analyze vast amounts of data, and gather intelligence across the market to better understand this demand. Slowing the runoff in securities holdings this spring and stopping in September will reduce the pace of reserve decline, allowing this detailed assessment to adapt appropriately as reserves decline to levels that were last seen almost a decade ago. The behavior of money markets with this lower—albeit still ample—level of reserves will feed back into the Desk’s thinking about the design of operations. And being operationally flexible will allow the Desk to adapt to changing economic and market conditions, as well as to the FOMC’s assessment of the appropriate level of reserves consistent with efficient and effective implementation.\n\nThe Banking System’s Demand for Reserves in an Ample-Reserves Regime\n\nLet me take a step back and provide some observations on the banking system’s demand for reserves. Banks engage in a variety of activities that create uncertainty about their liquidity positions. These uncertainties include a risk that depositors will want their money back sooner than expected. To manage this liquidity risk, banks need access to cash, which can come from several sources, including reserves already held at the Federal Reserve, money borrowed either in the private market or from the Federal Reserve’s lending facilities, or proceeds from asset sales or maturities.\n\nReserves are the most reliable and safest source of liquidity because they are immediately available for payments and do not have a changing market value. Nevertheless, before the crisis, banks typically tried to minimize reserve balances because reserves earned zero interest, whereas other highly liquid assets, such as Treasury bills, earned a sizable positive yield. Given this high opportunity cost, demand for reserves was relatively close to reserve balance requirements prior to the crisis.11\n\nToday, reserves are less expensive to hold than they were before the crisis because the Federal Reserve now pays IOER, which is the principal tool used to control short-term interest rates.12 However, demand for reserves has also shifted in other important ways. In particular, there are new factors driving reserve demand, and that demand is, and will likely continue to be, much higher than it was before the crisis. To start, the crisis appropriately changed attitudes toward risk and increased focus on managing liquidity risk. The subsequent creation of a liquidity regulatory framework for larger banks reinforced the benefits of holding unencumbered liquid assets for both domestic and foreign banks. While these liquidity regulations do not themselves impose any specific requirements to hold central bank reserves, many banks now use internal models that also estimate the very short-term liquidity they need to hold in order to be prepared for a stress scenario.13 These models make assumptions about a bank’s reduced access to funding and ability to sell securities during earlier stages of a stressed period. The bank may therefore decide to hold a portion of its liquidity buffer as reserves on an ongoing basis as a precautionary measure. Through our outreach to banks, we also hear that some are more reluctant to borrow from the Federal Reserve than they were prior to the financial crisis. For some banks, this reluctance increases the precautionary demand for reserves to meet large intraday payment obligations and manage unexpected volatility in deposits.\n\nTo get a more systematic reading of the factors driving reserve demand as well as the size of this demand, the Federal Reserve conducted a Senior Financial Officer Survey (SFOS) in September 2018 and again this February.14 The respondents to the September 2018 survey held two thirds of all reserves and about 60 percent of assets in the banking system, and they encompassed a diverse set of business models, including foreign banks and domestic banks of different sizes.\n\nThe survey results revealed that a number of important sources of reserve demand are present. As shown in Figure 4, which presents September 2018 survey responses, the majority of both domestic and foreign banks cited internal liquidity stress tests and meeting intraday payment flows as important or very important drivers of reserve demand. In addition, meeting potential deposit outflows specifically was important or very important for almost three quarters of domestic banks, compared to around a third of foreign banks.15 In addition, it is worth highlighting the considerable variation in survey responses, which reinforces my earlier point that the pre-crisis framework in which all banks had similar reserve management strategies no longer applies. The diversity of drivers also highlights how demand for reserves could change if banks move from one strategy to another or refine an existing one over time.\n\nThe survey can also be used as an input for quantifying the banking system’s demand for reserves. The surveys asked each bank to report the level of reserves it would hold (at the prevailing constellation of rates) before taking active steps to maintain or increase its reserve balances.16 This lowest comfortable level of reserves, or “LCLoR,” corresponds, conceptually, to the part of a bank’s demand curve where the slope could start to steepen. In the September 2018 survey, total LCLoR of all the respondents was $617 billion, and extrapolating the survey responses to the banking system as a whole resulted in estimates of U.S. banking system demand for reserves ranging from $800 billion to $900 billion.17 When we surveyed banks again in February, those results yielded estimates in the same range. Figure 5 shows total LCLoRs for foreign respondents and domestic respondents, and the extrapolated demand for the rest of the banking system. As you can see, each group has considerable demand for reserves, although this demand is still well below current reserve levels and far below the peak reserve levels reached in 2014.\n\nTo connect this back to the stylized framework, in a world with perfect information and no financial market frictions, an estimate of aggregate reserve demand derived from summing and scaling the reported demand of individual institutions might correspond to a level just to the right of the steeper portion of the demand curve. However, there are reasons why the amount of reserves a central bank needs to supply to remain in a floor system could deviate from such an estimate. First, an estimate of demand for reserves is simply an estimate; the true amount could be higher or lower. Second, an individual bank’s demand curve could change over time, both for reasons specific to the bank and because of changes in the economic or market environment. Third, the banking system’s demand for reserves could be higher than the sum of each bank’s individual demand if reserves are not distributed efficiently. This scenario could occur if there are financial market frictions to redistributing reserves that result in some banks persistently holding a surplus of reserves above their LCLoR. For example, banks now suggest that they face higher balance sheet costs to lend in federal funds, making it possible that this market would not be as efficient at redistributing reserves late in the day as it was prior to the crisis.18\n\nIn practice, assessing the lowest level of reserves necessary to remain on the flat, or flatter, part of the demand curve will entail not only periodically conducting the SFOS in collaboration with the Board of Governors, but also broader monitoring and analytical efforts in order to continuously inform our assessment of the demand for reserves and reserve conditions. The Desk’s current assessment is that reserves remain ample overall. Transacted rates in the federal funds market are relatively stable, and the vast majority of trades are within the target range. Additional monitoring of bank microdata also suggests that reserves are well supplied.19\n\nLet me walk through some observations based on the Desk’s ongoing monitoring and analysis. Though rates transacted in the federal funds market are relatively stable, the EFFR has risen one basis point relative to IOER in recent weeks and is now above IOER. Since mid-March, overnight repo rates have generally traded above federal funds rates, providing an attractive investment opportunity for the Federal Home Loan Banks (FHLBs) that invest a portion of their liquidity portfolios in both markets. As the dominant lenders in the federal funds market, FHLBs may have been able to negotiate higher overnight lending rates with banks that regularly use federal funds to fund non-reserve assets or meet payments, and on occasion with banks that borrow to improve their Liquidity Coverage Ratios (LCR) by borrowing from government sponsored enterprises.20\n\nIn line with a slightly higher EFFR, the overall share of above-IOER borrowing in markets underlying the overnight bank funding rate has been growing somewhat, as shown in Figure 6, although a large majority of activity is still transacted within a couple of basis points of IOER. As long as these rates remain relatively stable and at modest spreads above IOER, we don't see this as indicating that reserves are not well supplied.\n\nIndeed, an examination of daily reserve levels of individual banks shows that currently most banks remain well above their reported LCLoRs, suggesting that reserves remain ample. In fact, some banks with surplus reserves have been redistributing these reserves by shifting the composition of their HQLA (high quality liquid assets) by lending in repo markets when rates are attractive, suggesting secured markets are providing a means to redistribute reserves.\n\nIf competition for reserves were to increase, we might expect to see a more meaningful day-to-day relationship between changes in the level of reserves and overnight rates. Figure 7 shows a scatterplot of daily changes in reserve balances against daily changes in the spread between EFFR and the IOER rate. There continues to be no discernable relationship.\n\nIn sum, a wide range of information contributes to the Federal Reserve’s assessment of reserve demand and reserve conditions. The Desk’s current view is that reserves remain ample and well above the level the banking system demands at rates near IOER at this time. Our assessment of reserve demand is likely to change over time as we continue learning, and could move higher or lower based on new information from analysis of data, surveys, and outreach.\n\nSupplying Reserves in an Ample-Reserves Regime\n\nI will now look ahead and turn to how the Desk could supply reserves through open market operations to maintain an ample-reserves regime. After balance sheet runoff ends in September, the average level of reserves will decline more slowly, although there will be some meaningful transitory swings in reserve levels because of normal variability in the Federal Reserve’s other liabilities. As the level of reserves declines, the Desk will monitor medium-term forecasts of reserves and other indicators of reserve conditions. At some point, the FOMC will decide that the system has reached a level of reserves consistent with efficient and effective implementation.\n\nOnce this determination has been made by the FOMC, the Desk will need to conduct outright purchases of Treasury securities to supply reserves in order to offset the general decline in reserves from trend growth in non-reserve liabilities and ensure that reserves remain ample.21 In this regard, these purchases will have the same purpose as they did prior to the financial crisis—expanding the size of the SOMA portfolio to accommodate growth in currency and other liabilities. However, the size of these purchases will likely be larger in nominal terms because the growth of non-reserve liabilities is larger. For example, in 2018, currency increased by almost $100 billion, as shown back in Figure 3, compared to average increases of $40 billion a year in the early 2000s before the crisis.\n\nThese purchases could also be structured in size and timing to supply a sufficient amount of reserves so that normal variability in non-reserve liabilities would not require predictable repo market operations to offset the corresponding reserve level changes.22 The size of this “buffer” of reserves would depend on the Desk’s forward-looking assessment of the expected volatility in non-reserve liabilities. A buffer of reserves executed through Treasury purchases would diminish the need for the Desk to conduct frequent, sizable repo operations, which might be difficult to implement given reduced elasticity of primary dealer balance sheets for tri-party repo in the post-crisis era.2 Nonetheless, even with this type of approach to managing a buffer, there might be some occasions in which unanticipated changes in reserve conditions would still require the Desk to conduct repo operations to maintain reserve levels or keep the effective federal funds rate in the target range consistent with the directive from the FOMC.\n\nLet me make the discussion of the buffer more concrete by focusing on the TGA. The TGA is an account the Federal Reserve provides to the U.S. Treasury which, similar to a checking account, allows Treasury to deposit or withdraw cash every day. When Treasury takes in cash (for example, tax receipts), money is withdrawn from a private bank account that in turn comes out of that bank’s reserves holdings. So, when the TGA increases—and assuming there are no other changes on the Federal Reserve’s balance sheet—this corresponds to a decrease in the level of reserves. Figure 8 shows a scatterplot of weekly changes in the TGA against weekly changes in reserves over the past two years. There have been times when the TGA has increased by around $100 billion over the course of a week and there has been a similarly large decline in reserves.24 Figure 9 shows the distribution of weekly changes across all non-reserve liabilities. These changes have grown since the crisis and last year were as large as $100 billion. Managing a buffer of reserves above the banking system’s demand for reserves would help absorb these fluctuations in non-reserve liabilities.\n\nAs the level of reserves falls, the Federal Reserve will continue to learn and enhance its thinking about an efficient approach to open market operations that could incorporate both the trend growth and transitory volatility in non-reserve liabilities to maintain the excellent interest rate control experienced with higher levels of reserves.\n\nWhile my discussion tonight has focused on the structure of open market operations from the perspective of supplying sufficient reserves in an ample-reserves regime, the FOMC also announced in its March Principles and Plans that the Desk will be conducting Treasury purchases in the secondary market to reinvest mortgage-backed securities (MBS) principal payments starting in October (subject to a maximum of $20 billion per month). The size of these purchases will vary over time based upon MBS prepayments. Of course, these purchases will be altering the composition but not the overall size of the System Open Market Account portfolio and Federal Reserve balance sheet. At least initially, the FOMC will direct the Desk to conduct these MBS reinvestment purchases across a range of Treasury maturities to roughly match the maturity composition of securities outstanding. The Desk will provide more details on these operations in May.25\n\nClosing Thoughts on Continually Learning and Remaining Flexible\n\nIn conclusion, the FOMC made an important decision in January to continue to implement policy in an ample-reserves regime. Whereas the size of the balance sheet and the level of reserves have, for many years, been driven by Federal Reserve asset policies, at some point they will be determined by the demand for Federal Reserve liabilities, taking into account both the banking system’s demand for reserves and volatility in non-reserve liabilities.\n\nThrough outreach, surveys, and regular monitoring we have increased our understanding of the banking system’s demand for reserves. This learning is ongoing, and we expect the factors driving reserve demand to evolve over time. The Desk’s current assessment is that reserves are ample, and we will continue to monitor conditions closely to further enhance our understanding of reserve conditions and reserve demand.26\n\nLooking ahead, after balance sheet runoff ends in September, average reserve levels will likely continue to gradually decline. When the FOMC judges that reserves have reached a level consistent with efficient and effective implementation, the Desk will begin to execute gradual and mechanical purchases of Treasury securities in order to ensure that reserves remain ample. The size of these purchases will need to be larger than similar pre-crisis operations simply because trend growth in non-reserve liabilities is larger in nominal terms, and because proceeds from maturing MBS also will be reinvested.\n\nThese are unprecedented times, so active learning and maintaining operational flexibility will continue to be core principles that guide the implementation of monetary policy. The ample-reserves operating framework I have described today has auxiliary mechanisms, such as an ongoing process for assessing reserve demand, that allow for flexibility and adaptation for continued successful control of interest rates going forward.\n\nThank you. I would be happy to take a few questions.\n\nFigures"
    },
    {
        "title": "Thoughts on Cybersecurity from a Supervisory Perspective",
        "date": "Apr 12, 2019",
        "speaker": "Kevin J. Stiroh",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/sti190412",
        "content": "Good morning and welcome to the Federal Reserve Bank of New York.  We are very happy to be hosting this conference on “Cyber Risk to Financial Stability” for the Columbia School of International and Public Affairs (SIPA).1\n\nOne of the advantages of a talk on cybersecurity is that there is not a motivation problem—you don’t need to convince anyone that this is a fundamental risk for financial firms, the financial system, and the broader economy.  There is strong consensus on that point.  As a result, we can use our time more productively to talk about the substantive issues around things like identification, protection, detection, response, and recovery, the implications for financial stability, and the important work that needs to be done.\n\nThe focus of today’s conference is on the link between cyber threats and financial stability.  This is clearly an important topic and there is a growing field of research that explores issues around transmission channels, amplifiers, and behavioral dynamics.  As a few specific examples, recent papers map out the link from cyber risk to financial stability through a range of transmission channels such as interconnectedness, confidence, and data integrity, and a related paper focuses on the potential dynamics of a cyber-triggered run.2  I think these frameworks and perspectives offer a very useful point of departure for macroprudential policy discussions.  For today, however, I’m going to shift from a macroprudential view to a microprudential one and offer some thoughts on cyber risks from a supervisor’s perspective.  These are not independent views, so I hope my perspective adds to the broader discussion today.\n\nBefore proceeding, I’d like to emphasize that these are my views and do not necessarily reflect those of the Federal Reserve Bank of New York or the Federal Reserve System.\n\nAs you may know, the supervisory program for the largest banks that was implemented after the financial crisis looks at both individual firm safety and soundness and the potential impact that distress at a large bank can have on broader financial markets or the economy.3  More specifically, this framework expresses two complementary goals for the supervision of systemically important firms: (1) Enhanced resiliency of the firm to lower the probability of failure or inability to serve as an intermediary and (2) Reduced impact on the financial system or economy in the event failure or material weakness does in fact occur.  This is one way to link microprudential issues to the macroprudential concerns.\n\nFrom a supervisory perspective, cyber risk can be viewed through the lens of operational resiliency where a cyber attack threatens the ability of a firm to provide critical financial services.  An effective risk management framework with appropriate governance and controls is one way to mitigate those risks.  Just as capital and liquidity promote financial resilience, strong governance and controls support operational resilience.  The governance and controls portion of the large bank assessment framework includes an evaluation of risk management capabilities including independent risk management and controls and planning for the ongoing resiliency of a firm.4\n\nAs I indicated earlier, cyber risks are well-known and firms are expending considerable resources and management bandwidth to address them in terms of technology, process and personnel.  U.S. banks and other financial firms are projecting higher spending on cybersecurity each year as they face bigger threats and more attacks.5  Cumulatively, we have seen estimates as high as $1 trillion for global spend on cybersecurity through 2021.6\n\nOne area of supervisory focus is on maintaining resilience of core business services.  For cybersecurity, this includes continued progress on preventive and detective efforts.  It also includes things like a comprehensive process to assess cyber-related capabilities; identification of gaps in business resilience requirements such as recovery time objectives; risk monitoring and testing programs; and management reporting to facilitate appropriate prioritization.  Moreover, bank resiliency increasingly depends on the resiliency of third-party service providers.\n\nIn this sense, our approach to cybersecurity is embedded in the broader supervisory and risk management frameworks.  As such, we see notable similarities to other shocks that could impact a firm’s operational resiliency, safety and soundness, and ability to continue to provide financial services in a sustainable way.  From a process perspective, for example we expect firms to leverage traditional approaches to risk identification, measurement, mitigation, monitoring and reporting.  In terms of governance, we expect effective oversight from Boards of Directors.  There are also similarities to risk management of information technology with a focus on things like change management and information security controls, asset management, and the software development lifecycle. \n\nThere are, however, important differentiators that distinguish cyber risk from other operational shocks such as those related to natural events or human error.  These differentiators provide a useful map for risk managers and supervisors who must adapt to these evolving risks.\n\nOne obvious differentiator of cybersecurity risk is around motivation.  While there are exceptions, we don’t usually think of a credit or market risk event or a natural disaster happening with intentionality by a determined adversary.  Asset quality or market prices may change unexpectedly and weather events may prove disruptive, but they lack intent to harm.  By contrast, cyber events, by definition, involve an intention to steal, disrupt, or destroy.  According to one recent survey, the biggest drivers of cyber attacks were access to information followed by financial profit.7  Cybercriminals are increasingly motivated by data theft, rather than solely direct monetary theft.  In addition, the adversary is likely to evolve and may even actively respond during an attack making it necessary to have a dynamic response to this more complex threat.\n\nMotivation also impacts the varied nature of the threat landscape.  Cyber events are driven by nation states, organized crime, and political activists.  As result, the threat landscape changes not only with opportunity, data and technology, but also in response to global and domestic politics.  One study highlights that cyber incidents from nation states are on the rise.8  These types of cyber attacks have no physical boundaries as a malicious actor can successfully launch an attack on an institution located in another part of the world.  All of these factors impact how an institution will choose to prepare and respond.\n\nA second critical differentiator is the nature of disruption including potential impacts on data confidentiality, integrity, and availability.  Cyber attacks that involved data corruption or destructive malware are unique to a cyber threat and can have an immediate and devastating impact.  The question of data integrity has emerged as a critical factor in a cyber attack and introduces additional risk management challenges.  For example, the ability to respond and recover may be disrupted if there is data destruction or corruption in a scenario that is also likely to include considerable uncertainty.  Even if a firm can recover from a data corruption cyber-attack, when would customers and clients trust them as a counterparty?  Issues around confidence and interconnectedness are ways that a cyber event can have broader macroprudential implications.\n\nA third differentiator relates to the skill and human capital needed to build the most successful defenses.  Cyber security requires a different set of skills and abilities including systems development and acquisition lifecycles; general enterprise architecture and IT governance; and IT service management sub-disciplines such as asset management, and configuration management.  Even within the technology fields, cybersecurity efforts involve specialized disciplines that are not usually addressed by general IT specialists related to perimeter defense, endpoint security, and authentication.  Acquiring and retaining the critical talent for these activities is a growing challenge.\n\nAll of these issues present unique challenges from a risk management perspective.  Moreover, the fragmented regulatory landscape for cyber risk and lack of mature metrics and measurement tools add difficulty.  This is true both for the firm’s second and third lines of defense and Boards of Directors’ oversight, as well as for supervisors approaching this from an external perspective.\n\nAs I mentioned at the beginning of this talk, these issues are well-known and both the private sector and the public sector are actively working toward solutions to these difficult problems.  I expect we’ll see continued evolution of the risk management framework as the broader fintech ecosystem develops and cyber defenses co-evolve with new threats.   Challenging and complex issues include the most effective strategies around prevention, detection and response; communication protocols internally, and with clients, vendors, and the official sector; business continuity plans and contingency exercises; and the role of Boards of Directors.  Supervisors can contribute to this debate by continuing to emphasize the critical importance of a strong risk culture with the appropriate governance and controls framework.\n\nTo conclude, I believe that resiliency to a cyber event is an area where the incentives of the private and public sector are closely aligned.  Microprudential and macroprudential objectives are reinforcing.  As a result, there is an imperative to collaborate, share information, and learn from one another about threats, responses, and best-practice approaches.  Conferences like this one that bring together the private sector, the public sector and academics are a critical component to that necessary dialogue.\n\nThank you for your attention."
    },
    {
        "title": "Fulfilling Our Economic Potential",
        "date": "Apr 11, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil190411",
        "content": "Introduction\n\nThank you for that warm welcome. As you may be aware, last year I moved from San Francisco to take on the role of President of the New York Fed. In many ways California and New York are like different countries—the weather is a big adjustment, for starters!\n\nBut through my meetings with community leaders and visits to the Bronx and Brooklyn, I’ve come to appreciate that we’re facing a lot of the same issues in my new home that I saw in the San Francisco Bay Area. The main topics of discussion—displacement as a result of gentrification, inequitable growth, and a critical shortage of affordable housing—are all ones we were grappling with on the West Coast as well.\n\nThese issues are complex and challenging, and there are no easy answers, so I want to start by saying thank you for all the important work you do.\n\nThere are a couple of topics I want to talk about today. The first is why the work you do is so important, and the second is the role I see the Federal Reserve Bank of New York can play in supporting you today, and going forward.\n\nBefore I go any further I’ll give the usual Fed disclaimer that the views I express are mine alone and do not necessarily reflect those of the Federal Open Market Committee or anyone else in the Federal Reserve System.\n\nDual mandate\n\nApart from a stint running a pizza shop, I’ve worked in the Federal Reserve System for my whole career, nearly 25 years. And I’m not alone. When you ask people at a Federal Reserve Bank how long they’ve worked there, their response is often measured in decades, not years. What makes them stay? Working for a mission-driven institution. \n\nAnd speaking of our mission, the Federal Reserve has two goals: maximum employment and price stability. We want our economy to reach its potential in terms of jobs and growth and we want low, stable inflation. While these two goals are critically important, they are broad and high-level. The monetary policy decisions we make have important effects on the overall number of jobs and wages, but they don’t determine what economic growth looks like across communities, or who benefits from it. \n\nThat’s why our outreach and community-based work is a key priority. It’s an opportunity for us to take all of the data, all of the things we know about the economy, and help one another as we tackle the very real issues that shape the everyday lives of Americans.\n\nEquitable Growth\n\nWe’re closing in on the longest economic expansion on record, unemployment is at historically low levels, and inflation is close to our 2 percent target. From a pure monetary policy perspective, this is a healthy economy. But I’m acutely aware that not everyone is feeling the benefits of the economy’s good performance. \n\nRecent work by my colleagues at the New York Fed crystalized the economic challenges faced by some of those living in New York City. Since the early 1980s wage inequality has increased in the United States, but that increase in inequality has been particularly sharp in large urban areas like this one. \n\nIn New York City and Northern New Jersey, someone near the top of the income distribution earns seven times that of someone closer to the bottom. This contrasts with other parts of New York State, where the number is somewhere between four or five times.1\n\nThat’s not to say things are necessarily easier in areas with less inequality. In fact, many of the areas with less inequality are also challenged by limited job opportunities and declining populations. It does mean that not everyone is benefiting from growth equally.\n\nOne of the reasons for this is that the type of growth in the city has disproportionately benefited one type of worker. Growth in New York has generated demand for highly skilled workers, who are often more highly paid and highly mobile. This drives up housing prices and contributes to gentrification, which often goes hand in hand with displacement.\n\nWords like “gentrification” and “displacement” sometimes can feel impersonal or cliché. For some, gentrification conjures images of fancy coffee shops and high-priced fashion boutiques. But the reality can be heartbreaking. Families who’ve lived in a home for generations are driven out, communities break up, and the social fabric of a neighborhood is irreparably damaged.\n\nWe seek growth that enables everyone to fulfill their economic potential. We want enterprise that supports communities, and communities that have the tools to flourish through investment.\n\nThis is good for families, good for communities, and good for the economy.\n\nOf course, you don’t need me to tell you this. You face the challenges of inequitable growth every day through your work. And that’s why I’d like to talk about the role we can play to support you create a New York City that benefits every part of the community.\n\nWhen I think about our work at the New York Fed, both today and going forward, it boils down to three things: We connect organizations with data and research to amplify the power of their work, we convene stakeholders to share experiences and best practices, and we are a catalyst for initiatives and approaches that help tackle some of the most complex challenges confronting our communities.\n\nConnect\n\nSome of our greatest assets are our data, analysis, and research. And I don’t say that only because I am an economist by training! Data is at its best when it leads to meaningful action.\n\nI just came back from a trip to Puerto Rico and the U.S. Virgin Islands, which are part of the New York Fed’s district. The overwhelming feedback I received from community leaders there was how useful they found our research reports for thinking about how to strengthen their local economy.   \n\nAn example closer to home is the work we do looking at the credit behavior of households at the zip code level.\n\nAnalysis from the New York Fed is used by Financial Empowerment Centers across New York City to support people on low and middle incomes get out of financial trouble and build a credit profile. It also acts as a barometer for the success of programs, which helps practitioners learn which interventions are most effective.\n\nConvene\n\nAs well as connecting people with research and data, we play an important role creating spaces where different stakeholders convene, discuss the challenges and opportunities facing their communities, and share best practices.\n\nThis is where our workforce development program comes in, and it’s an area where Federal Reserve Banks from across the country have been collaborating to share knowledge among key stakeholders.\n\nThe work we’re doing bringing together schools and local employers is also in this vein. We owe it to young people to ensure that their investments in education pay off and that there’s a job at the end of all their hard work and study.\n\nFostering connections between community colleges and employers is one way we can help to make sure the next generation is ready for the opportunities our economy has to offer. We’ve published loads of research on this subject, and we’re putting some of our findings into practice with a series of roundtables, collaborating with colleges, workforce development organizations, and employers across the state.\n\nDisplacement and equitable growth are areas where we can do more. I’m pleased to share that later this year we’ll be hosting a series of listening sessions on these topics. We plan to bring together community development experts and nonprofits to better understand what’s happening across the district. We want to know what’s worked in the communities that have seen equitable growth, the challenges for those that haven’t, and what we can do about it.\n\nWe hope the findings from these sessions serve as a tool for those areas on the cusp of gentrification to ensure every part of the community benefits from local investment.\n\nCatalyze\n\nFinally, I’m going to turn to this idea of how we can be a catalyst for action.\n\nOur desire to create change is the reason we created resource guidebooks—publications that collate data, information, and contacts about specific communities. We’ve published guidebooks for parts of New York State and New Jersey, and we have another one in the works for Puerto Rico. They act as a starting point for local stakeholders and investors looking to support equitable growth in underserved areas. \n\nAnother example where we’ve acted as a catalyst is our Small Business Credit Survey. The findings revealed the challenge small businesses face accessing loans for expansion. The cost of originating smaller loans is often so great that banks won’t take them on. But as a result of the gap being identified, one bank is now partnering with the Community Reinvestment Fund to refer small businesses to the Community Development Financial Institutions (CDFI) Fund.2\n\nSmall businesses are vital to job creation and play an important role in our communities, so it’s essential we support them to get access to the credit they need to grow.\n\nI don’t want to talk about access to credit without saying a few words about the Community Reinvestment Act, or CRA. Our work supervising CRA compliance supports efforts to make sure credit gets to where it’s needed most.\n\nA large amount of work has been going on at the Board of Governors to review the modernization of the CRA, and we held two roundtables here in New York City to better understand what’s working and where we need change.\n\nWhat’s important is that efforts to modernize CRA maintain and strengthen the emphasis on ensuring all communities in our nation have access to affordable credit.\n\nThese principles—connect, convene, and catalyze—will continue to guide us forward as we grow our outreach programs and drive progress. They are the ways we contribute to an economy that works for everyone, above and beyond the high-level levers of interest rates and asset purchases.\n\nConclusion\n\nI’ll conclude where I began. As a policymaker I’m constantly weighing up the dual mandate, thinking about how we keep the economy on track, and balance maximum employment with low, stable inflation. But these decisions don’t affect the kinds of jobs that are created or who benefits from growth.\n\nThe type of growth we have, where it manifests, and who benefits from it play a huge role in shaping both the fabric of society and the lives of New Yorkers. That’s why our community and outreach programs are so important to me.\n\nI’d like to close by reemphasizing my gratitude for your commitment to this city. Community development work is complex, challenging, and never done. But it’s essential for building healthy communities that support everyone to reach their full economic potential.\n\nThank you. I look forward to your questions."
    },
    {
        "title": "The ‘New Normal’ for Growth",
        "date": "Apr 4, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil190404",
        "content": "Note: These remarks are based on those delivered at the Economic Club of New York on March 6, 2019.\n\nIntroduction\n\nGood morning, everyone. It’s a pleasure to be hosting this conference, and I’d like to start by welcoming you all to the New York Fed. I know there’s an opportunity to tour the gold vault later, and I definitely recommend it. There’s something very exciting about our more traditional approach to storing money… even though the gold bars pose certain logistical and practical challenges. If you drop one on your foot you’ll definitely know about it.\n\nCommunity Banks\n\nI’m going to start today by talking a bit about community banks. Aside from a short period running a pizza restaurant, I’ve spent all of my career in the Federal Reserve System. And I’ve had the opportunity to travel extensively throughout the United States. When you visit communities in more rural or remote parts of the country, away from big urban centers like New York and San Francisco, you really see the value of community banks. In many places you’re the only game in town when it comes to banking services and credit. You know the customers in your community because you are an integral part of the community.\n\nYou represent an essential part of our financial system and economy. But I know you didn’t come here today to hear me tell you what an important job you do. What I hope is that I can give you some insights that will be useful going forward.\n\nI know interest rates play a vital role in your businesses, so I’d like to spend a bit of time talking about the outlook and what that means for monetary policy. But before I do, I need to give the standard Fed disclaimer that the views I express are mine alone and do not necessarily reflect those of the Federal Open Market Committee or anyone else in the Federal Reserve System.\n\nWhere Things Stand\n\nFor the last decade we’ve all had our focus on recovery from the Great Recession. The path to getting back on track has been a long one, and there is a sense that the economy has now fully recovered. GDP growth came in at 3 percent in 2018 and I expect it to be around 2 percent this year.\n\nThe difference between last year’s numbers and this year’s forecast has caused some concern, and that’s where I’m going to keep my focus. Many people are hoping for GDP growth of 3 or 4 percent, similar to what we saw in the late 1990s. Less optimistic forecasts for 2019 growth are therefore, understandably, a source of worry.\n\nBut it’s key to remember that this GDP growth is taking place on a backdrop of historically low unemployment, and low, stable inflation. Unemployment is at 3.8 percent and inflation continues to run just under 2 percent.\n\nThe Fed is tasked with two goals: Low and stable inflation and maximum employment. From the perspective of monetary policy things are going very well. And the economic fundamentals indicate that slower growth is—far from a sign of doom and gloom—what we should expect.\n\nThe Economic Fundamentals\n\nThe 3 percent growth of 2018 was boosted by a number of tailwinds. Strong global growth, fiscal stimulus, and accommodative financial conditions all helped drive GDP growth and a tight labor market.\n\nThese tailwinds have calmed, and in fact reversed in some cases. Three developments contribute to this view: a downturn in global growth, heightened geopolitical uncertainty, and the effects of tighter financial conditions. \n\nIf you strip away these factors you’re left with the underlying drivers of potential growth, which are demographics and productivity growth. Both of these have slowed significantly. Baby boomers are retiring and fertility rates have come down, which has significantly slowed labor force growth relative to past decades. Productivity growth has also fallen considerably from the boom years of the late 1990s and early 2000s.\n\nI put the potential growth rate of the U.S. economy at about 2 percent, which is in line with the forecasts we’re seeing for this year.\n\nOf course, a number of different scenarios could play out. Geopolitical uncertainty and other factors holding back growth may recede, and the U.S. economy could resume the robust trajectory of last year. Or GDP growth could continue closer to trend, which is my own forecast. Finally, there’s always a chance that downside risks could knock GDP growth off course.\n\nBut what the data’s telling us is that this period of slow growth isn’t an aberration—it’s the “new normal” we should expect for the foreseeable future.\n\nMonetary Policy\n\nSo what does all this mean for interest rates?\n\nWith the economy running close to our dual mandate goals, monetary policy is in the right place.\n\nLooking ahead, the Fed will watch the data and use monetary policy to keep the expansion on track. When growth is well above trend, raising interest rates to keep the economy on a sustainable path is the right decision. Equally, when faced with a crisis like the Great Recession, it’s incumbent upon policy makers to use every tool at their disposal to get the economy back on course.\n\nIt’s important to remember the change in the long-run economic fundamentals means that interest rates are likely to be lower than what we saw in the 1990s, for example. In addition, the slope of the yield curve is unlikely to return to levels typically seen in the past. This “new normal” clearly has implications for banks, large and small.\n\nConclusion\n\nIn conclusion, the outlook is positive. GDP growth is on track, unemployment remains low, and there are no signs of inflationary pressures building. And while many of you will miss the heady days of the 1990s when growth rates were at 4 percent, the economic fundamentals indicate that these are fundamentally different times. But I’m optimistic for bankers like you because of your relationships with customers and the role you play in your communities.\n\nPlease enjoy the conference. I look forward to joining you later for the luncheon."
    },
    {
        "title": "Opening Remarks to Community Bankers Conference",
        "date": "Apr 4, 2019",
        "speaker": "Kevin J. Stiroh",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/sti190404",
        "content": "Welcome to the 17th Annual Community Bankers Conference.  We are very glad that so many of you from community banks in the Second District joined us today for our conference about what’s “On the Horizon for Community Banks.”  I am also pleased to welcome my colleagues from other regulatory agencies, including Doreen Eberley from the FDIC, who is our keynote speaker.\n\nBefore proceeding, let me emphasize that my remarks are my own and do not necessarily reflect those of the Federal Reserve Bank of New York or the Federal Reserve System.1\n\nIt has been more than 10 years since the financial crisis, and over that time we have seen the industry change in material ways.  Ten years ago there were 7,479 community banks nationwide. Today that number is 5,320, a 29% decline.  Some of this decline was due to failure, and some through consolidation or acquisition.  Your financial institutions remain today as a result of prudent risk management practices, resiliency, and the ability to successfully weather those stormy years.  \n\nToday, the data show, on average, strong metrics for the balance sheets of community banks in the Second District.  The quality of loans has improved due to more prudent underwriting and capital levels appropriately support the risk profile.  As of 4Q 2018 on average, Second District ratios were slightly above the national peer average for tier 1 and total risk-based capital ratios.  In terms of profitability, return on average assets was 0.95% in 2018 for community banks in the Second District, only slightly trailing the national average of 1.1% for all community banks.\n\nAnother relevant metric is the more diversified loan mix on the balance sheets of community banks. Commercial Real Estate (CRE) is a major source of asset growth, and in our geography, real estate continues to be in high demand and continues to outperform certain parts of the country.  For example, in the Second District non-owner occupied CRE concentration remains higher than the national peer average.  Our examiners, however, have noted more robust risk management practices and oversight, and commensurate levels of capital as mentioned earlier.\n\nAs supervisors, we will continue to stress to the region’s bankers that a financial institution’s decision to assume certain types of risk is a decision for the Board of Directors and senior management.  Also, as supervisors, we will continue to expect to see the appropriate level of risk management in place to manage and mitigate the risks assumed.  This means having the appropriate expertise on the management team, robust information systems and technologies to monitor and manage those risks, proper controls and oversight, reasonable limits, and an action plan to take corrective measures when necessary.  All business decisions have risks and there must be mitigating strategies in place to manage adverse occurrences.\n\nHearing your input and perspectives on these issues is an important part of the process for us, and I am fortunate to have the opportunity to meet with many community bankers in the Second District.  Some of the most common themes I hear include: an appreciation for the recent regulatory reforms implemented such as the streamlining of the Call Report; the extension of the examination cycle to 18-month for firms under $3 billion in total asset-size; and the current discussions around capital simplification.  The comment period on capital simplification will close on April 9th, and I hope you all shared input to influence that important policy decision.\n\nOne area where we receive mixed feedback from bankers is the issue of off-site examinations.  Some bankers love that idea, while others have stated they like the face-to-face interaction as it reduces the risk of miscommunication, misinterpretations or ambiguities.  Still others prefer a hybrid approach with certain portions of examination work conducted on-site and others off-site.  All felt critical meetings should be conducted in person.  Today’s technology makes all of these options possible, and we will continue to exercise judgment and thoughtful communication with bankers to determine the best option.  At the end of the day, our shared goal is a fair and accurate assessment for your institution, and we will continue to work together to accomplish this.\n\nTurning to today’s conference theme of “On the Horizon for Community Banking,” I have a few observations that I’d like to share.  These observations, in part, preview the day’s agenda and I look forward to the continued conversation on these important issues.\n\nFirst, our country and the banking industry have benefitted from steady economic growth, modest inflation, and low unemployment in recent years.  These are all positive and welcome outcome, but not all regions have benefitted equally here in the Second District.  Some portions of upstate New York have benefitted from an influx of technology companies and other industries, while other regions remain challenged due to a lack of population growth.  Later today we will hear from two of our economists who will share national and regional economic trends.\n\nAlso in the spirit of looking forward, I can share some views on what community bankers can expect from the supervisory process.  Examiners are keenly focused on emerging trends around funding costs and liquidity risks.  Bankers consistently share with us the challenge of raising core deposits to fund loan growth.  Community banks compete not only with other community banks for deposits, but also with credit unions and larger institutions.  And, these larger counterparts are actively courting customers through direct mailers and offering competitive rates.  Data in the Second District show cost across most deposit categories have increased with a slight uptick in the higher cost funding sources such as borrowings.  Data for the largest banks exhibit a comparable trend, but they control a larger market share.\n\nI expect another area of focus during the examination process will be interest rate risk and related challenges.  Bankers have shared with us that customers are demanding higher rates on deposits, yet those same customers are reluctant to pay higher rates on their loans.  What is a banker to do in trying to maintain a productive, long-term relationship?  We are seeing signs of lower net interest margins (NIM), where the average NIM in the Second District is 3.47 percent compared to the national peer average of 3.71 percent.\n\nExaminers will be reviewing initiatives by community bankers to partner with “fintech” firms.  We know there is competition from non-bank providers who have the capacity to provide automated loan pre-approvals in minutes and loan closings within days after approval.  Today, you will hear from a panel of your peers on how fintech is changing the way their firms do business, and how they mitigate the risks associated with outsourcing.  In my view, new innovations from the fintech space have the potential to fundamentally transform the provision of financial services.  As I’ve said elsewhere, this disruption brings both opportunity and risks and that need to be managed.2\n\nA third forward-looking topic is the coming implementation of accounting changes for reserves, known as the “current expected credit loss” approach or CECL. One of our colleagues from the Board of Governors will join us later to address a common question raised by our bankers on the challenge of gathering loss data in a historically low loan loss environment.\n\nWe will also discuss the challenges you share around keeping abreast of consumer compliance requirements.  During today’s luncheon, our head of consumer compliance will share remarks on recent enhancements to the examination process.\n\nI’d like to close with a few words about a specific topic that is core to the community bank business model, namely attracting new customers and growing deposits.  I believe success in the area will be increasingly linked to a willingness to invest in technology and new processes.  Customers today seem to prefer online banking, and millennials favor alternative banking options.  This suggests both an opportunity and a strategic imperative.  There may be no better time than now to reimagine transformation through investments in innovation, improved multi-channel delivery, more fintech partnerships, and expansion of digital payments.  Technologies to enable transformation are more powerful, readily accessible, easily implementable, and economical than ever before.  Of course, these new activities bring risk management and legal issues and challenges to consider—vendor management, data privacy, consumer protection and cybersecurity to name just a few.\n\nTo conclude, and again to echo the “on the horizon” theme of the conference, I believe we all need to be mindful of this new financial landscape as community banks strive to retain control of the customer experience.  This is critical to the relationship-based business model of many community banks and reflects the unique local knowledge of the market and customer needs.  As the industry evolves, this poses some challenges but also opportunities.  This is new territory for us all and I expect it will be part of the ongoing supervisory dialogue in the years ahead.\n\nThank you for your attention and for joining us today at the New York.  I will be available during today’s lunch session to discuss further, and I hope you enjoy today’s conference."
    },
    {
        "title": "The First Line of Defense and Financial Crime",
        "date": "Apr 2, 2019",
        "speaker": "Michael Held",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/hel190402",
        "content": "Good morning.  It’s an honor to join you at the 1LoD Summit.  As always, the views I express today are my own, not necessarily those of the Federal Reserve Bank of New York or the Federal Reserve System.1\n\nIntroduction\n\nI’m very grateful for the return invitation.  I have to be honest with you, though.  When Paul approached me to speak again this year, I was wary.  What would you all think of having to listen to me again?  The phrase “cruel and unusual punishment” came to mind.  Then I was reminded of an old joke.  To paraphrase W.C. Fields:  First prize, one Held speech.  Second prize, two Held speeches.  That’s a much more pleasant way to think of a return engagement.  So congratulations on second prize.\n\nI really do appreciate this opportunity for two reasons.  First, I want to express again my support for risk managers in the first line of defense.  Supervisors and the industry are often on opposite sides of the table—or opposite sides of the “v.,” as they say in my line of work.  But, in many instances, our goals are shared. “Safety and soundness”—the guiding principle of microprudential supervision—is a shared objective, even if supervisors and the industry sometimes disagree how the concept applies to particular facts.  I think we can agree that your work as first-line risk managers promotes safe and sound operations within your organizations.  So let me begin with my thanks for your continuing efforts.\n\nSecond, there are some things that I would like to get off of my chest.  My topic this morning is financial crime—more specifically, crime that converts and corrupts the payment system to achieve its ends.  This includes theft, fraud, money laundering, sanctions evasion, bribery, kleptocracy, cyber-terrorism, and electronic sabotage.  Combatting financial crime should not be an issue that finds supervisors and industry on opposite sides of the “v.”  It is in everyone’s interest—supervisors and their public constituents; the industry and its customers—to keep crime and the proceeds of crime out of the payment system.  Of course, no one is perfect.  But we must be honest with each other about what we’re doing, what we’re not doing, and how we can improve. \n\nLet me highlight just one example to demonstrate why I am so focused on financial crime right now.  As I’m sure many of you have read, in February 2016 international criminals used fraudulent wire transfer instructions to steal and launder many millions of dollars from Bangladesh Bank’s account at the New York Fed.  Bangladesh Bank is the central bank of Bangladesh.  Some of those funds have been recovered, but Bangladesh Bank is still working to recover most of its loss.  We at the New York Fed are still helping them to do so.  Simply put, it was not just Bangladesh Bank that was wronged. The New York Fed—my client and employer of more than 20 years—was wronged as well.  So I come to you today not as a dispassionate observer.  I come to you today bearing the scrapes and bruises and scars of our own experience at the New York Fed.   \n\nThis morning, I will speak about where we are and where I think we should go from here.  My message, in brief, is that we are all in this together.  The integrity of the payment system is critical to the U.S. and global economy.  Financial institutions and the official sector must do their utmost to protect the system from financial crime.\n\nWhere Are We Now?\n\nThe payment system can be used to facilitate many types of crime.  Fraud and theft have accompanied money transfers for as long as there has been money.  The Romans considered counterfeiting to be a form of falsum, which was a fraud against the public.2   And, of course, Willie Sutton provided the timeless, common-sense explanation for bank robbery.\n\nMoney laundering is, by contrast, a more recent development in criminal law.  In the United States, anti-money laundering law reflects an evolution in focus from banknote tracking to combatting narco-trafficking to counterterrorism.  Rather than take you through the legislative history of this evolution, I want to refer you to the Financial Crimes Enforcement Network—or “FinCEN,” for short.  Its website contains short and accessible histories of major anti-money laundering laws.3   These summaries highlight the important public goals of the Bank Secrecy Act and other anti-money laundering statutes—why they are more than just a compliance cost.  One theme that emerges is the increasing reliance on banks and other financial institutions to safeguard the payment system.  Our laws and regulations increasingly bring the industry into partnership with the government on combatting national security risks to the United States and financial crime.   \n\nAnti-money laundering is not just a matter of historical record.  Our rules continue to evolve.  For example, in 2016 the Treasury Department updated its regulations to require that banks and other financial institutions verify the identity of the natural persons—that is, the “beneficial owners”—who own or control companies that hold accounts.4   The beneficial owner rule, like other “know your customer” rules, is a regulatory floor, not a ceiling.  Covered firms can take the initiative to implement more stringent internal rules based on their risk.  Indeed, in December 2018, the federal banking agencies published guidance that encouraged innovation in anti-money laundering compliance—pilot programs that exceed the legal minima, or at least make compliance more efficient.5\n\nExperiments in ratcheting up internal thresholds are welcome.  As I have said in other contexts, one challenge in a rules-based regime is that the pace of rulemaking is not always commensurate with the pace of rule breaking.6   Complying with minimum legal thresholds may not be sufficient in all circumstances to appropriately mitigate risk to your firm or the payment system.7   Technology has greatly expanded access to the payment system.  Many of us—everyday consumers, that is—effect payments not only directly through our banks, but also through various “fintech” intermediaries.  There is much good in this, but technology is not risk-free.  The creative development and application of technology can create opportunities to improve the payment system.  Technology also creates new opportunities to compromise that system. \n\nTake, for example, the nascent challenge presented by the development of digital currencies.  The New York State Attorney General’s office estimates that more than 1,800 virtual currencies are exchanged around the world.8   Many digital currencies use distributed ledger technologies, which can help institutions achieve efficiencies in customer due diligence programs.  Distributed ledgers also promote traceability, which in theory assists law enforcement.  That said, the use of private exchanges for these digital currencies may facilitate anonymity, which tends to help the bad guys.\n\nMy concerns, however, are less about the technological frontier, and more about well-established risks to the payment system.   Recent press reports about financial crimes involving 1MDB, Danske Bank, Swedbank, and, yes, Bangladesh Bank make no mention of fancy new digital currencies.  Rather, if press reports are to be believed, these cases are to varying degrees about theft, fraud, greed, and corruption. \n\nLess covered in the press is trade-based money laundering—that is, using legitimate trade to hide illicit sources of funds.  The United States is particularly vulnerable to trade-based money laundering because more than half of the world’s trade is denominated in U.S. dollars.9   The Department of Homeland Security and the Drug Enforcement Administration have warned for years that a large amount of illicit narcotic payments occur through low-tech solutions like over- or under-invoicing of goods, false documentation, or phantom shipping.10   These methods give illegitimate transfers the appearance of ordinary transactions.  FinCEN has warned against the use of “funnel accounts” to facilitate trade-based money laundering of narcotics proceeds, and has provided the industry with a list of red flags associated with such activity.11 Trade-based money laundering can also be used to evade sanctions regimes, sometimes through money service businesses or general trading companies.12  \n\nThe extent of financial crime from a global perspective is simply staggering.  According to the United Nations Office on Drugs and Crime, the amount of money laundered globally in one year could be as much as $2 trillion.13   That’s five percent of global aggregate gross domestic product.    \n\nIf the scale of global financial crime is too large to contemplate, let’s focus instead on segments that are local.  A few years ago, the television show 60 Minutes covered an undercover operation that captured on video fifteen out of sixteen Manhattan lawyers offering advice on how an African official could secretly move funds into the country.14   The investigator posed as an adviser to an African minister of mining who managed to accumulate millions of dollars for personal use—expensive real estate, a jet, and a yacht.  In the end, no firms actually took on the client.  These were just preliminary meetings.  And they were a set-up.  No actual crimes were committed.  Still, only one lawyer out of sixteen told the undercover investigator to get lost.  What an eye-opener.  As a lawyer and a central banker—and as a New Yorker— I found this report deeply troubling.     \n\nWhere Do We Go From Here?\n\nSo that’s where I see us today.  Where do we go from here?  Let me share some ideas, which are not mutually exclusive.\n\nBuild Good Habits\n\nFor starters, let’s be honest about “looking the other way.”  Many financial institutions have, at times, turned a blind eye to evidence of money laundering, sanctions evasion, corruption, kleptocracy, and plain theft. Over time, some institutions become weak links in our system when they take on riskier clients, perhaps in order to chase profit, without developing the ability to manage those relationships in a responsible way.  Often these relationships start small: one low-value, ethically suspect transaction.  But it leads to another and another to the point where the money is simply too good to turn away, no matter how many red flags there are.  There may be less pecuniary explanations too.  Regardless of the profit potential, our sensitivity to warning signs can fade from lack of practice.  For every time we look the other way, it is incrementally easier to ignore the next instance.  On the flip side, each time we intervene, the next intervention is easier too.  Effective compliance gets better with practice.  Like ethics, it needs to be a habit. \n\nAnd don’t think that the bad guys don’t notice a firm’s reputation—or, dare I say, its culture.  Word spreads quickly.  A law firm or accounting firm or auditing firm can quickly become known for being “creative” in its approach to an internal investigation or issuing a tax or audit opinion. The same goes for corporate reputations for strong and weak compliance programs, or high or low tolerances for risk.  Clients may be attracted to firms with reputations aligned with their goals, for good or ill.  So, my first word of advice on where to go from here is to safeguard your corporate reputations by developing good habits.\n\nJust Because It’s Legal, Doesn’t Make It Right\n\nThat’s very high-level, prudential advice—not legal advice.  A second point I’d like to emphasize is to avoid cabined views of what’s permissible or impermissible.  Just because something is legal, doesn’t make it right.\n\nI’m a lawyer, and I like to say that legal expertise is what gets you into the room.  Knowing the law is necessary for professional legitimacy.  But, in my view, it is not sufficient.  Lawyers also have to consider the bigger picture—the purpose of relevant laws, the client’s needs, common sense, and fairness.  Those same inquiries should be on the minds of other professionals—including risk professionals in the first line of defense.\n\nI mentioned earlier the recent “beneficial owner” regulations.  Those rules are part of the customer due diligence regime for financial institutions.  The rules tell you the information firms are required to gather and retain to help keep financial crime out of the banking and payment systems.  They do not, however, directly answer the question, “Is this the type of customer we want to do business with?”  That question requires judgment that is not the exclusive domain of lawyers.  The answer will depend not on laws and regulations, but on your institution’s purpose and principles.\n\nManage Account Relationships\n\nOne area where the difference between what’s legal and what’s right is immediately relevant is the management of customer account relationships, including accounts for respondent banks.  I know that this can be tricky.  Banks do not want to take on unnecessary legal and reputational risk for the weaknesses and failures of account holders.  Nor do banks want to needlessly disrupt or impair the efficiency and speed of the payment system.  But that system is under attack.  Perhaps we need to think differently about risks, benefits, and costs. \n\nAn especially difficult decision is whether to close accounts for respondent banks with a demonstrable record of mismanaging their risks of money laundering and other financial crime.  I suspect that many firms will say that they already do this.  But they may not do it enough, frankly.  Or at the least, they may not ask enough questions when confronted with serious red flags about a respondent bank’s activities.  It is a process that requires careful attention.  If a particular bank, or other financial services provider, is unwilling, or even truly unable, to do their part to protect the payment system, then perhaps they should not be part of it.  \n\nTo be crystal clear:  I do not support the large-scale, regionally focused reduction of the availability of correspondent banking services, sometimes referred to as “de-risking.”  The industry has been criticized for “de-risking” with a broad sword.  I am recommending a scalpel.  My concern is how correspondent banks protect themselves and the payment system from specific institutions that have a demonstrated, and unremediated, history of unfairly exposing others to their risks.    \n\nClient-facing roles are critical to identifying customers that do not play by the rules.  The Federal Financial Institutions Examination Council—or “FFIEC”—publishes examples of red flags in its interagency exam manual.15   Many of these examples require an understanding of ordinary customer behavior in order to spot unusual behavior.  Moreover, reasons for suspicion may not be apparent from transmittal records, and can be outside the lens of automated compliance filters, despite their technological sophistication.  What makes one transmittal of funds different from another is purpose. An understanding of purpose often comes from meeting a customer in person and speaking with her about her business goals. \n\nImprove Communication\n\nOne final way forward is to improve communication and information sharing.  Here are some questions you might consider.\n\nWithin your firms, how does the front office communicate with the control and audit functions about money laundering risk?  Is there a frank discussion about common challenges? Or do those discussions resemble depositions or some other defensive exercise?\n\nBetween and among firms, do you take advantage of inter-bank information sharing opportunities pursuant to Section 314(b) of the USA PATRIOT Act?16   If you are a smaller, community-oriented bank, how are you using recent federal guidance that encourages collaborative arrangements that pool anti-money laundering resources?17   Regardless of size, does your firm insist that customers use the proper SWIFT message type with all the required fields completed in the right format?  And, if your firm processes high-risk activity with limited transparency, do you ask respondent banks to supplement payment messages with additional information? \n\nWhen communicating with the government, how helpful are your suspicious activity reports—or SARs, for short?  Are you filing them in the spirit in which they were intended, or for defensive purposes?  Are you picking up the phone to call law enforcement when you file a SAR that presents heightened concerns and perhaps should not wait to run through the formal reporting and referral process?  And, in response to a terrorist attack or other significant national security event, is your bank proactively searching its payment activity to see if it has potentially relevant information? \n\nOne question that I get asked from time to time is if law enforcement can share more information about the activity reported in SARs.  I believe law enforcement is always looking for ways to enhance the effectiveness of its information sharing.  I want to recognize FinCEN’s efforts to share more information via its public website and in its reports to Congress about the information it collects from the industry through SARs.  The New York Fed also tries to do its part.  It hosts three major conferences every year in which the FBI, FinCEN, local law enforcement, and financial firms discuss financial crime—especially terrorist financing, money laundering, and cyber-crime.  We do our best to act as a liaison between banks and law enforcement officials.  We’ll keep at it.\n\nConclusion\n\nStepping back, I’d encourage you to consider as well what your firm’s response to financial crime says about its purpose and culture.  What does it say about the role of the financial services industry in society?  And, to get personal for a moment, how does that response make you feel about where you work and how you make a living? \n\nFrom where I sit, gaps in AML and other payment system defenses make a terrible impression because of the terrible things funded through illegal payments.  When criminals take advantage of your firms and our financial system to make use of their ill-gotten gains, they make every institution involved play the fool.  And here I am going to use the “M” word: morals.  Now I know that whenever a lawyer starts talking about morals, eye rolling ensues—often justifiably.  But I do think that detecting and deterring financial crime is not just a legal obligation.  It is a moral one.    \n\nThanks for letting me get all of that off of my chest.  I’m honored that you wanted me back this morning.  Forums like this highlight important issues and choices.  Make the right choices, not just the legal ones.  I wish you all a good conference.  Thank you."
    },
    {
        "title": "Welcome Remarks at First New York Fed Fintech Conference",
        "date": "Mar 22, 2019",
        "speaker": "Kevin J. Stiroh",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/sti190322",
        "content": "Good morning and welcome to the first New York Fed Research Conference on FinTech.\n\nIt is exciting to see all of you in attendance today including the many scholars, industry participants and colleagues from the official sector who have joined us to further our understanding of the impact, implications, and direction of financial technologies. Thank you for helping us to broaden and deepen this important dialogue.  \n\nThe conference agenda covers a number of relevant topics for economists interested in fintech, which I think of simply as the intersection of finance and technology.  Today’s agenda includes changes in credit allocation, the role of blockchain and tokens in payment systems, and the use of machine learning for evaluating regulation.  These are all important issues.\n\nBefore proceeding, let me emphasize that the views I express are my own and do not necessarily reflect those of the Federal Reserve Bank of New York or the Federal Reserve System.\n\nI have a long interest in technology, particularly how it affects economic growth, productivity, and welfare.  I started my career as a research economist studying the impact of information technology on U.S. productivity and economic growth.1 As you know, a core belief among economists is that technological progress is a fundamental driver of economic growth and improvements in consumer welfare.  We see that in the “great inventions” of the late 19th century such as electricity and the internal combustion engine,2 and more recently in the impact of information technology.\n\nMy view as an economist is that technological progress is generally good for an economy.  That is not to say that there aren’t adverse consequences and risks that need to be addressed.  Recent studies, for example, have questioned whether the increasing use of digital technologies, robotics and artificial intelligence may lead to a decline in the compensation of workers or employment.3 Research suggests that productivity could grow about 2 percent annually over the next ten years, with about 60 percent of this growth coming from digital opportunities.4 This will likely lead to some disruptions and reallocations with new winners and losers among corporations, workers, and households.\n\nFocusing on a single industry, the financial sector has long benefited from technological innovations—think about the automated teller machine, practical applications of asset pricing models, and credit-scoring technologies that have been around for decades.  But, the pace of technological change seems to be accelerating as financial firms embrace more sophisticated predictive algorithms, develop new products and consumer interfaces, and leverage new tools such as cloud services.  One survey suggests that banking firms are more concerned about the speed of technological change than any other sector.5\n\nThis is relevant to me because, in addition to being a recovering economist, I am now a supervisor of financial institutions.  My perspective as a supervisor is that implementing innovations, while generally beneficial, can introduce new risks and add complexity.  These risks need to be understood and managed.  Moreover, in addition to deciding how to implement and manage new digital strategies, financial institutions must also re-evaluate legacy systems, organizations, risk management frameworks, and skillsets. \n\nThe Federal Reserve has long taken a risk-focused supervisory approach with the understanding that the level of scrutiny should be commensurate with the level of risk. Each type of fintech innovation poses its own set of risks and supervisors can look at these changes through a familiar risk framework focused on things like credit, operational, reputation, or strategic risk.\n\nOperational risk, for example, could increase with high-frequency trading, system integration, or cyber events.  Partnerships with new fintech firms or third-party service providers could increase reputation risk.  The potential for new entrants creates strategic risk to incumbent business models.  Artificial intelligence algorithms have the potential to introduce unintended biases that lead to potential fair lending violations or compliance issues.  \n\nThese risks need to be managed to reap the largest benefits from innovation.  Some may see this perspective as implying that the official sector is an impediment to the adoption of new technologies, but I don’t think that is necessarily the case.  The five federal banking agencies, for example, recently encouraged financial firms to explore innovative approaches to meet their compliance obligations related to money laundering and further strengthen the industry against illicit activity.6 That said, not all fintech-related risks are knowable now, and we must be vigilant in monitoring new risks that may emerge in this rapidly evolving space.\n\nI am fortunate to have this supervisory perspective in New York.  The New York Fed’s district holds a unique position in the fintech landscape.  New York City is an attractive destination for fintech entrepreneurs and innovators who aim to operate in the epicenter of the financial services industry.  According to the New York City Economic Development Corporation, since 2008, the City’s tech workforce has grown nearly three times faster than the nation’s.7 New York City’s metro area also benefits from one of the largest concentrations of academic institutions and a strong investor community.  These factors have created a growing and active fintech ecosystem that benefits from its proximity to Wall Street.  We can learn much from a sustained dialogue with this community.\n\nAt the New York Fed, we are working to think more systematically about fintech.  A key aim of today’s conference is to further the New York Fed’s engagement and communication with stakeholders in this space.  This will give us greater insight into cutting-edge developments in specific areas of interest, including technology, regulation, law, and economic research.\n\nTo further our engagement efforts, the New York Fed has created a Fintech Advisory Group. The primary purpose of the Fintech Advisory Group is to build the Bank’s institutional knowledge of fintech and provide the New York Fed senior leaders with a more complete picture of existing and emerging technologies, the application and market penetration of these technologies, and the resultant or expected impact on the financial system.  The Fintech Advisory Group will convene for the first time in the coming weeks and will meet several times per year.\n\nWe also have much to learn from an academic perspective, both on the impact of technology in the financial system and the emerging risks.   That is one reason why conferences such as this one are so important.\n\nThrough these ongoing efforts, we aim to engage with a wide range of firms and participants in the fintech landscape and to participate actively in the ongoing dialogue about the changes that technology is bringing to the financial system.\n\nThank you for your attention, and I hope you enjoy the conference."
    },
    {
        "title": "The Federal Reserve's Experience Purchasing and Reinvesting Agency MBS",
        "date": "Mar 7, 2019",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/pot190307",
        "content": "Thank you for the invitation to speak at today’s workshop discussing asset purchases by central banks. We all have the opportunity to learn from different central banks’ experiences in recent years.1\n\nBetween the middle of 2008 and late 2014 the Federal Reserve expanded the size of its balance sheet from around $900 billion to about $4.5 trillion. During that time, the Fed conducted three large-scale asset purchase (LSAP) programs, two of which specifically targeted agency mortgage-backed securities (MBS) and programs to reinvest principal paydowns. These programs altered the composition of the System Open Market Account’s (SOMA) domestic securities portfolio from one consisting entirely of U.S. Treasury securities to one including approximately 40 percent agency MBS (Figure 1).2 Overall, the Federal Reserve purchased over $4 trillion in agency MBS—fairly evenly split between LSAP and reinvestment purchases—with holdings peaking just below $1.8 trillion.3 I will focus my remarks today on the Federal Reserve’s experience with these purchases of agency MBS.\n\nBefore I continue, I will note that the views I express are my own and do not necessarily reflect those of the Federal Reserve Bank of New York or the Federal Reserve System.\n\nLSAP Transmission Channels\n\nThere are many broad transmission channels through which LSAPs can provide additional monetary accommodation:\n\nThe purchases of different types of assets will work through these channels in varying degrees. For example, sovereign assets will generally work primarily through the first five channels, while certain non-sovereign assets would also work through the credit or prepayment channels.6 Overall, the purchase of non-sovereign assets can be helpful by providing additional channels for monetary accommodation to be realized. In addition, it can allow central banks to overcome functional limitations that could exist if they were restricted to LSAPs in only one market—although the Federal Reserve is lucky to have very deep and liquid U.S. Treasury and agency MBS markets.\n\nPurchases of Agency MBS\n\nFor an asset class to be valuable as part of an asset purchase program, it must be deep enough to allow a meaningful amount of purchases and liquid enough to allow transactions to be conducted at a fast pace without causing a material deterioration in market functioning. With over $6.3 trillion outstanding and an estimated average daily trading volume of over $200 billion in 2018, agency MBS meet these criteria.7\n\nThe depth and liquidity of the agency MBS market are assisted by the existence of the to-be-announced (TBA) MBS trading convention. This structure provides investors with the ability to trade a wide range of MBS with a handful of similar characteristics without requiring them to analyze the underlying details of each individual mortgage security.8 It transforms a market of hundreds of thousands of individual mortgage securities into a limited number of homogenous TBA contracts.9 In addition, TBA market trading conventions provide mechanisms for adjusting previously agreed-on settlement terms. Dollar rolls can be used to extend the settlement date by one month, and coupon swaps can be used to switch the specific TBA contract being purchased for one with a different coupon rate. These trading tools have been used to facilitate efficient settlement and can alleviate market stress.10\n\nHowever, adding a new asset class can also bring new challenges, requiring a central bank to learn more about an asset class and to build or refine the internal systems needed to operate in that market.11 Furthermore, the clearing, settlement and custody of agency MBS require unique attention.12 Finally, there may be political economy perspectives to consider when buying assets other than sovereign government debt, such as benefiting specific economic sectors or private entities. Many might argue that the main issuers in agency MBS gained private benefits at the expense of the taxpayers in the period prior to the financial crisis.13 An aversion to perceptions that agency MBS allocate credit across sectors of the economy could be a consideration in any future decision to undertake purchases.\n\nLessons Learned (and To Be Learned) about MBS Purchases\n\nSince 2008, the Fed purchased agency MBS through two of its LSAP programs and through its reinvestment program.14 As I look back at these experiences, it is worth considering some lessons learned, and what remains to be learned, about their use.\n\nLesson #1: MBS Purchases Provided Clear Benefits\n\nResearch largely provides evidence that the LSAPs that the Fed conducted since reaching the zero lower bound (ZLB) in 2008 put downward pressure on longer-term interest rates and eased broader financial conditions, helping to improve macroeconomic outcomes. Estimates of the magnitude and persistence of the effects of the Fed’s purchase programs vary, as do assessments of the relative importance of the possible channels through which such programs are believed to work.15 Further, the importance of specific channels could have varied given distinct market conditions and economic environments at the time each program was initiated.\n\nIn LSAP 1, announced in November 2008, agency MBS purchases had a stated purpose to \"reduce the cost and increase the availability of credit for the purchase of houses, which in turn should support housing markets and foster improved conditions in financial markets more generally.\"16 This was a period of severe financial market turmoil and deep recession, so private sector buyers of MBS were on the sidelines, causing agency MBS spreads to be “abnormally” high due to market illiquidity and dysfunction. Research suggests that this purchase program reduced the portion of agency MBS spreads that was specifically considered to be “abnormal” by about 70 basis points over its first six months.17 In the framework noted above, these LSAPs worked primarily through the market functioning channel. However, research also suggests that borrowing costs were lowered more broadly through a portfolio balance effect as lower agency MBS spreads are required to induce existing investors to rebalance their portfolios into other securities.18 Overall, the Desk's purchases, aided by the FOMC’s lowering of its overnight federal funds target to the ZLB, resulted in lower mortgages rates, which in turn reduced the cost of home purchases and allowed existing borrowers to refinance at more attractive terms.\n\nLSAP 3, an outcome-based program that included purchases of $40 billion of agency MBS per month, was launched in September 2012, after the liquidity and market functioning of agency MBS had largely normalized. As the pace of economic growth remained modest, purchases were intended to provide overall policy accommodation by putting additional downward pressure on longer-term interest rates, making broader financial conditions more accommodative, and providing further support to mortgage markets.19 Consistent with these objectives, research suggests that MBS purchases in LSAP 3 worked primarily through the reduction of the general level of interest rates, with both agency MBS and Treasury security purchases having a similar effect on agency MBS yields and likely operating through the first four channels I noted earlier.20 While MBS purchases also lower MBS yields through the prepayment channel, a further benefit of agency MBS purchases during LSAP 3 may have been in facilitating a faster pace of long-term debt purchases by the Fed, relative to only purchasing sovereign debt. In doing so, agency MBS purchases enhanced the effects of the duration and portfolio balance channels by increasing the expected stock of the central bank's total longer-term debt holdings. In contrast to LSAP 1, it seems clear that LSAP 3 predominantly worked through channels other than the market function channel.\n\nLesson #2: The Fed Could Purchase Large Amounts of MBS without Damaging the Market\n\nAbsent a conflicting policy objective, a central bank should aim to promote good market functioning in its operations to help ensure effective transmission to the rest of the financial system. As such, the Desk considered how to execute asset purchase programs in ways that avoided market disruption. If the Fed became too dominant a buyer or holder, it could have reduced the tradeable supply of targeted securities and discouraged trading among market participants, leading to diminished liquidity and price discovery, which could ultimately raise borrowing costs and undermine the program’s policy goal.\n\nPurchases of agency MBS exceeded $100 billion per month at times in LSAP 1 and at times exceeded $70 billion per month in LSAP 3 when including reinvestment-related purchases (Figure 2). Staff actively monitored indicators of market functioning for signs of any material deterioration, and I do not believe the Fed’s purchase programs had any material unintended adverse effects.21 Transaction volumes dropped off from 2012 into 2013 as LSAP 3’s agency MBS purchases commenced, but remained fairly constant at those levels following the end of LSAP 3 (Figure 3). Bid-offer spreads implied from actual transactions have remained fairly constant since 2011, other than brief spikes following significant moves in long-term interest rates during the taper tantrum in 2013 and following the 2016 U.S. Presidential election (Figure 4). Finally, MBS fails were fairly high from 2009-2011, but fell off fairly substantially after the Treasury Market Practices Group recommended a fails charge in February 2012 (Figure 5).22\n\nLesson #3: Maintain Flexibility\n\nThe Fed also evolved its purchase strategies and put in place active policies to help prevent market dysfunction as a result of its operations. The initial purchase strategy targeted purchases in line with the outstanding stock of agency MBS, guided by commonly referenced market indices.23 This guidance was relaxed when the FOMC increased the size of LSAP 1 in March 2009, after which supply and demand conditions were considered. Eventually, in 2011, the Desk targeted purchases towards newly issued TBA securities, which tend to be the most liquid and readily available.24 In addition, purchases were spread across settlement months when necessary, and dollar rolls were used in a more transparent way to facilitate the orderly settlement of unsettled purchases.25\n\nLesson #4: Reinvestments Matter\n\nRelatively quickly, the Fed came to learn that the reinvestment of principal payments received from securities held in its portfolio represented a distinct balance sheet tool that could help to maintain the FOMC’s desired level of monetary policy accommodation.26 This is because, in addition to the acquisition of securities through asset purchases, the period over which the central bank intends to hold its securities helps to shape market participants’ expectations about the size and duration of assets available to the public. Extending the holding period through reinvestments prolongs this so-called stock effect on the various transmission channels.\n\nInitially after the completion of LSAP 1 in 2010, the Fed was not reinvesting agency-related principal payments and the level of accommodation provided through each of the transmission channels likely partially reversed. Though these paydowns occurred over time, they amounted to a considerable reduction in the Fed’s agency MBS portfolio, with the $1.25 trillion purchased through the end of LSAP 1 in mid-2010 falling to about $800 billion by late 2011, when reinvestment of agency MBS principal paydowns into agency MBS were initiated. Once they began, cumulative agency MBS reinvestments summed to about $2 trillion through 2018—essentially 100 percent turnover of just over $2 trillion in agency MBS purchases through LSAPs. Figure 6 highlights the evolution of certain portfolio characteristics since the end of 2014.\n\nWhile much has been learned from our experience purchasing agency MBS, there is still room for further study. Very little is known about the effects that agency MBS purchases have on other non-sovereign assets. For example, consistent with the portfolio balance channel, market participants widely expressed a belief that agency MBS purchases had a positive effect on corporate bonds, and even equities, which may more directly influence economic outcomes. However, there has been little academic analysis to quantify this hypothesis, posing a potentially important area for additional study (Figure 7).\n\nBalance Sheet Normalization\n\nFollowing the end of agency MBS purchases in late 2014, principal paydowns were fully reinvested until late 2017. Starting in late 2017, the Federal Reserve has decreased its agency MBS holdings by reinvesting principal maturities only to the extent that they exceed monthly caps that gradually increased up to a maximum level (Figure 8).27 This gradual and predictable approach is intended to reduce the balance sheet’s size at an appropriate pace while supporting good market functioning and mitigating the risk of sharp or outsized asset price reactions.28 Consistent with a gradual decline in the Fed’s agency MBS holdings, 30-year MBS option-adjusted spreads (OAS) have moved modestly higher, and respondents to the Desk’s January surveys expect approximately an additional 10 basis point increase in OAS in 2019 (Figure 9).29 This gradual increase in OAS may reflect the FOMC’s gradual and predictable approach to balance sheet normalization, but could also reflect additional factors. While uncertainty exists as to the precise impact of LSAPs and the relative importance of the various transmission channels, there is even less experience to draw upon in evaluating LSAPs’ reversal, making this another area for additional study.\n\nPreparedness for Future Potential MBS Operations\n\nA separate challenge for the central bank to consider is how to maintain operational and analytical readiness over the long term as current operational needs evolve. For example, the Fed had no operational experience transacting in the agency MBS market prior to the crisis. It initially relied on outside investment advisors to support its first purchase program, before eventually bringing trading operations in house and then later, developing capabilities to conduct auctions on its proprietary auction platform.30 Doing so involved building out new operational practices and procedures, legal documentation, governance and reporting processes, counterparty administration, technological systems, and scaling learning curves for both central bank staff and market participants. In order for the Desk to be prepared for a wide variety of scenarios in the future, including sales or additional purchases, the Desk tests its capabilities through small-value exercises. These exercises are matters of prudent advance planning and portfolio administration by the New York Fed, and no inference should be drawn about the timing of any change in the stance of monetary policy in the future from them.\n\nIn the near term, Figure 8 shows that current market pricing implies that no further agency MBS reinvestment operations will be necessary during the normalization process. But since principal payments on agency MBS are sensitive to changes in long-term interest rates and other factors, it is possible they could rise above the cap and require some agency MBS reinvestment purchases in the future.31 In light of this possibility and in the interest of operational readiness, the Desk has been purchasing up to $300 million of agency MBS for monthly periods in which principal payments fall below the cap and there are otherwise no reinvestments.32\n\nAgency MBS sales are not part of the process of normalizing the size of the balance sheet; however, it was noted in the minutes of the December 2018 FOMC meeting that “Several participants commented on the possibility of reducing agency MBS holdings somewhat more quickly than the passive approach by implementing a program of very gradual MBS sales sometime after the size of the balance sheet had been normalized.”33 To date, the Federal Reserve's experience conducting sales of agency MBS has been limited to small-value exercises that have been run semi-annually since 2015. There would be more to learn about selling agency MBS prior to any decision to use gradual sales to normalize the composition of the balance sheet.34 From a practical standpoint, sales may be complicated in that they would most likely be conducted through sales of individual securities, rather than through the TBA market, because the securities in the SOMA portfolio would most likely be more valuable than those that are cheapest-to-deliver into TBAs.35\n\nIn addition, the structure of the agency MBS market is expected to change with the June 2019 implementation of the Federal Housing Finance Agency’s Single Security Initiative. Under this initiative, Fannie Mae and Freddie Mac MBS are expected to be harmonized such that they can be traded in a single TBA contract, known as Uniform MBS (or UMBS). This change is expected to improve the liquidity of the agency MBS market and would simplify any future agency MBS purchase operations for the Desk. The Desk is developing its operational readiness for transacting in UMBS and will likely conduct operational readiness operations in UMBS later this year.36\n\nConclusion\n\nIn discussing its balance sheet reaction function in its Policy Normalization Principles and Plans, the FOMC has noted that it is prepared to alter size and composition of the balance sheet if future economic conditions were to warrant a more accommodative monetary policy than can be achieved solely by reducing the federal funds rate. However, the Committee has given no indication whether agency MBS would be included in future securities purchase programs. From an implementation perspective, the Federal Reserve is fortunate to have the option of implementing monetary policy in a large and liquid non-sovereign market with direct implications for existing and prospective homeowners, in addition to the U.S. Treasury market where it has the capacity to buy large amounts at a fast pace as illustrated by former Chair Janet Yellen in a speech at Jackson Hole in 2016.37 However, future policymakers will need to weigh these benefits versus credit allocation concerns in making that determination.\n\nFigures"
    },
    {
        "title": "The Economic Outlook: The ‘New Normal’ Is Now",
        "date": "Mar 6, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil190306",
        "content": "Introduction\n\nThank you for the kind introduction. It’s a great pleasure to be here today and a huge honor to be joining the board of the Economic Club of New York. This institution has been an essential forum for debate for many decades, and I sincerely hope today will be no exception.\n\nI’m going to start with a spoiler: At a certain point during this speech I will talk about interest rates. I imagine that if I don’t, you’ll feel you’ve been shortchanged. So before I get going, I’ll give the usual Fed disclaimer that the views I express are mine alone and do not necessarily reflect those of the Federal Open Market Committee or anyone else in the Federal Reserve System.\n\nR-star\n\nNow that’s out of the way, I want to take you back a long time ago, to a galaxy far, far away. It’s 2001: Destiny’s Child is topping the charts, you were marveling over your Motorola flip phone, and I was a young economist at the Board of Governors in Washington, D.C. At the time, I was burning the midnight oil, along with a fellow economist, Thomas Laubach, trying to understand what the burst of productivity growth generated by the tech boom meant for interest rates, both in the short term and in the long run. This led us to develop a model for measuring r-star.\n\nI know what’s going through your mind: “What is this r-star that you speak of?” R-star is how economists describe the neutral or natural rate of interest. It’s the rate that we expect to prevail in the long run when interest rates are neither providing a boost to the economy nor trying to cool things down. It’s neither accommodative, nor contractionary. In other words, it’s the “normal” interest rate we expect in “normal” economic times.\n\nG-star\n\nOne of the major factors determining r-star is… you guessed it, g-star! G-star is what economists mean when they describe trend growth, sustainable growth, or potential growth of the economy. The two main drivers of g-star are labor force growth and productivity growth.\n\nWe started trying to understand where r-star was when productivity growth was very high. During the boom GDP growth averaged over 4 percent per year. Our estimate of r-star for that time was above 3 percent, a full percentage point higher than before the tech boom started.\n\nGiven that this research was originally focused on the reasons r-star had risen, it’s ironic that I and others have ultimately dedicated much more of our careers to understanding why it’s dropped. In recent years, r-star has been averaging well below 1 percent. And it’s actually now lower than at any time before the Great Recession.\n\nOne of the reasons for this dramatic decline in r-star lies in changes to the major factors determining potential growth: labor force growth and productivity growth.\n\nBaby boomers are retiring and fertility rates have come down. Both of these demographic shifts have significantly slowed labor force growth relative to past decades.\n\nProductivity growth has also fallen considerably from the boom years of the late 1990s and early 2000s. All the rapid changes in technology that we see around us every day may make this seem counterintuitive. But for the moment, being able to order a Nintendo Switch and have it arrive the same day is, shockingly, not increasing productivity in a meaningful way. I could go on for quite some time on this topic, but will leave it to another day!\n\nI should note that these demographic and productivity trends are not unique to the United States. In fact, we’ve seen similar slowdowns in growth and sharp declines in r-star in other advanced economies.1\n\nSo why did I take you back to simple days of the millennium bug, when you were playing Snake on your phone and ordering takeout from a paper menu?\n\nThe Economic Outlook\n\nUnderstanding the fundamentals driving g-star and r-star provides a helpful backdrop for what’s going on in the economy today, and an indication of what we should expect in the future.\n\nWhat is the current economic outlook?\n\nThe potential growth rate, or g-star, currently appears to be about 2 percent. That may sound low to many of you, but remember that labor force and productivity trends have slowed considerably relative to the past, and that’s unlikely to change anytime soon.\n\nBy comparison, actual GDP growth for 2018 came in at just above 3 percent. A number of positive tailwinds gave the economy this extra boost. Strong global growth, fiscal stimulus, and accommodative financial conditions all helped drive strong headline GDP growth and a tight labor market.\n\nThese tailwinds have calmed, and in fact reversed in some cases, and I expect growth to slow considerably relative to last year, to around 2 percent. Three developments contribute to this view: a downturn in global growth, heightened geopolitical uncertainty, and the effects of tighter financial conditions.\n\nStarting with global growth, the outlook in both Europe and China has become less bright, with the downgrade to the outlook in Europe notable. This means less demand for our exports.\n\nIn addition, there’s geopolitical risk on the horizon that’s creating angst. I know we’re all on tenterhooks waiting to see what will happen at the end of this month to our good friends across the pond. And concerns around trade negotiations continue to loom large. These geopolitical risks leave an imprint on the economy as businesses put off hiring and investment decisions until the air has cleared.\n\nMoving away from the global context, back to the United States, the tightening of financial conditions that occurred late last year will likely restrain consumer and business spending this year. In fact, we have already seen a sustained slowing in housing construction, in part reflecting less favorable financing costs.\n\nNow, I know this talk of slowing growth is causing uncertainty, some hand-wringing, and even fear of recession. But slower growth shouldn’t necessarily come as a surprise. For quite some time, the economic fundamentals have pointed to GDP growth much lower than what we saw in the 1990s, for example.\n\nIn fact, my 2 percent growth forecast is right in line with g-star. That means slower growth isn’t necessarily cause for alarm. Instead, it’s the “new normal” we should expect. And, it’s important to remember that this is happening at a time when the labor market is very strong.\n\nFrom the perspective of monetary policy, the overall picture of the economy is about as good as it gets: very low unemployment, sustainable growth, and inflation just about at our 2 percent goal.\n\nGiven this favorable situation, when you look at monetary policy, things are looking pretty normal as well. My current estimate for r-star is 0.5 percent, so when you adjust for inflation that’s near 2 percent, the current federal funds rate of 2.4 percent puts us right at neutral.\n\nWhat Does the Future Hold?\n\nSo what does the future hold?\n\nWith a strong labor market, moderate growth, and no sign of any significant inflationary pressures, the baseline outlook is quite favorable, as I’ve said.\n\nOf course, there are a number of different scenarios that could play out over the year ahead. Geopolitical uncertainty and other factors holding back growth may recede, and the U.S. economy could resume the robust trajectory of last year. Or GDP growth could continue closer to trend, which is my own forecast. Finally, there’s always a chance that downside risks could knock GDP growth off course.\n\nWhat will the response of the Fed be? My short answer: It depends!\n\nI promised talk of interest rates, and here it is. I’ve said it before, and no doubt, I’ll say it as long as I work for a central bank. But in the current conditions the phrase takes on even more importance. Our response will always be “data dependent.”\n\nWhen growth is well above trend, raising interest rates to keep the economy on a sustainable path is the right decision. Equally, when faced with a crisis like the Great Recession, it’s incumbent upon policy makers to use every tool at their disposal to get the economy back on course.\n\nThe base case outlook is looking good, but various uncertainties continue to loom large. Therefore, we can afford to be flexible and wait for the data to guide our approach. In that context, the FOMC decided to keep interest rates where they were at the most recent meeting, and noted the importance of patience in determining future policy actions.\n\nTo use a nautical metaphor, guiding the U.S. economy is like steering a large ship. Monetary policy decisions can leave a wake several miles long, with implications that reach far into the future. We’ll consider the full range of data, the headline statistics, the market indicators, and we’ll listen to our business contacts on the ground, as we aim to keep the economy on its current course of a strong labor market, sustainable growth, and 2 percent inflation.\n\nConclusion\n\nR-star provides a helpful framework, not just for thinking about monetary policy, but also for understanding the current progress of GDP growth and its likely trajectory over the coming years.\n\nModerate growth, both in the U.S. and the global economy, is far less alarming when you know what the underlying factors might be. And all the signs have been pointing to a slower growth trend. You may have had Destiny’s Child—Beyoncé—turned up too loud to spot the signs, but they have certainly been there.\n\nSlow trend growth and low r-star in the current global context presents certain challenges to monetary policy, another topic for a future day. In everything that we do, we need to consider all the information, and exercise data dependence and flexibility, as we navigate the course ahead."
    },
    {
        "title": "Policy Efficiency in Supervision",
        "date": "Mar 1, 2019",
        "speaker": "Kevin J. Stiroh",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/sti190301",
        "content": "Introduction\n\nGood morning.  Thank you very much for the kind introduction.  \n\nI am very happy to participate in this conference on “Bank Regulation, Lending, and Growth,” sponsored by Columbia University and the Bank Policy Institute. The agenda covers a number of important banking issues and I will talk about the idea of policy efficiency from a supervisory perspective.1\n\nBefore beginning, let me emphasize that the views I express are my own and do not necessarily reflect those of the Federal Reserve Bank of New York or the Federal Reserve System.\n\nPublic policy strives to be efficient, but efficiency is not a simple concept, particularly in the context of the supervision of large financial firms where the world is complex, the official sector has multiple objectives, and impact lags are long and uncertain.2  For example, supervisors care about the safety and soundness of individual financial firms, about the provision of financial services to support the real economy, about consumer welfare and financial inclusion, and about financial stability.  More generally, supervision and regulation are designed to both support productive financial intermediation and limit disruption and financial stability risks.3\n\nToday, I will talk about a simple framework for one view of policy efficiency in a world of two policy objectives – financial intermediation and financial stability.  As in traditional finance theory, one can think about an “efficient policy frontier” that maps the trade-offs among these objectives.  One might choose a set of policies that supports a high level of financial intermediation and also accepts a higher risk of disruption or crisis.  This might be achieved, for example, with relatively light oversight or low capital requirements.  Alternatively, one might choose a different set of policies, such as more intensive oversight and stringent capital requirements that likely reduces risk, but also limits intermediation.\n\nAn efficient policy is one that minimizes risk for a desired level of intermediation—it is on the efficient policy frontier.  This efficiency idea is distinct from the policy choice itself, as any of these policy regimes can be efficient in the sense that risk is as low as possible for a desired level of intermediation.  I’ll emphasize that this framework is agnostic to the merits of one policy choice or another.  That is, it is a positive framework and not a normative one, and I won’t look at this from a normative perspective.  Policy preferences can vary and they are set by policymakers.\n\nI believe this simple framework is useful when assessing and discussing changes to supervisory or regulatory policy that can be designed with different purposes in mind.  For example, a policy change could be designed to increase efficiency and move toward the frontier.  This could entail changing rules, regulations, or supervisory practices in order to promote more financial intermediation without increasing risk.  Obvious examples could be eliminating a complex compliance regime for an activity that banks no longer perform or reducing redundancy in data reporting.  It seems like a good idea to always pursue these types of efficiency gains.\n\nAlternatively, a policy change could be designed to achieve a different risk and return profile.  This could be a shift from a mix of policies designed to achieve a low-intermediation and low-risk outcome to a mix of policies designed to support higher intermediation while accepting greater risk.  Either of these regimes can be efficient and consistent with policymakers’ preferences, but the expected outcomes would be quite different.\n\nFrom a public policy perspective, in my view, it is important to be clear and distinguish between these two types of policy change.  For example, one might expect a greater number of bank failures during stress if there were less effective supervisory oversight or higher borrowing costs in a world with higher capital requirements.4 Those may not be desired outcomes, but they would be predictable ones.  From a risk management perspective, this might be viewed as an accepted risk of a particular policy choice.  By contrast, a policy change designed to increase efficiency might not come with those undesirable outcomes.\n\nIn practice, however, it can be difficult to differentiate among drivers of policy change due to measurement and assessment challenges, the potentially long impact horizon, and different views from a social and private perspective.  I’ll turn now to describe this efficiency framework and then discuss some of the practical issues.\n\nAs a point of departure, I’ll begin with an idea that I expect is familiar to most of you.  Standard finance theory suggests that investors face a trade-off between risk and return that outlines the set of feasible options across portfolios.  An investor can choose a portfolio with higher expected returns and higher risk, or a portfolio with lower expected returns and lower risk.\n\nI think there is a useful analogy for supervisory and regulatory policy.  On one hand, the official sector cares about the provision of financial services such as credit, deposit-taking, payments, and risk transfer, where effective intermediation supports economic growth and consumer well-being.  This is the basic function of the financial sector and can be thought of as the return on banking.  On the other hand, the official sector also cares about reducing disruption in the provision of those services and broader financial stability risks.  It is only a decade since the financial crisis in 2008-2009 that imposed such an enormous social cost on the U.S. economy, so those risks remain salient for many of us.\n\nIn this context, the risk of disruption or a financial crisis matters because of the potential reduction of necessary financial services and economic activity in the future.  As I discussed in a talk at this conference a few years ago, a short-term discussion of the balance between firm resilience and the provision of financial services may be reframed over a longer horizon as a dynamic trade-off between the provision of financial services today and in the future.5  This risk/return framework extends that perspective if one cares about the volatility of those financial services across different states of the world and over time.\n\nThe idea of multiple objectives suggests a policy trade-off with an efficient frontier.  In this framework, an efficient financial sector regime can only realize more intermediation if it accepts a higher risk of financial instability and less intermediation in the future.  A second premise is that the official sector can impact outcomes relative to this frontier through a combination of rules, regulation, and supervisory policies.  For example, a reduction in loan documentation requirements, risk management reviews and supervisory oversight might allow increased lending in the short-run, but potentially lowers loan quality and increases risk over time. \n\nA particular set and calibration of policies is efficient if the only way to increase financial intermediation is to accept higher risk of disruption or crisis.  Equivalently, an efficient outcome is one where the only way to reduce risk is to accept less intermediation.  This is not to say that all components of supervisory and regulatory policy come with this trade-off,6 but at some point, policy interventions become less effective, intermediation costs rise, and a trade-off is likely to emerge.  Those points map the efficient policy frontier.  The official sector can either push the financial sector along a given frontier as policy preferences change or closer to the frontier as policy becomes more efficient.\n\nTo be more concrete, consider the following chart with financial intermediation on the Y-axis and financial stability risk on the X-axis.  Point A represents the outcome from a conservative policy mix with relatively low financial intermediation, but also relatively low financial stability risk.  In this environment, supervisors might impose tight lending standards, so projects with positive net present value and low risk are potentially left unfunded.  You might see high capital charges on relatively low-risk activities, restrictions or prohibitions on certain risky activities, or relatively frequent exams.  Intermediation and credit growth might be slow on average, but more stable with less risk of a disruption to financial services or a financial crisis.\n\nBy contrast, point B represents the outcomes from a less conservative policy regime that supports more financial intermediation, but comes with a higher risk of disruption or financial crisis.  In that environment, banks might be more inclined to assume a “risk-on” posture with looser underwriting standards, so riskier projects are funded.  Oversight would be low and banks would feel less constrained.  On average, greater intermediation would likely support more economic activity in the short-run but with a higher probability of disruption later on.7\n\nFinally, point C represents an outcome from a policy regime that is not efficient.  A different mix of policies could increase intermediation or decrease risk (or both) with no associated trade-offs.  Point C is never a good outcome.  Examples of such policies might be a burdensome compliance regime for activities that a bank doesn’t engage in, redundant data requests from the official sector, or a risk management system that does not address the most material risks.\n\nTo summarize, both point A and point B can be thought of as efficient outcomes, where the only way to support more financial intermediation is to accept higher risk.  To be clear, one point on the efficient policy frontier is not necessarily better than another, and changes in policy to move between them reflect policymakers’ choices to promote social welfare.  By contrast, point C is inefficient, and changes in the policy mix that push outcomes toward the efficient frontier are likely to be welfare-enhancing.  The official sector should always be looking for opportunities to move the financial sector toward the frontier.\n\nAs a practical application of this framework, consider the Federal Reserve’s commitment to tailoring and ensuring that supervision is scaled appropriately to the risks associated with different types of institutions.8  Recent examples could include the stress testing distinction between large, complex firms and large non-complex firms; expanded eligibility for the 18-month exam cycle; relief from supervisory assessments, stress testing requirements, and other prudential measures for bank holding companies with less than $100 billion; the Bank Exams Tailored to Risk (BETR) program; and the multi-agency proposal to better align prudential standards with the risk profile of large institutions.9 \n\nTailoring allows different firms to operate with different parameters of the policy regime.  The largest, most systemically important firms, for example, impose larger potential risks to society than other firms, which implies a different risk/return trade-off and policy mix than for smaller institutions.  Without the ability to tailor the regime to reflect the risks associated with different types of institutions, we would likely be left with a one-size-fits-all approach that is not optimal for any type of firm or for the financial system as a whole.\n\nThe simple framework that I just described is just that—simple.  The real world is much more complicated, and I’d like to spend a few minutes discussing some practical issues.\n\nThe chart presented above is clearly illustrative and actual policy analysis requires detailed estimates of the potential benefits and costs of each component and the cumulative effect.  For example, the Basel Committee published a framework to identify optimal levels of bank capital and used that framework as an input into the calibration of Basel III capital requirements.10  This approach considered how the probability of a financial crisis varies with bank capital, the social costs of banking crisis, the link between bank capital and bank funding costs, and the impact of higher loan spreads on economic activity.\n\nMoreover, this efficient frontier is not fixed and is likely to vary over time as technology, official sector tools, and bank business models evolve.  For example, technology-driven innovations that rely on bigger datasets and more sophisticated predictive algorithms might change the risk/return opportunities associated with certain intermediation services.  Similarly, supervisory innovations like stress testing or more sophisticated horizontal assessments could make official sector oversight more effective.  Finally, changes in business strategies like a shift to a more diversified business model might change the overall risk of certain activities.  One can think of these changes as shifting the efficient policy frontier where there is more intermediation for a given level of risk.  Alternatively, constraints on supervisory authorities or practices could shift the frontier in the opposite direction if use of effective tools is prohibited.\n\nGiven the complexity of these relationships, it is not surprising that there is a wide range of estimates and theories about the optimal type and level of oversight of financial institutions.  Nonetheless, we need to understand better these relationships in order to make policy choices more effective.  This is an area where I believe the official sector, industry participants, and researchers can all continue to make progress.\n\nA second interesting issue reflects the perspective on risk and return.  In traditional portfolio investment problems, it is clearly the risk and return to the private investor.  When thinking about policy, however, the appropriate perspective of the official sector is for society as a whole, which likely differs from the perspective of an individual firm.  That is, the official sector and private sector might measure the axes differently—particularly the risk axis—because the official sector cares about spillovers, externalities, and other market failures that private actors may ignore.11 \n\nThis broader perspective is a critical part of the supervisory framework for large banks that was implemented after the financial crisis.  In particular, the official sector recognizes the potential impact that stress at a large bank could have on financial markets or the real economy.  As a result, the Federal Reserve supervisory program pursues two complementary goals for large bank supervision—one, to enhance resilience to lower the probability of failure or inability to provide intermediation services and, two, to reduce the impact on the financial system or economy in the event failure or material weakness does in fact occur.12\n\nThis difference in perspective potentially introduces some divergence in views and assessment of what an efficient policy looks like.  A set of policies might be socially efficient—that is,  on the frontier when risk and return are measured for society as a whole – but perceived as inefficient by the industry or the public, who may focus on the private impact on individual firms or sectors.  This can lead to some debate about the stated goals and assessment of the efficacy of a particular policy mix.  I believe this is an area where better communication can help and the official sector can be clear on expected outcomes and explain any divergence between social and private perspectives.\n\nA third issue is that it is not always easy to distinguish policy changes that move outcomes along the frontier from policy changes that move outcomes toward the frontier.  For example, is the shift to an 18-month exam cycle for community banks or a shift to less frequent stress testing for some banks an increase in policy efficiency or a shift to a different policy stance with different risk/return features?  As I mentioned, both can be defensible policy moves, but they do have different implications.  Moreover, a given policy change might have both effects as the stance of policy changes and the policy mix becomes more efficient.  The proposal around the stressed capital buffer, for example, simplifies the capital regime and also relaxes some assumptions.13  Finally, these policy changes might interact and blend together.14\n\nHow do you know the policy intent?  The most obvious answer is to look at the stated rationale and motivation associated with the policy change.  For example, the analysis by the Basel Committee on the long-term economic impact of stronger capital and liquidity requirements accepted the potential costs of higher loan spreads and reduced output levels, so this can be viewed as movement to the left along a frontier. \n\nIt is also useful to think carefully about the expected impacts—both desired and undesired—and make ex ante predictions.  What is the likely behavioral response?  How might the balance sheet and the income statement change?  What might financial markets reflect?\n\nAt the risk of taking this simple framework too far, let’s think about what might happen to market indicators in each of two scenarios—a movement toward the frontier that makes policy more efficient and movement along the frontier toward a less conservative policy stance.  If the former is really an elimination of inefficient constraints (move from C to B, for example), one might expect equity prices and Sharpe ratios to rise, while CDS spreads or bonds ratings are flat as opportunities expand and risk doesn’t increase.  By contrast, in a movement along the curve, say from A to B, one might expect equity prices to rise as intermediation opportunities increase, while CDS spreads and ratings deteriorate as risk increases.  One might expect to see measures of increased distress in some states of the world over time. \n\nOf course, the world is much more complex and risks materialize over long periods of time in response to many factors, and there are important differences between social and private outcomes, but I think it is helpful to think through these types of signals ex ante.  By making predictions and assessing whether observed outcomes are consistent with those predictions, we can better understand the complex dynamics in financial institutions and develop better policy.\n\nFinally, I’ll emphasize that I have focused on one specific type of policy efficiency that looks at the ability to achieve multiple objectives.  Alternatively, one could think about efficiency more directly in terms of resources and burden, both on the private sector and the official sector.  For example, the Federal Reserve aims to minimize compliance burden while achieving its policy goals.15   That perspective is about allocative efficiency of scarce resources and the deadweight loss associated with the misallocation of those resources.  As part of the official sector, we have an obligation to be good stewards of resources and should always try to achieve a given set of policy objectives in the least-cost manner.\n\nTo conclude, I have suggested a simple framework to help distinguish changes in policy to achieve a different level of intermediation and an acceptable level of risk from changes in policy to increase efficiency.  The first reflects the preferences of policymakers, and the second should be a standing goal.  While it is difficult to make this distinction in practice, I believe it is useful to keep the difference in mind as we discuss and explain our work.\n\nIn my view, communication is vital, and we should explain our goals as clearly as possible and acknowledge all expected outcomes of policy changes.  By recognizing the full set of expected outcomes, including those that are not desired, I believe we can more effectively communicate the goals of supervisory policy, increase accountability and transparency, and better fulfill our mission to promote a safe, sound and stable banking and financial system.\n\nThank you for your attention.\n\nChart"
    },
    {
        "title": "Reform of Culture in Finance from Multiple Perspectives",
        "date": "Feb 26, 2019",
        "speaker": "Kevin J. Stiroh",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/sti190226",
        "content": "Introduction\n\nI would like to begin by thanking the Global Association of Risk Professionals for the opportunity to speak at this Risk Convention.1  As a bank supervisor, I am obviously very interested in the topics of risk management and risk measurement.  An important part of my job is to interact with the financial industry to better understand the issues you face and how we can work collaboratively toward our shared objective of promoting sustainable financial intermediation and a safe, sound, and stable banking and financial system.\n\nAs risk professionals, you are all steeped in the intricacies of risk management and risk measurement.  Value-at-risk, the three-lines-of-defense model, and controls and testing frameworks are second nature.  This is particularly true when we talk about the most familiar and well-studied financial risks such as credit, market, and liquidity risk, and even non-financial risks such as operational, compliance, and legal risk.\n\nWe all acknowledge the critical importance of these issues, but today I will talk about misconduct risk and the need to develop a healthy corporate culture to mitigate it.  I’ll focus on the importance of addressing these issues from multiple perspectives including those of boards of directors, senior management, industry groups, and the official sector.  This topic might be less familiar to some—the terms are newer and the assessment frameworks are less developed—but I believe it is critically important.  Both the official sector and many of your firms have put considerable emphasis on improving corporate cultures and reducing misconduct risk, but we continue to see striking cases of misconduct both in the U.S. and abroad.  Clearly, more needs to be done.\n\nMany of the insights I will share today are drawn from interactions with industry participants and members of the official sector through outreach and industry conferences and workshops on the topic, including events hosted by the Federal Reserve Bank of New York.2  I am confident that continued commitment and collaboration will contribute to better outcomes in this vital area.\n\nBefore proceeding, I’ll emphasize that I am speaking for myself and not the Federal Reserve Bank of New York or the Federal Reserve System.\n\nInfluencing Culture from Multiple Perspectives\n\nPrudent risk management is a fundamental aspect of the business of banking, which means it is a critical factor in the effective oversight of banks.  Supervisors focus on financial resilience by requiring firms to maintain sufficient capital and liquidity and emphasize operational resilience by promoting effective corporate governance, risk management, and recovery planning.3  In my view, misconduct risk—defined as the potential for behaviors or business practices that are illegal, unethical, or contrary to a firm’s stated beliefs, values, policies, and procedures—is an integral part of an effective risk management framework.  Bad conduct threatens a firm’s resiliency by diverting management attention, harming a firm’s reputation, depleting a firm’s capital, and affecting the composition of its workforce.4\n\nTo further develop this idea, the team working on this at the New York Fed introduced the concept of a firm’s “cultural capital,” which is a type of intangible asset that impacts what a firm produces and how it operates.  It is analogous to physical capital, like equipment, buildings and property, or to human capital, like the accumulated knowledge and skills of workers, or to reputational capital, like the franchise value or brand recognition.  This is an intangible asset, so we generally feel the impact rather than see the thing itself.  Nonetheless, I believe a firm’s cultural capital can be measured, assessed, and ultimately impacted in ways that can improve outcomes and enhance a firm’s resiliency.\n\nIn a firm with a high level of cultural capital, for example, misconduct risk is low and observed structures, processes, formal incentives, and desired business outcomes are consistent with the firm’s stated values and risk appetite.  Unspoken patterns of behavior reinforce this alignment. Problems are escalated to senior managers routinely, as employees feel empowered to raise their hands and believe that their efforts will result in meaningful responses.  And senior leaders advance through the organization because, in addition to strong business performance, they set a credible tone from above by modeling behaviors consistent with the firm’s values. \n\nBy contrast, in firms with low levels of cultural capital, formal policies and procedures do not reflect “the way things are really done.”  Misconduct results from the norms and pressures that drive individuals to make decisions that are not aligned with the values, business strategies, and risk appetite set by the board and senior leaders.  Employees do not speak freely when they have concerns, and directors and senior managers do not find out about improper conduct until it is uncovered by the authorities. Rules may be followed to the letter, but not in spirit. All of this increases misconduct risk and potentially damages the firm and the industry over time.\n\nTo be clear, I am not saying this is easy.  Culture is difficult to assess because it is not a single, point-in-time metric, but a multi-dimensional concept with different implications for different parts of an organization.  This complexity suggests that no single metric, solution, approach, or template will work for every firm in every circumstance.5\n\nConsider the different roles of key players in mitigating misconduct risk and building and sustaining a healthy culture.  The board of directors, senior management, staff, investors, industry groups, and the official sector might all see and assess aspects of a firm’s culture differently. Today, I will focus on a few of these stakeholders.\n\nBoards of Directors\n\nIn 2017, the New York Fed hosted a culture measurement workshop that included a panel of board members from various financial firms.  There was consensus among the panelists that board members play a critical role in a firm’s governance structure and culture.  Moreover, corporate boards independently assess whether management is setting the right tone and hold management accountable for any gaps between a firm’s values and its actual business practices.  Proposed guidance from the Federal Reserve suggests that the responsibilities of an effective board of directors include setting strategy and risk tolerance, supporting independent risk management, managing information flows, ensuring appropriate expertise, and holding senior management accountable for effective execution.6   \n\nBehavioral indicators can help boards understand their firms’ cultures and identify potential trouble spots within the organization.  Corporate boards use various methods to obtain information and assess culture, such as meeting with many levels of employees, receiving third-party assessments, and conducting 360 reviews of the CEO and top leaders.  Organizations like the U.K. Banking Standards Board (BSB) can provide outside benchmarking data against which to measure the culture within one’s own firm. 7  Corporate boards for financial firms may also import practices and lessons from other industries.  Finally, the board members that participated in the New York Fed’s workshop agreed that a board’s own culture matters; it needs independent voices, diverse perspectives, and opportunity for debate.\n\nSenior Management\n\nThe Federal Reserve published a proposal on corporate governance that states that senior management is responsible for the execution of strategy and day-to-day operations consistent with the firm’s stated risk appetite, ensuring safety and soundness, overseeing business lines and risk management, and ensuring compliance with internal policy, procedures, regulations and laws.8 Development of a firm’s cultural capital links directly to these core responsibilities and is evidenced by behavior throughout the firm.\n\nTo address these concerns and build cultural capital, some firms have established culture and conduct committees or groups that meet regularly to review cultural indicators and assessment efforts. Providing these groups with sufficient stature, developing the proper infrastructure to support them, and defining clear roles and responsibilities are essential for success.  It is critical that there is alignment between the board, senior management, and middle and lower management on issues of conduct and culture, as employees take cues from their direct managers even more than the tone from the top.\n\nThe performance management framework established by senior management sets powerful incentives to influence conduct and culture within a firm.9  Financial incentives, however, are not the only drivers of culture and conduct. Employees alter their behavior based on a variety of formal and informal, intrinsic and extrinsic motivations.  Intrinsic motivations could include growth opportunities, independence, power, and social factors, while extrinsic motivations could include pay, benefits, profit sharing, or other forms of awards.10  With this range of motivating factors and the impact they have on the culture in mind, a number of firms have integrated “how” factors into an employee’s performance evaluations by assessing the methods used to achieve a performance goal alongside the “what” components such as contributing to their group’s financial performance.\n\nIndustry Benchmarking\n\nAnother perspective is from the industry as a whole.  I think we can gain important insights by looking across the financial industry, identifying best practices, and understanding what approaches are most effective in positively influencing conduct and culture.  Conferences and workshops like those hosted regularly by the New York Fed and others can help.  In addition, standardized metrics and indicators can provide insights when assessing changes over time and across firms. \n\nThe BSB, for example, provides one lens on assessing culture change for a broad cross-section of financial services firms operating in the U.K.11  Each firm participating in the survey has the ability to see its outcomes relative to others and the industry average.  This type of industry-wide assessment gives firms insight into areas they can individually focus on, as well as areas they may want to address as an industry.    \n\nThe U.K. is not alone in this approach.  In December 2017, the chief executives of the five main retail banks in Ireland informed the Department of Finance of their intention to establish an Irish Banking Standards Board.12\n\nIn the U.S., the idea of industry benchmarking has been raised and discussed in the past, but no industry action has emerged to move this idea forward.  At the New York Fed’s workshop on culture measurement and assessment, several participants recommended either joining the BSB’s benchmarking survey or creating a similar U.S. organization.13 Some noted that it would be helpful to include non-banks in the effort, and to collaborate with other corporations that have experience building strong cultures.  Participants also discussed the importance of sharing “lessons learned” and developing best practices as an industry.  In my view, these types of industry-led initiatives can offer useful insights as firms compare and contrast their efforts and outcomes with peers. \n\nOfficial Sector\n\nA final perspective is that of the official sector.  In my view, there is a clear role that follows the traditional focus on prudent risk management and the goal of building resilience.  This reflects both the direct impact to firms mentioned above and the potential for industry-wide spillover if conduct and performance issues at one firm cause customers, counterparties, and investors to lose trust in the industry as a whole. This type of spillover suggests a role for the official sector to coordinate efforts and push the industry forward.\n\nTo be clear, it is not the role of the official sector to set the internal culture of a firm—that responsibility lies squarely with the board of directors and senior management.  Rather, the job of the official sector is to promote standards and practices that will mitigate misconduct risk and promote efficient and sustained financial intermediation and financial stability.\n\nGlobal supervisors have taken a number of approaches when it comes to understanding, influencing, and assessing culture at firms. In the whitepaper I mentioned earlier, my New York Fed colleagues and I included a section describing a selection of approaches that the official sector has employed.  The Financial Stability Board Working Group on Governance Frameworks (FSB WGGF) has published a range of tools for supervisors to use in assessing firms’ management of cultural drivers of misconduct, as well as a summary of supervisory approaches and practices in supervising for culture.14  This variation reflects differences in mandates, tools, and authority.  Looking across jurisdictions, we see supervisory efforts such as specialized units of behavioral experts, risk culture assessment frameworks, and supervisory guidance that directs supervised institutions to develop and promote a sound corporate culture.\n\nThis variation in approach is a valuable feature of the official sector focus on conduct and culture reform.  As the official sector innovates, experiments, and introduces new approaches, we’ll develop a better understanding of how to best promote a healthy culture and mitigate misconduct risk.\n\nMeasurement and Assessment\n\nI’ll now turn to the question of measurement and assessment.  As all risk managers know, you can’t manage what you can’t measure.  This is equally true in the area of misconduct risk.  At the same time, while hard data is important, I don’t believe it is sufficient to convey all of the complexities of a firm’s culture or all of the ways it can impact business outcomes.\n\nIn recent years, we have seen firms develop dashboards that seek to bring together various metrics from across their organization.  This is an important and necessary step.  Data sources that we’ve seen include internal quantitative information such as compliance and policy violations, completed trainings, audit results, and risk metrics like limit breaches, policy violations and external metrics like customer complaints, stakeholder perceptions, and client surveys.  There also are qualitative data based on employee surveys.  Some firms have begun to do more frequent pulse surveys with small cohorts of staff.  Other firms have found that long-term trends are more insightful than short term changes.\n\nThese types of data, while necessary, are not sufficient to make progress. Supplementing hard data with interviews, focus groups, dialogue, and external benchmarking can greatly enhance an organization’s ability to manage and strengthen its culture.  Measurement and metrics are best used as a starting point to spur further analysis and probing as boards and senior management seek to identify trouble spots and build strong cultures.\n\nAs we continue to see the impact of technology and big data in other parts of financial services, one interesting question is how innovation and enhanced technology will support the measurement and management of culture. The potential of big data analytics to revolutionize approaches in many areas of business has been talked about for years, and is now beginning to become a reality.  For example, we might see firms routinely leverage broader data to make stronger predictions about potential misconduct risk, which could be useful to help focus scarce compliance resources.   \n\nSome may view the official sector as an impediment to the adoption of new technologies, but I don’t think that needs to be the case.  In the context of compliance risk management in the U.S., the five federal banking agencies recently encouraged financial firms to explore innovative approaches to meet their compliance obligations related to money laundering and further strengthen the industry against illicit activity.15\n\nSimilarly, in the U.K., Rob Gruppetta, head of the Financial Crime Department at the Financial Conduct Authority, has spoken about the “balancing act” the FCA faces as a regulator trying to encourage progress and innovation while also mitigating danger and imprudent risk-taking.  He points out that data from new technologies has also allowed the FCA to be more consistent, effective, and risk-based in its supervisory approach toward conduct risk.16  This leads to a question about how firms can use similar tools and techniques to mitigate misconduct risk and promote healthy cultures. \n\nAs with any innovation and application of new technology, critical questions are emerging.  For example, reliance on complex algorithms or machine learning tools raises issues around “explainability” and a clear audit trail.  Lawyers will inevitably opine on a variety of issues, including data protection and privacy.  Beyond strictly legal issues, firms will have to wrestle with questions about fundamental fairness and the consequences that techniques perceived as unduly intrusive and untrusting may have on employee morale and industry retention.  As new technologies spread and their use grows in areas related to compliance and internal assessments, we will need to confront these issues.\n\nConclusions\n\nTo conclude, the culture of a financial firm, and of the industry as a whole, is a complex phenomenon that is not easy to measure or manage.  Collectively, however, we must not ignore this issue.\n\nLooking ahead, members of the financial industry with different roles and perspectives can each take practical steps to move this dialogue forward.  Financial firms should continue to develop and implement programs to build cultural capital, with directors, senior management, and staff each having distinct responsibilities and perspectives.  Supervisors around the globe should remain committed to staying up-to-date on industry advances while continuing to develop and implement innovative supervisory approaches.  We should continue to collaborate with the industry, the academic community, and other parts of the official sector. We should continue to monitor progress on industry benchmarking as more jurisdictions take up these efforts. And, perhaps most importantly, we should continue to engage in discussions like this one, so we work together to identify lessons learned and best practices as we pursue our common goals of a safe, sound, and stable banking system.\n\nI think we should be optimistic that with the right tools and technology, the right level of commitment, and the right people at the table, we can build cultural capital and mitigate misconduct risk. We all have a role to play in mobilizing our organizations to advance this critical work.\n\nThank you for your attention."
    },
    {
        "title": "SOFR and the Transition from LIBOR",
        "date": "Feb 26, 2019",
        "speaker": "Michael Held",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/hel190226",
        "content": "Thank you for that kind introduction, and for the opportunity to speak to you all today.  As always, my remarks reflect my own views and not necessarily those of the Federal Reserve Bank of New York or the Federal Reserve System.1\n\nWhen I was thinking about what to talk about today, I recalled how everybody’s favorite question at these kinds of events is, “What keeps you up at night?”  Of course, at the top of my own list is whether my daughter is going to get into a good kindergarten in Brooklyn.  But not far below that is reference rate reform.  Today I’ll keep the former to myself, and speak instead about reference rates.\n\nIt’s not just me who’s concerned.  The Securities Industry and Financial Markets Association (SIFMA) named reference rate transition from the London Interbank Offered Rate (LIBOR) to its alternatives as one of two leading fixed-income market developments for 2019.2   And the Financial Stability Oversight Council (FSOC) has repeatedly identified reference rate transition as a financial stability risk.3  Today I’d like to give you an update on the state of that transition.  Here’s the preview: The task is immense.  But it is not insurmountable.  Much work has been accomplished, and much work remains.  Every firm that has exposure to LIBOR needs to prepare now for the risk—indeed, the likelihood—that LIBOR will cease in the near future.  I’m optimistic that we can get there.  But, as Tim Geithner liked to say, hope is not a plan.\n\nHow Did We Get into This Fix?\n\nBefore discussing solutions, let’s review what we’re trying to solve.\n\nIt became apparent after the financial crisis that LIBOR was being manipulated.  Financial firms misstated their LIBOR submissions—often in collusion with each other—to make better returns on their swap books.  During the financial crisis, they also submitted artificially low rates to avoid signaling financial weakness.  Manipulation was possible because of the way LIBOR submissions were made.  Banks were asked to estimate the rate at which they could borrow from other banks, not rates at which they actually borrowed.  Their quotes were hypothetical—guesses if you will—and were therefore particularly easy to compromise.\n\nAt the same time, the way banks fund themselves has changed.  The unsecured London interbank market, which LIBOR was designed to measure, was active when LIBOR was created, but that just isn’t how banks finance themselves any more.  The Fed estimates that on a typical day there are currently around six to seven actual market transactions—totaling about $500 million—that could underpin one- and three-month U.S. dollar LIBOR across all of the panel banks.  For the six-month tenor, there are only two or three transactions per day.  At the one-year tenor the average is one transaction per day, and on many days there are none.4   That means that the majority of panelist submissions each day are based solely on “expert judgment.” \n\nAlthough actual transactions underlying LIBOR have diminished, its use as a benchmark has become ubiquitous.  The gross notional value of all financial products tied to U.S. dollar LIBOR is around $200 trillion—about 10 times U.S. GDP.5  That includes $3.4 trillion of business loans, $1.8 trillion of floating-rate notes and bonds, another $1.8 trillion of securitizations, and $1.3 trillion of consumer loans held by about four million individual retail consumers, including around $1.2 trillion of residential mortgage loans.  The remaining 95% of exposures are derivative contracts, which we learned in the financial crisis have consequences for both Wall Street and Main Street. \n\nSo, every day, the payments on $200 trillion of exposures are calculated based on a handful of transactions worth a few hundred million dollars at most.  That’s like a very tall, very broad building built on a very narrow foundation.  Imagine an upside-down pyramid.  It’s not stable, and, as we will discuss shortly, it’s getting more rickety by the day.  You don’t want to be standing near it when it comes down.\n\nThere is, thankfully, a coordinated global effort to address LIBOR’s shortcomings.  That work has focused on creating more reliable alternatives.  The Financial Stability Board (FSB)6 and the International Organization of Securities Commissions (IOSCO)7 have published roadmaps for reform.  Their shared principle is that benchmarks should be based on observable, arms-length transactions rather than estimates. \n\nTo meet this expectation, an FSB steering group has coordinated working groups for each of the major IBOR benchmarks.  The mandates of these groups are to identify alternative rates and to begin transitioning to them. \n\nIn 2014 the Federal Reserve convened the Alternative Reference Rates Committee (ARRC) to plan the transition away from U.S. dollar LIBOR.8   The ARRC conducted two public roundtables, published a written consultation, and created an advisory group of end users across market sectors.  The ARRC’s criteria for U.S. dollar LIBOR’s replacement included methodological quality, accountability, governance, and ease of implementation.  In the end, the ARRC recommended a new rate that the New York Fed had proposed in cooperation with the Treasury Department’s Office of Financial Research.9   The new rate is called the Secured Overnight Financing Rate (SOFR). \n\nThe New York Fed has published SOFR every day since early April of last year.10   SOFR measures the cost of overnight borrowings through repo transactions collateralized with U.S. Treasury securities, which is the deepest and most liquid money market in the U.S.  It is based on actual transactions and takes in more transactions than any other Treasury repo rate available, recently around a trillion dollars each day.  SOFR is relevant to the cost of borrowing for a wide array of market participants, was constructed to meet the best practices for benchmarks set out by IOSCO, and is built to accommodate future market evolution.\n\nI want to pause here to acknowledge that, as an overnight rate, SOFR has been the subject of criticism for sometimes being volatile from day to day, especially near quarter- and year-end.  I think such criticism is misplaced.  Most users of SOFR don’t use a single day’s rate to determine their payments; they use average rates, which are not much affected by daily volatility.\n\nParallel work has been going on in other jurisdictions whose currencies are used in major benchmarks.  Working groups for sterling, euro, Swiss franc and yen IBORs11 have all endorsed overnight rates as their preferred alternatives to those rates.12   Three of the alternative rates are (or will be) administered directly by central banks, reflecting the recognition that crucial interest rate benchmarks are a public good, and their integrity and sustainability are key to financial stability.\n\nI’ll note that, at the outset of these reform efforts, the idea was not to completely replace LIBOR.  LIBOR was being reformed to address its weaknesses as far as possible in the expectation that it could continue to be used.  Efforts to “fix” LIBOR proceeded alongside the program to identify better alternatives.  The idea was that if there were a more attractive alternative, firms would gradually move away from LIBOR of their own accord, with a minimum of disruption, and that LIBOR then would be seldom, if ever, used.  LIBOR could still be inherently unstable despite the reform efforts.  Perhaps it would go away eventually, maybe dying of loneliness, or perhaps it would stagger on for a small range of transactions—but in any event it would cease to be a problem.\n\nWhile this work was underway, new regulations brought the production of LIBOR under the supervision of the UK Financial Conduct Authority (FCA) and introduced minimum requirements for benchmarks used in the EU.  In 2017, Andrew Bailey, the head of the FCA, spotlighted the possibility that LIBOR itself might cease to exist altogether.13  Bailey announced that the FCA had been exerting an increasing amount of persuasion to get panel banks to keep submitting quotes, and had gotten their commitment to continue until the end of 2021.  But he warned that the FCA could not ensure that LIBOR would be published at all after that.\n\nSuddenly, instead of just the general threat that LIBOR could cease to be produced someday, there was a specific expiration date on the horizon.  The challenge to the market was no longer to gradually start writing new contracts on alternative rates like SOFR instead of LIBOR, but to prepare for the risk that LIBOR could entirely cease to exist within a fairly short time frame.  That was a much bigger job, because when you looked at the underlying contracts that used LIBOR, they didn’t provide very well for LIBOR simply disappearing.  I blame the lawyers, and our congenital lack of imagination.  Even when financial contracts deal with the possibility that a LIBOR quote might not be available, they seem to have been drafted assuming that the gap is temporary, not permanent.  In a permanent cessation of LIBOR, the fallback solutions in existing contracts become impractical or materially change the economics.  That’s a situation that invites litigation, and in the case of LIBOR that litigation would be on a massive scale.  So while this prospect might not be unwelcome to certain members of the litigation bar, this is not good news for those interested in maintaining financial stability. \n\nFor example, many contracts state that if a calculation agent cannot find a LIBOR quote for a given day on the usual Reuters screen, it will call three large London banks and ask them what their borrowing rate is for that day—basically try to privately replicate LIBOR.  Imagine if every calculation agent for every transaction tried to do this every time they had to set LIBOR.  There would be chaos, especially when those London banks had already decided to get out of the LIBOR-estimating business.  Why would they even respond?\n\nFor some types of transactions—floating-rate notes, for example—there is a further fallback: to fix the interest rate at whatever the last LIBOR quote was.  This is not a very satisfactory solution to either the issuers or the borrowers who thought they had an instrument that protected them against interest rate risk.  Other transactions like syndicated loans may revert to prime rate loans—again, not what borrowers may want.  And, for derivatives, there simply may be no further fallback.  You can imagine the litigation risk when the reference rate for a 20-year contract disappears and there’s no clear path to replace it. Now imagine 190 trillion dollars’ worth of those contracts.  This is a DEFCON 1 litigation event if I’ve ever seen one.\n\nWhere Are We Now?\n\nThose are the issues that the ARRC, the other IBOR working groups, and central banks in general have confronted over the last few years.  Let me turn now to the work at hand.\n\nThere are two urgent tasks for every market participant with LIBOR exposure.  First, if you find yourself in a hole, stop digging.  What market participants must do right now is stop writing new contracts on LIBOR and start using SOFR or at least another robust alternative.  And, especially if you need to keep using LIBOR, make sure your new contracts have strong and workable fallback language.  That was one of the original recommendations the various official sector groups made as far back as 2013, but the people writing the contracts don’t seem to have been paying attention.  As I’ll describe in a moment, very soon there will be specific fallback language that represents the consensus best practice to use in each type of U.S. dollar LIBOR-linked financial product going forward.\n\nThe second urgent task is to deal with the trillions of dollars of existing contracts that extend past 2021 and don’t have effective fallbacks.  That’s a more serious problem for some types of instruments than others.  The large majority of business loans in existence today will mature or be renegotiated before the end of 2021 in the ordinary course.  So the loans can be converted entirely to SOFR loans before the clock runs out on LIBOR, or at least fallback provisions can be added.  There’s also good news on derivatives, relatively speaking.  A large percentage of derivative contracts are also shorter-term.  Over-the-counter derivatives that do extend past 2021 can be amended to incorporate new rates and fallback provisions through a protocol procedure that the International Swaps and Derivatives Association (ISDA) will put in place.  That’s still a lot of work for everyone who has LIBOR derivatives, but it’s a relatively clear way forward.  And for exchange-traded derivatives, the exchanges themselves can specify fallbacks through rulebook amendments.  The exchanges have indicated they will adopt the same basic fallback methodologies as ISDA.  This is all good news.\n\nConsumer loans present different issues.  The documentation generally gives the lender discretion to unilaterally choose a comparable rate if LIBOR goes away.  That sounds simple.  But, in practice, the knot of reputational, operational, and legal considerations involved in changing the interest rate basis on consumer loans will require attention and resources to unravel.  And over 40% of LIBOR-based residential mortgage loans currently outstanding extend past 2021.\n\nOne of the more difficult challenges is the one posed by floating-rate notes, securitizations, and preferred stock whose payments are tied to LIBOR.  These securities either have no fallbacks at all to handle a LIBOR cessation, or they effectively become fixed-rate instruments.  And, in practice, it is very difficult, if not impossible, to add the kind of provisions that will be standard for new issuances going forward.\n\nA year after Andrew Bailey’s 2017 speech, he reiterated that firms should treat the discontinuation of LIBOR as an event that will happen and that they should be preparing for it.14   For my part, that’s also the message I would ask you to take away today.  You can think of this as a call to arms or as a threat or as simply a warning.  What’s important is that you act on it.  To tackle the issues a LIBOR cessation presents for cash products, the Fed reconstituted the ARRC in 2018 to broaden its membership and its mandate.  “ARRC 2.0” now has dedicated workgroups for floating-rate notes, syndicated and bilateral business loans, consumer products, and securitizations.  Those workgroups include market participants that weren’t all members of “ARRC 1.0” but are active in their specific sectors.  There are also working groups dealing with legal, regulatory, and accounting issues.  Thanks to these working groups, for the first time market participants and their lawyers now have not just general concerns and warnings, but specific actionable language to react to and start to use in contracts across different products.  Again, this is good news.\n\nThe ARRC’s recommendations are voluntary, and the Federal Reserve is not mandating what fallbacks to use.  But there’s a lot of value in a common approach.  If LIBOR ceases and different instruments that use it fall back to different rates or at different times, basis risk will be higher and hedging more difficult.  So the ARRC started its fallback work by adopting a set of guiding principles to apply across all product types.15   Among other things, those guidelines suggested that, to the extent practicable and appropriate, market participants should (1) maintain consistency of fallbacks across asset classes and minimize basis risk between products; (2) use SOFR or a benchmark based on SOFR as the replacement rate; (3) minimize value transfer over the life of the contract if the fallback is triggered; and (4) include specific triggers that activate the fallback.  \n\nFollowing these overall principles, the ARRC commenced consultations on specific fallbacks for four types of cash products: floating-rate notes,16 syndicated business loans,17 bilateral business loans,18 and securitizations.19   In general, the consultations proposed that following a trigger event—such as LIBOR being discontinued—the instrument would instead pay interest at SOFR or a rate based on SOFR, adjusted so the new rate is comparable to the old one.  ARRC’s consultations recognized that different solutions may be necessary or preferable for different markets and products, but promoted common approaches as much as possible. \n\nThe comment periods for the consultations have closed, and the ARRC is now reviewing the feedback.  Final recommendations are expected to be published very soon.\n\nYou may have noticed that I didn’t mention an ARRC consultation on fallbacks for derivatives, even though ARRC’s original purpose was to address benchmark transition in derivative products.  That’s because the FSB’s Official Sector Steering Group asked ISDA to do that for derivatives on all of the IBORS.  So, alongside the ARRC consultations for cash products, ISDA has been conducting consultations on fallback provisions for IBORS in multiple currencies,20 and has said it will be putting out a U.S. dollar LIBOR consultation in the near future.  One of the key questions being discussed is how closely aligned the cash products fallbacks and the derivatives fallbacks will be.\n\nThe ARRC has also been an essential vehicle for engaging the official sector to address regulatory obstacles to LIBOR transition.  For example, replacing fallback language in legacy, uncleared swaps presents a host of issues under the swap margin rules.  ARRC’s regulatory working group is actively engaged with the Commodity Futures Trading Commission, the Federal Reserve, and other regulators to address those.  On the accounting side, the Financial Accounting Standards Board has ruled that SOFR is eligible for hedge accounting treatment.\n\nWhile all the consulting and coordinating and kvetching has been going on, market participants have gone ahead and begun the transition to actually using SOFR.  SOFR swaps are being quoted by dealers and cleared by central counterparties.  CME Group and LCH are clearing SOFR-linked swaps and are moving quickly to using SOFR for discounting curves and interest on collateral. CME Group and Intercontinental Exchange have listed SOFR-linked futures. Over $56 billion in floating-rate financing based on SOFR has been issued in all sectors of the debt markets.21   And the Treasury Borrowing Advisory Committee has discussed the possibility of issuing SOFR-linked floating-rate notes.\n\nThese are all reasons for optimism.  But I’m a realist—some would say a pessimist.  It is not yet time to chill the champagne.  As I said at the beginning of my remarks, much work remains.\n\nLife After LIBOR and How We Get There\n\nLooking forward, the risks from LIBOR transition are both macro- and micro-prudential—a risk to the system is always built up from risks to individual firms.  On the macro scale, as I mentioned earlier, the FSOC has identified reference rate transition as a financial stability risk.  The President of the New York Fed, John Williams, was just appointed co-chair of the FSB’s steering group.  He succeeds Fed Chairman Jerome Powell in that role.  From the beginning, the Fed’s mantra has been that the transition away from LIBOR has to be led by the private sector.  But the Fed clearly has a role to play.  The New York Fed will continue to be engaged through our work with the ARRC, our coordination with the other international transition efforts, and our role as administrator of our own benchmark rates.\n\nAt the micro level, sound risk management demands that every firm assess its vulnerabilities if LIBOR ceases to be published, and engage in appropriate planning to address those risks across its balance sheet.  Firms must have a safe and sound process for addressing all aspects of this transition.  They also need to select alternative rates that are robust and resilient, in line with current best practice.  At the Fed, we’ve been thinking about this risk across all portfolios, from small community banks to the largest institutions.  Our focus so far has been on outreach and education.  We have been encouraging examiners to raise awareness with their supervised institutions, and we’ve provided them with some tools to enage in this discussion.  We’ve also been reviewing our own contracts that use reference rates to make sure we’re walking the talk.\n\nAnecdotally, the largest financial firms seem to be more aware of the challenges than smaller ones.  Last September, the UK Prudential Regulation Authority and FCA sent a “dear CEO” letter to the heads of large banks and insurance companies to seek assurance that their senior managers and boards understood the risks of LIBOR transition and were taking appropriate action ahead of end-2021.22   As the larger firms prepare for the transition, they are beginning to work on the operational challenges the transition will present.  You all know that those kinds of system changes are never simple or straightforward.  The potential death of LIBOR is causing everyone in the market a lot of grief, and everyone goes through the five stages of grief at their own pace.  But the fact that firms are starting to grapple with implementation means that they’ve got past denial and bargaining and settled down to acceptance.\n\nSmaller firms may be lower down on the preparation curve, although presumably their total risks are also smaller.  In December, the Federal Financial Institutions Examination Council presented a one-hour webinar on LIBOR transition that drew about 1,300 participants from supervised firms.  Of those who responded to a polling question during the webinar, about one-fifth said it was the first time they were hearing about the topic.\n\nTo continue forward, several issues must be resolved.  For example, what events should trigger the shift from LIBOR to SOFR or another alternative rate?  A definitive end to LIBOR is an obvious trigger.  But, short of that, what events should prompt the switch?  What if LIBOR was nominally available, but the UK FCA used its supervisory power to declare that LIBOR is unrepresentative of the underlying market?  Last month, an FCA official highlighted the possibility that this is exactly how the end of LIBOR might come about, and urged market participants to seriously consider this possibility when crafting their fallback triggers.23   In their various consultations on fallback language, the ARRC has proposed that such a declaration by the FCA would trigger the switch to an alternative rate.  A clear majority of respondents supported that proposal.  I think every market participant should ask how the fallback triggers in their contracts would respond. \n\nIn addition, the precise mechanism of changing from LIBOR to, say, SOFR will vary.  The solution is not a simple substitution.  There are inherent differences between SOFR and LIBOR that will need to be adjusted for in the transition.  For one thing, most contracts that reference LIBOR key off of three-month or six-month LIBOR.  Mortgages often use one-year LIBOR.  But SOFR is an overnight rate—there’s no such thing as three-month or six-month SOFR.\n\nOne solution to this difference is to use the daily SOFR for each day during the term, either averaged or compounded each day until the end of the period.  ISDA has indicated that derivatives will fall back to this kind of average, and the SOFR floating rate notes issued recently use a similar approach.  To facilitate this method, the New York Fed is preparing to produce a backward-looking compounded average alongside the daily SOFR. \n\nSome market participants may need to use a forward-looking “term SOFR” created by constructing a yield curve off of trading in forward-looking SOFR derivatives, particularly for legacy contracts that already reference a term LIBOR rate.  Creating a new rate like that is part of the ARRC’s transition plan, although it’s more concept than reality at this point.  But under all of the cash product consultations put out to date, the first preferred fallback is to that kind of term rate, if such a rate were to be created that was up to IOSCO standards and endorsed by the ARRC.\n\nAnother difference between SOFR and LIBOR is that SOFR is a “near risk-free rate” because the underlying repo transactions are secured by Treasuries.  LIBOR, on the other hand, was based on unsecured transactions and was intended to include the price of bank funding risk.  Although most of the current uses of LIBOR have no actual need to reflect bank funding risk, that difference does still affect the level of the index, and so will need to be adjusted for in contracts that start out referencing LIBOR and then switch to SOFR.\n\nThere are various technical ways to adjust for both the term difference and the credit difference between SOFR and LIBOR.  The goal is to create a value-neutral adjustment mechanism that doesn’t create winners and losers as LIBOR contracts reset.  Ideally, the adjustments would also be easily understood, transparent, and based on objective factors, not on discretion.  That’s a very tall order.  This is another area where the ongoing consultations are very important.\n\nConclusion\n\nChange is hard.  It’s hard to say goodbye to LIBOR when it’s been around for so long and is embedded in so many operations and processes that have been in place for decades.  For the last several years, change has clearly been coming to the use of LIBOR, but there was a fog of uncertainty about what that change was going to look like, and so the reaction of many firms seemed to be to do nothing except wait and see what happens.  Now the time has come—I would argue it came a while ago, but in any case it’s here now—the time has come for every firm with LIBOR exposure to actively grapple with the risks that are coming your way whether you like it or not.  The way forward for the vast majority of LIBOR-based instruments is rapidly becoming clear.  It’s easy and tempting to justify inaction by criticizing the available solutions as imperfect or as just too hard to do.  But inaction at this point in the LIBOR transition is short-sighted and futile and only extends the uncertainty.  We need decisive action by everyone in the market to avoid damage to individual firms and the financial system."
    },
    {
        "title": "Models Only Get You So Far",
        "date": "Feb 22, 2019",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/pot190222",
        "content": "It is a pleasure to deliver the lunchtime address during the Atlanta and New York Fed's first annual research day on quantitative tools for monitoring macroeconomic and financial conditions.1 My remarks will focus on some insights from the book Superforecasting by Philip Tetlock and Dan Gardner, with respect to how we interpret, use, and evaluate the results from our quantitative tools.2 If you have read the book, subtitled \"The Art and Science of Prediction,\" you will probably be aware that it is much less mathematical than the discussions we have been having today, but at the same time is more complex in how it articulates the role of forecasting in good decision-making. Before I continue, I should note that these remarks reflect my own personal views and not necessarily those of the New York Fed or the Federal Reserve System.\n\nThe insights in Superforecasting grew out of a forecasting tournament sponsored by the Intelligence Advanced Research Projects Activity (IARPA). In the wake of the controversy surrounding the intelligence community's assessment of the existence of weapons of mass destruction in Iraq, IARPA set out to enhance the accuracy, precision, and timeliness of intelligence forecasts.3 The Federal Reserve and the broader economics and financial community have, of course, had to face similarly humbling shortcomings in our failure to forecast the Great Recession, despite the numerous signals that were available to forecasters and policymakers prior to 2008. On this issue, it is useful to recall a quote from an unusual source on economic forecasting, the Queen of the United Kingdom, who in November 2008 asked, \"Why did nobody notice it?\" We should not treat this as a rhetorical question. What are some of the underlying reasons individuals and organizations fail to predict? What should we change about our mindsets and practices to improve the chances that we \"notice it\" next time, whenever that may be?\n\nAt the New York Fed, we've made investments in response to these critical questions. We've created a team that is raising awareness of the challenges that make it so difficult to \"notice,\" innovating on how we approach analysis and decision-making, and making these approaches an essential part of what it means to work at the Bank. This is much in the spirit of what IARPA sought to do by sponsoring their tournament.\n\nTo compete in the IARPA tournament, Tetlock recruited participants online from outside the intelligence community. A small number of these participants quickly separated themselves from the pack and consistently maintained their exceptional performance. Tetlock dubbed these individuals \"superforecasters\" and subsequently focused his team on aggregating the forecasts of these standout performers—a strategy that draws from the \"wisdom of the crowd.\" As you well know, this strategy relies on some degree of independence in the information, approaches, and insights producing the forecasts. We often think of independence in a very statistical sense, but Superforecasting conveys a useful framing of independence as how one assembles diverse teams and how team dynamics can be established to maintain this diversity.4 This approach won the IARPA tournament by a substantial margin with the use of one subtle but important tweak. Tetlock extremized the aggregated prediction probabilities—so, for example, a probability of 70 percent became 85 percent.\n\nSo what can we learn from these non-technical superforecasters who outperformed the intelligence community? Tetlock lists a number of characteristics that generate individual diversity and maintain that diversity in a team dynamic. My quick summary of these lessons is to be humble, always question, listen to alternative views, and—very comfortingly for Bayesians like me—always express your forecast as a distribution rather than a point forecast, and crucially update that forecast when new information arrives. Further, constantly assess why forecasts worked and didn't work.\n\nIn contrast, the book is full of examples of forecasters and pundits whom Philip Tetlock refers to as hedgehogs; knowing one central truth, they take data as either affirming their views or, if not, discarding the data as unreliable, irrelevant, or uninteresting. The impact of the Great Recession has been helpful in removing some of this type of hubris, but memories are short, and Tetlock's work shows that making lasting improvements in processes that produce good judgment and decision-making requires structure and constant practice.\n\nMuch of Tetlock's message is around the importance of quantifying statements. He uses a statement by former Microsoft CEO Steve Ballmer as a prime example of a statement that requires quantification: In 2007, Ballmer said, \"There is no chance that the iPhone is going to get any significant market share. No chance.\" Depending how we quantify \"no chance\" and \"significant market share,\" Ballmer was either accurate or inaccurate. Tetlock argues convincingly that quantifying statements in a manner that allows accuracy to be objectively evaluated is a critical part of good prediction. This is clearly part of the science of prediction, and I would argue not an issue for the sophisticated tools we use to monitor and forecast in central banks.5 However, our tools are so sophisticated and so cutting-edge (as the papers presented today illustrate) that they can become their own form of hedgehog. At the recent Brookings-Yale conference looking back 10 years after the crisis, both former Federal Reserve Chairman Bernanke and Vice Chair Kohn expressed the view that too much weight was given to the predictions of the Fed's workhorse model FRB/US.6\n\nSome of you might observe that model uncertainty is a well-developed field, and certainly staff of the Federal Reserve Board and the Reserve Banks all appreciate the importance of using a diverse set of models with different transmission mechanisms, estimation approaches, and solution methods. Further, the Great Recession has generated considerable interest in approaches that allow for nonlinear reactions and fat tails, as Marco Del Negro will explore this afternoon. While I am incredibly sympathetic to the idea that more sophisticated modeling can improve judgment and decision-making—especially as I spent much of my career working on such technical issues—this approach has limits. If one had to summarize the message of the Superforecasting book in one line, it would be the well-known quip, \"Forecasting is hard, especially about the future.\"7\n\nTo illustrate some of the learnings from the book, I invite you to consider a standard problem: What is the probability the U.S. economy will be in a recession over some period of time? This is similar to the focus on binary outcomes in the IARPA tournaments.\n\nOne simple, wisdom-of-the-crowd-based approach to this question would be to poll you all right now given a specific time frame. Rather than disturb your lunch, we can instead consider using the wisdom of the market. Given my role in the Markets Group at the New York Fed, I'd like to focus on what financial market participants' beliefs and market prices can tell us about this question and what we should do with the answers from them.8 Let's first consider asking market participants directly and then use observed asset prices to answer the question.\n\nPrior to the January 2019 FOMC meeting, as part of its routine policy expectations survey, the New York Fed asked market participants to report what percent chance they attach to the U.S. economy currently being in an NBER recession, and what percent chance they attach to the U.S. economy being in an NBER recession in six months.9 We have been asking survey recipients these questions for over 10 years. As shown in Figure 1, the median respondent assigned a 2 percent probability to the U.S. currently being in a recession and a 12 percent probability to the U.S. being in a recession in six months, with a fairly small degree of dispersion around these predictions.\n\nSeparately, we posed a newer and more complicated question, asking respondents to provide a probability distribution of the timing at which they believe the U.S. economy will first enter a recession. The bubble chart shown in Figure 2 illustrates the range of results, with the relative size of each bubble indicating the relative number of responses for each probability. Perhaps not surprisingly, responses were much more dispersed. Respondents placed the highest probability on a U.S. recession first occurring in 2020 or 2021, assigning a roughly 25 percent probability to each, on average. At about 17 percent, the distribution's average probability of a recession starting in 2019 or earlier suggests a good match with the shorter-horizon question. Interestingly, our respondents demonstrated an ability to calibrate their probability assessments over a longer horizon, something some of Tetlock's superforecasters struggled with.\n\nHowever, historical perspective may offer a cautionary tale. Figure 3 shows the evolution of results from our survey's recurring questions on the probability that the U.S. economy is currently in an NBER recession and that it will be in an NBER recession in six months, taken over the mid-2007 to late-2008 time period. This period, of course, encompasses the start of the Great Recession, which the NBER dated as beginning in December 2007. Although the probabilities in our survey responses were starting to trend upward in the second half of 2007, they remained well below 50 percent.10 You might wonder why we ask these two questions. We ask these two questions so that when the current probability of a recession is high, the probability of a recession in six months' time allows us to calculate a probability of switching to an expansion. For example, by December 2008 nearly all our respondents were sure the U.S. economy was in a recession; thus, the 70 percent probability of being in a recession in six months can be interpreted as a 30 percent probability of an expansion in June 2009. The NBER subsequently declared July 2009 as the turning point into an expansion.\n\nAnother approach to predicting recessions, which is a form of the wisdom of the crowd, is to use financial market asset prices. The well-known argument is that because large amounts of money are on the line, this different form of aggregation of diverse views will tend to be more accurate. Of course, asset prices also include compensation for risk, and much of modern asset price theory is focused on how fluctuations in this compensation drive much of the asset market volatility we observe. In the technical language of modern asset price theory, the probabilities from our surveys are physical ones, whereas those that use asset prices without adjustment for risk aversion are risk neutral probabilities.\n\nThe classic variable to use here is the term spread, the difference between a long-term interest rate and short-term interest rate. The seminal paper showing the power of the term spread to predict recessions, measured as the 10-year Treasury rate less the three-month rate is by Arturo Estrella and Rick Mishkin. The paper, written while they were both working in the New York Fed's Research Group in the mid-1990s, formed the basis of many internal discussions in 1999-2000 and 2006-2007 as the U.S. yield curve inverted.11 Much of the internal discussion was around trying to understand whether the properties of the yield curve were different. The current estimate from this model is a 23.6 percent chance that the U.S. economy will be in an NBER-defined recession in January 2020.\n\nJust like Tetlock, Arturo liked to extremize the results of the model, so for example a probability of at or above 30 percent is moved to 100 percent and below 30 percent is moved to zero. One reason to extremize the results is that the prediction object is a recession in one particular month, rather than the probability of a recession over a period of time. Marcelle Chauvet and I analyzed this more general problem.12 Our goal was to avoid the problem with the probit model that a constant forecast of, for example, a 25 percent probability that the economy will be in recession in 12 months' time implying after one year of such monthly predictions a 97 percent probability the economy would be in a recession in at least one month of the 12. This is similar to the issue I mentioned earlier, where some of the superforecasters appear to have had difficulty calibrating their probability assessments over varying time horizons. Such a poor calibration would run afoul of the Brier-based scoring system used in the IARPA tournament.13 It is clear that extremizing the results lessens the impact of the poor calibration on the scoring system, but it leaves open the question of whether a better calibration is possible.\n\nAnother financial market indicator we could look at is the stock market. As Paul Samuelson famously stated, the stock market has predicted nine of the last five recessions. An alternative statement—and this is also true of yield curve inversions—is that no U.S. recession has occurred since the Second World War without a yield curve inversion or stock market correction (or worse) proceeding it. As a recent San Francisco Fed Economic Letter carefully illustrated, yield curve inversions have the additional record of indicating only one false positive in the U.S., during the mid-1960s.14\n\nIf we cared equally about forecasting recessions and expansions, these properties would be well-reflected in the Brier score; however, if we view recessions—or more accurately, elevated probabilities of a recession—as more concerning than elevated probabilities of an expansion, then Brier scoring is not sufficient. As Tetlock writes, \"One problem is that Brier scores treat false alarms the same as misses.\" The issue of asymmetric loss functions over outcomes is well understood by economists, and more recent research around robustness to ambiguity in probability assessments provides more formal methods of \"extremizing\" probabilities based on loss functions and the level of ambiguity.15\n\nAnother issue Tetlock raises is that prediction of binary outcomes of clearly stated problems, while useful for tracking forecast accuracy and learning good technique, leaves a lot of information to still be determined. For the question I posed on the probability of a recession, we would also want to have forecasts of how moderate or deep the recession would be if it occurred. This is particularly true in the current environment of low natural rates of interest, where recessions are more likely to be associated with trips to the zero lower bound (ZLB) and the ability to further reduce the policy rate is constrained.16\n\nA topic beyond the scope of Superforecasting—but very relevant for the forecasting of recessions—is that policy decisions given the probability assessment can affect the actual outcome. This relationship suggests that extremizing results is much more useful for forecast tournaments than in every day practice for central bankers. Further, economic outcomes can be partly determined by multiple equilibria or animal spirits, which themselves can be influenced by current decisions and communications. For example, we have heard from market contacts that FOMC participants' recent discussions of the link between yield curve inversion and recessions could encourage some economic agents to believe a recession will occur if the yield curve does invert. This brings me to the fundamental question of why a particular pattern of asset price moves tends to precede recessions. Let's suppose for now we have estimated and evaluated the best possible model to incorporate the statistical implications of these asset price patterns on recession probabilities and of the depth of a recession based on the available data. Is the model showing a causal relationship or is it just a predictive relationship with a different causal factor driving both asset prices and the real economy? And if the latter, what are the causal factors?\n\nThis is a different question than the one we were debating in our internal discussions in 1999-2000 and 2006-2007. We spent much of that time arguing about whether structural change, particularly lower compensation for interest rate risk, meant that asset price indicators were less relevant than before. An alternative approach would be to ask, if the probability of recession is elevated, what might be causing this phenomenon? And then ask the complementary question: If the probability of recession is unusually low, what might be causing that phenomenon? In this latter case, given the asymmetry in loss between elevated recession probabilities and elevated expansion probabilities, a very helpful follow-up question would have been whether beliefs in central banks contributing to the economic stability of the Great Moderation had made us too complacent about recession risks.\n\nTetlock emphasizes the importance of post-mortems once outcomes are known. In particular, they allow one to assess whether a forecast turned out to be accurate because of luck or because of some mechanism articulated in advance. This is critical to learning in a complex environment where feedback may be ambiguous or hidden. Without active assessment and reassessment, we're prone to hindsight bias and the belief that we will surely see it coming next time.\n\nIn the post-mortem I wrote about the failure to forecast the Great Recession, the focus was on the phenomenon and links that we failed to see or were to slow to realize the power of.17 There was nothing on the debate about whether the signal from the yield curve was misleading due to low term premia because ex-post, given the depth of the Great Recession, this debate seems a little silly. So then the question we should be asking ourselves is how could we have arrived at a clearer focus on the relevant forces before the Great Recession? What structures, processes, and behaviors should be in place to help us question our assumptions?\n\nInstitutionalizing such practices thus remains an essential challenge for our ability to monitor and forecast economic and financial conditions, and to explain their possible implications to policymakers. I'd therefore like to return to the investments we've made at the New York Fed through the Applied Critical Thinking (ACT) Function, led by my colleague Meg McConnell. ACT is dedicated to promoting awareness of how bias and complexity impede a central bank's ability to achieve its objectives, and to establishing alternative mindsets and methods to confront these challenges. One of ACT's key initiatives, inspired by the learnings in Superforecasting, is implementing a tournament at the New York Fed with the goal of giving Bank staff the opportunity to assess their use of the practices for which Tetlock advocates.\n\nIn conclusion, models and quantitative tools, like the ones we are discussing, can take us a long way. But they are designed to answer specific questions that are at best strong building blocks to the broader questions that policymakers grapple with. Being rigorous in our evaluation of these building blocks is critical, as is openness to other forms of information and approaches. The insights from Superforecasting are relevant for both the evaluation of specific tools and asking questions not addressed by these tools. Tetlock ends the book with Ten Commandments for good forecasting. However, in the spirit of being humble around forecasting problems, he adds an eleventh commandment: \"Don't treat commandments as commandments.\" A version of this I would like to leave you with is to not treat any particular model or quantitative tool as the whole truth; no matter how sophisticated the underlying technical details may be, keep questioning assumptions about how the economy works and be always open to contradictory information, not just confirming evidence.\n\nAs a closing example of the mindset underlying this philosophy I am going to quote Warren Buffett quoting Meg McConnell: 18\n\nFigures"
    },
    {
        "title": "Discussion of 'Prospects for Inflation in a High Pressure Economy: Is the Phillips Curve Dead or Is It Just Hibernating?' by Peter Hooper, Frederic S. Mishkin, and Amir Sufi",
        "date": "Feb 22, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil190222",
        "content": "The Phillips curve is the connective tissue between the Federal Reserve’s dual mandate goals of maximum employment and price stability. Despite regular declarations of its demise, the Phillips curve has endured. It is useful, both as an empirical basis for forecasting and for monetary policy analysis.\n\nThe paper by Peter Hooper, Rick Mishkin, and Amir Sufi—HMS for short—provides an in-depth analysis of key aspects of inflation dynamics in the United States and how they may have changed. This study is particularly timely against the backdrop of an emerging consensus that the Phillips curve has become nearly flat. If this is true it would fundamentally alter the employment-inflation trade-offs the Fed confronts.\n\nThere is a lot to like about this paper: It synthesizes existing research, new analysis, and bridges the study of wages and prices. Too often these are studied in isolation. But most significant, as someone who has discussed papers at the USMPF that reached the century mark in page count, this year’s paper came in at a relatively svelte 65 pages. I hope this represents a durable structural shift, and isn’t merely a reflection of the fact that this year’s paper has three authors rather than the usual four.\n\nThe paper covers a lot of ground, so I will only summarize a few key takeaways. First, HMS find evidence of asymmetric effects of unemployment on both price and wage inflation, with very low rates of unemployment leading to disproportionately large movements in inflation. That is, the Phillips curve is very much alive in very tight labor markets. Second, they find that the dynamics of price inflation have changed in important ways; specifically, it has become less sensitive to the business cycle and less persistent. Finally, they draw the policy conclusion that Fed policymakers should not be complacent about the possibility of a resurgence of inflation in the context of a tight labor market.\n\nIn my remarks, I will discuss two issues. First, that price inflation dynamics have changed significantly in recent decades. I will argue that a number of structural factors have contributed to the appearance of a flattening of the Phillips curve and that a modified formulation that accounts for these changes displays little sign of the Phillips curve flattening. Second, I will turn to the issue of anchoring of inflation expectations and the need for the Fed to be vigilant in maintaining the anchor in the right place. Before I continue, I’ll give the standard Fed disclaimer that the views I express are mine alone and do not necessarily reflect those of the Federal Open Market Committee or anyone else in the Federal Reserve System.\n\nHas the Phillips Curve Changed, or Is it a Trick of the Data?\n\nThroughout the paper, HMS use a reduced-form Phillips curve regression as a basis for their analysis. For my remarks, I simplify the specification and assume the inflation rate depends on the unemployment gap (and variants thereof), four lags of inflation, and a constant (relative to HMS, I have dropped the import price and inflation expectations terms).\n\nOverall, the results for the core personal consumption expenditures (PCE) price index inflation using this simplified specification are quite similar to those reported in HMS. The full-sample (1961-2017) estimated coefficients using the core PCE price inflation rate are reported in the first column of the upper part of Table 1. The second column reports results corresponding to the “spline” specification of HMS, where positive values of the unemployment gap are added to the equation to capture asymmetry in the response of inflation to labor market tightness versus slack. The third and fourth columns of Table 1 report the corresponding estimation results for the subsample of 1961-1987, and the final two columns report the results for the subsample 1988-2017. Like HMS, the latter sample shows far less intrinsic persistence in inflation and a severe flattening of the Phillips curve relative to the full-sample results.\n\nInterestingly, I find evidence in the early (pre-1988) sample that suggests that inflation was, if anything, more sensitive to the unemployment gap when unemployment was high, rather than low. Although these estimates are imprecise, this pattern is the opposite of the claim by HMS, which relies on full-sample estimates to draw conclusions. This also runs counter to HMS’s historical narrative about the 1960s and the dangers of a red-hot labor market. I will not dwell on this further, but it is a reminder that findings of nonlinearities in Phillips curves are often not robust.\n\nThe apparent breakdown in this simple price Phillips curve in the past 30 years reflects a number of structural changes in the U.S. economy. The Federal Reserve’s success in re-anchoring inflation expectations at a low level can explain the decline in inflation persistence seen in the data.1 However, the role of well-anchored expectations in flattening the Phillips curve is not obvious, and as HMS note, this flattening is not as clear in the wage inflation equations. This suggests other forces are at work.\n\nSeveral usual suspects come to mind, including the effects of supply shocks, increased globalization of goods markets and supply chains, and changes in the market structure for consumer goods. For example, Alberto Cavallo has shown how Amazon and other online retailers have fundamentally changed price dynamics for consumer goods, with prices changing more rapidly and more affected by exchange rate changes.2 In today’s globalized and rapidly changing economy, standard reduced-form Phillips curve models that focus primarily on the demand-driven movements in inflation are less relevant. Other factors besides the state of the business cycle dominate price movements for a wide variety of goods.\n\nCSI to the Rescue\n\nIn theory, one could try to modify the model to account of these factors, but for the present purpose, a more straightforward approach is to focus on prices for categories of consumer spending that are less susceptible to supply shocks, international trade, and changing market structure. This approach follows in the tradition of “core” measures of inflation that remove the categories that are particularly prone to supply shocks.3 A number of related measures have been developed along these lines, including work at both the New York and San Francisco Feds.4\n\nIn my remarks I will use the “Cyclically Sensitive Inflation” measure, or CSI, developed by Jim Stock and Mark Watson.5 By design, CSI includes the categories that are cyclically sensitive and less prone to measurement error or other non-cyclical drivers. In practice, this measure more heavily weights services with well-measured market-based prices and significantly down-weights goods. Figure 1 compares the four-quarter moving averages of CSI to that of core PCE price inflation, with the shaded regions indicating NBER-dated recessions.\n\nRegression results using CSI display neither a flattening of the short-run Phillips curve nor signs of nonlinearities. The lower part of Table 1 reports the estimation results using CSI inflation. Starting with the linear specification, the full-sample and early-sample results are very similar to those using core PCE price inflation. This is not altogether surprising since the two series track each other closely during the first half of the sample. The big difference is in the later sub-sample, where the coefficient on the unemployment gap is not that much smaller than in the early sample, and larger than in the full-sample estimation. Moreover, there is little evidence of nonlinearity in the Phillips curve relationship in either the full or later samples. Evidently, both the flattening of the Phillips curve and evidence of nonlinearities depend on including goods and other categories that are primarily influenced by non-cyclical factors.\n\nThese results suggests that the Phillips curve is alive and kicking when inflation is measured using categories that are cyclically sensitive, rather than buffeted by supply and other shocks. Based on this analysis, the trade-offs between employment and inflation the Fed faces haven’t fundamentally changed.\n\nThat does not imply there have not been meaningful changes in inflation dynamics—inflation persistence has fallen significantly for the CSI as well. This means that transitory shocks no longer get embedded in inflation expectations, but instead have relatively short-lived effects on inflation. This is a very favorable development for the economy and monetary policy, which can safely “look through” transitory shocks.\n\nAnchoring Inflation Expectations at the Target\n\nThis brings me to my second point of the importance of anchoring inflation expectations at the target level. As HMS correctly stress, policymakers cannot take for granted that inflation expectations will remain well anchored. They highlight the risk that very tight labor markets could eventually lead to a resurgence of inflation and unmoor expectations, as in the 1960s. I concur that we must remain vigilant regarding a sustained takeoff in inflation.\n\nWe must be equally vigilant that inflation expectations do not get anchored at too low a level. So far during this expansion, core and overall PCE inflation has averaged about 1.5 percent, well below the Fed’s 2 percent target. Taking a longer perspective, over the past 25 years, core and overall inflation have both averaged 1.8 percent.\n\nThis persistent undershoot of the Fed’s target risks undermining the 2 percent inflation anchor. In this regard, research by Ulrike Malmendier and Stefan Nagel is sobering.6 They find that inflation expectations are heavily influenced by the inflation experience in one’s own lifetime, which implies that decades of too low inflation can become embedded in expectations. Indeed, we have seen some worrying signs of a deterioration of measures of longer-run inflation expectations in recent years, as seen in Table 2.\n\nImportantly, this sustained undershoot of the inflation target is likely to be a recurring dilemma for the Fed and central banks in Japan and Europe that have had similar experiences over the past decade. This problem of inflation running chronically below the target stems in part from the limited ability of central banks to offset economic downturns due to the lower bound on interest rates in a low-neutral-rate environment.7\n\nImplications for the Monetary Policy Framework\n\nThe risk of the inflation expectations anchor slipping toward shore calls for a reassessment of the dominant inflation targeting framework.8 A number of alternative frameworks and strategies have been proposed that hold the promise of better achieving the inflation goal and holding fast the inflation anchor.9 In this regard, I am very pleased that the Federal Reserve is undertaking a review of our policy framework this year, a topic that Federal Reserve Vice Chairman Clarida will discuss in his lunch remarks today.\n\nIn summary, the Phillips curve is alive and well. I wholeheartedly agree with the authors that we must not be complacent about inflation expectations becoming unmoored, whether at too high or too low a level.\n\nTables and Figures"
    },
    {
        "title": "Monetary Policy: A 'Data Dependent' Approach",
        "date": "Jan 18, 2019",
        "speaker": "John C. Williams",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2019/wil190118",
        "content": "Introduction\n\nGood morning everyone. I wish every conference I went to had such an incredible opening—Joe’s certainly a hard act to follow. I have a son who’s a very talented artist, but he didn’t get those genes from me. So I’m going to stick to what I know—monetary policy.\n\nToday I’ll talk about the economy, how I’m assessing the outlook, and what that means for monetary policy. It’s early in the day, so I’ll try and keep the recitation of facts and figures to a minimum. But I know there are a lot of bankers in the room and that the direction of interest rates plays a vital role in your business, so, rest assured, I won’t skimp on that topic.\n\nGiven that I’ve promised to talk about interest rates, before I say another word I’ll give the standard Fed disclaimer that the views I express are mine alone and do not necessarily reflect those of the Federal Open Market Committee or anyone else in the Federal Reserve System.\n\nThe Dual Mandate\n\nI want to start this morning by reminding everyone what the goals of the Federal Reserve are. It’s easy to think of the Fed as just setting interest rates, and of course that’s a major part of what we do. But we do so in order to achieve two goals set by Congress: maximum employment and price stability. In Fed-speak it’s called the dual mandate.\n\nThere are serious debates taking place amongst economists and policymakers about what maximum employment actually looks like, and they probably deserve another speech of their own. But I promised not to deluge you with numbers, so I’ll focus on a few highlights.\n\nThe unemployment rate last year averaged just under 4 percent, the lowest such annual figure since 1969. The latest jobs numbers also showed very strong growth, with over 2.6 million jobs added last year. The ongoing strength in the labor market has led to encouraging gains in wages. No matter how you cut it, the labor market is strong, consistent with our maximum employment goal.\n\nWhen we say price stability, our goal is to keep inflation around 2 percent.1 Those of you who are of my generation will remember the runaway inflation of the 1970s and early 1980s. Obviously that’s something we must avoid repeating. But the challenge of more recent years actually hasn’t been too high inflation. Instead, it’s been inflation that’s persistently too low. Over the past decade, inflation has more often than not come in below our goal.\n\nBut things are looking better. Underlying measures of inflation have been running just below 2 percent over the past 12 months, and I expect them to be right at 2 percent this year. And I don’t see any worrying signs of inflationary pressures building. So, from the perspective of the Fed’s dual mandate as a whole, things are looking very good.\n\nOf course, for this healthy jobs market and low and stable inflation to persist, we need solid growth in the economy. While GDP growth in 2018 looks to have been a robust 3 percent, we’re hearing anxiety both in the markets and the commentary, about what’s to come in 2019.\n\nI wish I could now tell you with certainty what will happen to the economy, but anyone who promises they can see into the future is a charlatan. However, what I can do is provide you with some insight into how I assess the health of the economy and what that means for my view on the monetary policy decisions before us.\n\nData Dependent\n\nI often say I’m data dependent, and I’ve talked about the major economic indicators, GDP growth, inflation, and employment. But these headline numbers aren’t the only things that inform my view on monetary policy, or the economy more broadly. I’m looking at a whole raft of information about what’s going on and where risks may be lurking.\n\nEmployment and unemployment numbers provide a useful summary measure of what’s happening in the labor market. But “data” doesn’t stop there. There are many sources of information on labor market conditions that I regularly consult. They range from very granular data that zero in on individuals’ experiences in the labor market, to indices of job openings, how many people are quitting their jobs, and labor market participation, to name a few.\n\nBut it’s not just these hard numbers that matter. I also find surveys of households and businesses provide valuable and timely perspectives about economic conditions. For example, there’s a questionnaire that asks people whether jobs are “hard to get.” It’s helpful because it gives the perceptions of normal people (and not economists!) about whether there are a lot of open positions. This survey continues to report that it’s relatively easy to find work, which is a very good sign about the strength of the economy.2\n\nBeyond the economic data and surveys, financial market indicators are a critical source of up-to-the minute information on how investors view the economy. In addition, stock prices and interest rates affect the spending decisions of households and businesses, and the value of the dollar affects demand for exports and imports, so these are important factors shaping the economy’s trajectory.\n\nMore broadly, I look at a wealth of data on international financial market developments and banking conditions, which feed into my understanding of the global outlook.\n\nFinally, and perhaps least well known, I spend a great deal of time speaking with members of the public, at our Board and Advisory Group meetings, community and business leader forums, and events like today’s. I get immense value from their insights, whether it’s the challenges they face in filling positions, their plans for capital investments, or how global developments are influencing their decisions. These discussions provide a huge amount of detail and texture to the economic picture. One of the great strengths of the Federal Reserve System is our extensive networks of contacts in our communities, and we share what we learn from these conversations when we meet at the FOMC.\n\nBeing \"data dependent\" means taking this holistic view when thinking about the economy and monetary policy. Not all of these types of data draw headlines the way shifts in the stock market do, but together they provide a much richer and more complete depiction of what’s happening. And they also tell me whether the signals we’re getting from the markets are reflected in the economy as a whole.\n\n2018 Tailwinds\n\nIn thinking about where the economy is heading, it’s worth reflecting on where we were 12 months ago.\n\nAt the start of last year the message I was getting—from business leaders, economic data, and the markets—was one of overwhelming positivity. This reflected three tailwinds: strong global growth, fiscal stimulus, and quite accommodative financial conditions. Each of these was giving the economy an extra boost. Indeed, these tailwinds provided even more of a boost over most of 2018 than forecasters had anticipated.\n\nFast forward 12 months and indicators in Europe and Asia point to less optimism about global growth. The initial lift from the fiscal stimulus will likely wane over the course of this year. And financial conditions have become less accommodative over the past several months. In addition, we are now seeing some emerging headwinds to growth from the partial government shutdown in the United States and elevated geopolitical uncertainties abroad.\n\nOther “data” confirm that the winds in our sails have calmed. The headlines have been about investors’ concerns around growth, which have contributed to volatility in markets. This is also showing through in surveys of households and businesses, who say that they are somewhat less confident about future economic prospects. For example, the latest Empire State Manufacturing Survey and New York Fed Business Leaders Survey both point to slowing growth in the region. And there is no doubt that my colleagues and I are hearing the same thing directly from our business and financial market contacts, who are more focused on the possibility of a weaker economic outlook this year.3\n\nBut let me be clear: a softer economic outlook doesn’t mean we should prepare for doom and gloom. On the contrary, it’s likely we’ll see GDP growth somewhere between 2 and 2½ percent this year. That’s a step down from 2018, but still consistent with a healthy, growing economy.\n\nSupporting a Strong Economy\n\nSo, how should the Fed respond to an outlook of slowing growth and one that’s less certain than, say, this time last year? In a word: carefully.\n\nAt the start of 2018, when the economy was growing well above trend and interest rates still were still quite low, gradually raising rates was the obvious and necessary choice. Twelve months later, the tailwinds have lost their gust, interest rates are closer to normal levels, and inflation is tame. The approach we need is one of prudence, patience, and good judgment. The motto of “data dependence” is more relevant than ever.\n\nIf growth continues to come in well above sustainable levels, somewhat higher interest rates may well be called for at some point. However, if conditions turn out to be less robust, then I will adjust my policy views accordingly.\n\nI assure you that I have my eyes wide open and my ear to the ground when it comes to thinking about how the economic outlook will unfold in the year ahead, and as ever I’ll be guided by the data in all its forms.\n\nThe Balance Sheet\n\nAside from my views on interest rates, the other issue I get asked about is the normalization of the balance sheet. When we first announced our plans in June 2017, I was poised to expound on them in great detail.4 But when I raised the topic I was sorely disappointed to be met with glazed eyes and general indifference. Sadly, nobody seemed to care!\n\nNow that’s changed, and I’m getting more interest on this topic.\n\nIt’s important to remember why the Fed took unconventional monetary policy actions in the first place. Large-scale asset purchases, forward guidance on the future path of interest rates, and seven years where the federal funds rate hovered close to zero, were all measures necessary for the full recovery from the Great Recession.\n\nA decade later and the FOMC has slowly but surely been moving monetary policy “back to normal.” We’ve moved interest rates closer to neutral, scaled back on forward guidance, and are over a year into the process of winding down the balance sheet.5\n\nOur goals throughout the policy normalization process have been twofold. First and foremost, to support a strong, sustainable expansion of the economy. Second, to do this in a predictable and transparent way that creates as little disruption as possible.6 So far, this plan has worked very well, with the economic expansion nearing record duration.\n\nBut it is important to stress that if circumstances change, I will reassess our choices regarding monetary policy, including the path of balance sheet normalization. Data dependence applies to all that we do. And, as always, if the outlook deteriorates in a material way, we stand prepared to deploy all our policy tools as appropriate in support of the economy.\n\nConclusion\n\nI’d like to conclude where I began, with the dual mandate. There’s sometimes a view that Fed policymakers are fixated on two data points: inflation and employment. And the reason is that’s how we’re judged by Congress at doing our job.\n\nBut underpinning these is a strong economy, and ultimately that’s what we’re always using monetary policy to support. So, as the year unfolds I’ve got my eyes wide open, looking at the data and listening to experience—whether it’s coming from household surveys, market indicators, or from leaders in my District.\n\nThe economy is strong, the outlook is healthy, and my number one priority is using monetary policy to keep it that way. In short, I’m watching, listening, and prepared to adjust my views depending on the data."
    }
]