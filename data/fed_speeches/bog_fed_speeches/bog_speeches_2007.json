[
    {
        "content": "November 30, 2007\n\nGovernor Randall S. Kroszner\n\nAt the Philadelphia Fed Policy Forum, Philadelphia, Pennsylvania\n\nIn my remarks today, I would like to explore the role of information in the development of new financial products and then draw some lessons about risk management and regulation. In particular, I will examine the role that investment in information gathering, processing, and evaluating plays in supporting the price discovery process and how such investment can lead toward a tendency to greater standardization as markets for innovative financial products mature. Examples from both history and current experience will help to illustrate this tendency with respect to loan work-outs and restructurings. I will then conclude by considering how a regulatory approach that encourages transparency and sound risk management, such as Basel II, can be valuable in fostering a robust environment for the introduction of innovative financial products.\n\nExperimentation and Learning in New Instrument Development\nTypically, when a new product is being developed, there is an initial experimentation phase in which market participants learn a great deal about the product’s performance and risk characteristics. This phase involves gathering and processing information and modeling the performance of the product in various scenarios and under different market conditions. It may then take time for market participants to understand what, exactly, they need to know to value a product. During the early phases, a fair amount of due diligence is appropriate, given the greater uncertainty associated with innovative products. The investment in gathering, processing, and evaluating information then, as I will discuss, often leads to greater standardization of products and contract terms, which can enhance liquidity of products as their markets mature.\n\nIn the initial experimentation phase, the terms and characteristics of a new product are adjusted in response to market acceptance--or lack thereof. During this period, market participants are seeking and providing information so that they can properly value the product, judge its potential for risk and return, assess its market acceptance and liquidity, and determine the extent to which the risks of the product can be hedged or mitigated.\n\nWhen a product’s track record is not well established, there should be a strong market demand for information in order to facilitate price discovery. Price discovery is the process by which buyers’ and sellers’ preferences, as well as any other available market information, result in the “discovery” of a price that will balance supply and demand and provide signals to market participants about how most efficiently to allocate resources. This market-determined price will, of course, be subject to change as new information becomes available, as preferences evolve, as expectations are revised, and as costs of production change.\n\nIn order for this process to work most effectively, market participants must utilize information relevant to value that product. Of course, searching out and using relevant sources of information--as well as determining what information is relevant--has its own costs. To underscore the last point, with new instruments, it may not even be clear exactly what information is needed for price discovery--that is, some market participants may not know what they do not know and they may therefore terminate the information-gathering stage prematurely, unwittingly bearing the risks and costs of incomplete information.\n\nPrice Discovery\nDue diligence is an important part of the price discovery process. The due-diligence process allows market participants to “trust but verify” market-provided information through a range of activities, from assessing risks and exposures through stress-testing to assessing the enforceability of the contracts that define the legal relationship among originators, sponsors, investors, and guarantors. The due diligence is complemented by risk-management structures that allow participants to interpret, understand, and act appropriately in response to the information in the market.\n\nRecently we have seen how a lack of information and inadequate due diligence and risk management have created problems in the market for certain structured finance products. Let me focus a moment on structured investment vehicles, or SIVs. SIVs have been created with a variety of terms and characteristics--for example, different underlying assets, different levels of liquidity support or guarantees, and various triggers that require the forced sale of assets or liquidation of the structure. Although SIVs or similar vehicles have existed for many years, many recent SIV structures involved a much higher level of complexity of the underlying credit risks, legal structures, and operations. This complexity--and the lack of information about where the underlying credit, legal, and operational risks resided--made these products more difficult and costly to value than many investors originally thought. Investors suddenly realized that they were much less informed than they assumed and, not surprisingly, they pulled back from the market.\n\nWe have seen similar problems in the subprime residential mortgage-backed securities market and the related derivatives markets. The lack of long historical data on the performance of these instruments, and their correlations with other assets and instruments, made it difficult to assess their overall risk-return profile, especially in times of stress. Moreover, in the subprime residential mortgage-backed securities market, many market participants were willing to proceed without conducting robust due diligence and without establishing appropriate risk-management structures and processes. They did not follow “trust but verify,” that is, they instead accepted the investment-grade ratings of these securities as substitutes for their own risk analysis. Ratings keyed to expected default or credit loss do not adequately capture the full range or magnitude of risks to which a product may be subject, including--as we have seen most dramatically--market liquidity risks. In addition, some originators may not have demanded sufficient information about the purchased assets underlying these structures and therefore may not have fully appreciated the credit risk of the assets and the consequential risk that the structures would come back on balance sheet when the assets defaulted.\n\nWhen the problems in the subprime mortgage market began to emerge and delinquencies exceeded rating agency estimates and the defaults predicted by limited historical data, we had moved beyond our past experience with these instruments. Information was not readily available about the extent to which the economic context had changed, or even whether underlying loans would or could be modified to prevent default. When ratings were downgraded, investors lost confidence in the quality of the ratings and hence the quality of the information they had about subprime investments. Lack of information, a disrupted price-discovery process, and a stressed environment led to a reassessment of risk, not only in the subprime market but also in the residential mortgage market across the board.\n\nOf course, this is not the first time that participants in a market for an innovative product have suffered losses. In the early 1990s, participants in the collateralized mortgage obligation (CMO) market and the markets for structured notes and certain types of interest rate derivatives did not have adequate information about the potential volatility and prepayment risk involved. Consequently, market participants did not appropriately model these risks and suffered significant losses when market interest rates rose sharply in the mid-1990s. As in the case of the residential mortgage-backed securities market today, the general market reaction was a flight away from these instruments. However, over time, the market was restored as market participants came to better understand the risks and as standardized methods were developed to measure the risks and model the value of these instruments under alternative scenarios. Increased information and standardized pricing conventions, such as the use of option-adjusted spreads, moved these instruments from the experimentation and learning phase to the phase of broad market acceptance.\n\nWhen market participants realize that they do not have the information necessary for proper valuation of risks, the price-discovery process can be disrupted, and market liquidity can become impaired. A significant investment in information gathering, processing, and evaluation may be necessary to revive the price discovery process. This revival is likely to take time and the market may not look the same when it re-emerges.\n\nLet me describe in a bit more detail the ways in which these investments will take place and hence why recovery of price discovery may be a gradual process. First, market participants will likely need to collect more-detailed data in a more systematic manner in order to better understand the nature and risks of the instruments and their underlying assets. Second, investments in enhanced systems to warehouse and model data related to these instruments will facilitate a better understanding of their risks, particularly under stress conditions. Third, investors need to ensure that they have the so-called human capital expertise--that is, the people--to understand, interpret, and act appropriately on the results of the modeling and analysis of the information gathered. The pay-off from these investments will be a greater understanding of risks and greater ability to value the instruments.\n\nThe Development of Greater Standardization in a Market\nAnother consequence of information investments is a tendency towards greater standardization of many of the aspects of an instrument, which can help to increase transparency and reduce complexity. As was demonstrated in the CMO market, as the market gains information about a product and develops a level of confidence in that information, the product tends to become increasingly standardized. Standardization in the terms and in the contractual rights and obligations of purchasers and sellers of the product reduces the need for market participants to engage in extensive efforts to obtain information and reduces the need to verify the information that is provided in the market through due diligence. Reduced information costs in turn lower transaction costs, thereby facilitating price discovery and enhancing market liquidity. Also, standardization can reduce legal risks because litigation over contract terms can result in case law that applies to similar situations, thus reducing uncertainty.\n\nThe benefits of the development of standardization for enhancing the liquidity of financial markets have a long history. One particularly clear example dates back to the development of exchange-traded commodities futures contracts in the mid-1800s. The standardization of the futures markets improved the flow of information to market participants, reducing transaction costs and fostering the emergence of liquid markets.\n\nIn the early days of the Chicago Board of Trade, in the mid-1850s, standardization took the form of creating “grades” or quality categories for commodities such as wheat, allowing for the fungibility of grains stored in elevators and warehouses, and breaking the link between ownership rights and specific lots of a physical commodity. Traders no longer needed to verify that a certain quantity of grain was of a sufficiently high grade because the exchange established a system of internal controls in the form of grain inspectors and a self-regulatory system to arbitrate disputes. The grain inspectors charged a set fee to certify the quality of the grain for any receipt traded at the board, a system with parallels to the mechanisms employed today by the rating agencies.1\n\nIn effect, standardization and related controls reduced traders’ information requirements and, thus, their transaction costs. In 1865, the Chicago Board of Trade standardized the delivery dates for the contracts, thus fostering the emergence of liquid markets in which traders could readily hedge the risk of price changes in the commodities and contracts. A final step toward standardization came years later with the adoption of the clearinghouse for the exchange as the common counterparty to all of the contracts traded on the exchange. With a central counterparty, the costs and uncertainties of failures and restructurings were significantly reduced, thereby reducing work-out costs and enhancing liquidity of the contracts traded on the exchange.2\n\nThe benefits of standardization can be realized not only on organized exchanges but also in over-the-counter markets. In more recent times, for example, the creation of the International Swaps and Derivatives Association (ISDA) master agreement for over-the-counter swaps and derivatives contracts has brought about the benefits of standardization while also allowing for product flexibility and customization. The ISDA master agreement provides standard definitions and a general outline for the contract but allows latitude in customizing terms. The master agreement also sets forth a template for workout procedures if a counterparty defaults, allowing parties to the agreement to adjust their risk-management strategies in light of the agreed-upon work-out process. This standardization reduces uncertainty about the instruments, which lowers transaction costs and facilitates price discovery and market liquidity.\n\nThe examples from the long- and more recent- past may hold some valuable lessons for how improvements in standardization could help to address some of the challenges in the subprime market. Uncertainty about the work-out process and the options that are available, for example, could be contributing to the difficulties in reviving price discovery and liquidity in the market for subprime residential mortgage-backed securities. Part of the valuation challenge is gauging the extent of the difficulties that borrowers will have in making payments and being able to stay in their homes given the reduction in house price appreciation--or actual declines in some areas--and the large number of interest rate resets coming on many adjustable-rate mortgages. From now until the end of next year, monthly payments for an average of roughly 450,000 subprime mortgages per quarter are scheduled to undergo their first interest rate reset. In addition, tightening credit conditions as reported in the Federal Reserve’s Senior Loan Officer Opinion Surveys on Bank Lending Practices suggest that refinancing may become more difficult.\n\nLenders and servicers generally would want to work with borrowers to avoid foreclosure, which, according to industry estimates, can lead to a loss of as much as 40 percent to 50 percent of the unpaid mortgage balance. Loss mitigation techniques that preserve homeownership are typically less costly than foreclosure, particularly when applied before default. Borrowers who have been current in their payments but could default after reset may be able to work with their lender or servicer to adjust their payments or otherwise change their loans to make them more manageable.\n\nIt is imperative that we work together as a financial services community to look for ways to help borrowers address their mortgage challenges, particularly for those who may have fewer alternatives, such as lower-income families. The Federal Reserve and other regulators have been active in encouraging lenders and servicers to take a proactive approach to work with borrowers who may be at risk of losing their homes. For example, the agencies have issued statements underscoring that prudent workout arrangements that are consistent with safe and sound lending practices are generally in the long-term best interest of both the investor and the borrower and have had numerous meetings with interested parties to foster the development and implementation of work-out arrangements.\n\nGiven the substantial number of resets from now through the end of 2008, I believe it would behoove the industry to go further than it has to join together and explore collaborative, creative efforts to develop prudent loan modification programs and other assistance to help large groups of borrowers systematically. I am not suggesting a one-size-fits-all approach, but a bottom-up approach designed to appropriately balance the needs of all parties. Getting to borrowers who have been making payments but are at risk of falling behind before they actually do become delinquent, for example, can help to preserve work-out and refinancing options.\n\nSome industry participants and consumer groups have begun to work collaboratively to develop loan-modification templates, standards, and principles that can help to streamline the work-out and modification process. This can reduce transaction costs and potentially provide timely relief to a wider range of borrowers. A systematic approach to loan modifications would likely reduce some of the uncertainties in the market for such subprime mortgage-backed securities, helping to restore price-discovery and liquidity. This would help to ease the tightening of credit conditions in the market.\n\nI am privileged to serve as a board member of NeighborWorks America, a national nonprofit that partners with the HOPE NOW Alliance. This alliance is developing ways to facilitate the flow of information between servicers and distressed borrowers and to work toward clarification of loan-modification procedures. Increased standardization and certainty could also benefit investors in the mortgage market by improving information flows and the price-discovery process, thereby improving market liquidity while at the same time helping to avoid foreclosures and promoting sustainable homeownership.\n\nA Regulatory Environment That Encourages Sound Risk Management and Transparency\nRecent market events have underscored the need for better market information about new products, robust due diligence to verify that information, and risk-management strategies to utilize the information in management decisionmaking. The supervisory agencies and the industry both are addressing the need for improved risk management in light of the market disruptions\n\nThe newly adopted Basel II capital framework for large internationally-active banking organizations, for example, is an important advance that encourages the types of investment in information I discussed earlier. The Basel II framework is comprised of three pillars. Pillar 1 requires information gathering and robust modeling techniques to better take into account the risks of different types of instruments and securities than under the traditional Basel I framework. It also provides incentives for more robust risk management in connection with certain higher-risk activities, such as securitization and other off-balance-sheet activities. Pillar 2 emphasizes the further stress testing and analysis of the data in conjunction with an ongoing evaluation of the institution’s capital adequacy in light of its risks through the internal capital adequacy assessment process. Pillar 3 reflects the need for better information through investments in data gathering and analysis that are reflected in enhanced public disclosures and regulatory reporting. More-comprehensive and more-transparent information allows investors to better understand the banking organization’s risk profile and thus reduces transaction costs and facilitates price discovery and market liquidity. The three pillars of Basel II promote precisely the three types of investment in information discussed earlier that facilitate the price discovery process.\n\nIn addition to supervisory initiatives, industry leaders’ efforts to influence the adoption of sound practices and codes of conduct can efficiently and effectively facilitate market-correcting behaviors. To this end, the industry is actively engaged in efforts to improve sound practices for risk management through improved stress-testing practices to cover contingent exposures, marketwide events, and potential contagion and enhanced due diligence and modeling for new products. As they look into the causes of the recent market disruptions and determine the appropriate response, both supervisory and industry groups are carefully analyzing the weaknesses in risk management and the lack of transparency in complex structures--and the implications of that lack of transparency for proper valuations.\n\nConclusion\nThe recent market disruptions have dramatically underscored the importance of gathering and analyzing information about innovative products. When the price-discovery process for a product is disrupted, both investors and sellers need to engage in a period of information gathering, processing, and analysis in order to re-establish a market price. This can be a gradual process and one that results in fundamental changes to the market for the product. Efforts underway by both supervisors and the industry should encourage improvements in risk analysis and management and, thus, price discovery. We are hopeful that our efforts to increase the standardization of loan-modification options and processes for subprime loans will help to provide more information to lenders, investors, homeowners, and communities faced with potential mortgage loan defaults while at the same time helping to provide more timely relief for borrowers in distress.\n\nFootnotes\n\n1. See Randall S. Kroszner (1999), “Can the Financial Markets Privately Regulate Risk? The Development of Derivatives Clearing Houses and Recent Over-the-Counter Innovations,”  Journal of Money, Credit, and Banking, vol. 31 (August), p. 600. Return to text\n\n2. See Kroszner, “Can the Financial Markets Privately Regulate Risk?”, p. 601. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20071130a.htm",
        "title": "Innovation, Information, and Regulation in Financial Markets",
        "date": "11/30/2007"
    },
    {
        "content": "November 29, 2007\n\nGovernor Frederic S. Mishkin\n\nTo the Undergraduate Economics Association, Massachusetts Institute of Technology, Cambridge, Massachusetts\n\nA key element of the macroeconomics revolution in recent decades has been the recognition that the dynamic structure of the economy is not purely mechanistic but instead reflects the fundamental role of expectations in the economic decisions of households and firms.1 That insight inevitably put a spotlight on the expectations of the monetary authorities themselves and has led central banks around the world to refine their communications about policy objectives and the macroeconomic outlook.  Indeed, to give the public more information regarding its plan for conducting monetary policy in accordance with its congressional mandate, the Federal Reserve on November 14 announced several important enhancements to its communication strategy.  And the Federal Reserve's first \"Summary of Economic Projections\" was published last week in conjunction with the minutes of the October meeting of the Federal Open Market Committee (FOMC). Here is a brief overview of the main elements of the enhanced communication strategy:\n\nIn the remainder of this speech, I will discuss how the modern science of monetary policy provides a strong rationale for the Federal Reserve's new communication policy, and then I'll describe how a number of key principles of monetary policy making are evident in the latest set of FOMC projections, which were released last week.\n\nThe Rationale for the Federal Reserve's Enhanced Communication Strategy\nThe modern approach to the formulation of monetary policy is generally characterized by an optimization problem in which policymakers maximize a specific objective function subject to a set of constraints. At first glance, this problem looks a lot like the typical optimal control problem that MIT undergraduates might study in an engineering course--say that of directing a rocket ship to the moon.  However, there is a crucial difference: A rocket ship responds only to forces currently acting on it and does not try to anticipate how its controllers will change their settings. In contrast, expectations about future monetary policy and economic conditions play a key role in economic agents' decisions and thus are important in both the objective function and the constraints of the optimal monetary policy problem.  This role of expectations is crucial in understanding why central bank communication is so important for achieving good macroeconomic outcomes.3\n\nObjective Function\n\nThe modern science of monetary policy proceeds under the assumption that the central bank's purpose is to maximize the well-being of households in the economy; the objective function specifies exactly what should be maximized.  Broadly speaking, the objective function has two components, each expressed as a deviation from a goal and each with a negative sign: one component is the deviation of inflation from its optimal level, and the other is the deviation of output (or employment) from its so-called natural level, the level that would be attained if prices and wages were completely flexible. Those components of the objective function capture the essence of the Federal Reserve's dual mandate to promote price stability and maximum employment, a mandate established by the Congress for the ultimate good of the public.\n\nThe intertemporal nature of these objectives is crucial, as the well-being of households depends not only on their consumption and leisure at the present moment but also on their expectations about how that standard of living will evolve over time. Thus, policies that are intended to maximize the well-being of households must reflect both the present state of the economy and its expected path in future periods.\n\nConstraints\n\nIn a market-oriented economy like ours, the science of monetary policy explicitly reflects the extent to which households and firms have the freedom to make their own economic decisions.  The overall level of expenditures in the economy--which macroeconomists refer to as aggregate demand--is the sum of all the spending decisions of individual households and firms.  Aggregate demand exhibits a strong forward-looking component because, as noted, the spending decisions of households include their expectations regarding the future path of the economy. Similarly, the overall inflation rate of the economy ultimately reflects the price-setting decisions of individual firms, and expectations have a crucial influence on those decisions as well.  As a result, the evolution of aggregate supply also exhibits a significant forward-looking component.\n\nHowever, both aggregate supply and aggregate demand tend to exhibit a substantial degree of inertia, which reflects the significant costs incurred by households and firms in making sudden large adjustments in their consumption, staffing levels, and so forth.  Largely because of those inertial elements, monetary policy actions have little or no instantaneous effect on the economy; in fact, the peak effect on aggregate demand may not appear for at least a year, and the peak effect on inflation is subject to even longer lags.  Therefore, because the variables the central bank wants to influence have a forward-looking component, and because the effects of policy actions are unlikely to appear for at least a year, macroeconomic projections should have a central role in the monetary policy plan and should appear in the public description of that plan.\n\nThe critical role of expectations in the evolution of the economy elevates the importance of central-bank communication and suggests that it should encompass some discussion of views beyond that of the quantitative projections.  Broadening the scope of communication by including a description of policymakers' expectations helps households and businesses to make more-informed decisions and can help anchor the private sector's inflation expectations more firmly by clarifying and underscoring the central bank's intention to keep inflation low and stable over the long run. Increased communication also enhances the performance of financial markets by providing investors with greater information about policymakers' intentions.  In all of these ways, central-bank communication that more fully portrays policy considerations facilitates the effectiveness of monetary policy and contributes to the overall stability and growth of the economy.\n\nThe enhancements to our communication strategy were developed through a comprehensive process over the past year or so, but they have turned out to be particularly timely in light of recent economic and financial developments. As you are well aware, the U.S. economy has been experiencing a bout of financial instability over the past few months. Heightened uncertainty in the credit markets impedes the ability of the financial markets to channel funds to households and businesses that have productive investment opportunities.\n\nAs I discussed in a speech in early November, the heightened uncertainty flows from an increase in risk, which can be broken down into two components: valuation risk, which arises when the market realizes that the complexity of a security or the opaqueness of its underlying creditworthiness prevents it from accurately assessing the value of the security; and macroeconomic risk--that is, the risk that a financial disruption will cause significant deterioration in the real economy, which can, in turn, worsen the financial disruption (Mishkin, 2007d). The Federal Reserve's recent policy actions are intended to help lower macroeconomic risk and thereby improve the functioning of financial markets. In addition, our enhanced communications will help reduce macroeconomic risk by providing additional information regarding our policy strategy and our assessment of the economic outlook.\n\nThe Science of Monetary Policy and the Longer-Term Outlook\nI would now like to consider the important information conveyed about the FOMC's longer-term policy strategy in the Federal Reserve's newly released set of macroeconomic projections. In that context, I will also highlight how our longer-term strategy fulfills the dual mandate and embeds some of the key implications of the modern science of monetary policy.\n\nThe Longer-Term Projections for Inflation\n\nThe projections for both overall and core inflation at the three-year horizon (which is currently 2010) fall in the range of 1.5 percent to 2.0 percent; the central tendency of these projections is 1.6 percent to 1.9 percent. As I noted earlier, each FOMC participant's projection is made under the assumption of \"appropriate\" monetary policy--that is, the path of policy calibrated to achieve outcomes for economic activity and inflation that are most consistent with our dual objectives of price stability and maximum employment. For that reason, the longer-run inflation projections provide information about each FOMC participant's assessment of the inflation rate that best promotes those dual objectives--a rate that I will refer to as the \"mandate-consistent inflation rate.\"\n\nWhen the underlying level of inflation is reasonably close to its mandate-consistent rate, monetary policy should be able to achieve that rate of inflation in coming years while keeping economic activity near its maximum sustainable level. Although overall inflation is currently above the mandate-consistent rate, reflecting the effects of energy price increases, core inflation has been well-behaved. Thus, I believe this description essentially characterizes the current macroeconomic environment, and my own projection of inflation at a three-year horizon is equal to my assessment of the mandate-consistent inflation rate.\n\nThe range of projections for inflation in 2010--1.5 percent to 2.0 percent--is consistent with a fundamental implication of the modern science of monetary policy--namely, that the well-being of the economy is maximized by maintaining a low and stable inflation rate over time.4 Low inflation promotes social welfare by simplifying the savings and retirement planning of individual households and by facilitating firms' production and investment decisions, all of which helps promote higher productivity growth. Furthermore, an environment of overall price stability contributes to economic efficiency by reducing the variability of relative prices and by minimizing the distortions that arise because the tax system is not completely indexed to inflation. Finally, and importantly, low and stable inflation promotes equity and reduces poverty, as the poorest members of society typically do not have access to the sorts of financial instruments that would help protect them against inflation.\n\nAt the same time, recent research on the science of monetary policy (as well as the analysis of several historical episodes) has underscored the pitfalls that can result from maintaining a zero or negative inflation rate over time. First, such a strategy tends to raise the likelihood of episodes in which short-term nominal interest rates reach the zero lower bound and thereby hinder the conduct of monetary policy. Second, an economy with a zero average inflation rate will almost inevitably experience some episodes of deflation, which may adversely affect financial markets and thereby hamper the performance of the macroeconomy. Third, if inflation is at or below zero, downward rigidities in nominal wage adjustment might induce some distortions in labor market efficiency, with unfavorable consequences for the level of employment. In light of these considerations, as Chairman Bernanke (2007) recently noted, \"the (properly measured) long-run inflation rate that best promotes the dual mandate is likely to be low but not zero.\"\n\nThe science of monetary policy has also emphasized the importance of establishing the central bank's strong and credible commitment to keeping inflation low and stable over the long run because that commitment helps to stabilize inflation expectations; in turn, stable expectations aid the functioning of the economy and the efficacy of policy. The increased information that these projections convey regarding FOMC participants' views of the mandate-consistent inflation rate, combined with the FOMC's continuing commitment to keeping inflation low and stable, should help anchor inflation expectations even more firmly.\n\nBetter information on FOMC participants' views on the mandate-consistent inflation rate is also essential for achieving the other element of the dual mandate--maximum employment. By increasing the likelihood of achieving a low and predictable inflation rate, providing information on the mandate-consistent inflation rate plays a crucial role in facilitating long-term growth in employment and labor productivity. Furthermore, although the economy will inevitably be buffeted by various shocks, the knowledge that the monetary authority will take steps to move inflation gradually toward its mandate-consistent rate over time also helps minimize the deviations of employment and output from maximum sustainable levels.\n\nThe Longer-Term Projections for Growth and Unemployment\nFor 2010, the central tendency of the range of projections for the growth of real GDP is 2.5 percent to 2.6 percent, and the projections for the unemployment rate have a central tendency of 4.7 percent to 4.9 percent. The full range of FOMC participants' projections is 2.2 percent to 2.7 percent for real GDP growth and 4.6 percent to 5.0 percent for the unemployment rate.\n\nAs indicated in the forecast narrative, these longer-run projections were heavily influenced by participants' assessments of the sustainable rates of output growth and employment. However, assessments of potential output growth and the sustainable unemployment rate are fundamentally different from assessments of the mandate-consistent inflation rate. In particular, as I emphasized in a speech last spring, the Federal Reserve can determine and achieve the long-run average rate of inflation in keeping with its dual mandate, but the Federal Reserve most emphatically cannot choose the level of maximum sustainable economic activity--no central bank can control the level of real economic activity or employment over the longer run (Mishkin, 2007a).\n\nIn addition, the maximum sustainable levels of output and employment cannot be known with any assurance (Mishkin, 2007b). The relatively narrow range of participants' longer-term projections for real GDP growth and the rate of unemployment must not be confused with the uncertainty attending those projections. As is explained in the special section on forecast uncertainty that was published last week with the FOMC's \"Summary of Economic Projections,\" the uncertainty attending each participant's projections is distinct from the diversity of views about the most likely outcomes. Forecast uncertainty is concerned with the risks associated with a particular projection rather than with divergences across a number of different projections.\n\nIn that regard, macroeconometric analysis indicates that the sustainable unemployment rate and the growth rate of potential output have varied quite substantially over recent decades, thereby exacerbating the challenges of constructing estimates of these characteristics at any given point in time. For example, Staiger, Stock, and Watson (1997a,b) obtained 95 percent confidence intervals for a measure of the sustainable unemployment rate that are at least 2 percentage points wide. Thus, although FOMC participants' projections for the unemployment rate in 2010 provide information about their assessments of the sustainable unemployment rate, the analysis of Staiger, Stock, and Watson highlights the degree of uncertainty about these assessments--namely, the true value of that rate might well be as low as 3-3/4 percent or as high as 5-3/4 percent. Such calculations help illustrate the fundamental point that the FOMC projections--just like any forecast--should be interpreted with considerable caution; they can be highly uncertain and may vary considerably over time in response to incoming information about the structure of the economy and the macroeconomic outlook.\n\nThe Science of Monetary Policy and the Shorter-Term Projections\nNow let us take a brief look at the Federal Reserve's macroeconomic projections for 2007 through 2009. These projections are useful for understanding the Federal Reserve's near-term policy strategy, and again, I would like to highlight how this strategy fulfills the dual mandate and embeds key implications of the modern science of monetary policy.\n\nThe science emphasizes that monetary policy makers need to think in terms of a plan for the appropriate paths for inflation and economic activity that best promotes the dual mandate of price stability and maximum sustainable employment. If economic activity is well below its maximum sustainable level, then monetary policy should aim at increasing output and employment toward sustainable levels. If inflation is above the mandate-consistent rate, monetary policy should aim at reducing inflation to that rate. Providing projections for the short run as well as for the longer run encourages FOMC participants to think in terms of desirable paths for inflation and output, a discipline that the science suggests will produce better policy outcomes. In addition, the projections provide households and businesses with information that can help them understand what the monetary authority is trying to achieve, thereby increasing the likelihood of good economic outcomes.\n\nThe Inflation Outlook\nIn the projections of FOMC participants, core inflation edges down slightly and then remains fairly stable at about 1-3/4 percent toward the end of the decade. Meanwhile, given an anticipated flattening out of energy prices, overall consumer inflation in these projections declines toward the core inflation rate; it drops from about 3 percent this year to about 2 percent in 2008 and then drops a bit further near the end of the decade.\n\nThese projections are consistent with a point I made in an earlier speech--namely, in recent decades core inflation has tended to be a good predictor of overall inflation further ahead (Mishkin, 2007c). Furthermore, these projections demonstrate that FOMC participants do not think that the Federal Reserve should, or even could, achieve an immediate reduction of overall inflation to its mandate-consistent rate. The recent high level of overall inflation reflects rapidly rising energy prices that the Federal Reserve obviously cannot control. For this reason, any attempt to bring overall inflation down too quickly would entail large losses of output and employment, and hence such an approach would be inconsistent with the Federal Reserve's dual mandate, which places a balanced emphasis on both price stability and maximum employment.\n\nThe Outlook for Economic Activity\nThe midpoint of the range of projections for real GDP growth declines noticeably from about 2-1/2 percent for 2007 to roughly 2 percent in 2008; then it returns to about 2-1/2 percent in 2009 and 2010. The projection narrative indicates that the near-term weakness in the economy importantly reflects the sharp contraction in the housing market and the recent strains in the credit markets. As I have already noted, monetary policy works only with a lag and hence cannot offset these near-term effects; rather, the recent cuts in the federal funds rate are intended to help bring economic activity back to maximum sustainable levels over time, and such an outcome can be seen in the broad contours of the FOMC projections.\n\nThere is considerable diversity in the views of participants about the outlook for near-term growth. The range of forecasts for the growth of real GDP in 2008 is from 1.6 percent to 2.6 percent--twice as wide as in the previous set of FOMC projections, which were published last June.\n\nAs part of the enhanced projections process, FOMC participants provide a qualitative assessment of the magnitude of uncertainty surrounding their projections relative to average levels of uncertainty. As a benchmark for these qualitative assessments, the forecast narrative provides information about the average magnitude of projection errors of private and government forecasters over the past twenty years. These averages of projection errors indicate that the degree of uncertainty about the growth outlook is even wider than the range of the participants' forecasts. For example, at a one-year forecast horizon, the root mean squared prediction error is 1.3 percentage points for the growth of real GDP and 0.6 percentage points for the unemployment rate, as compared to 1.0 percentage points and 0.4 percentage points respectively for participants' forecasts of real GDP growth and unemployment. Furthermore, in the latest report, FOMC participants indicated that the current degree of uncertainty about GDP growth is even higher than the typical level of uncertainty over the past two decades.\n\nAlthough the current projections involve relatively stable trajectories for economic activity and inflation, the regular communication of Federal Reserve projections would become particularly useful in a setting in which the economy started from a position further away from its long-run balanced-growth path. In such circumstances, the task of monetary policy is to help guide the economy back toward the balanced-growth path, and the appropriate speed of that convergence will depend on the nature of the shocks that have affected the economy.\n\nThe Enhanced Communication Strategy and Federal Reserve Accountability\nI have argued that the Federal Reserve's enhanced communication strategy will improve the public's understanding of our objectives and policy strategies and thereby enable households and businesses to make better decisions. In addition, enhanced communication can help anchor the public's inflation expectations, which promotes both price stability and higher economic growth. As we have seen, the new communication strategy is consistent with what the modern science of monetary policy suggests is needed to achieve good monetary policy outcomes.\n\nBeyond that, however, I would like to emphasize another crucial reason for the Federal Reserve to enhance its communication strategy. We at the Federal Reserve have the task, delegated by the Congress, of conducting monetary policy to enhance the welfare of our citizens. In a democratic society, that assignment requires that we, as public servants, be accountable to the public and the Congress\n\nOf course, given the myriad unforeseen events and disturbances that can affect the economy, a central bank cannot be held accountable solely in terms of actual economic outcomes. For this reason, accountability can be enhanced by having the central bank explain the broad outcomes that its policy strategy is aimed at fostering, and then, to the extent that the economy deviates from those outcomes--as it surely will--the central bank should explain the factors that caused the deviation and how it plans to achieve the desired outcomes in the future.\n\nIn that regard, the Federal Reserve's enhanced communication strategy will be useful in enabling the FOMC to explain its policy decisions and strategies more fully in the context of its medium-term objectives for economic activity and inflation and the risks to those objectives. As a result, the public and the Congress can better assess whether our forecasts of the economy are reasonable and whether we are pursuing a policy strategy that is consistent with achieving the dual mandate of price stability and maximum sustainable employment. The result should be increased accountability that is consistent with basic democratic principles.\n\nI also hope that the increased prominence given to participants' economic projections will, on the one hand, help focus greater attention on our medium-term objectives and on our progress toward meeting these objectives and, on the other hand, remove some attention from the question of whether the FOMC will change its target for the federal funds rate at its next meeting. The near-term path of interest rates is highly uncertain and depends on the implications of the incoming data, which in some cases are evolving right up to the time of the meeting. What is more certain--and far more important--is the commitment of the FOMC to move interest rates as needed to foster outcomes consistent with our dual mandate of maximum sustainable employment and price stability.\n\nConclusion\nAs I have outlined here, I believe that the recent enhancements to the Federal Reserve's communication strategy are a major advance in achieving greater transparency. Moreover, the approach we have taken is consistent with what the science of monetary policy tells us will result in better outcomes for price stability and maximum employment. In addition, this communication strategy can help improve the performance of financial markets and increases the Federal Reserve's accountability to the public and the Congress.\n\nOver the years, the Federal Reserve has striven to improve its communication strategy, and our recent enhancements should be seen as an important further step in this direction. We have learned from our earlier experiences with increasing the transparency of our communications, and we expect to continue learning. We will also continue to examine how we can improve our communication with the public and the markets so that we can better achieve our congressionally mandated goals.\n\nReferences\nBenigno, Pierpaolo, and Michael Woodford (2003). \"Optimal Monetary and Fiscal Policy: A Linear-Quadratic Approach,\" NBER Macroeconomics Annual, vol. 18 (no. 1), pp. 271-333.\n\nBernanke, Ben S. (2007). \"Federal Reserve Communications,\" speech delivered at the Cato Institute 25th Annual Monetary Conference, Washington, November 14.\n\nClarida, Richard, Jordi Gali, and Mark Gertler (1999). \"The Science of Monetary Policy: A New Keynesian Perspective,\"  Journal of Economic Literature, vol. 37 (December), pp.1661-707.\n\nErceg, Christopher J., Dale W. Henderson, and Andrew T. Levin (2000). \"Optimal Monetary Policy with Staggered Wage and Price Contracts,\"  Journal of Monetary Economics, vol. 46 (October), pp. 281-313.\n\nGiannoni, Marc P., and Michael Woodford (2005). \"Optimal Inflation-Targeting Rules,\" in Ben S. Bernanke and Michael Woodford, eds., The Inflation-Targeting Debate. Chicago: University of Chicago Press, pp. 93-162.\n\nGoodfriend, Marvin, and Robert G. King (1997). \"The New Neoclassical Synthesis and the Role of Monetary Policy,\" NBER Macroeconomics Annual, vol. 12 (no. 1), pp. 231-83.\n\nKing, Robert G., and Alexander L. Wolman (1999). \"What Should the Monetary Authority Do When Prices Are Sticky?\" in John B. Taylor, ed., Monetary Policy Rules. Chicago: University of Chicago Press, pp. 349-98.\n\nLevin, Andrew T., Alexei Onatski, John C. Williams, and Noah Williams (2005). \"Monetary Policy under Uncertainty in Micro-Founded Macroeconometric Models,\" NBER Macroeconomics Annual, vol. 20 (no. 1), pp. 229-87.\n\nMishkin, Frederic S. (2007a). \"Monetary Policy and the Dual Mandate,\" speech delivered at Bridgewater College, Bridgewater, Va., April 10.\n\n_________ (2007b). \"Estimating Potential Output,\" speech delivered at the Conference on Price Measurement for Monetary Policy, Federal Reserve Bank of Dallas, Dallas, May 24.\n\n_________ (2007c). \"Headline versus Core Inflation in the Conduct of Monetary Policy,\" speech delivered at the Conference on Business Cycles, International Transmission, and Macroeconomic Policies, HEC Montreal, Montreal, October 20.\n\n_________ (2007d). \"Financial Instability and Monetary Policy,\" speech delivered at the Risk USA 2007 Conference, New York, November 5.\n\nRotemberg, Julio J., and Michael Woodford (1997). \"An Optimization-Based Econometric Framework for the Evaluation of Monetary Policy,\" NBER Macroeconomics Annual, vol. 12 (no. 1), pp. 297-346.\n\nSchmitt-Grohé, Stephanie, and Martin Uríbe (2005). \"Optimal Fiscal and Monetary Policy in a Medium-Scale Macroeconomic Model,\" NBER Macroeconomics Annual, vol. 20 (no. 1), pp. 383-425.\n\nStaiger, Douglas, James H. Stock, and Mark W. Watson (1997a). \"The NAIRU, Unemployment, and Monetary Policy,\"  Journal of Economic Perspectives, vol. 11 (Winter), pp. 33-49.\n\n_________ (1997b). \"How Precise Are Estimates of the Natural Rate of Unemployment?\" in Christina D. Romer and David H. Romer, eds., Reducing Inflation: Motivation and Strategy. Chicago: University of Chicago Press, pp. 195-242.\n\nWoodford, Michael (2003). Interest and Prices: Foundations of a Theory of Monetary Policy. Princeton, N.J.: Princeton University Press.\n\nFootnotes\n\n1. Note that my remarks reflect my own views and not necessarily those of others on the Board of Governors or the Federal Open Market Committee. I appreciate the very helpful comments and assistance of Spencer Dale, William English, Andrew Levin, and Jonathan Wright. Return to text\n\n2. Also specified in the Federal Reserve's legislative mandate is the goal of moderate long-term interest rates, but this goal is generally viewed as consistent with the dual mandate because long-term interest rates can remain low only in a stable macroeconomic environment; further discussion of this topic is in Mishkin (2007a,d). Return to text\n\n3. A clear formulation of the New Keynesian approach to the science of monetary policy can be found in Clarida, Gali, and Gertler (1999) and in Woodford (2003), both of which build on the pioneering work of Goodfriend and King (1997) and of Rotemberg and Woodford (1997). Academic economists and central-bank researchers have followed this approach in numerous subsequent studies; some examples include King and Wolman (1999); Erceg, Henderson, and Levin (2000); Giannoni and Woodford (2005); Benigno and Woodford (2003); Levin and others (2005); and Schmitt-Grohé and Uríbe (2005). Return to text\n\n4. There is no perfect indicator of the “true” inflation rate; for example, the PCE price deflator exhibits some upward bias that corresponds to unmeasured improvements in the quality of goods and services that consumers purchase. Thus, given PCE inflation of about 1-1/2 percent to 2 percent, the “true” inflation rate faced by consumers might well be less than 1 percent. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/mishkin20071129a.htm",
        "title": "The Federal Reserve’s Enhanced Communication Strategy and the Science of Monetary Policy",
        "date": "11/29/2007"
    },
    {
        "content": "November 29, 2007\n\nChairman Ben S. Bernanke\n\nAt the presentation of the Citizen of the Carolinas Award, Charlotte Chamber of Commerce, Charlotte, North Carolina\n\nThe focus of my brief remarks this evening will be the Charlotte region and how the area and the economy have changed since I regularly visited my grandparents here some four-and-a-half decades ago. First, though, I would like to share a few thoughts on the U.S. economy and the considerations that we at the Federal Reserve will be weighing as we prepare for our policy meeting on December 11, less than two weeks from now.\n\nThe Federal Open Market Committee (FOMC), the monetary policy making arm of the Federal Reserve System, last met on October 30-31. At that meeting, the Committee cut its target for the federal funds rate, the key policy interest rate, by 25 basis points (1/4 of a percentage point), following a cut of 50 basis points in September. Economic growth in the period leading up to the October meeting had proven quite strong, as confirmed by this morning’s figures on third-quarter gross domestic product (GDP). At its meeting, however, Committee members took the view that tightening credit conditions--the product of ongoing stresses in financial markets--and some intensification of the correction in the housing sector were likely to restrain economic activity going forward. Specifically, growth appeared likely to slow significantly in the fourth quarter from its rapid third-quarter rate and to remain sluggish in early 2008. The Committee expected that economic growth would thereafter gradually return to a pace approaching its long-run trend as the drag from housing subsided and financial conditions improved. Inflation was seen as edging down next year, approaching rates consistent with price stability; however, the Committee remained concerned about the possible effects of higher energy costs and the lower foreign exchange value of the dollar, especially the risk that they might lead to an increase in the public’s long-term inflation expectations.\n\nHow has the economic picture changed in the month since that meeting? As is often the case, the incoming economic data have been mixed. In the market for residential real estate, indicators of construction and home sales have continued to be weak. In contrast, the labor market remained solid in October, with some 130,000 new jobs added to private-sector payrolls and the unemployment rate remaining at 4.7 percent. Claims for unemployment insurance have drifted up a bit in recent weeks, although, on average, they have remained at a level consistent with moderate expansion in employment. We will, of course, have the labor market report for November next week, and in the coming days we will continue to draw on anecdotal reports, surveys, and other sources of information about employment and wages. Continued good performance by the labor market is important for maintaining the economic expansion, as growth in earnings helps to underpin household spending.\n\nWith respect to household spending, the data received over the past month have been on the soft side. The Committee will have considerable additional information on consumer purchases and sentiment to digest before its next meeting. I expect household income and spending to continue to grow, but the combination of higher gas prices, the weak housing market, tighter credit conditions, and declines in stock prices seem likely to create some headwinds for the consumer in the months ahead.\n\nCore inflation--that is, inflation excluding the relatively more volatile prices of food and energy--has remained moderate. However, the price of crude oil has continued its rise over the past month, a rise that will be reflected in gasoline and heating oil prices and, of course, in the overall inflation rate in the near term. Moreover, increases in food prices and in the prices of some imported goods have the potential to put additional pressures on inflation and inflation expectations. The effectiveness of monetary policy depends critically on maintaining the public’s confidence that inflation will be well controlled. We are accordingly monitoring inflation developments closely.\n\nThe incoming data on economic activity and prices will help to shape the Committee’s outlook for the economy; however, the outlook has also been importantly affected over the past month by renewed turbulence in financial markets, which has partially reversed the improvement that occurred in September and October. Investors have focused on continued credit losses and write-downs across a number of financial institutions, prompted in many cases by credit-rating agencies’ downgrades of securities backed by residential mortgages. The fresh wave of investor concern has contributed in recent weeks to a decline in equity values, a widening of risk spreads for many credit products (not only those related to housing), and increased short-term funding pressures. These developments have resulted in a further tightening in financial conditions, which has the potential to impose additional restraint on activity in housing markets and in other credit-sensitive sectors. Needless to say, the Federal Reserve is following the evolution of financial conditions carefully, with particular attention to the question of how strains in financial markets might affect the broader economy.\n\nIn sum, as I have indicated, we will be receiving a good deal of relevant information in the coming days. In making its policy decision, the Committee will have to judge whether the outlook for the economy or the balance of risks has shifted materially. In doing so, we will take full account of the implications for the outlook of both the incoming economic data and the ongoing developments in the financial markets.\n\nEconomic forecasting is always difficult, but the current stresses in financial markets make the uncertainty surrounding the outlook even greater than usual. We at the Federal Reserve will have to remain exceptionally alert and flexible as we continue to assess how best to promote sustainable economic growth and price stability in the United States.\n\nCharlotte and the Carolinas: Personal Connections\nI’d like now to speak a bit about Charlotte and the region from a personal as well as an economic perspective. My family has a long connection with Charlotte. My maternal grandparents, originally immigrants from Eastern Europe, moved here from Connecticut when my mother was a teenager, and she finished high school here. My parents met while attending different campuses of the University of North Carolina--my father at UNC-Chapel Hill, my mother at UNC-Greensboro (then a women’s college). I was raised from early childhood in the small town of Dillon, South Carolina, about two hours from here. My family settled in Dillon because my paternal grandfather bought a drug store there in 1941, and my father and his brother followed in his footsteps as town pharmacists. In Dillon, a town that was always very short of the more regular kind of doctor, my father and uncle were popularly known as Dr. Phil and Dr. Mort, and the prescriptions they dispensed were often accompanied by their free advice on maintaining good health.\n\nI often visited my maternal grandparents’ home on Cumberland Avenue in Charlotte, sometimes with my parents and sometimes on my own, and I have many fond memories of those visits. A short walk from their home was a park where my grandfather often took me to feed the ducks that lived on a lake there. The name of that spot--Freedom Park--was sufficiently like my grandparents’ surname--Friedman--for me as a small child to conclude that it was actually called Friedman Park. I was suitably impressed by the honor the city authorities had apparently given my grandparents. Grandpa Friedman taught me to play chess when I was five or six; he let me win at first, but after a few years I was no longer a pushover, and the games became very, very serious. Grandma Friedman was a wonderful cook, and if you dig deep enough into the archives of the Charlotte Observer, you will find a large photo of a much younger me under the headline, “Ben Loves Grandma’s Blintzes,” together with her recipe for that dish. Unfortunately, my grandmother died when I was thirteen, and when my grandfather came to live with us in Dillon, the regular trips to Charlotte ended. I am pleased to say, though, that my connection to this city has since been re-established, as my parents have retired to Charlotte, and my brother (a lawyer in town) and his family live here, too. So I still feel like an honorary Charlottean as well as a Carolinian.\n\nIn my periodic visits to the Carolinas, I have been enormously impressed by the social and economic changes that have emerged in what has aptly been called the New South. This transformation has not been easy. In Dillon in the 1960s, I attended a segregated public school; but I did have African-American friends, and one of them was instrumental in persuading me to attend Harvard University--a critical step, as it turned out, in my life and career. Now, in Dillon, Charlotte, and elsewhere in the Carolinas, I see increasing cooperation among people of different races and backgrounds to achieve common civic and economic goals.\n\nThe Transformation of the Economy in the Carolinas\nEconomically speaking, Carolinians have faced the same challenge confronting many other parts of the country, that is, to replace jobs lost in old-line manufacturing industries by creating jobs in services such as health care and hospitality while simultaneously adapting to globalization and advancing technology. Here as elsewhere, the Carolinas have met this challenge through education and by building on regional strengths. As I’ve stressed on previous occasions, the quality of the workforce is the single most important factor in an economy’s success. In a rapidly changing world, economically valuable skills can be maintained only through learning that extends beyond traditional schooling to encompass training and re-training well into the middle years of life.\n\nNorth Carolina offers a good example of these trends. In the past decade, the state has lost about one-third of the manufacturing jobs it had at the beginning of the decade--a loss of about 250,000 jobs. About 60 percent of the losses occurred in the textile and apparel industries. In the textile mills in particular, employment across the state is down two-thirds from the level of ten years ago. In the furniture industry, which accounts for the largest share of the remaining job losses in North Carolina manufacturing, employment in the state has dropped from 82,000 in 1999 to less than 51,000. The Charlotte area itself has experienced a number of plant closings, including the 2003 shutdown of the Pillowtex plant in nearby Kannapolis.\n\nThere is, of course, another side to the coin of economic change here. Despite losing an average of 25,000 manufacturing jobs each year over the past decade, North Carolina has managed a net increase of 44,000 jobs per year in total nonfarm employment over the same period. Those two numbers together imply that, on average, North Carolina has enjoyed an annual net gain of 69,000 nonmanufacturing jobs. The largest net increases have been in education and health care, professional and business services, and the leisure and hospitality sector. Thus, like many other vibrant regions of the country, the Charlotte area has grown by developing a high-productivity service economy.\n\nIndeed, what happened to the former Pillowtex site itself is a good metaphor for the transformation under way in the region. Though the loss of manufacturing jobs is painful, the ongoing development of the Pillowtex site as the North Carolina Research Campus illustrates this region’s ability to shift resources from industries that are shrinking to those that are expanding The North Carolina Research Campus is a public-private, 350-acre life sciences hub near Charlotte that includes partnerships with Duke University, the University of North Carolina, the North Carolina Community College System, and other institutions of higher education. This is one high-profile example, but the transformation has also been happening in less dramatic fashion through the development of hundreds of smaller businesses throughout the region.\n\nEven within the manufacturing sector, a number of firms--typically smaller operations with relatively few employees--have begun to exploit nontraditional niches. Some recent examples of emerging industrial operations across the state include primary metal manufacturing, machinery production, and the manufacture of nonwoven fabrics (Employment Security Commission of North Carolina, 2007). That last category includes a remarkably wide variety of engineered fabrics, ranging from those used to make doctors’ and nurses’ operating-room garb to some used in roofing materials; those products are especially interesting because they represent a small but fast-growing segment of specialty textiles within the broader textile industry.\n\nThe transformation of this region has been aided by its reputation as a desirable location in which to live and work. Census data and statistics from interstate moving companies indicate a heavy flow of people moving into Charlotte from other states, including large numbers of educated workers. Overall, the area has gained an average of 39,000 net new residents every year since 1997. (You probably feel that you see all those people every day in traffic.) Without a doubt, Charlotte’s status as one of the preeminent financial centers of the country lies behind much of the inflow.\n\nImportance of Charlotte as a Financial Services Center\nCharlotte’s roots as a financial center stretch back two centuries. From 1800 to 1848, the city was the center of U.S. gold production, and a branch of the U.S. Mint operated here from 1837 to 1913. More recently, North Carolina’s legal framework has been important to the growth of the banking system. Because the state had long allowed in-state branch banking, homegrown banks here had a head start when interstate banking became possible--first regionally, in the mid-1980s, and then nationally with the 1994 passage of the Riegle-Neal Interstate Banking and Branching Efficiency Act (Hills, 2007).\n\nNorth Carolina’s early adoption of branch banking is a good example of a “first mover” gaining a strategic advantage. The banking statutes allowed banks in North Carolina to become larger than their counterparts in other states and helped them develop expertise in running larger branch networks. The result has been a rapid increase in the size of banks located in the state: In 1970, only three banks from the entire South, including two from North Carolina, were among the fifty largest U.S. banks ranked by assets, today, three of the top ten U.S. banks are headquartered in Charlotte alone (Hills, 2007).\n\nOne of the key advantages of Charlotte and other metropolitan centers in North Carolina has been the ability to attract and retain educated workers: Among adults aged 25 or older, 31 percent in metro centers hold at least a bachelor’s degree, versus 17 percent in rural areas (U.S. Census Bureau, 2006). In some cases, growing urban areas like Charlotte are the beneficiaries of a positive dynamic: The city’s modern, service-oriented economy attracts skilled and educated workers; the presence of a skilled workforce attracts new firms to the area and also promotes the development of amenities such as high-end restaurants and cultural activities; these opportunities and amenities then attract additional highly skilled workers.\n\nThe Challenge of Education in North Carolina\nCities like Charlotte will probably continue to attract highly educated and skilled workers from other areas of the country, but improving the skills of local workers--especially those displaced by industries in decline--remains critical for both urban and rural areas in the state. Four-year institutions play an important role in meeting that challenge, but they are not the sole means for developing workforce skills. For example, in the 2004-05 school year, the North Carolina Community College System served nearly 780,000 students in fifty-eight institutions. The average community college student in the state is thirty years old and likely working while attending school (North Carolina Community College System, 2006). Because they offer education closely tailored to employer demands in the local workplace, community colleges in North Carolina, as elsewhere, play an essential role in training and retraining workers. Moreover, they do so at a relatively low cost. In general, we must move beyond the view that education is something that takes place only in K-through-12 schools and four-year colleges, as important as those are. Education and skills must be provided flexibly and to people of any age.\n\nI will close my comments on education with a pitch for financial literacy. In today’s complex financial marketplace, a basic understanding of financial tools and markets and an appreciation of the need to budget, save, invest, and borrow wisely are critical to the financial health of every individual. The Federal Reserve is advancing financial literacy locally through the Charlotte Branch of the Federal Reserve Bank of Richmond. The Branch has active partnerships with organizations involved in financial literacy and economic education, including among others Jump$tart, Junior Achievement, LifeSmarts, Communities in Schools, the North Carolina Council on Economic Education, and the North Carolina Bankers Association. In short, advancing financial literacy is a high priority at the Federal Reserve.\n\nConclusion\nI’d like to conclude by again expressing my gratitude to the Charlotte Chamber of Commerce for honoring me with its Citizen of the Carolinas Award. I am indeed proud to consider myself a citizen of the Carolinas and of the region. Thank you very much.\n\n\n\nReferences\nEmployment Security Commission of North Carolina (2007). “Employment and Wages by Industry, 1990 to Most Recent,”  www.ncesc.com/lmi/industry/industrymain.asp.\n\nHills, Thomas D. (2007). “The Rise of Southern Banking and the Disparities among the States following the Southeastern Regional Banking Compact (225 KB PDF),”  Balance Sheet, vol. 11, pp. 57-104, http://studentorgs.law.unc.edu/ncbank/balancesheet.\n\nNorth Carolina Community College System (2006). “Get the Facts,”  press release, July 3, www.ncccs.cc.nc.us/News_Releases/GetTheFacts.htm.\n\nU.S. Census Bureau (2006). “2005 American Community Survey,” www.census.gov/acs .",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20071129a.htm",
        "title": "National and regional economic overview",
        "date": "11/29/2007"
    },
    {
        "content": "November 28, 2007\n\nVice Chairman Donald L. Kohn\n\nC. Peter McColough Series on International Economics, Council on Foreign Relations, New York, New York\n\n\n\nI thought it might be useful to start this session with a few thoughts on some of the issues facing central banks as they deal with the consequences of the recent turbulence in financial markets.1  This list is not comprehensive:  I have concentrated on the issues associated with our roles as monetary policy makers and providers of liquidity--and even in that category I cannot address all the issues in the short time allotted.\n\nLike every other period of financial turbulence, this one has been marked by considerable uncertainty.  Central banks, other authorities, and private-market participants must make decisions based on analyses made with incomplete information and understanding.  The repricing of assets is centered on relatively new instruments with limited histories--especially under conditions of stress; many of them are complex and have reacted to changing circumstances in unanticipated ways; and those newer instruments have been held by a variety of investors and intermediaries and traded in increasingly integrated global markets, thereby complicating the difficulty of seeing where risk is coming to rest.\n\nOperating under this degree of uncertainty has many consequences.  One is that the rules and criteria for taking particular actions seem a lot clearer in textbooks or to many commentators than they are to decisionmakers.  For example, the extent to which institutions are facing liquidity constraints as opposed to capital constraints, or the moral hazard consequences of policy actions, are inherently ambiguous in real time.  Another consequence of operating under a high degree of uncertainty is that, more than usually, the potential actions the Federal Reserve discusses have the character of \"buying insurance\" or managing risk--that is, weighing the possibility of especially adverse outcomes.  The nature of financial market upsets is that they substantially increase the risk of such especially adverse outcomes while possibly having limited effects on the most likely path for the economy.\n\nMoral Hazard\nCentral banks seek to promote financial stability while avoiding the creation of moral hazard.  People should bear the consequences of their decisions about lending, borrowing, and managing their portfolios, both when those decisions turn out to be wise and when they turn out to be ill advised.  At the same time, however, in my view, when the decisions do go poorly, innocent bystanders should not have to bear the cost.\n\nIn general, I think those dual objectives--promoting financial stability and avoiding the creation of moral hazard--are best reconciled by central banks' focusing on the macroeconomic objectives of price stability and maximum employment.  Asset prices will eventually find levels consistent with the economy producing at its potential, consumer prices remaining stable, and interest rates reflecting productivity and thrift.  Such a strategy would not forestall the correction of asset prices that are out of line with fundamentals or prevent investors from sustaining significant losses.  Losses were evident early in this decade in the case of many high-tech stocks, and they are in store for houses purchased at unsustainable prices and for mortgages made on the assumption that house prices would rise indefinitely.\n\nTo be sure, lowering interest rates to keep the economy on an even keel when adverse financial market developments occur will reduce the penalty incurred by some people who exercised poor judgment.  But these people are still bearing the costs of their decisions and we should not hold the economy hostage to teach a small segment of the population a lesson.\n\nThe design of policies to achieve medium-term macroeconomic stability can affect the incentives for future risk-taking.  To minimize moral hazard, central banks should operate as much as possible through general instruments not aimed at individual institutions.  Open market operations fit this description, but so, too, can the discount window when it is structured to make credit available only to clearly solvent institutions in support of market functioning.  The Federal Reserve's reduction of the discount rate penalty by 50 basis points in August followed this model.  It was intended not to help particular institutions but rather to open up a source of liquidity to the financial system to complement open market operations, which deal with a more limited set of counterparties and collateral.\n\nThe Effects of Financial Markets on the Real Economy \nRelated developments in housing and mortgage markets are a root cause of the financial market turbulence.  Expectations of ever-rising house prices along with increasingly lax lending standards, especially on subprime mortgages, created an unsustainable dynamic, which is now reversing.  In that reversal, loss and fear of loss on mortgage credit have impaired the availability of new mortgage loans, which in turn has reduced the demand for housing and put downward pressures on house prices, which have further damped desires to lend.  We are following this trajectory closely, but key questions for central banks, including the Federal Reserve, are, What is happening to credit for other uses, and how much restraint are financial market developments likely to exert on demands outside the housing sector?\n\nSome broader repricing of risk is not surprising or unwelcome in the wake of unusually thin rewards for risk taking in several types of credit over recent years.  And such a repricing in the form of wider spreads and tighter credit standards at banks and other lenders would make some types of credit more expensive and discourage some spending, developments that would require offsetting policy actions, other things being equal.  Some restraint on demand from this process was a factor I took into account when I considered the economic outlook and the appropriate policy responses over the past few months.\n\nAn important issue now is whether concerns about losses on mortgages and some other instruments are inducing much greater restraint and thus constricting the flow of credit to a broad range of borrowers by more than seemed in train a month or two ago.  In general, nonfinancial businesses have been in very good financial condition; outside of variable-rate mortgages, households are meeting their obligations with, to date, only a little increase in delinquency rates, which generally remain at low levels.  Consequently, we might expect a moderate adjustment in the availability of credit to these key spending sectors.  However, the increased turbulence of recent weeks partly reversed some of the improvement in market functioning over the late part of September and in October.  Should the elevated turbulence persist, it would increase the possibility of further tightening in financial conditions for households and businesses.  Heightened concerns about larger losses at financial institutions now reflected in various markets have depressed equity prices and could induce more intermediaries to adopt a more defensive posture in granting credit, not only for house purchases, but for other uses a well.\n\nLiquidity Provision and Bank Funding Markets\nCentral banks have been confronting several issues in the provision of liquidity and bank funding.  When the turbulence deepened in early August, demands for liquidity and reserves pushed overnight rates in interbank markets above monetary policy targets.  The aggressive provision of reserves by a number of central banks met those demands, and rates returned to targeted levels.  In the United States, strong bids by foreign banks in the dollar-funding markets early in the day have complicated our management of this rate.  And demands for reserves have been more variable and less flexible in an environment of heightened uncertainty, thereby adding to volatility.  In addition, the Federal Reserve is limited in its ability to restrict the actual federal funds rate within a narrow band because we cannot, by law, pay interest on reserves for another four years.\n\nAt the same time, the term interbank funding markets have remained unsettled.  This is evident in the much wider spread between term funding rates--like libor--and the expected path of the federal funds rate.  This is not solely a dollar-funding phenomenon--it is being experienced in euro and sterling markets to different degrees.  Many loans are priced off of these term funding rates, and the wider spreads are one development we have factored into our easing actions.  Moreover, the behavior of these rates is symptomatic of caution among key marketmakers about taking and funding positions, and this is probably impeding the reestablishment of broader market trading liquidity.  Conditions in term markets have deteriorated some in recent weeks.  The deterioration partly reflects portfolio adjustments for the publication of year-end balance sheets.  Our announcement on Monday of term open market operations was designed to alleviate some of the concerns about year-end pressures.\n\nThe underlying causes of the persistence of relatively wide-term funding spreads are not yet clear.  Several factors probably have been contributing.  One may be potential counterparty risk while the ultimate size and location of credit losses on subprime mortgages and other lending are yet to be determined.  Another probably is balance sheet risk or capital risk--that is, caution about retaining greater control over the size of balance sheets and capital ratios given uncertainty about the ultimate demands for bank credit to meet liquidity backstop and other obligations.  Favoring overnight or very short-term loans to other depositories and limiting term loans give banks the flexibility to reduce one type of asset if others grow or to reduce the entire size of the balance sheet to maintain capital leverage ratios if losses unexpectedly subtract from capital.  Finally, banks may be worried about access to liquidity in turbulent markets.  Such a concern would lead to increased demands and reduced supplies of term funding, which would put upward pressure on rates.\n\nThis last concern is one that central banks should be able to address.  The Federal Reserve attempted to deal with it when, as I already noted, we reduced the penalty for discount window borrowing 50 basis points in August and made term loans available.  The success of such a program lies not in loans extended but rather in the extent to which the existence of this facility helps reassure market participants.  In that regard, I think we had some success, at least for a time.  But the usefulness of the discount window as a source of liquidity has been limited in part by banks' fears that their borrowing might be mistaken for accessing emergency loans for troubled institutions.  This \"stigma\" problem is not peculiar to the United States, and central banks, including the Federal Reserve, need to give some thought to how all their liquidity facilities can remain effective when financial markets are under stress.\n\nConclusion\nIn response to developments in financial markets, the Federal Reserve has adjusted the stance of monetary policy and the parameters of how we supply liquidity to banks and the financial markets.  These adjustments have been designed to foster price stability and maximum sustainable growth and to restore better functioning of financial markets in support of these economic objectives.  My discussion today was intended to highlight some of the issues we will be looking at in financial markets as we weigh the necessity of future actions.  We will need to assess the implications of these developments, along with the vast array of incoming information on economic activity and prices, for the future path of the U.S. economy.  As the Federal Open Market Committee noted at its last meeting, uncertainties about the economic outlook are unusually high right now.  In my view, these uncertainties require flexible and pragmatic policymaking--nimble is the adjective I used a few weeks ago.  In the conduct of monetary policy, as Chairman Bernanke has emphasized, we will act as needed to foster both price stability and full employment.\n\nFootnotes\n\n1.  These are my views and are not necessarily those of my colleagues on the Federal Open Market Committee. Return to text",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20071128a.htm",
        "title": "Financial Markets and Central Banking",
        "date": "11/28/2007"
    },
    {
        "content": "November 16, 2007\n\nGovernor Randall S. Kroszner\n\nAt the Conference on Competitive Markets and Effective Regulation, Institute of International Finance, New York, New York\n\nThe disruption in financial markets over the past few months has altered the economic landscape appreciably.  This morning, I would like to talk about the analytical framework that I use to help guide my thinking about the appropriate path for monetary policy.  Just as an analytical risk-management framework is fundamental to the safe and sound operation of large banking and financial institutions, it is also, I believe, essential for sound monetary policy-making.\n\nThe Federal Open Market Committee recently announced that it will increase the frequency and expand the content of its economic projections.  A clear understanding of the risk-management framework should help improve the public's comprehension of these expanded announcements and thereby, I believe, improve the efficacy of monetary policy actions and the overall functioning of the economy.  After briefly outlining the risk-management approach, I will discuss the application of this analytical framework to the current economic and financial environment.  The views I will express today are my own and do not necessarily reflect those of my colleagues on the Federal Open Market Committee (FOMC).\n\nMonetary Policy and Uncertainty about the Outlook\nIn thinking about an analytical framework for guiding policy decisions, it is helpful to understand two basic principles of monetary policy making:  First, monetary policy must be set on the basis of forecasts; and, second, because forecasts are subject to substantial uncertainty, policymakers must adopt aspects of risk management in their approach.  After all, as the Nobel laureate Niels Bohr once said (in a comment later attributed to Yogi Berra), \"prediction is very difficult, especially when it's about the future.\"  But, of course, policymakers cannot wait until the economy's overall performance comes clearly into view before judging the most appropriate stance for monetary policy.  Rather, given the long and variable lags between changes in interest rates and changes in economic activity, as well as lags in receiving data about economic activity, monetary policy must be forward looking.\n\nMoreover, not only must policymakers decide on the path the economy is most likely to take over the medium term, they must also judge how the macroeconomic risks are arrayed around that path.  In the case of the FOMC, when risks appear to be too heavily weighted to one side or the other, it may be appropriate to adjust the stance of policy to better align the array of future possible outcomes with our dual mandate of promoting maximum sustainable employment and stable prices over the longer term.\n\nFor some time now, central bankers have used principles of risk management to help inform their monetary policy decisions.  In essence, through risk management, monetary policymakers consider economic scenarios that may have a relatively low probability of occurring but may have very adverse consequences if they do occur.  Households, business managers, and policymakers all face the need to reduce the risks surrounding such relatively improbable but potentially high-cost events.  Buying auto insurance is an example of risk management intended to lessen the adverse (financial) consequences of an automobile accident, and driving carefully is a risk-management technique that can reduce the probability of having an accident.  Thus, as with most economic decisions, we face a trade-off, in this example between the benefits of risk management in mitigating very adverse outcomes versus the costs of auto insurance and the additional travel time required by more careful driving.\n\nIn the case of monetary policy, the possibility of adverse consequences arises in part from the uncertainty that surrounds the outlook for economic activity and inflation at any given time--uncertainty often referred to as macroeconomic risk.  Generally, the main benefit of policy actions to lower macroeconomic risk is that they reduce the probability of a very adverse outcome occurring while raising the odds of achieving an outcome relatively close to the forecasted central tendency.  In the language of statistics, the actions are intended to move some of the probability mass from the tail of the distribution of possible outcomes toward the center of the distribution.  But, of course, there's no such thing as a free lunch--what I mean is that here, too, we face a tradeoff:  The cost of the risk-management action is the possibility that it may have increased the odds of inflation moving beyond some acceptable range or of economic activity moving significantly away from its longer-run sustainable path.\n\nFinancial Market Turmoil and Risks to the Outlook\nTurning from the abstract to the concrete, the FOMC's decisions to ease policy at its September and October meetings were, in my view, governed in part by an attempt to manage the macroeconomic \"tail risks\" facing the U.S. economy.  To no small extent, stresses in financial markets contributed significantly to those macroeconomic risks.\n\nEarly in the summer, losses on securities backed by subprime home mortgages sparked concerns about the performance of a range of securities with exposure to those mortgages, and investors quickly pulled back.  With secondary markets under significant strain, a number of large originators announced substantial changes to their subprime-mortgage programs, and the volume of newly issued securities backed by subprime mortgages fell precipitously and stayed low.  The same forces also damped investors' willingness to fund other types of nonconforming mortgages (that is, loans that do not qualify for sale to Fannie Mae and Freddie Mac).  The issuance of securities backed by mortgages in so-called \"alt-A\" pools--which consist of loans to borrowers who typically have higher credit scores than subprime borrowers but whose applications may have other risky attributes--declined markedly.  Prime jumbo home-purchase loans continued to be originated, but the spread of rates on such loans over those on conforming loans was considerably higher than earlier in the year, and banks reportedly tightened lending standards and other loan terms, as well.  In terms of the macroeconomic outlook, this substantial tightening in mortgage markets seemed likely to increase the odds of a deeper and more long-lasting contraction in the housing market.\n\nThe mounting losses from securities backed by subprime mortgages led investors to lose confidence in structured finance products more generally--investors apparently had relied heavily on credit-rating firms to determine the quality of these often-complex instruments rather than perform their own independent evaluations.  Once losses began to mount, the earlier lack of due diligence by investors brought them to the realization that they had an insufficient amount of information about these products, and the normal price-discovery mechanism began to break down.1  The concerns about structured credit products led to severe problems in markets for asset-backed commercial paper (ABCP), where spreads spiked and programs had difficulty issuing paper with maturities longer than a few days.\n\nThe largest banks began to worry about difficult-to-forecast expansions of their balance sheets--they recognized that they might have to provide backup funding to commercial paper programs that were no longer able to over their paper, and they faced substantial challenges syndicating the leveraged loans they had underwritten.  As a result, banks became very protective of their liquidity, and interbank funding markets came under considerable pressure.  Moreover, the extent to which banks were protecting their liquidity and the pressures on their balance sheets raised in my mind the possibility that banks could tighten lending standards and terms significantly and thereby exert a material drag on economic growth.\n\nPolicy Deliberations in September and October\nFrom my perspective, the outlook for future economic growth had thus weakened appreciably by the time of the September FOMC meeting, and the downside risks associated with that weakened outlook had increased markedly.  Most notably, the incoming data and the continuing strains in mortgage markets suggested that the outlook for housing activity had become gloomier.  Moreover, although the data in hand did not provide direct evidence of spillovers from the housing sector to other segments of the economy, a heightened sense of uncertainty about economic and financial conditions had the potential to lead households and businesses to be cautious about spending.  Moreover, conditions in financial markets could be expected to improve slowly at best; and even if conditions did begin to normalize, credit conditions appeared likely to remain much tighter than they had been in the spring.\n\nThe actions taken at the September meeting were intended to help forestall some of the adverse effects on the broader economy that might otherwise arise from the disruptions in financial markets--that is, to reduce the macroeconomic tail risk, and to promote moderate growth over time--or, put another way, to increase the likelihood of achieving a desirable path for economic activity.  Economic growth appeared likely to run below its potential for a while.  Given incoming inflation data to the favorable side and reasonably well anchored inflation expectations, the costs of easing policy, measured in terms of heightened inflation risks, seemed relatively low.\n\nBy the time of the FOMC meeting on October 31, it was evident that real gross domestic product (GDP) had grown at a solid pace in the third quarter.  However, the information that had come in during the preceding six weeks suggested an intensification of the housing correction.  In addition, although some financial markets showed signs of reduced stress, normal price discovery was still absent from many markets.  In particular, mortgage markets remained significantly impaired, and survey information suggested that banks had tightened terms and standards considerably for a range of credit products, including mortgages.\n\nAll told, FOMC members saw the stance of monetary policy as being still somewhat restrictive, partly because of the effects of tighter credit conditions on aggregate demand.  Accordingly, the FOMC lowered the target federal funds rate an additional 25 basis points, to 4-1/2 percent.  The further reduction in the target rate was intended to lessen the extent of macroeconomic risk in the economy and to increase the likelihood of achieving moderate growth over time.  In terms of the potential inflation costs associated with that action, the incoming data on consumer prices continued to be encouraging and inflation expectations appeared to remain reasonably well anchored.  But, the recent run-up in energy prices and the fall in the foreign exchange value of the dollar suggested to me that, since the September FOMC meeting, somewhat greater inflation risks had raised the costs of easing policy to manage the macroeconomic risks.  Nonetheless, on balance, I viewed the benefits of that action as being greater than the costs.\n\nLooking forward, one feature of monetary policy to keep in mind is that, all else equal, each successive action in the same direction tends to lower the incremental benefits and to raise the incremental costs of additional actions.  For example, unless underlying economic conditions or risks change substantially, reductions in the target federal funds rate tend to be associated with decreasing incremental benefits in terms of further mitigating tail risks and with increasing incremental costs in terms of the potential for inflation to increase.  In the current context, I would be especially concerned if inflation expectations were to become unmoored and will watch both market-based and survey-based measures of inflation expectations closely.\n\nIn sum, in September and again in October, I believed that achieving the FOMC's statutory mandate to promote price stability and maximum employment would best be accomplished by lowering the target federal funds rate.  With those actions, however, the downside risks to economic growth now appear to be roughly balanced by the upside risks to inflation.  I would add that the limited data and information received since the October FOMC meeting have not changed my thinking in this regard.\n\nEconomic Outlook\nWith the September and October policy actions as a backdrop, I would now like to provide a more detailed description of where I think the U.S. economy is most likely to be headed in the near-term and further ahead.\n\nIn the near term, the economy will probably go through a rough patch during which a number of economic data releases may be downbeat.  Home sales seem likely to weaken further given the difficulties faced by some potential buyers in obtaining a mortgage and, perhaps, some concerns on their part about buying into a falling market.  Moreover, with the inventory of unsold homes already quite high relative to sales, a further weakening of demand is likely to prompt additional cutbacks in construction.\n\nIn the mortgage market, two considerations suggest that conditions for subprime borrowers will get worse before they get better.  First, the bulk of the first interest rate resets for adjustable-rate subprime mortgages are yet to come.  On average, from now until the end of 2008, nearly 450,000 subprime mortgages per quarter are scheduled to undergo their first reset, eventually causing a typical monthly payment to rise about $350, or 25 percent.  Second, the weakness in house prices and the resulting limit on the build-up of home equity will hinder the ability of subprime borrowers to refinance out of their mortgages into less expensive loans; as a result, more borrowers will be left with a mortgage balance that exceeds the value of the house.\n\nThe likely consequences of these two factors--imminent interest rate resets and the difficulty of refinancing--will be yet higher rates of delinquencies and foreclosures over the next several quarters and, in turn, additional downward pressure on house prices.  The overhang of unsold homes also will weigh heavily on the prices of newly built and existing homes.  From a risk-management perspective, these housing-related factors together pointed the FOMC toward its recent easings in policy to mitigate the likely resulting drag on economic activity over the coming quarters.\n\nElsewhere in the economy, increases in consumer spending can be expected to be limited for a while by the effects of sluggish home prices on household balance sheets.  Consumer spending will also be constrained, although probably to a lesser extent, by the drain on aggregate purchasing power caused by mortgage resets; that drain will likely be exacerbated by the current run-up in energy prices.  Meanwhile, heightened uncertainty in the business sector could lead to reductions in capital spending plans.  Nonetheless, indicators of business sentiment from a variety of national and regional surveys have remained generally favorable.  Moreoever, conditions in the labor market, although a bit softer recently, are still relatively solid, and foreign demand for U.S. goods and services remains strong.\n\nLooking further ahead, the current stance of monetary policy should help the economy get through the rough patch during the next year, with growth then likely to return to its longer-run sustainable rate.  As conditions in mortgage markets gradually normalize, home sales should pick up, and homebuilders are likely to make progress in reducing their inventory overhang.  With the drag from the housing sector waning, the growth of employment and income should pick up and support somewhat larger increases in consumer spending.  And as long as demand from domestic consumers and our export partners expands, increases in business investment would be expected to broadly keep pace with the rise in consumption.\n\nSuch developments would not be likely to fuel a rise in inflation expectations or in actual inflation.  Sizable increases in energy and food prices have contributed to a pickup in headline inflation this year, but the developments on core inflation (which excludes prices for food and energy items) have been moving in a more favorable direction.  For example, data released yesterday show that the overall consumer price index (CPI) rose 3.5 percent over the twelve months ending in October, a gain about 2 percentage points greater than that of the preceding twelve months.  In contrast, core CPI inflation was 2.2 percent over the twelve months ending in October, 1/2 percentage point less than the rate a year ago.\n\nAgainst this backdrop, inflation expectations have remained reasonably well anchored.  The prices of oil and other commodities continue, of course, to be a source of major uncertainty for the overall inflation outlook.  Currently, quotes from futures markets suggest that investors expect food and energy prices to come off their recent peaks next year.  That said, I think it's also fair to say that political and economic developments around the world, not to mention the vagaries of the weather, make any forecast of oil and other commodity prices highly uncertain.  Moreover, spillovers from the latest run-up in crude oil prices could begin to put upward pressure on core inflation.\n\nSo, to sum up, the economy seems poised to grow for a while at a noticeably slower pace than it did during the summer, in part because of lower home sales, less residential construction, and generally smaller increases in consumer and business spending.  A sequence of data releases consistent with the rough patch for economic activity that I expect in coming months would not, by themselves, suggest to me that the current stance of monetary policy is inappropriate.  I will, of course, continue to carefully assess the implications of the incoming economic data and financial market developments for economic growth prospects and the outlook for inflation.\n\nFederal Reserve Communications\nLet me close by saying a few words about the other important decision made by the FOMC at the October meeting--a decision to adjust our communications strategy.  As Chairman Bernanke explained in more detail two days ago, the Committee decided that it would release its economic projections four times per year rather than semiannually and that it would extend those projections from two years to three.  The new information will include a description of the economic considerations underlying the forecasts, a discussion of the sources of risk to the overall outlook, and a sense of the dispersion of views among policymakers.  The changes adopted by the FOMC are an important advance:  They will provide additional insight into the Committee's outlook, they will help households and businesses better understand and anticipate our policy decisions, and they will enhance our accountability for the decisions we make.  The Committee's decision to provide this expanded information represents the latest step in an ongoing process, extending back at least thirty years, to foster that accountability and improve the public's understanding of U.S. monetary policy making.  I hope that my remarks this morning also prove helpful in fostering a better understanding of how the risk-management tradeoffs affect the policy deliberations of the FOMC as it pursues its dual mandate of promoting maximum employment and price stability over the longer term.\n\nFootnotes\n\n1.  Randall S. Kroszner (2007), \"Recent Events in Financial Markets,\" speech delivered at the Institute of International Bankers Annual Breakfast Dialogue, Washington, D.C., October 22. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner2007116a.htm",
        "title": "Risk Management and the Economic Outlook",
        "date": "11/16/2007"
    },
    {
        "content": "November 14, 2007\n\nChairman Ben S. Bernanke\n\nAt the Cato Institute 25th Annual Monetary Conference, Washington, D.C.\n\nMontagu Norman, the Governor of the Bank of England from 1921 to 1944, reputedly took as his personal motto, \"Never explain, never excuse.\"  Norman's aphorism exemplified how he and many of his contemporaries viewed the making of monetary policy--as an arcane and esoteric art, best practiced out of public view.  Many central bankers of Norman's time (and, indeed, well into the postwar period) believed that a certain mystique attached to their activities and that allowing the public a glimpse of the inner workings would only usurp the prerogatives of insiders and reduce, if not grievously damage, the effectiveness of policy.\n\nNorman's perspective on central banking now seems decidedly quaint.  Over the past few decades, central banks around the world have worked assiduously to become more open about their activities.  In fact, Norman's own institution, the Bank of England, has in recent years been a leading exponent of increased transparency in central banking.  Monetary policy makers have adopted a range of methods to improve their communication with the public, including timely announcements of policy actions, expanded testimony before members of the legislature, the release of minutes of policy meetings, frequent public speeches, and the regular publication of reports about the economy and monetary policy.  This increased openness is a welcome development for several reasons.  Most importantly, monetary policy makers are public servants whose decisions affect the life of every citizen; consequently, in a democratic society, they have a responsibility to give the people and their elected representatives a full and compelling rationale for the decisions they make.  Good communications are a prerequisite if central banks are to maintain the democratic legitimacy and independence that are essential to sound monetary policy making.\n\nIn addition, a considerable amount of evidence indicates that central bank transparency increases the effectiveness of monetary policy and enhances economic and financial performance in several ways.  First, improving the public's understanding of the central bank's objectives and policy strategies reduces economic and financial uncertainty and thereby allows businesses and households to make more-informed decisions.  Second, if practitioners in financial markets gain a better understanding of how policy is likely to respond to incoming information, asset prices and bond yields will tend to respond to economic data in ways that further the central bank's policy objectives.  For example, if market participants understand that arriving information about the economy increases the likelihood of certain policy actions, then market interest rates will tend to move in a way that reinforces the expected actions, effectively supporting the goals of the central bank.  Third, clarity about the central bank's policy objectives and strategy may help anchor the public's long-term inflation expectations, which can substantially improve the efficacy of policy and the overall functioning of the economy.  Finally, open discussion of the central bank's analyses and forecasts invites valuable input and feedback from the public.\n\nThe benefits of an open and accountable policymaking process have spurred the Federal Reserve, along with other major central banks, to take a number of actions over the years to increase its transparency.  Appropriately, given the unique position of the Federal Reserve and the sensitivity of financial markets to its communications, these steps have generally been incremental in nature; but, taken together, they have substantially increased the ability of the American public to understand and to anticipate monetary policy decisions.\n\nThe Congress has also long been aware of the importance of Federal Reserve transparency and accountability; in particular, a series of resolutions and laws passed in the 1970s set clear policy objectives for the Federal Reserve and required it to provide regular reports and testimony to the Congress.1  Since 1975, the Federal Reserve has presented testimony twice each year to the Congress on the conduct of monetary policy.  These semiannual presentations have become an important vehicle for the U.S. central bank to make known its views on the outlook and on the appropriate stance of policy.  Other notable milestones in the Federal Reserve's progress toward greater openness include:  in 1979, the first release of semiannual economic projections; in 1983, the first publication of the Beige Book, which summarizes information about economic conditions received from the Federal Reserve System's business contacts; in 1994, the decision to release a postmeeting statement when policy actions had been taken; in 2000, the beginning of the practice of issuing a statement after each meeting of the Federal Open Market Committee (FOMC) and including in the statement an assessment of the balance of risks to the Committee's objectives; in 2002, adding the FOMC roll call vote to the postmeeting statement; and in 2005, the speeding up of the release of the minutes of FOMC meetings, from a delay of some six or seven weeks to just three weeks.\n\nIn testimony to the Congress at the time of my nomination as Chairman, in 2005, I pledged to continue the trend toward greater openness sustained under Chairman Greenspan.  In so doing, I stressed the importance of continuity with the policies and strategies that have served the American economy well.  Any further changes, I promised, would come only pursuant to a consensus within the FOMC that those changes would enhance the Committee's ability to pursue its dual mandate of achieving maximum employment and price stability.\n\nToward that end, the FOMC has engaged in extensive deliberations over the past year or so to consider further steps toward greater transparency.  Guided by a subcommittee chaired by Board Vice Chairman Donald Kohn, the FOMC reviewed the full range of our communications with the public.2  As indicated in a statement issued by the FOMC today, these discussions have led to a decision to increase the frequency and expand the content of the publicly released economic projections that are made by Federal Reserve Board members and Reserve Bank presidents.  As I mentioned, the Federal Reserve has published economic projections for almost thirty years, and, indeed, the Federal Reserve was the first major central bank to release such projections.3  Today's announcement builds on that foundation.  In the remainder of my remarks I will describe the changes that we plan to make, and then explain why I believe that, collectively, they represent an important further step toward greater transparency.\n\nToward More Informative Economic Projections\nBecause monetary policy affects spending and inflation with a lag, policy decisions must be based on an assessment of medium-term economic prospects.  Thus, the Committee cannot fully explain its policy decisions without sharing its economic outlook with the public and the Congress.  To provide more-timely information about the evolving outlook, the Federal Reserve will release FOMC participants' economic projections four times each year, rather than twice each year as we have done previously.\n\nProjections will continue to be released in February and July of each year to coincide with the semiannual Monetary Policy Report and the associated testimony to the Congress.  Two additional sets of projections will be published in conjunction with the minutes of the FOMC meetings held around the beginnings of the second quarter and the fourth quarter of the year (in 2008, the April and October meetings).  The first expanded set of projections will be released next week, on November 20, together with the minutes of the October FOMC meeting.  The horizon of the projections will be extended from two years to three.  The projections released next week will extend through 2010.4\n\nEach of the participants in the FOMC meeting--including the Federal Reserve Board members and all the Reserve Bank presidents‑‑will, as in the past, provide projections for the growth of real gross domestic product (GDP), the unemployment rate, and core inflation (that is, inflation excluding the prices of food and energy items).  In addition, participants will now provide their projections for overall inflation.  Both overall and core inflation will continue to be based on the price index for personal consumption expenditures (PCE).5\n\nProjections will continue to be made independently by each FOMC participant under the assumption of \"appropriate\" monetary policy, that is, the future evolution of the federal funds rate judged by that participant to be the one most likely to foster economic outcomes that satisfy the Federal Reserve's dual mandate.  Following past practice, we will publish the central tendency and the range of the projections for each variable and each year.6  We will also publish a comparison with the previous set of quarterly projections; a chart showing central tendencies and ranges for each variable; and charts showing the distribution of participants' projections and how that distribution has changed since the previous release.\n\nAccompanying the numerical projections will be a discussion--a projections \"narrative\" if you will--that summarizes participants' views of the major forces shaping the outlook, discusses the sources of risk to that outlook, and describes the dispersion of views among policymakers.  By providing a medium-term perspective, the narrative will complement the discussion of shorter-term developments contained in the minutes.  We will also provide qualitative information about participants' views on both the uncertainty and the balance of risks surrounding the outlook, together with quantitative historical information on the typical range of projection errors.7  Of course, the specific material provided and its form of presentation may change over time as we gain experience and receive feedback.\n\nBenefits of the Enhanced Projections\nThe enhanced projections will provide the public with several types of useful information.  In particular, I find it helpful to think of the projections as functioning in three different ways:  as a forecast, as a provisional plan, and as an evaluation of certain long-run features of the economy.\n\nMost obviously, the projections reflect the economic forecasts of FOMC participants and as such should provide the public with greater and more-timely insight into the Committee's views of the economic outlook and the risks to that outlook.  Of course, because our knowledge of the structure of the economy is incomplete and future economic disturbances are often unforeseeable, economic forecasting is a highly uncertain enterprise.8  The only economic forecast in which I have complete confidence is that the economy will not evolve along the precise path implied by our projections.  Nevertheless, as I have already noted, because policy affects spending and inflation with a lag, Committee members have no choice other than to make medium-term forecasts--provisional and subject to uncertainty though they may be.  Providing more information about these forecasts, including discussions of the factors underlying the forecasts and of FOMC participants' assessments of the risks to the Committee's objectives, should improve the public's understanding of the rationale for the current stance of monetary policy and any changes to that stance.  The public will also be better able to judge the extent to which the Committee's rationale is reasonable and persuasive.\n\nThe projections also function as a plan for policy--albeit as a rough and highly provisional one.  As I mentioned earlier, FOMC participants will continue to base their projections on the assumption of \"appropriate\" monetary policy.  Consequently, the extended projections will provide a sense of the economic trajectory that Committee participants see as best fulfilling the Federal Reserve's dual mandate, given the initial conditions and the constraints posed by the structure of the economy.  To illustrate, consider the question of the length of time over which a central bank should aim to restore price stability following an unwanted increase in inflation.  A central bank that places weight on both employment and price stability, like the Federal Reserve, would not attempt to disinflate immediately or establish a fixed time frame for the restoration of price stability.  Rather, the optimal expected time required for completing the disinflation would depend on a host of factors, including the size of the initial deviation from price stability, the initial state of the real economy (for example, the level of unemployment), whether the rise in inflation resulted from transitory or more persistent sources, the extent to which inflation expectations are well anchored, and so on.  In circumstances in which disinflationary policy is necessary, the extended economic projections would make clear that the Federal Reserve is committed to maintaining price stability, but they would also provide some indications about what the Committee views as the most appropriate pace of disinflation, given the state of the economy and the requirements of the dual mandate.  In like fashion, the speed at which policy aims to return the economy to its sustainable rates of growth and employment following a period of resource slack should depend in part on the nature and extent of inflation risks, among other considerations.  More generally, the extended projections will convey additional information about the Committee's policy strategies and thus help augment the Committee's transparency, predictability, and accountability.\n\nFinally, the extended projections will embody information about FOMC participants' evaluations of certain long-run features of the economy, evaluations determined both by the economy's structure and by the Committee's policy objectives.  Because of the extension of the projection horizon to three years, participants' inflation projections will convey more information regarding their views about the measured rate of inflation that, in the long run, is consistent with the Committee's dual objectives of maximum employment and price stability.  Were price stability the only objective mandated for the Federal Reserve, the FOMC presumably would strive to achieve zero inflation, properly measured--that is, the optimal measured inflation rate would deviate from zero on average only by the amount of the estimated measurement error in the preferred inflation index.  But under the Federal Reserve's dual mandate, the determination of the appropriate long-run inflation rate must take account of factors that may affect the efficient functioning of the economy at very low rates of inflation, such as the risk that the zero lower bound on nominal interest rates might hinder the effectiveness of monetary policy.  Thus, the (properly measured) long-run inflation rate that best promotes the dual mandate is likely to be low but not zero.\n\nUltimately, households and businesses care about the overall, or \"headline,\" rate of inflation; therefore, the FOMC should refer to an overall inflation rate when evaluating whether the Committee has met its mandated objectives over the long run.  For that reason, the Committee has decided to publish projections for overall inflation as well as core inflation.  In its policy statements and elsewhere, the Committee makes frequent reference to core inflation because, in light of the volatility of food and energy prices, core inflation can be a useful short-run indicator of the underlying trend in inflation.  However, at longer horizons, where monetary policy has the greatest control over inflation, the overall inflation rate is the appropriate gauge of whether inflation is at a rate consistent with the dual mandate.\n\nFOMC participants will continue to couch their inflation projections in terms of PCE inflation, rather than, say, inflation as measured by the consumer price index, because the PCE index is generally thought to provide the single most comprehensive and theoretically compelling measure of consumer prices.  That said, no single measure of inflation is perfect, and the Committee will continue to monitor a range of measures when forming its view about inflation prospects.\n\nThe lengthening of the projection horizon will also allow the public to infer more about FOMC participants' current judgments about the rate of GDP growth and the unemployment rate that the economy can sustain in the long run.  Over time, effective monetary policies foster rates of growth and unemployment close to their long-run sustainable rates.  However, in contrast to inflation, which in the long run is determined by monetary policy, the rates of economic growth and unemployment that can be sustained in the long run are determined by many factors outside the control of central banks.  Among these factors are the advance of technology, entrepreneurial activities, the growth in the size of the labor force, the rate at which workers acquire new skills, tax and regulatory policies, and the efficiency of labor markets in matching workers with positions.  Consequently, the long-run sustainable rates of economic growth and unemployment should be viewed as constraints on what monetary policy can achieve and not as variables that policymakers can freely choose.  In addition, estimates of sustainable rates of growth and unemployment have been shown to be highly uncertain at any point in time; and they may vary significantly over time in light of new information and changes in the structure of the economy.  Thus, the longer-run projections of growth and unemployment should be treated with considerable caution.\n\nRelationship to Inflation Targeting\nAs you may know, I have been an advocate of the monetary policy strategy known as inflation targeting, used in many countries around the world.  Inflation targeting is characterized by two features:  an explicit numerical target or target range for inflation and a high degree of transparency about forecasts and policy plans.  The steps being taken by the Federal Reserve, I must emphasize, are intended only to improve our communication with the public; the conduct of policy itself will not change.  Nonetheless, in light of the changes to communications we are undertaking, one might fairly ask how the Federal Reserve's approach relates to inflation targeting.\n\nA superficial drawback of inflation targeting is its very name, which suggests a single-minded focus on inflation to the exclusion of other goals.  In fact, the practice of monetary policy in an inflation-targeting regime is not necessarily inconsistent with a dual mandate such as that given to the Federal Reserve; indeed, most if not all inflation-targeting central banks today practice \"flexible\" inflation targeting, meaning that they take account of other economic goals besides price stability--notably economic growth, employment, and financial stability--when making policy decisions.  Moreover, a broad consensus exists among central banks, whether they have an explicit numerical target for inflation or not, that maintaining low and stable inflation over time is the best means by which monetary policy can promote economic efficiency and maximize the nation's economic welfare.  Thus, at least since the stabilization of U.S. inflation in the 1980s, the Federal Reserve's approach to monetary policy has had much in common with that of central banks that describe themselves as inflation targeters.\n\nNevertheless, some aspects of inflation targeting may be less well suited to the Federal Reserve's mandate and policy practice.  In particular, although inflation-targeting central banks certainly pay attention to economic growth and employment, their formal accountability is often largely couched only in terms of a price-stability objective.  Likewise, the communication strategies of inflation-targeting central banks tend to be focused on the formal inflation objective and the horizon over which that objective will be achieved.  As I have emphasized today, the Federal Reserve is legally accountable to the Congress for two objectives, maximum employment and price stability, on an equal footing.  My colleagues and I strongly support the dual mandate and the equal weighting of objectives that it implies.  Of course, as I have discussed, the Federal Reserve's influence over these objectives differs importantly in the long run:  Monetary policy determines the long-run inflation rate, whereas the factors that influence the sustainable rates of growth and employment in the long run are largely outside the central bank's control.  Still, over time, monetary policy must strive to foster rates of growth and employment close to their long-run sustainable rates.  The Federal Reserve must thus be accountable for the effects of its policies on the real economy as well as on inflation.  The enhanced projections that I have described today will provide additional information pertinent to both halves of the Federal Reserve's mandate.\n\nAt a more technical level, the Federal Reserve differs from most inflation-targeting central banks in that it provides information about the independent projections of Committee members rather than a single collective forecast.  To some extent, that difference reflects the relatively large size of the FOMC and the geographic dispersion of Committee participants; those factors would make the development of a detailed consensus forecast quite difficult as a practical matter.  But, as I will discuss briefly, such a diversity of viewpoints can enhance the quality of policy decisions.\n\nThe Diversity of the Committee\nAn important strength of the Federal Open Market Committee is its diversity.  The Board members and Reserve Bank presidents who sit around the table at each meeting of the FOMC bring a wide range of perspectives to the deliberations that reflect the participants' professional backgrounds, the regions of the country with which they are most familiar, and their differing approaches to economic and policy analysis.  The task participants face at each meeting is to forge a rough consensus regarding the outlook, the risks to the Committee's objectives, and the appropriate policy response.  Of course, it is not always possible--indeed, it would be rather unusual--to come to a set of conclusions that fully represent the views of every participant.  But the process of searching for common ground is itself an important aspect of how the Committee operates.  Diversity of views drives the Committee to adopt an eclectic approach and thus serves to limit the risk that a single viewpoint or analytical framework might become unduly dominant.\n\nThe changes to the projections process announced today preserve the important role played by this diversity of perspectives.  As I have noted, Committee participants will continue to produce individual projections that reflect their judgments about the state of the economy and their approaches to policy.  From the internal perspective, I expect the more frequent sharing of projections and the additional information they contain will improve our discussions and policy debates.  From the external perspective, the public will gain additional and more frequent information about both the central tendencies and diversity of participants' views.  In particular, the additional narrative material that will accompany the numerical projections will illuminate both the consensus of opinion and the differences in judgments that may emerge.\n\nConclusion\nThe communications strategy of the Federal Reserve is a work in progress.  I believe that the changes announced by the FOMC today are an important advance:  The changes will provide a more-timely insight into the Committee's outlook, will help households and businesses better understand and anticipate how our policy decisions respond to incoming information, and will enhance our accountability for the decisions we make.  But the changes are also evolutionary, in that they build on long-established practices; in that respect, they represent just one more step on the road toward greater transparency at the Federal Reserve.  The Committee will continue to look for ways to improve the accountability and public understanding of U.S. monetary policy making.\n\nFootnotes\n\n1.  The key measures were the House Concurrent Resolution 133, in 1975; the Federal Reserve Reform Act of 1977; and the Full Employment and Balanced Growth Act of 1978 (the Humphrey-Hawkins Act). Return to text\n\n2.  Gary Stern, president of the Federal Reserve Bank of Minneapolis, and Janet Yellen, president of the Federal Reserve Bank of San Francisco, were the other members of the subcommittee. Return to text\n\n3.  Economic projections were first published in 1979 to fulfill the Board's legislated requirement to report on \"prospects for the future.\"  Return to text\n\n4.  The projection period of the first three releases each year will cover the current year and the subsequent two years.  The fourth release each year will add a year to the projection horizon.  Thus, the first three sets of projections in 2008 will be for the period 2008 through 2010, whereas the fourth set of projections will extend to 2011. Return to text\n\n5.  Participants will no longer provide projections for the growth of nominal GDP.  These now seem relatively less useful to the public, given participants' projections for real GDP growth and overall inflation. Return to text\n\n6.  The range for each variable in a given year includes all participants' projections, from lowest to highest, for that variable in the given year.  The central tendencies exclude the three highest and three lowest projections for each variable in each year. Return to text\n\n7.  A Board staff paper discussing the historical forecasting record of the Federal Reserve and other institutions will be released on November 20, simultaneously with the release of the expanded projections. Return to text\n\n8.  The historical data we will provide on forecast errors will starkly illustrate this point. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20071114a.htm",
        "title": "Federal Reserve Communications",
        "date": "11/14/2007"
    },
    {
        "content": "November 13, 2007\n\nGovernor Randall S. Kroszner\n\nAt the Standard & Poor's Bank Conference 2007, New York, New York\n\n\n\nI would also like to offer thanks and extend congratulations to all the parties involved in the successful adoption of Basel II. This includes staff at each of the U.S. banking agencies, who worked tirelessly and with incredible determination and patience to see this rulemaking to its completion, as well as the principals at the other agencies, who worked very hard to find common ground and develop a rule that would serve the public interest and satisfy each of our agencies' objectives. Of course, I would also like to thank the many industry participants--some of whom may be here today--who spent considerable time and effort providing valuable comments on our proposals over the past several years. Your contributions made the final rule a much better product. Developing Basel II was like running a marathon, and even though some of us may have hit the wall and wanted to drop out at mile 20, we persevered and successfully reached the finish line. I am proud of what we have all accomplished.\n\nCompletion of the U.S. Final Rule\nIn the banking industry, most of the innovation and evolution in risk-management practices occur on a continuous basis, generally in small steps. Updates to banking regulations, on the other hand, typically occur in large jumps. As was the case with Basel I nearly twenty years ago, I consider the adoption of Basel II to be a major step forward in banking regulation in the United States. Importantly, we are also working on an additional proposal, known as the standardized approach, to offer non-core banks a set of regulatory capital requirements that have more risk sensitivity than the current Basel I rules, but less complexity than the advanced approaches in the Basel II final rule.\n\nOne of the main reasons we were able to complete the Basel II final rule successfully, I believe, was our renewed focus on the fundamental rationale for developing Basel II: enhancing the safety and soundness of the U.S. banking system by providing more-risk-sensitive capital requirements for our largest, most complex banks and improving risk management practices at those institutions. Moreover, we endorsed the notion that the U.S. rule would foster international consistency and be less burdensome on banks if it adhered more closely to the international Basel II framework finalized in 2006--and if it also aligned as closely as possible with what banks themselves were doing for risk management.\n\nThese were principles that I emphasized as I represented the Federal Reserve in interagency discussions, and I believe my counterparts shared these views. Perhaps our ability to refocus stemmed from a fresh set of comments received on our proposals, our renewed commitment to getting things right, and the infusion of some new approaches brought to the table. Regardless, we of course owe a huge debt of gratitude to our predecessors, who broke the hard ground in the long U.S. rulemaking process.\n\nReasons for Adopting Basel II\nI would like to return briefly to our reasons for adopting Basel II, since it is useful to remember why we undertook so much effort to see it through. While the existing Basel I capital regime was a major step forward when introduced in 1988, it has become outdated for large, complex banking organizations. Retaining Basel I for these institutions would have widened the gap between their regulatory capital requirements and their actual risk profiles, generating further incentives for regulatory arbitrage to take advantage of that gap.\n\nIn contrast to the simple risk-bucketing approach of Basel I, in which exposures to obligors of varying creditworthiness were given the same capital treatment, the new Basel II rules require banks to distinguish among the credit quality of individual borrowers. For example, under Basel I almost all first-lien residential mortgage exposures are subject to the same risk weight regardless of the borrower's creditworthiness, whereas Basel II provides for a more refined differentiation of low- versus high-credit-quality mortgage borrowers. Likewise, Basel I is inadequate for dealing with capital markets transactions such as highly structured asset-backed securities. Basel II, on the other hand, provides a much more refined approach by requiring banks to hold capital commensurate with the actual risks of such transactions. Recent market events highlight why a robust and independent assessment of risk on the part of banks is so important. The enhanced risk-sensitivity of the Basel II advanced approaches creates positive incentives for banks to lend to more-creditworthy counterparties and to lend against good collateral, by requiring banks to hold more capital against higher-risk exposures.\n\nThe Federal Reserve's role as the nation's central bank reinforces our belief in the importance of maintaining prudent and risk-sensitive capital requirements for financial institutions. Financial stability is enhanced when banks' regulatory capital measures adequately reflect risk, as well as when banks continually improve their risk-management practices. Since the Basel II regime is far superior to the current Basel I regime in aligning regulatory capital requirements with risk and fostering continual improvements in risk management for our largest and most complex banking organizations, I believe it will contribute to a more resilient financial system as a whole.\n\nIn addition, let me emphasize that the Basel II regulatory capital framework establishes a more coherent relationship between regulatory measures of capital adequacy and the day-to-day risk management conducted by banks. That is, it builds on risk-management tools, such as credit-risk rating systems and economic capital, that are already in use at sophisticated financial institutions. As a result, Basel II will be better able than the current system to adapt over time to innovations in banking and financial markets and will reduce incentives for arbitrage that arise from the gap between what the regulators require and what sound economic risk management requires.\n\nMoving Ahead with Basel II Implementation\nNext Steps for Supervisors\nI used the analogy of running a marathon earlier, describing how the final rule represented a finish line of sorts for the U.S. banking agencies. Alas, I'm afraid that we cannot rest because in fact we have simply passed the baton from the runner in the first stage of the race--rule finalization--to the runner in the next stage--implementation. Successful implementation of Basel II will require additional hard work and determination. As most of you know, the agencies have for some time been preparing for Basel II implementation by working to integrate Basel II into our day-to-day supervisory processes. With completion of the final rule, we must now be ready to pace ourselves through another long, intensive, but ultimately rewarding, effort.\n\nThe agencies are already working hard to foster consistency across banks and across the agencies. We are building upon the cooperation already established through our work on the final rule and our efforts to prepare supervisory staff for the Basel II qualification process. Our supervisory staffs have been meeting regularly for some time to align qualification approaches, iron out any differences, and ensure that each bank subject to Basel II is treated appropriately and consistently. We also remain attentive to the way in which the framework is implemented in other countries, so that we can minimize the burden placed on banks by having to meet multiple national rules. I hope our decision to align the definition of default for wholesale exposures more closely with the definition used internationally, for example, sends a positive signal about our intentions to increase cross-border consistency and reduce unnecessary burdens that can distract banks from one of the fundamental goals of Basel II--improving risk management.\n\nOf course, the agencies need to move ahead with Basel II implementation carefully and with our eyes wide open. The advanced approaches are a significant change from our current, time-tested, risk-based capital rules, and we have therefore embedded the transitional safeguards set forth in the agencies' 2006 proposal into the U.S. Basel II rule. These safeguards will help ensure that capital levels remain strong and that we have sufficient opportunity to assess the framework before full implementation. Importantly, we also are retaining the leverage ratio and our existing prompt-corrective-action framework.\n\nAs noted in the agencies' July press release, we are committed to a robust and transparent study of the framework during the transitional phase to assess its overall effectiveness, and we will address any material deficiencies that we identify. This study should include active and meaningful dialogue among the agencies, the industry, market participants, Congress, and other interested parties. This is consistent with my view that whenever regulators undertake a major regulatory change, a careful and thorough empirical review of the effectiveness of the regulation is extremely valuable. Such a review can help assess whether the goals for the rule are being met, whether the benefits of the rule exceed the costs, and how the rule can be made more effective and less burdensome.\n\nIn addition to this study, during and after the transitional phase we will be relying upon ongoing, detailed analyses to evaluate continuously the results of the new framework in operation. A primary objective of this ongoing review will be to ensure that capital levels remain prudent. For example, we will respond if we see unreasonable declines in capital requirements at individual institutions that do not appear to be supported by either those banks' own internal capital-adequacy assessments or by our supervisory view of those institutions' risks and how well those risks are managed.\n\nAs has long been the case with our capital rules, we expect that adjustments to the capital framework will be made over time to address industry and market developments, any potential shortcomings in the rule identified in our review and analysis during implementation, and new and improved techniques of risk management.\n\nNext Steps for Bankers\nCompletion of the Basel II rulemaking process means that banks adopting the new rule must also gear up their efforts. Of course we recognize the substantial work that bankers have undertaken over the past several years to prepare themselves for Basel II. But, understandably, they have had to wait for completion of the final rule to see how the agencies would articulate certain requirements--some of them quite detailed. Therefore, it would seem that bankers need to read the rule very carefully and take time to understand how their own bank will be able to meet its requirements.\n\nAs stated in the final rule, and as the U.S. agencies articulated several years ago, the key instrument in the qualification process is a bank's implementation plan. This written implementation plan, approved by a bank's board of directors, must describe in detail how the bank complies, or intends to comply, with the rule's qualification requirements.\n\nSpecifically, the plan must describe how the bank intends to address the gaps it has identified between its existing practices and the qualification requirements set forth in the rule for the advanced approaches, covering all consolidated subsidiaries. The implementation plan also must include objective, measurable milestones--including delivery dates--and a target date when the bank expects its advanced approaches to be fully operational. The bank must establish and maintain a comprehensive and sound planning and governance process to oversee implementation efforts, and must demonstrate to its supervisor that it meets the qualification requirements.\n\nBanks subject to the final rule on a mandatory basis, the core banks, have up to six months to adopt an implementation plan. Of course, banks may always submit their plans earlier, and I understand that a number of core banks are working toward that goal. This deadline for submission of plans by core banks is intended to prevent delays in starting implementation efforts. However, the final rule provides flexibility and gives banks adopting Basel II ample time to fully meet the qualification requirements once they have adopted an implementation plan. Specifically, a bank's plan may include developmental goals for full implementation for up to thirty-six months from the effective date of the final rule.\n\nAs supervisors, we will take the qualification requirements seriously, expecting banks to meet both the letter and the spirit of those requirements. Thus, we strongly recommend that banks undertake their own sober and frank appraisal of their ability to meet the final rule. Systems development can take time, for example, and it is important to make sure that these systems function appropriately. While I believe that expeditious adoption of Basel II will have significant benefits, it is of the utmost importance that the implementation not be rushed but be undertaken thoughtfully and deliberately.\n\nAfter a bank has submitted a credible implementation plan to its primary supervisor, it must then begin a parallel run lasting at least four consecutive calendar quarters, during which the bank's supervisor must determine the bank's compliance with the qualification requirements to be satisfactory. During the parallel run, a bank remains subject to the Basel I risk-based capital rules for all applicable regulatory and supervisory purposes, but the bank also must calculate its capital ratios using the advanced approaches and report pertinent information to its supervisor. It is only upon notification from its supervisor that a bank can move into a series of three transitional periods (each lasting at least one year), during which the cumulative reductions of the bank's risk-based capital requirements are limited. Supervisory approval is needed to move to a subsequent transitional floor-level and then to move from the transitional floors to stand-alone use of the Basel II rules.\n\nImportantly, as bankers move forward with implementation, they should not lose sight of Pillars 2 and 3, which may ultimately be more important to the success of Basel II than Pillar 1, which has received the bulk of the attention so far. Under Pillar 2, banks are required to have an internal process--which will be subject to rigorous supervisory review--for ensuring that they are holding enough overall capital to support their entire risk profile. Thus, Pillar 2 should be a key area of focus for banks implementing Basel II. The preamble to the final rule describes the steps that supervisors will take under Pillar 2, namely that supervisors will take into account a bank's internal capital-adequacy assessment process--known as its ICAAP--as well as the bank's compliance with the minimum capital requirements set forth in this rule, and all other relevant information.\n\nThe agencies expect banks to implement and continually update the fundamental elements of a sound ICAAP--identifying and measuring material risks, setting capital-adequacy goals that relate to risk, and ensuring the integrity of internal capital-adequacy assessments. A bank is expected to hold adequate capital against all of its material risks, particularly those risks not covered or not adequately quantified in the risk-based capital requirements--such as liquidity risk or interest-rate risk in the banking book. In general, a bank's ICAAP should reflect an appropriate level of conservatism to account for uncertainty in risk identification, risk mitigation or control, quantitative processes, and any use of modeling. In most cases, this conservatism will result in levels of capital or capital ratios above minimum regulatory requirements to be regarded as adequate.\n\nPillar 3 is a key mechanism for banks to communicate to market participants about their risk profiles, their associated levels of capital, and the manner in which they are meeting the requirements in the final rule. In addition to providing information about its various components of regulatory capital and its minimum capital requirements and ratios, a bank must disclose information about how it measures and manages credit risk, operational risk, equity risk, and interest-rate risk in non-trading activities, as well as the range of risks related to securitizations. For example, a bank has to describe the operation of its credit risk rating system as well as the data used in parameter estimates for credit losses.\n\nSome of these disclosure requirements will be new for banks but others are already required by, or are consistent with, existing U.S. generally accepted accounting principles, Securities and Exchange Commission disclosure requirements, or bank regulatory reporting requirements. As a strong believer in market discipline and the importance of information in market transactions, I believe Pillar 3 will improve bank disclosures about risk profiles and enhance discussions between bankers and market participants about risk-management practices.\n\nOf course, while we want to promote consistency, we must also allow bankers some flexibility in meeting the Basel II requirements and permit a reasonable amount of diversity of practices across banking organizations. Such flexibility will allow banks to use and readily improve their existing risk-measurement and risk-management practices. More to the point, as supervisors we should actively encourage such improvements. As we move forward, we encourage banks to raise issues as they try to meet the rule's requirements; in other words, we want banks to maintain an ongoing dialogue about implementation with their supervisors, who stand ready to answer questions and assist banks in interpreting Basel II requirements.\n\nStandardized Approach Proposal for Non-Core Banks\nBefore concluding, I would like to discuss the agencies' additional plans for revising capital rules, specifically plans for those banks not subject to the advanced approaches of Basel II. Some commentators on the earlier Basel II and Basel IA proposals voiced concerns that adoption of a new capital framework for the largest and most complex U.S. banking organizations could disadvantage other U.S. banking organizations, particularly the smaller banks. We understand that banks not required to adopt Basel II are facing a choice about whether to opt-in to the advanced approaches. Some of these banks may be sophisticated institutions that exhibit sound risk management but do not quite meet the criteria to be core banks. The agencies recognize that such institutions should be afforded an alternative for more-risk-sensitive capital requirements, but one that is not as complex as the advanced approaches.\n\nIn this regard, the agencies have responded by committing to proposing a \"standardized\" approach instead of Basel IA. Specifically, the staffs are currently working on a notice of proposed rulemaking that would implement some of the simpler approaches for both credit risk and operational risk from the Basel II framework--referred to as the standardized approach. The proposal is being developed as an optional risk-based capital framework for all banking organizations that are not required to adopt the advanced approaches. We also expect to retain our existing Basel I-based regulatory capital framework for those smaller banks that would prefer to remain under that regime.\n\nThe proposal for the standardized approach will take into consideration relevant commentary received in response to the Basel IA and Basel II proposed rules that were published in late 2006 and should, in essence, modernize the Basel I-based rules without imposing a substantial implementation burden. Among other things, the proposal is being designed both to provide greater differentiation across corporate exposures based on borrowers' underlying credit quality and to recognize a broader spectrum of credit-risk mitigation techniques. The agencies are also considering how to implement Pillars 2 and 3 of the Basel II framework in the standardized proposal in a manner that is commensurate with banks' complexity and risk profiles. Our goal is to realize the benefits of these two pillars without imposing excessive regulatory burden and without creating competitive advantages or disadvantages for different types of banks.\n\nI expect this proposal to be presented to the Board for consideration within the next several months, and I encourage all interested parties to review and comment on this proposal once it has been issued. We are keenly aware of the need for capital requirements to make sense from the standpoint of both safety-and-soundness and competitiveness; we recognize that a one-size-fits-all approach is probably not the best for our banking system, in light of our wide range of institutions. We remain sensitive to the principle that if we have multiple regulatory capital frameworks, they must work together to improve the safety and soundness of our entire banking system without artificially creating competitive inequalities. Our goal is to have the standardized approach ready for implementation concurrently with the start of the first Basel II transition phase.\n\nConclusion\nThe U.S. banking agencies have reached an important milestone in adopting the final rule for Basel II. Our focus on the fundamentals of improving risk management consistent with safety and soundness, and on international consistency, has been key to achieving this success. Obviously, however, effective implementation of Basel II is as important as, if not more important than, the rulemaking process. It is imperative that we observe how the new rule works in practice--assessing carefully both its advantages and its limitations. I am confident that both banking organizations and the supervisory community are up to the challenge. It is also important to modernize the existing Basel I-based regulatory capital framework to improve the risk sensitivity of capital requirements at the non-core banks, by offering a standardized option.\n\nFinally, we should all bear in mind that implementation of Basel II--and, more significantly, the improvements in risk measurement and management that will be required--will not be a one-time event, but rather an ongoing process. Basel II is designed to accommodate innovation and change as markets and risk-measurement and -management evolve over time. As one marathon is completed, yet another begins.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20071113a.htm",
        "title": "Implementing Basel II in the United States",
        "date": "11/13/2007"
    },
    {
        "content": "November 07, 2007\n\nGovernor Kevin Warsh\n\nTo the New York Association for Business Economics, New York, New York\n\nDetractors have argued that this end-of-history thesis is a \"mirage\" (Kagan, 2007). Intervening geopolitical events appear to have upended the notion that History is all but written, that ideological conflicts will inexorably abate. At the very least, the experience of the past dozen years suggests that the hopes and aspirations for a less confrontational political and ideological era may be delayed. But even as the geopolitical situation has become more complex, and as conflicts among ideologies become more obvious, I wonder whether we are seeing--in certain other spheres of social interaction--some coalescing of views, some mutual understanding of what makes for a strong and enduring system. Here, befitting an audience of business leaders and economists, I am thinking of economic policy and, in particular, monetary policy in market-based economies.2\n\nAs recently as several months ago, some may have been tempted to believe that, in the realm of economic policy, we were on the precipice of the end of history. The seemingly benign financial and economic conditions of the past few years may have appeared to be approaching this nirvana.3\n\nIf the end of economic history were at hand, what would it look like? You would almost assuredly find strong, synchronized global economic growth; favorable inflation readings and anchored inflation expectations; low global risk premiums and low term premiums; muted volatility across asset markets; a relatively free flow of products and services across national boundaries; interconnected financial markets; deep, robust, and highly liquid secondary trading; the democratization of credit and growing access to capital; and, finally, a stance of monetary policy approximating its natural equilibrium.\n\nThink of the economic environment of several months ago, prior to the more recent financial turmoil. The appearance of these sorts of benign indicators could have provided useful testimony in support of the end-of-history thesis in the economic realm. But then what is to be made of the global liquidity shock that crested in August and continues to manifest itself to this day?\n\nIn my view, recent financial turmoil should not be considered an accident of history. Rather, it is History's latest reminder to policymakers and market participants alike that we ought to be humble in our convictions and cautious in our deeds as we seek a better understanding of what makes a strong and stable economic and financial system. The lessons learned from natural experiments in economics during recent decades offer great promise that macroeconomic performance can be improved. But as empiricists outside of our \"dismal science\" would remind us, there are few control groups in economics. As a result, the definitive account has not been written. Nor has the practical conduct of policy yet evolved to the point at which financial crises and economic downturns can be wholly avoided. As a result, I will argue, the end of history may have to wait awhile.\n\nLet me first discuss the evolution of financial markets. Next, I will highlight the evolution of monetary policy and financial stability. Finally, I will turn to more recent financial and monetary events.\n\nEvolution of Financial Markets\nDuring the past several years, the cause of economic freedom and the culture of capitalism have appeared firmly on the march. Founding ideologies aside, countries' economies are more connected by virtue of increased trade of products and services. Free markets, technological innovation, and instant communications are the watchwords of the global economy. And the dissemination of financial innovation has gained new converts. London, Hong Kong, Singapore, and Shanghai increasingly seek to challenge New York as the center of global financial markets. Competitive national ambitions, perhaps, have demanded an embrace of economic liberalism to build wealth even among those regimes that may prefer something closer to autocracy in their non-economic dealings. Democratizing credit to enlarge the global middle class, efficiently allocating capital, and dispersing risk exposures--these remain compelling practices for private market participants throughout the world.\n\nThese financial market tailwinds bolstered worldwide economic growth, which is near record levels. In 2006, the U.S. economy grew 3 percent, while growth outside the United States averaged nearly 4 percent as all major regions racked up solid gains. So far this year (through the third quarter in the United States and through the second quarter elsewhere), U.S. output growth managed to continue apace, while growth for the rest of the world picked up to more than 4 percent. Clearly the global economy has been firing on all cylinders, with emerging Asia taking the lead. There can be little doubt that this macroeconomic performance is due, at least in part, to the increased adoption of liberal economic practices around the globe. Perhaps, then, the case for the end of economic history is not without some evidentiary backing and persuasive appeal.\n\nEvolution of Monetary Policy and Financial Stability\nIn a recent survey paper, David Laidler identifies three tenets that embody the current consensus on the conduct of monetary policy in advanced economies (Laidler, 2007). First, market economies are inherently self-righting; second, open economies perform best under flexible exchange rates; and third, central bankers should focus on price stability as their long-run objective.4\n\nMonetary policy in the past couple of decades can, almost assuredly, claim far more successes than failures. Look no further than measures of consumer price inflation in the advanced economies. Median inflation (as defined by the International Monetary Fund) has held near 2 percent for the decade. This performance has helped anchor inflation expectations, which, in turn, has helped damp the pass-through of supply-related price shocks. The low inflation rate has also permitted central banks to respond more forcefully to output fluctuations and more opportunistically to ensure low and stable prices.\n\nThe improved inflation performance has come not at the expense of, but in conjunction with, output stability. Researchers have documented the so-called Great Moderation, in which the U.S. economy has achieved a marked reduction in the volatility of both real gross domestic product and core inflation over the past twenty years or so. Most economists would probably attribute at least some portion of the Great Moderation in the growth of real output to monetary policy.\n\nDespite the record of steadier economic performance, bouts of financial market instability continue to arise. Indeed, in my judgment, the inexorable march of technological change and creative destruction in a market economy all but assures the occurrence of such episodes (Schumpeter, 1942). To be sure, it is the recurrence of such episodes that supplies kindling to the debate on the appropriate balance between flexible, competitive financial markets and regulations that could limit the exuberance of market responses, irrational or otherwise.\n\nIn the episode at hand, early-cycle increases in housing prices, escalating returns of leveraged buyouts, and lower all-in financing costs made possible by structured-finance products highlighted new profit-making opportunities for providers and users of capital. As economic historians remind us, more entities are thus drawn into the activity until competition pushes its boundaries beyond the point of positive returns (Kindleberger, 1996). Eventually, some shock to the system exposes the fragility, and the shock is often followed by fear and overreaction before a new equilibrium is established.\n\nIn these circumstances, financial markets often suffer from episodes of widespread illiquidity amid a rise in risk aversion among investors. Policymakers and market participants know with certainty that investors' risk perceptions and preferences will change and that stresses will recur, but predicting their onset, scope, or duration is exceedingly difficult. Surely, policymakers and market participants have advanced in their knowledge and in the adaptability of their tools to help mitigate these negative effects on real economic activity. But as the events of 2007 make clear, the latest chapter in economic history should remind us of the habitual, but perhaps not immutable, drivers of human behavior in financial markets.\n\nRecent Financial Developments\nThe consequences of the liquidity shock of 2007 on the financial markets and the real economy are still playing out in real time. It is premature to delineate lessons learned with complete assurance. The facts, nonetheless, can perhaps be placed in some narrative context, drawing on the experience of prior bouts of financial instability.\n\nHistory, of course, is far from a perfect teacher. After all, history does not repeat itself; it only appears to do so. In that regard, the causes and consequences of market turmoil are still insufficiently understood for the end of history to be declared. And as I briefly discuss the phases often accompanying financial market turmoil--retrenchment, reliquification, revaluation, review and refinement--you will recognize that these phases are neither discrete nor complete as year-end approaches.\n\nAfter several years of strong domestic and global growth, financial markets appeared highly accommodative for issuers and investors. Many willing investors purchased complex financial products convinced that they would achieve outsized returns because the future would look like the recent past. On the other side of the trade, originators operated under the presumption that secondary markets would remain liquid. And the resulting market-clearing prices across a range of asset classes were predicated on a world of modest risk premiums, low credit spreads, and plentiful liquidity. Market confidence ultimately begot complacency (Warsh, 2007).\n\nBy mid-summer, complacency was upended. What followed was a rapid period of retrenchment, the first phase of financial market turmoil. As concerns about losses on subprime mortgages and securitized products intensified, investors withdrew liquidity and markets became impaired. Investors revisited, almost anew, the quality of the information about their assets. Financial intermediaries also pulled back from making markets in many products, and the engines of financial innovation were all but turned off. As the strains in financial markets intensified, many of the largest financial institutions became jealously protective of their liquidity and balance sheet capacity. Amid heightened volatility and diminished market functioning, they became more concerned about the risk exposures of their counterparties and other potential contingent liabilities. For some, that process lasted days and weeks; for others, it may yet continue for many months.\n\nIn the second phase--reliquification--financial institutions decided on the new liquidity levels and capital ratios at which they were prepared to conduct business. Many banks became markedly less willing to provide funding to customers, including other banks. Given reduced confidence in their ability to quantify and price risks, balance sheet capital remained a scarce commodity. As a result, both overnight and term interbank-funding markets were pressured considerably. Even today, some banks face potentially large needs for dollar funding, and their efforts to manage their liquidity may be contributing to pressures in global money markets and foreign exchange swap markets. More broadly, many financial institutions appear hesitant to put opportunistic capital to work.\n\nThe next phase--revaluation--requires that the new prices be established in accordance with the new financial environment. The process of price discovery appears to be quicker and more assured among corporate credits. Think, for example, of the markets for high-yield and leveraged loans, in which risk spreads have returned to more moderate levels. We have seen significant evidence that the process of revaluation in other previously disrupted markets is also under way. Investors are differentiating risks, for example, among asset-backed commercial paper programs, and many spreads have moderated. Revaluation, however, may be a slower, tougher slog in the mortgage markets for those vintages in which the underlying asset quality is less certain. How quickly asset markets substantially complete the revaluation phase depends on the speed with which stakeholders regain comfort in their ability to value these assets.\n\nFinancial markets rarely normalize in a steady, linear fashion. More often, as market sentiments sway between fear and greed, asset prices fluctuate and seek support (volume) before establishing new trading ranges. That pattern is particularly pronounced when the underlying economic fundamentals are less certain. Hence, the next phases--review and refinement--are the hardest to predict with precision. As circumstances dictate, some financial institutions will review and refine their capital ratios, risk metrics, and business imperatives and proceed forward. Others may find that, in the course of review, they return to the phases of retrenchment and reliquification. Central bankers are prudent to stay alert to these changes.\n\nWithout a doubt, then, this is a time of testing. Stakeholders will need to discern whether they are witnessing some impairment of the financial sector, or merely a realignment of the competitive landscape. Moreover, I suspect that some of the more complex structured products and investment strategies will be substantially modified; others will die. But, in the end, an improved understanding of how to create, bundle, distribute, and assess risk will not be forgotten, at least not for a cycle.\n\nRecent Monetary Policy Actions\nThe financial shock of 2007 and its potential threat to the economy changed the view held by the Federal Open Market Committee (FOMC) of the most appropriate stance of monetary policy. The stresses led to greater financial restraint on economic growth. Impaired price discovery impeded the flow of capital. At the time of our September meeting, the downside risks to the real economy appeared to have increased; greater uncertainty could have led lenders and investors to pull back further. Moreover, data on inflation were relatively stable, and inflation expectations appeared to remain anchored. From a risk-management perspective, these circumstances warranted a strong policy action. By doing more and sooner, the FOMC intended its policy action to counterbalance the tighter financial conditions and help forestall some potentially adverse effects of financial market disruptions on the real economy. To that end, the FOMC cut its target for the federal funds rate 50 basis points at its September meeting.\n\nSubsequent economic information received suggests that activity in the third quarter expanded at a solid pace, even though housing activity subtracted about 1 percentage point from real activity. Some indicators, however, suggest that activity may have slowed in the current quarter, and the prospect that such perceived softness may prove real and enduring is understood. In addition, stresses remain evident in certain asset and credit markets, even though financial market conditions today appear much improved from their August nadir. There are also important reasons to be concerned about the outlook for inflation. Although recent readings on core inflation have been favorable, prices of crude oil and other commodities have increased. These changes most likely will put upward pressure on overall inflation in the short run. Moreover, the decline in the foreign exchange value of the dollar could lead to higher prices for imported goods. If these same forces cause inflation expectations to become less reliably anchored, then inflation could increase in the longer run as well.\n\nAs you know, last week, the FOMC reduced its target rate an additional 25 basis points, to 4-1/2 percent. Combined with the action of the September meeting, the FOMC judged that the cumulative policy easing of 75 basis points reduced the downside risks to growth, and that these risks were now roughly balanced by the upside risks to inflation. Should incoming data materially change our forecast, or risks to our forecast, for growth and inflation, so too would our view on the appropriate stance of monetary policy.\n\nConclusion\nUltimately the ability of the economy to withstand shocks is a function of the flexibility and resiliency of labor, product, and capital markets; strong and resilient market infrastructures; and good macroeconomic policies, not the least of which is monetary policy. Much progress has been made in our understanding of monetary policy in market-based economies. Indeed the economic trends of the past generation show great promise, albeit interrupted by periods of genuine distress. In my view, we almost invariably end up with better macroeconomic outcomes than if we viewed stability as the sine qua non of policy. Recent events serve as an important reminder that the next chapter of history is still being written. Perhaps, that is always the case. As in the political realm, the path to the end of history may well prove to be prone to advance, overshoot, and correct. We must continue to deepen our understanding of financial markets and monetary policy, equipped with ample humility on a long, productive, and admittedly uneven path.\n\nReferences\n\nBeattie, Alan (2007). \"Master of the Universe (Retired),\" a review of The Age of Turbulence: Adventures in a New World by Alan Greenspan, in Financial Times, September 22-23.\n\nFukuyama, Francis (1989). \"The End of History?\" National Interest, vol. 16 (Summer), pp. 3-18.\n\nKagan, Roger (2007). \"End of Dreams, Return of History,\" Policy Review  (August/September), www.hoover.org/publications/policyreview.\n\nKindleberger, Charles P. (1996). Manias, Panics, and Crashes: A History of Financial Crises, 3rd ed. London: Wiley.\n\nLaidler, David (2007). \"Successes and Failures of Monetary Policy Since the 1950s,\"  University of Western Ontario, RBC Financial Group Economic Policy Research Institute Working Papers 20072. London, Ontario: University of Western Ontario, October.\n\nSchumpeter, Joseph (1942). Capitalism, Socialism and Democracy. New York: Harper.\n\nWarsh, Kevin (2007). \"Financial Market Developments,\" speech delivered at the State University of New York at Albany's School of Business, Albany, N.Y., September 21.\n\nFootnotes\n\n1. Francis Fukuyama advanced an affirmative view of this question when he published \"The End of History?\" (Fukuyama, 1989). Noting that the historical origins of the phrase can be found in the writings of Hegel and, later, Marx, Fukuyama wrote that \"what we may be witnessing is not just the end of the Cold War, or the passing of a particular period of postwar history, but the end of history as such: that is, the end point of mankind's ideological evolution and the universalization of Western liberal democracy as the final form of human government\" (p. 4).  Return to text\n\n2. The views expressed here are my own and do not necessarily reflect those of other members of the Board of Governors or of the Federal Open Market Committee. I am grateful for the assistance of Nellie Liang and Steven Sharpe, of the Board's staff, who contributed to these remarks.  Return to text\n\n3.  A discussion of this recent \"nirvanic\" period is in Beattie (2007). Return to text\n\n4.  The Federal Reserve's dual mandate is not inconsistent with Laidler's third tenet. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/warsh20071107a.htm",
        "title": "The End of History?",
        "date": "11/7/2007"
    },
    {
        "content": "November 06, 2007\n\nChairman Ben S. Bernanke\n\nAt the ACCIÓN Texas Summit on Microfinance in the United States, San Antonio, Texas\n\nLast month I had the pleasure of meeting with someone very well known to this audience but not so well known to Americans generally: Dr. Muhammad Yunus. Perhaps more than any other individual, Dr. Yunus inspired the movement that has become known as microfinance. In 1976, Dr. Yunus founded the Grameen Bank in Bangladesh, which became one of the pioneers of the concept of offering small loans to people deemed too poor or insufficiently creditworthy to qualify for traditional bank loans.\n\nThe organization and the larger movement it helped spawn have financed the entrepreneurial aspirations of many thousands of people. The great majority of those who have benefited from Grameen Bank loans have been women, particularly poor rural women. Microfinance has offered borrowers, in Dr. Yunus's words, \"a fair chance to unleash their energy and creativity\" (Yunus, 2006). His innovative thinking and dedication to poverty relief through the extension of credit were honored in 2006 by the award of the Nobel Peace Prize. And the movement itself was recognized when the United Nations declared 2005 to be the International Year of Microcredit.1\n\nThe microfinance, or microcredit, movement has spread throughout the world--to other parts of Asia, Africa, Latin America, and, more recently, to the United States. Although the social and economic contexts differ widely across countries, the fundamental purpose of microfinance programs remains the same: to offer small loans and other financial services to low-income people to help them increase their incomes through entrepreneurship and self-employment.\n\nAcción Texas has been an exemplar of the movement in the United States. I am very pleased to speak at your summit meeting today for many reasons, not the least of which is the opportunity to visit again with Janie Barrera, the president of Acción Texas. I had the pleasure of working with Janie when she was a member of the Federal Reserve Board's Consumer Advisory Council, which has been an invaluable resource for the Board over the years on all aspects of consumer protection regulation and community development initiatives. Soon after I became a member of the Board in 2002, Janie collaborated with the Federal Reserve Bank of Dallas to invite me and one of my fellow Board members, Susan Bies, to Brownsville, Texas. We toured local housing and community development projects and visited a small business that had gotten its start with the help of a microloan from Acción Texas.\n\nIn the remainder of my remarks I will speak about the development of the microfinance movement in the United States, putting it into an international context and discussing as well how it fits into the broader landscape of small business financing in this country. I will close with some thoughts on the challenges facing the U.S. movement as it continues to grow and mature.\n\nThe Development of the U.S. Microfinance Movement\nAlthough the United States came relatively late to the microfinance movement, experimentation in the 1980s and 1990s laid the groundwork for the lively network of programs we see today. Acción has been at the forefront of the development of microfinance in the United States. Acción International began its microlending activities in Latin America in 1961 and established an affiliate organization in the United States, Acción USA, in 1991. Over the years, the U.S. Acción network has grown to become one of the country's largest microfinance providers. Since its founding, the U.S. Acción network has loaned $180 million to nearly 20,000 borrowers in thirty-five states.2\n\nOf course, the operational details of U.S. microfinance programs differ significantly from those in overseas programs, but as I mentioned, they share similar goals and core values. As it does in developing countries, the microfinance movement in the United States seeks to expand economic opportunities for individuals and to foster community economic development by providing small loans and other business services to people who have been traditionally underserved by mainstream financial institutions. Loan features--including size, collateral requirements, and repayment terms--are typically more flexible than those of standard bank loans and are tailored to the needs of low- and moderate-income entrepreneurs.\n\nIn the United States, however, credit is only one part of the microfinance package. To a greater extent than overseas, microfinance programs here have expanded their offerings to deliver education, training, and various other services to nascent entrepreneurs. The goals of these supplemental activities are twofold: to improve the survival rate of the borrowers' start-up businesses and to mitigate credit risks for the lender. Several factors have driven the U.S. microfinance industry to diversify beyond simply lending. The complexity of the U.S. market for financial services requires greater financial management skills than are typically needed in developing countries. Here, even very small businesses are likely to have to deal with factors--such as taxes, licenses, and zoning laws--that can prove daunting hurdles to the inexperienced, aspiring business owner (Assanie and Virmani, 2006). By contrast, entrepreneurs in developing countries tend to operate in the informal sector, often out of the sight of regulators and tax authorities. Yet another difference between the U.S. context and that of the developing world is that, in the United States, aspiring entrepreneurs may have access to alternative sources of credit. Although they may not be able to obtain traditional small business loans, some can qualify for credit cards, home equity credit lines, or other alternatives to microcredit, whereas many of Grameen Bank's clients in Bangladesh, for example, have no such alternatives. Thus, while lending remains a very important part of U.S. microfinance programs, it is not as central to the broader mission as is typically the case in the developing world.\n\nIn helping local enterprises get under way, microfinance organizations help deliver the social benefits often associated with such businesses. For example, microentrepreneurs often involve their family members in their businesses, providing them valuable work experience; and extra income can confer important advantages on future generations, such as a chance for a better education. In addition, entrepreneurs may benefit communities and local economies in multiple ways, as this story of a woman who resides in one of Houston's poorest neighborhoods illustrates. Observing the lack of grocery stores in her community, she approached Acción Texas for funds to open a small organic food store and restaurant. With the help of the microloan, she created a viable business while also improving the options for food shopping in her community. She also provides various services, including neighborhood cooking classes that promote healthy eating habits.\n\nThe Place of Microfinance in the Landscape of Small Business Finance\nAlthough comprehensive data on U.S. microfinance as a whole is scarce, many U.S. microfinance institutions measure and track their own performance. Acción Texas, for instance, reports that it loaned $42 million between 1994 and 2005. It estimates that those loans created 982 new jobs and generated about $78 million in economic activity (including earnings of about $25 million and local tax revenue of $4.5 million).3 Thus, despite gaps in the aggregate data, we can get some sense of how microfinance fits into the overall picture of small business finance.\n\nSmall businesses, generally defined as firms having fewer than 500 employees, have always played a vital role in the U.S. economy. Together, they employ more than half of private-sector workers and produce more than half of private-sector output (Board of Governors, 2007). The enterprises that microlenders finance are, of course, the very smallest of small businesses, but such firms make up a substantial share of the U.S. small business sector: 20 percent of small businesses in the United States have only one individual working in the firm, and 40 percent have two to four people working. Among these smaller firms, nearly 25 percent were founded or acquired by a new owner within the past four years.\n\nThus microenterprises not only provide a path to economic self-reliance for owner-entrepreneurs and benefit their local communities, but they are also important for the economy as a whole. There is some truth to the popular image of the successful firm which had its beginnings in someone's garage. Microenterprises can grow into small businesses, and small businesses can grow into large firms. Thus, microfinance plays the role of business incubator by compensating for the difficulties faced by very small firms and startups in obtaining credit from established financial intermediaries. These difficulties arise because lending to small businesses is typically considered riskier and more costly than lending to larger firms. Small businesses are often more susceptible to changes in the broader economy and generally have a much higher rate of failure than larger operations, although the survival rate of small firms increases with age (Knaup, 2005).Collateral may be used to help mitigate the risk to lenders, but the smallest and youngest firms often have few assets available to pledge. Besides being riskier, lending to small firms can be more expensive. It costs more per dollar loaned both to evaluate their credit applications and to monitor their ongoing performance. Many small businesses lack detailed balance sheets and other financial information used by underwriters in making lending decisions. And the small firm does not issue publicly traded debt or other securities whose values in the marketplace serve as a signal of its profit expectations.\n\nOf course, despite these challenges, many smaller businesses do manage to obtain the credit and capital they need. Community banks, which rely on personal relationships and knowledge of the local market to assess credit risks, have long been a source of funding for small business. The development of more-sophisticated techniques in small business loan underwriting, including the use of credit scoring, has helped make small business lending more attractive to larger institutions as well (Cowan and Cowan, 2006). And research demonstrates that internal finance‑‑that is, financing from the personal resources of owners, family, friends, and business associates--can help offset a lack of access to capital and is crucial to both new and established small enterprises (Rosen, 1998; Holtz-Eakin, Joulfaian, and Rosen, 1994a,b). For some potential low-income entrepreneurs, however, none of these options is feasible. Microfinance was designed to bridge this gap.\n\nThe Future of Microfinance in the United States\nAs I have emphasized, microenterprise development programs in the United States are about much more than the extension of credit, though access to credit remains a central concern. Many programs take a holistic approach, offering interconnected services that complement lending activities and are targeted at entrepreneurs at each stage of business development. Services being offered include up-front business training; specialized technical assistance; mentoring programs; sector-specific advice and support; networking opportunities; coordinated sales and marketing programs; and the development of formal links with banks, local community colleges, and other institutions (Edgcomb and Klein, 2005). Of course, many start-up businesses don't make it; that's an inescapable aspect of the risks that small business entrepreneurs face. But the services provided by microenterprise programs offer borrowers a strong foundation in the fundamentals of running a business and give their businesses a better chance to grow and flourish in a competitive marketplace.\n\nThese services benefit the lender by making the borrowers more creditworthy, but providing these services to budding entrepreneurs is labor intensive and requires considerable expertise. Because microfinance clients are rarely able to pay for these services, the costs have generally been underwritten by philanthropic efforts and public-private partnerships. Whether U.S. microfinance programs can become financially self-sustaining is a key question for the future.\n\nCurrently, microenterprise organizations are experimenting with business models in the effort to promote self-sustainability. Some are trying to enhance their profitability by offering a wider array of fee-based services, such as check cashing and the facilitation of remittances. Others have turned to technology to reduce their costs. Acción USA, for instance, has reduced transaction, underwriting, and servicing costs through an Internet lending initiative.4 It has also reduced its training costs through online and distance-learning courses. Another web-based effort, MicroMentor, matches inexperienced entrepreneurs with more experienced businesspeople, thereby providing important assistance to new business owners at a relatively low cost (http://www.micromentor.org/ ). The Association for Enterprise Opportunity, the principal trade association for microenterprise programs, serves as a forum for learning about innovations, developments, and best practices in this field (http://www.microenterpriseworks.org/ ).\n\nAnother promising avenue for the future of microfinance is the development of more partnerships with mainstream banking institutions. Mainstream banks typically don't offer the array of supportive services found at microlenders. But by partnering with a microlender that incubates very small businesses, mainstream institutions can gain new customers when the borrowers \"graduate\" from the microfinance program and seek larger loans. And these new customers will be more creditworthy borrowers because of the early support they received from the microfinance organization. Acción Texas and other microfinance organizations have established several mutually beneficial partnerships with large banking institutions. Such partnerships serve as two-way referral systems between the microlenders and large banks and help break down the barriers between mainstream institutions and underserved entrepreneurs.\n\nConclusion\nTo sum up, I want to affirm the important role that microfinance plays in bringing the opportunity for entrepreneurship to people who otherwise might not have it. Although some businesses will inevitably fall by the wayside, those that flourish and grow are likely to have better management and better long-term prospects than they would have without the support of microenterprise programs. Successful microbusinesses provide jobs as well as valuable products and services to their communities. Not least important, they can provide economic independence and self-reliance for the owner-entrepreneurs. The full benefits of this movement are difficult to calculate. Indeed, one important challenge for the future is to find ways to better measure the impact and cost effectiveness of microfinance programs. What is clear is that the microfinance movement has grown and adapted considerably during its short history in the United States. I hope that microfinance organizations will sustain their energetic spirit of innovation and experimentation as they strive to become more self-sufficient and adapt to our ever-changing economy.\n\nReferences\n\nAssanie, Laila, and Raghav Virmani (2006). \"Incubating Microfinance: The Texas Border Experience,\" Federal Reserve Bank of Dallas, Southwest Economy (September/October), pp. 3-7.\n\nBoard of Governors of the Federal Reserve System (2007). Report to the Congress on the Availability of Credit to Small Businesses. Washington: Board of Governors of the Federal Reserve System, October.\n\nCarr, James H., and Zhong Yi Tong, eds. (2002). Replicating Microfinance in the United States. Washington: Woodrow Wilson Center Press.\n\nCowan, Charles D., and Adrian M. Cowan (2006). \"A Survey-Based Assessment of Financial Institution Use of Credit Scoring for Small Business Lending (690 KB PDF).\" Washington: U.S. Small Business Administration, Office of Advocacy, November.\n\nEdgcomb, Elaine L., and Joyce A. Klein (2005). \"Opening Opportunities, Building Ownership: Fulfilling the Promise of Microenterprise in the United States.\" Washington: Microenterprise Fund for Innovation, Effectiveness, Learning and Development (FIELD) at the Aspen Institute, February, www.fieldus.org/Projects/MovingForward.html.\n\nHoltz-Eakin, Douglas, David Joulfaian, and Harvey S. Rosen (1994a). \"Entrepreneurial Decisions and Liquidity Constraints,\"  RAND Journal of Economics, vol. 24 (Summer), pp. 334-47.\n\n_________ (1994b). \"Sticking It Out: Entrepreneurial Survival and Liquidity Constraints,\"  Journal of Political Economy, vol. 102 (February), pp. 53-75.\n\nKnaup, Amy E. (2005). \"Survival and Longevity in the Business Employment Dynamics Data,\" Monthly Labor Review, vol. 128 (May), pp. 50-56.\n\nRosen, Harvey S. (1998). \"The Future of Entrepreneurial Finance,\"  Journal of Banking and Finance, vol. 22 (August), pp. 1105-07.\n\nYunus, Muhammad (2006). \"Nobel Lecture,\"  acceptance speech delivered at the Nobel Peace Prize ceremony, Oslo, December 10, www.nobelprize.org/nobel_prizes/peace/laureates.\n\nFootnotes\n\n1. Additional information is available on the United Nations website, \"International Year of Microcredit,\" www.yearofmicrocredit.org.   Return to text\n\n2. Acción USA, About Us: Our Impact,  www.accionusa.org/site/c.lvKVL9MUIsG/b.1388811/k.46F7/ACCIONs_Impact_on_Small_Businesses.htm.  Return to text\n\n3.  Acción Texas, \"Economic and Community Impact of Acción Texas, 1994-2005,\"  www.acciontexas.org/economic_impact_report.php. Return to text\n\n4.  Acción USA, Get a Loan , https://secure.accionusa.org. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20071106a.htm",
        "title": "Microfinance in the United States",
        "date": "11/6/2007"
    },
    {
        "content": "November 05, 2007\n\nGovernor Frederic S. Mishkin\n\nAt the Risk USA 2007 Conference, New York, New York\n\n\n\nAfter operating for years under very favorable conditions and ample liquidity, financial markets came under stress last summer and have not yet fully recovered.  This ongoing episode has reminded investors and policymakers alike that financial instability, if allowed to develop fully, could have severely negative consequences not only for the functioning of financial markets but also, importantly, for the macroeconomic prospects of our country as well as others.  It is this connection with the real side of the economy that makes financial stability a central concern for me and my colleagues at the Federal Reserve and at other central banks around the world.\n\nPolicymakers, particularly those in a central bank, are faced with the questions of what they should do to prevent financial instability and what their responses should be when financial instability threatens to compromise economic performance.  To start answering these questions, we must first understand the nature of financial instability and how it might affect the macroeconomy.1\n\nThe Nature of Financial Instability\nThe financial system performs the function of efficiently channeling funds to individuals or corporations with worthy investment opportunities.  If shocks interfere with the information flows that are necessary for a smooth functioning of the financial system, the system can be disrupted and financial instability can arise.  By disrupting the flow of credit, financial instability, in turn, becomes a threat to economic performance.2\n\nThe information that is necessary for the efficient functioning of the financial system is by its nature asymmetric:  Often, one party to a financial contract (typically the lender) has much less accurate information about the outcome of an investment than does the other party (typically the borrower).  As I have explained in more detail in a recent speech, such asymmetry leads to two prominent difficulties for the functioning of the financial system:  adverse selection and moral hazard (Mishkin, 2007).\n\nAdverse selection arises when investments that are most likely to produce an undesirable (adverse) outcome are the most likely to be financed (selected).  For example, investors who intend to take on large amounts of risk are the most likely to be willing to seek out loans because they know that they are unlikely to pay them back.  Moral hazard arises because a borrower has incentives to invest in high-risk projects, in which the borrower does well if the project succeeds but the lender bears most of the loss if the project fails.\n\nHistorically, banking institutions and other financial intermediaries have played a major role in reducing the asymmetry of information because they are well placed to collect information from borrowers and to engage in long-term relationships with clients.  In more recent times, improved transparency and financial innovation--in the form of new financial products as well as new types of institutions that have become active in markets--have also contributed to the efficient flow of information across the system.  The continuity of this flow helps keep adverse selection and moral hazard in check and is crucial to the process of price discovery--that is, the ability of markets to collect information and properly evaluate the worth of financial assets.\n\nDuring periods of financial distress, information flows may be disrupted, and price discovery may be impaired.  The high risk spreads and reluctance to purchase assets that are characteristic of such episodes are natural responses to the increased uncertainty resulting from the disruption of information  Two types of risks are particularly important for understanding financial instability.  The first is what I will refer to as valuation risk:  The market, realizing the complexity of a security or the opaqueness of its underlying creditworthiness, finds it has trouble assessing the value of the security.  For example, this sort of risk has been central to the repricing of many structured-credit products during the turmoil of the past few months, when investors have struggled to understand how potential losses in subprime mortgages might filter through the layers of complexity that such products entail.\n\nThe second type of risk that I consider central to the understanding of financial stability is what I call macroeconomic risk--that is, an increase in the probability that a financial disruption will cause significant deterioration in the real economy.  Because economic downturns typically result in even greater uncertainty about asset values, such episodes may involve an adverse feedback loop whereby financial disruptions cause investment and consumer spending to decline, which, in turn, causes economic activity to contract.  Such contraction then increases uncertainty about the value of assets, and, as a result, the financial disruption worsens.  In turn, this development causes economic activity to contract further in a perverse cycle.\n\nDeterioration of balance sheets during a recession can also intensify problems of adverse selection and moral hazard because it removes an important channel through which information asymmetries are mitigated--the use of collateral.  If a borrower defaults on a loan backed by collateral, the effects of the adverse selection problem are less severe because the lender can take title to the collateral and thus make up for the loss.  In addition, the threat of losing the collateral gives the borrower more incentives not to take unmanageable risks that might ultimately lead to a default, and it thus reduces the moral hazard problem.  These mechanisms work only as long as the collateral is of sufficient quality; during macroeconomic downturns, the value of collateral may fall, problems of adverse selection and moral hazard again become central, and lenders become much less willing to lend.  Again, these events can result in an adverse feedback loop.\n\nShocks of various natures can interfere with the information flow in financial markets and thereby precipitate financial instability through valuation and macroeconomic risk.  Historical examples of such shocks include higher interest rates, problems in the banking sector, increases in uncertainty, and asset market effects on balance sheets.  Of those, the last two appear to have been especially prominent in the ongoing episode of financial instability.\n\nInterpreting the Recent Episode of Financial Instability\nOne could argue that the valuation of financial products backed by mortgages and corporate loans has always been uncertain, as the ability of borrowers to repay their debt ultimately depends on the performance of the economy.  Yet, especially in very recent years, investors appeared to be less concerned about macroeconomic uncertainty or about the attendant problems of adverse selection and moral hazard inherent in asset-backed products.  Thus, abundant credit flowed cheaply to borrowers regardless of the risks involved.\n\nHowever, beginning in the spring and continuing to the present time, a considerable amount of uncertainty has surrounded markets' valuations of many structured-finance products--part of the flurry of innovative financial instruments that have become popular among market participants in recent years.  Generally, increased uncertainty in financial markets makes it harder for lenders to screen good credit risks from bad and ultimately makes information more asymmetric, thereby possibly exacerbating the adverse selection problem.  Consequently, lenders may become less willing to lend, and that reluctance may lead to a decline in investment and aggregate activity.  During the recent turmoil, the opaqueness of structured-credit products contributed to market uncertainty until investors in those products (who were ultimately lenders to households and corporations) withdrew from the market and left borrowers without an important source of credit.\n\nIn the housing market, where price appreciation has slowed or even turned to depreciation in many areas, delinquencies and defaults have risen of late, especially in the variable-rate subprime sector.  In addition, the decline in house prices has induced a clear deterioration in the collateral behind home mortgages.  As a consequence, lenders have responded by tightening standards and terms and, ultimately, by reducing credit.\n\nSimilarly, the collateral offered by many financial institutions to back the borrowing they needed to finance their operations also became questionable.  As a result, these institutions found credit much more difficult to obtain, or much more costly, or both.  Funding difficulties for financial institutions clearly have the potential to turn into tighter credit conditions for households and nonfinancial businesses alike.\n\nThe Role of the Federal Reserve\nAgainst this backdrop, what role should the Federal Reserve perform to pursue its objectives?  To answer this question, we must first understand exactly what those objectives are.  The Federal Reserve was created by the Congress in 1913 to provide an effective backstop against the recurring episodes of financial panic that were relatively frequent at the time.  Even so, the interest of the Congress was not financial stability per se.  Rather, the Congress was concerned that financial panics were often followed by sharp contractions in economic activity, and it recognized that a stabilization of the financial system would lead to a stabilization of the whole U.S. economy.\n\nOriginally, the preamble to the Federal Reserve Act of 1913 stated that the Federal Reserve System was created \"to furnish an elastic currency, to afford means of rediscounting commercial paper, to establish a more effective supervision of banking in the United States, and for other purposes.\"  Later, in 1977, the Congress amended the act to introduce macroeconomic objectives explicitly.  Accordingly, it stated that \"the Board of Governors of the Federal Reserve System and the Federal Open Market Committee shall maintain long run growth of the monetary and credit aggregates commensurate with the economy's long run potential to increase production, so as to promote effectively the goals of maximum employment, stable prices, and moderate long-term interest rates.\"   Because long-term interest rates can remain low only in a stable macroeconomic environment, these goals are often referred to as the dual mandate--that is, the Federal Reserve seeks to promote the two coequal objectives of maximum employment and price stability.  But although the main interests of the Federal Reserve are macroeconomic in nature, well-functioning financial markets are ancillary to good economic performance.  Conversely, financial instability can compromise economic growth and price stability.  Because of this intimate connection with economic performance, the Federal Reserve has a clear interest in promoting the stability of financial markets.\n\nThe Federal Reserve has various tools at its disposal to promote financial stability.  In a speech two weeks ago, I discussed its role as a liquidity provider (Mishkin, 2007).  Today, I will instead focus on how monetary policy can be used as an effective instrument to keep markets stable and to counter the macroeconomic effects of a system that has become unstable.\n\nAs a general principle, a sound monetary policy is one that will foster the objectives of price stability and maximum sustainable employment.  Such a policy can make financial instability less likely.  In my view, the reason that this is so resides once again in the informational asymmetries that pervade our financial system.  For example, in an economy that experiences severe swings in output growth, lenders will be more reluctant to lend and will demand higher interest rates because of the higher risks that borrowers will default.  But this situation is likely to exacerbate the adverse selection problem, as only riskier borrowers will be willing to take out loans at higher interest rates.  Similarly, in an environment of high inflation, lenders will not be willing to lend for long periods.  Debt contracts will then tend to have short maturities, thereby increasing the system's exposure to cash flow and liquidity problems.\n\nFinancial instability, however, can arise even if macroeconomic fundamentals are good and monetary policy is sound, simply because of shocks that are unforeseen by policymakers or that cannot be prevented from occurring.  In this case, monetary policy can also be useful because it can help forestall the negative macroeconomic consequences of financial instability.  An easier monetary policy provides a direct stimulus to the economy, as it generally leads to lower interest rates across the term structure.  Lower rates reduce the cost of capital for borrowers and therefore encourage investment.  They also generally boost asset prices, thereby increasing wealth and encouraging consumer spending.\n\nResearchers have also identified other channels through which monetary policy is effective.  One important one is the credit channel.  The credit-channel view holds that monetary policy has additional effects because interest rate decisions influence the cost and availability of credit by more than would be implied by the associated movement in risk-free interest rates (Bernanke and Gertler, 1995; Bernanke, 2007a). For example, an easier monetary policy strengthens the balance sheets of borrowers.  This stronger financial position, in turn, enables the borrower to reduce its potential conflict of interest with the lender, either because the borrower is able to self-finance a greater share of its investment projects, or because it can offer more or better collateral to guarantee its liabilities.  As a result, firms and households will find it easier to increase their spending.\n\nIn addition to having beneficial macroeconomic effects, monetary policy can also help directly restore stability in financial markets after a period of financial instability.  As we have seen, financial instability can basically be viewed as a disruption of information; therefore, its resolution requires a restoration of information flows.  Monetary policy can contribute to this process by minimizing market uncertainty.\n\nI noted a moment ago that periods of financial instability are characterized by valuation risk and macroeconomic risk.  Monetary policy cannot have much influence on the former, but it can certainly address the latter--macroeconomic risk.  By cutting interest rates to offset the negative effects of financial turmoil on aggregate economic activity, monetary policy can reduce the likelihood that a financial disruption might set off an adverse feedback loop.  The resulting reduction in uncertainty can then make it easier for the markets to collect the information that enables price discovery and to hasten the return to normal market functioning.\n\nTo achieve this result most effectively, monetary policy needs to be timely, decisive, and flexible.  Quick action is important for a central bank once it realizes that an episode of financial instability has the potential to set off a perverse sequence of events that pose a threat to its core objectives.  Waiting too long to ease policy in such a situation would only risk a further deterioration in macroeconomic conditions and thus would arguably only increase the amount of easing that would eventually be needed.\n\nDecisive action is also important.  In circumstances when the risk of particularly bad economic outcomes is very real, a central bank may want to buy some insurance and, so to speak, \"get ahead of the curve\"--that is, ease policy more than it otherwise would have simply on the basis of its modal economic outlook.  However, because monetary policy makers can never be certain of the amount of policy easing that is needed to forestall the adverse effects of disruptions in financial markets, decisive policy actions may, from time to time, go too far and thus produce unwelcome inflationary pressures.  That's why I said that flexibility is also an important characteristic of monetary policy during a time of financial turmoil.  If, in their quest to reduce macroeconomic risk, policymakers overshoot and ease policy too much, they need to be willing to expeditiously remove at least part of that ease before inflationary pressures become a threat.\n\nSome may see a monetary policy that actively addresses episodes of financial instability along the lines that I have just described as promoting excessive risk-taking and thus increasing the probability of future crises.  In other words, such a policy might appear to create some moral hazard problems of its own.  I question, however, the validity of this view.  As I pointed out earlier, the Federal Reserve has a mandate from the Congress to promote maximum employment and stable prices, and it will choose its monetary policy actions so as to best meet that mandate.  That said, as pointed out recently by Chairman Bernanke, it is not the responsibility of the Federal Reserve--nor would it be appropriate--to protect lenders and investors from the consequences of their financial decisions (Bernanke, 2007b).  Indeed, the Federal Reserve can hardly insulate investors from risk, even if it wished to do so.  And the fact that investors who misjudged the risks they were taking lost money over the past few months as well as during most other episodes of financial turmoil, independently of the monetary policy actions taken by the Federal Reserve, certainly corroborates this argument.   The point is that, although the Federal Reserve can and should offset macroeconomic risk with monetary policy decisions, investors remain responsible for dealing with valuation risk.  Indeed, monetary policy is and should be powerless in that respect.  It is solely the responsibility of market participants to do the hard work of price discovery and to ascertain and manage the risks involved in their investments.\n\nThe Federal Reserve's Recent Monetary Policy Decisions\nWhat I just said should serve as a framework for understanding the recent decisions of the Federal Reserve to ease policy, first by 50 basis points on September 18 and then by another 25 basis points last week.  The first action was larger than markets expected at the time--indeed, quotes from the federal funds futures market as well as survey data indicated that most investors had anticipated a cut of only 25 basis points in the target federal funds rate ahead of that meeting.   As reported in the minutes, the Federal Open Market Committee (FOMC) judged that a policy easing of 50 basis points was appropriate to help offset the effects of tighter financial conditions on the economic outlook.  Had the FOMC not eased policy, it would have faced a risk that the tightening of credit conditions and an intensifying housing correction would lead to significant broader weakness in output and employment.  In addition, it would have faced the possibility that the impaired functioning of financial markets would persist for some time or worsen, which would create an adverse feedback loop not dissimilar to what I earlier called macroeconomic risk.  The cut of 50 basis points at that meeting was the most prudent action from a macroeconomic standpoint, even given the Federal Reserve's objective of price stability.  Indeed, with economic growth likely to run below its potential for a while and with incoming inflation data to the favorable side, the easing of policy, even if substantial, seemed unlikely to affect adversely the outlook for inflation.\n\nIt should be clear at this point that the FOMC's decision was made purely on macroeconomic grounds--that is, policy was eased solely to offset macroeconomic risk.  The changed policy stance would not have interfered with the ongoing adjustments in the pricing of financial instruments--that is, the policy action, even if larger than investors had expected, would not have had any effects on valuation risk.\n\nThe response of the markets to the easing of monetary policy in September was encouraging.  Financial market functioning improved after the decision was announced, an outcome that partially allayed the risks of a coming credit crunch and thus suggested that macroeconomic risk may have been reduced.  Still, conditions in several markets remained strained.  In part, those tensions certainly reflected the fact that valuation risk was still substantial and would not be reduced quickly.   Indeed, the process of price discovery is ongoing, and it will likely be some time before it is completed.\n\nAt the FOMC meeting last week, the federal funds rate target was lowered by another 25 basis points.  Our economy grew at a solid pace in the third quarter and was boosted importantly by personal consumption and business expenditures, an indication of considerable underlying strength in spending before the recent financial turbulence.  However, the pace of economic expansion is expected to slow in the near term, largely because of the intensification of the housing correction.  The combined 75 basis points of policy easing put in place at the past two meetings should help forestall some of the adverse effects on the broader economy that might otherwise arise from the disruptions in financial markets and should help promote moderate growth over time.\n\nGoing into the meeting, I was comforted by the lack of direct evidence to date of serious spillovers of the housing weakness and of tighter credit conditions on the broader economy.  But with an unchanged policy interest rate, I saw downside risks to the outlook for growth.  I was mindful, in particular, of the risk that still-fragile financial markets could be particularly exposed to potential adverse news on the housing situation, or on the macroeconomy more generally, and that renewed strains in financial markets could feed back adversely on economic performance.  My vote to ease policy at the meeting was motivated by my wish to reduce those risks.  The FOMC perhaps could have waited for more clarity and left policy unchanged last week, but I believe that the potential costs of inaction outweighed the benefits, especially because, should the easing eventually appear to have been unnecessary, it could be removed.\n\nIn voting to ease policy, I carefully considered the effect of that decision on our other objective--price stability.  I reasoned that the anticipated softening of economic growth and perhaps the emergence of some slack in the labor market might reduce those pressures, and I judged that a cut of 25 basis points in the target federal funds rate would not materially alter that modal outlook.  However, I recognized the risk that, even if readings on core inflation have improved modestly this year, recent increases in energy and commodity prices, among other factors, may put renewed upward pressure on inflation.  Consequently, in considering appropriate future adjustments to policy, I will monitor inflation developments carefully.\n\nOverall, I think that the cumulative policy easing the FOMC put in place at its past two meetings reduced significantly the downside risks to growth so that those risks are now balanced by the upside risks to inflation.  In these circumstances, I will want to carefully assess incoming data and gauge the effects of financial and other developments on economic prospects before considering further policy action.  As always, my colleagues on the FOMC and I will act to foster our dual objectives of price stability and sustainable economic growth.\n\nConclusions\nAs I have argued here, under the mandate it has been given by the Congress, the Federal Reserve has a responsibility to take monetary policy actions to minimize the damage that financial instability can do to the economy.  I hope I was clear in communicating to you that policies to achieve this goal are designed to help Main Street and not to bail out Wall Street.  Pursuing such policies does help financial markets recover from episodes of financial instability, and so it can help lift asset prices.  But this does not mean that market participants who have been overly optimistic about their assessment of risk don't pay a high price for their mistakes.  They have, and that is exactly what should happen in a well-functioning economy--which, after all, is what the Federal Reserve is seeking to promote.\n\n_________ (2007b).  \"The Recent Financial Turmoil and Its Economic and Policy Consequences,\" speech delivered at the Economic Club of New York, New York, October 15.\n\nBernanke, Ben S., and Mark Gertler (1995).  \"Inside the Black Box: The Credit Channel of Monetary Policy Transmission,\"  Journal of Economic Perspectives, vol.  9 (Autumn), pp. 27-48.\n\nMishkin, Frederic S. (1997).  \"The Causes and Propagation of Financial Instability:  Lessons for Policymakers (145 KB PDF),\" in Maintaining Financial Stability in a Global Economy, proceedings of a symposium sponsored by the Federal Reserve Bank of Kansas City, Jackson Hole, Wyo., August 28-30, pp. 55-96.\n\n_________ (2007).  \"Financial Instability and the Federal Reserve as a Liquidity Provider,\" speech delivered at the Museum of American Finance Commemoration of the Panic of 1907, New York, October 26.\n\nFootnotes\n\n1.  Note that my remarks here reflect my own views and not necessarily those of others on the Board of Governors or the Federal Open Market Committee.  I thank Roberto Perli for his excellent comments and assistance on this speech. Return to text\n\n2.  A more detailed discussion of my views on what causes financial instability and of the effect of such instability on economic activity is in Mishkin (1997). Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/mishkin20071105a.htm",
        "title": "Financial Instability and Monetary Policy",
        "date": "11/5/2007"
    },
    {
        "content": "November 05, 2007\n\nGovernor Randall S. Kroszner\n\nAt the Consumer Bankers Association 2007 Fair Lending Conference, Washington, D.C.\n\nGovernor Kroszner presented identical remarks at the Symposium on Housing Affordability, Washington, D.C., on November 5, 2007\n\n\n\nI am delighted to have been invited to speak to you about the important and pressing problems in the subprime mortgage market.  My remarks today will address those problems, with particular focus on how they are affecting borrowers.  The sharp increases in subprime mortgage loan delinquencies and foreclosures this year have created personal, economic, and social distress for many homeowners and communities.  I will discuss first the forces that caused that distress and then turn to the prospects for troubled borrowers.  Finally, I will address the critical question of what can be done to keep the affected families in their homes and alleviate the other difficulties they and their communities will face.\n\nBackground on the Subprime Mortgage Market\nSubprime loans are associated with high credit risk because the borrower lacks a strong or lengthy credit history or has other characteristics that are associated with high probabilities of default.  The expansion of subprime lending since the mid-1990s has been quite substantial, with the number of subprime mortgage loans now totaling 7-3/4 million, or 14 percent of the overall mortgage market.  Technological advances and financial innovations that reduced the costs of lending to higher-risk households contributed importantly to the expansion of the subprime market.  In particular, improvements in information processing allowed lenders to standardize their underwriting techniques and to better manage risks by adjusting the terms of loans to reflect the expected probability of default.  Ongoing growth in the secondary market for mortgage loans also contributed to the growth of subprime lending by lowering transactions costs for investors and by spreading risk more broadly, especially through the process of securitization.  That process allows intermediaries to pool large numbers of mortgages and sell the resulting cash flows to investors, often as components of structured securities.\n\nDespite the positive aspects of the longer-term expansion of access to mortgage credit, it came with features that increased risks to households, the financial system, and the broader economy.  Not surprisingly, subprime loans are more likely to default; for the borrower, this can mean the loss of a home and reduced access to future credit.  Such outcomes can be even more likely if loan products have complex repayment terms that are not fully understood, or if the borrowers have unrealistic expectations of their future income or house prices.  On the lender side, the originate-to-distribute model can leave lenders with weaker incentives to maintain strong underwriting standards.  In particular, originators who securitize may inadequately screen potential borrowers unless investors provide oversight and insist on practices that align originator incentives with the underlying risk.  The originate-to-distribute system is thus not only a potential source of risk to the financial system but also raises concerns regarding consumer protection.\n\nSources of the Recent Problem\nWith this background in mind, let me turn now to the recent problems in the subprime mortgage market.  As is widely known, delinquency rates on subprime mortgages have increased sharply over the past year.  The distress has been concentrated among the two-thirds of subprime borrowers who have variable-rate mortgages; more than 17 percent of those mortgages are in serious delinquency, about a tripling of the share since mid-2005.1   Near-prime loans are showing a rise in serious delinquencies as well, although it is much smaller than for subprime.  Serious delinquencies encompass foreclosures, and those are also up sharply--lenders initiated foreclosure proceedings for an average of 320,000 mortgage loans per quarter in the first half of this year, up from 240,000 loans per quarter over the preceding two years.2  This increase in foreclosures has been largely associated with subprime mortgages.\n\nMany factors contributed to the sharp increases in subprime delinquencies and foreclosures, both separately and in combination.  First, the unemployment rate in an area can significantly undermine the ability of people in that area to repay their mortgages.  States in the Midwest hit hardest by job cuts in the auto industry, such as Michigan and Ohio, are among the states with the highest rates of new foreclosures.\n\nA second key factor has been the slowing of house prices:  One national index, which rose at close to a double-digit pace from 2000 through 2005, has slowed to show only small gains for the past several quarters, and some areas are seeing outright price declines.  By damping the growth of home equity, sluggish house-price appreciation makes it harder for homeowners struggling with payments to obtain better terms through refinancing or to withdraw accumulated equity to finance their obligations.  In addition, borrowers with mortgages \"under water\"--that is, the house is worth less than the mortgage balance--may be tempted to walk away from their loans.  That outcome may be particularly likely for those who purchased properties purely for investment purposes; indeed, the Mortgage Bankers Association has found that a disproportionate share of serious delinquencies are associated with non-owner-occupied properties in some of the states that have seen the largest increases in delinquencies.\n\nA third factor that contributed to the sharp rise in payment problems among subprime mortgages appears to have been a loosening of underwriting standards for such mortgages in late 2005 and 2006.  Investors, having seen several years in which mortgages showed extremely strong performance, apparently did not demand sufficient information from sellers of mortgages and related products during this later period.3  As I already noted, inadequate monitoring can leave originators with weak incentives to maintain strong underwriting standards under the originate-to-distribute model.\n\nMany subprime originators, for example, engaged in so-called risk-layering--in which they made loans to borrowers not only with weak or short credit histories but also with other risk factors.  More than one-third of subprime mortgages originated in the second half of 2005 and 2006, for instance, carried a second lien, up from an average of only about 10 percent over the preceding three years.4  The greater prevalence of such \"piggyback loans\" contributed to higher initial cumulative loan-to-value ratios.  In addition, the share of subprime mortgages with full documentation fell to about 60 percent in late 2005 and 2006 from about 70 percent over the preceding three years.  Another underwriting practice in the subprime mortgage market that can add to risk is the failure to escrow taxes and insurance, which can result in payment shock to the borrower if the borrower did not fully anticipate the costs of taxes and insurances on the property.\n\nThese and other shifts in underwriting standards, coupled with slowing house price appreciation or even depreciation, are the most likely explanation for the pronounced rise we have seen in defaults occurring within a few months of origination--before most borrowers would have experienced significant changes in their payment obligations or in their financial situations.\n\nFinally, another factor that could affect subprime delinquencies is the substantial payment increase often experienced at the first interest rate reset.  For the most common type of subprime variable-rate loan, the so-called \"2/28\" loan, this reset occurs after two years, before which payments are typically based on a fixed below-market rate.  In early 2007, the typical subprime mortgage experiencing a first reset had its rate increase from 7 percent to 9-1/2 percent, producing an increase of 25 percent to 30 percent in the monthly payment.  This increase translates into an additional monthly debt obligation of $350 per month for the average subprime variable-rate mortgage.\n\nIn the past, many subprime borrowers have avoided such payment increases by refinancing; for example, about two-thirds of subprime 2/28s originated in 2003 and 2004 were terminated through a refinancing or home sale by the time of the first scheduled reset.  Prepayments on subprime variable-rate loans originated in late 2005 and 2006, however, have occurred at a slower pace, likely in part because the combination of sluggish house price appreciation and high initial cumulative loan-to-value ratios has left some borrowers with too little equity to qualify for new loans.\n\nProspects for Subprime Borrowers\nLooking ahead, two considerations suggest that conditions for subprime borrowers have the potential to get worse before they get better.  First, all indications are that housing activity is continuing to weaken.  Incoming data in recent weeks show that sales and new residential construction have declined further.  In such an environment, house prices in the aggregate are likely to remain sluggish for some time.  Second, the bulk of resets is yet to come:  On average, in each quarter from now until the end of next year, monthly payments for more than 400,000 subprime mortgages are scheduled to undergo their first interest rate reset.  That number is up from roughly 200,000 per quarter during the first half of 2007.  Delinquencies and foreclosures are therefore likely to continue to rise for a number of quarters.\n\nMany of the borrowers facing resets will still have solid payment records and enough home equity to refinance.  But others will face challenges from not only low levels of home equity but also from considerably tighter credit conditions.  The Federal Reserve's recent surveys of senior loan officers at banks have showed a significant tightening of standards on subprime loans.  In addition, many lenders that dealt only in subprime loans have gone out of business, and other large lenders have cancelled some subprime lending programs.  The issuance of new securitized pools of subprime loans has dwindled in the past couple of months, and judging from the few deals that are being placed, spreads are extremely wide.  The supply of funds for subprime loans is likely to remain low for some time as investors gather information and reevaluate the risks.\n\nHelping Borrowers and Mitigating Losses\nThese circumstances call for a high degree of collaboration and innovation to identify solutions that can keep borrowers confronting foreclosure in their homes.  The loss of a home to foreclosure is distressing to families and communities and can cause significant financial and social difficulties.  We should also pay particular attention to communities that may face more challenges than others, such as African-American families, who, according to data collected under the Home Mortgage Disclosure Act, account for a disproportionate share of higher-price (and thus more likely to be subprime) loans.5\n\nIt is imperative that we work together as a financial services community to look for ways to help borrowers address their mortgage challenges, particularly those who may have fewer alternatives, such as lower-income families.  Toward this end, the Federal Reserve's Community Affairs Offices have been convening lenders, community leaders, and government officials around the country over the past two years to help identify strategies to provide resources to assist borrowers confronting foreclosure.  Most recently, the Federal Reserve Bank of Chicago sponsored symposiums in Chicago, Indianapolis, and Detroit to discuss factors contributing to increasing foreclosure rates and to highlight innovative intervention programs.  Another meeting will be held in Waukesha, Wisconsin, in December.\n\nIn addition, the Federal Reserve and other banking supervisory agencies have issued statements in recent months urging both lenders and servicers to look for ways to work with borrowers having difficulty in meeting their mortgage loan obligations.6  These statements note that prudent workout arrangements that are consistent with safe and sound lending practices are generally in the long-term interests of both the financial institution and the borrower.  The September statement identifies prudent strategies for loss mitigation for servicers, including loan modifications, deferral of payments, extension of loan maturities, capitalization of delinquent amounts, and conversion of adjustable-rate mortgages into fixed-rate mortgages or fully indexed, fully amortizing adjustable-rate mortgages.\n\nLenders and servicers generally would want to work with borrowers to avoid foreclosure, which, according to industry estimates, can lead to a loss of as much as 40 percent to 50 percent of the unpaid mortgage balance.  Loss mitigation techniques that preserve homeownership are typically less costly than foreclosure, particularly when applied before default.  Borrowers who have been current in their payments but could default after reset may be able to work with their lender or servicer to adjust their payments or otherwise change their loans to make them more manageable.\n\nComprehensive data about how many loan workouts and modifications have actually occurred are not available, but some reports suggest that the numbers may be limited thus far.  One possible contributing factor is that many borrowers are not seeking help or advice from their lenders because they believe that lenders cannot or are not willing to help them.  Industry and consumer advocates who testified at our hearings on the home equity lending market last year told us that the greatest barrier to working with troubled borrowers is in simply making contact with them.  These witnesses told us that lenders can reach troubled borrowers through trusted community advocates and that partnerships between community groups and lenders can reduce the number of homes lost to foreclosure.  In addition, the analysis necessary to identify the best sustainable workout solutions that take into account the specific circumstances of individual borrowers can require a great deal of time and resources.  Trusted counselors can also help here.\n\nOne national nonprofit organization, NeighborWorks America (NeighborWorks), is making important strides in helping facilitate loan workouts and modifications.7  As a member of the board of NeighborWorks, I have the privilege of seeing firsthand how this organization has mobilized its national network of affiliates and partners to respond to the threat of foreclosures in various communities throughout the country.  Through its Center for Foreclosure Solutions, NeighborWorks has partnered with mortgage and insurance companies to help train and develop foreclosure counselors, conduct outreach to borrowers in trouble, and promote research to help inform strategic solutions for families and communities.  A primary tool of the center is a hotline--1-888-995-HOPE (4673)--that borrowers in financial distress can contact any time--twenty-four hours a day, seven days a week--to seek assistance from a mortgage counselor.\n\nSo far this year, the hotline has received more than 100,000 calls, with more than half of those calls received in the third quarter alone.  As more subprime ARMs reset over the course of coming quarters, services aimed at preventing foreclosures will continue to be strained, and all stakeholders will need to be innovative in identifying strategies to help preserve sustainable homeownership.\n\nA recent collaborative initiative that may help to alleviate some of the resource challenge is the Hope Now alliance.  This collaboration among counselors, servicers, investors, and other mortgage market participants aims to increase outreach efforts to contact at-risk borrowers through a national direct-mail campaign, encouraging them to either call their lender or a credit counselor.  The alliance will work to expand the capacity of an existing national network to counsel, refer, and connect borrowers to servicers.  Participating servicers have agreed to work toward cross-industry technology solutions to more effectively link servicers and counselors to better serve the homeowner.\n\nA promising effort started by the states is the new Foreclosure Prevention Working Group.  Consisting of eleven state attorneys general plus the Conference of State Bank Supervisors and the state bank regulatory agencies, the Working Group has held conversations with mortgage servicers and will continue to pursue opportunities for preventing foreclosures and encouraging increased loan modifications.\n\nAll these efforts are important, but there is more to be done to deal with the significant challenges ahead.  First, at this point, we are hearing that many modifications are done on a case-by-case basis.  That is understandable given the complexity of the products and the unique circumstances of each borrower.  Given the substantial number of resets from now through the end of 2008, however, I believe it would behoove the industry to join together and explore collaborative, creative efforts to develop prudent loan modification programs and other assistance to help large groups of borrowers systematically.\n\nSecond, I believe that modernization of programs administered by the Federal Housing Administration, which has considerable experience helping low- and moderate-income households obtain home financing, could also help avoid foreclosures.  FHA modernization could give the agency the flexibility to work with private-sector lenders to expedite the refinancing of creditworthy subprime borrowers and to design products that improve affordability through such features as variable maturities or shared appreciation.\n\nThird, we must pursue initiatives to prevent these problems from recurring, and the Federal Reserve is making strides in this direction.  We are engaged in a rigorous review of the mortgage-related rules under Regulation Z, which implements the Truth in Lending Act (TILA), and we intend to issue proposals before the end of the year to ban several deceptive advertising practices and require important consumer disclosures earlier in the mortgage process to better enable consumers to compare and shop among loan products.\n\nWe also plan to issue proposed regulations under the Home Ownership and Equity Protection Act to address unfair or deceptive mortgage lending practices that would apply to subprime loans offered by all mortgage lenders.  We will address some practices associated with subprime lending, such as prepayment penalties, failure to offer escrow accounts for taxes and insurance, stated-income and low-documentation lending, and the failure to give adequate consideration to a borrower's ability to repay.  For example, as I mentioned earlier, failure to escrow for taxes and insurance can lead to a situation akin to payment shock for borrowers.  It is a common practice for these payments to be escrowed in the prime markets, and I see no reason that escrows should not be standard practice in the subprime markets too.\n\nAll told, the spirit of innovation is essential to addressing the issue that is before us now as lenders, investors, regulators, community leaders, and borrowers.  As stakeholders and beneficiaries of the mortgage market, we all share an interest in working together to implement solutions that maintain a robust, transparent credit environment that promotes access to responsible mortgage lending.\n\nFootnotes\n1.  Seriously delinquent mortgages are more than ninety days in arrears or in foreclosure.  Estimates are based on data from First American LoanPerformance. Return to text\n\n2.  Foreclosure estimates are based on data from the Mortgage Bankers Association, adjusted to reflect the limited coverage of the association’s sample. Return to text\n\n3.  For a more comprehensive treatment of the causes and results of the failure of investors to exercise due diligence, refer to my recent speech to the Institute of International Bankers, in which I noted that the episode underscores the importance of the Russian proverb:  “Trust but verify” (Randall S. Kroszner, 2007, “Recent Events in Financial Markets,” speech delivered at the Institute of International Bankers Annual Breakfast Dialogue, October 22). Return to text\n\n4.  The estimates in this paragraph are based on analysis of loan-level data from LoanPerformance by Federal Reserve Board staff. Return to text\n\n5.  Robert B. Avery, Kenneth P. Brevoort, and Glenn B. Canner (2007), “The 2006 HMDA Data,” Federal Reserve Bulletin, vol. 93. Return to text\n\n6.  Board of Governors of the Federal Reserve System (2007), “Working with Mortgage Borrowers,” Division of Banking Supervision and Regulation, Supervision and Regulation Letter SR 07-6 (April 17); and “Statement on Loss Mitigation Strategies for Servicers of Residential Mortgages,” Supervision and Regulation Letter SR 07-16 (September 5). Return to text\n\n7.  NeighborWorks America is a registered service mark. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20071105a.htm",
        "title": "The Challenges Facing Subprime Mortgage Borrowers",
        "date": "11/5/2007"
    },
    {
        "content": "October 26, 2007\n\nGovernor Frederic S. Mishkin\n\nAt the Museum of American Finance Commemoration of the Panic of 1907, New York, New York\n\nIn my remarks today I would like to address the issue of financial instability from two perspectives:  First, I will offer a conceptual framework that helps us understand why periods of financial instability arise, and second, I will describe how the Federal Reserve has used the provision of liquidity to reduce the damage to the economy during such periods of turmoil.1\n\nThe interest of the Federal Reserve in financial stability does not arise out of a concern for the functioning of financial markets as such or out of a desire to aid distressed investors or institutions.  Rather, the Federal Reserve vigorously promotes financial stability because of the intimate connection between a stable financial system and solid macroeconomic performance.  The financial system, comprising financial markets and institutions, channels funds to those individuals or firms that have productive investment opportunities.  The better the financial system performs this role, the more efficiently credit flows from lenders to borrowers and the more efficiently the economy operates.  A period of financial instability arises when a shock to the financial system prevents it from channeling funds efficiently to productive uses; as I will describe, such a shock generally relates to problems in the flow of information.  Unless checked, such a seizing-up of the financial system can lead to a precipitous drop in lending and a steep decline in economic activity.  Therefore, a stable financial system is of vital importance for the Federal Reserve if it is to pursue its statutory goals of maximum sustainable employment and stable prices.\n\nAsymmetric Information and Financial Instability\nWe can better understand how financial instability can arise if we recognize the problem of asymmetric information‑‑when one party to a financial contract has much less information than the other party.  For example, the borrower is usually much better informed than the lender concerning the potential risks and returns associated with the investment projects to be financed by a loan.  Asymmetric information leads to two basic problems: adverse selection and moral hazard.  Generally speaking, adverse selection arises when investments that are most likely to produce an adverse outcome are the most likely to be selected.  For example, investors who intend to take on large amounts of risk are the most likely to be willing to seek financing.  When the problem of asymmetric information is particularly severe in the marketplace, lenders may decide to cut back on lending even though good credit risks exist.2  Clearly, minimizing adverse selection requires that lenders screen out bad credit risks.\n\nMoral hazard occurs when the lender is subjected to the hazard in which the borrower has an incentive to engage in activities that are undesirable from the lender's point of view, that is, activities that make it less likely that the loan will be repaid.  For example, a borrower may have an incentive to invest in high-risk projects in which the borrower does well if the project succeeds but the lender bears most of the loss if the project fails.  Again, if this conflict of interest between the borrower and lender becomes particularly pronounced in the marketplace, many lenders might decide that they would rather not make loans, which leads to suboptimal lending and investment.  To minimize moral hazard, lenders must impose restrictions (known as restrictive covenants) that penalize borrowers for engaging in certain activities; lenders must monitor those activities, and they must enforce the restrictions if the borrower violates them.\n\nOver time, the financial system has developed a variety of mechanisms to deal with informational asymmetries.  These mechanisms include a strong network of financial intermediaries, sound and resilient financial markets, innovative financial products that spread risk to the investors most willing and able to bear it, and effective supervisory and regulatory frameworks.  However, even the strongest financial system is not immune to shocks.  When such shocks interfere materially with information flows, the problems of adverse selection and moral hazard can overwhelm the mechanisms intended to control them. As lenders pull back and liquidity becomes scarce, a period of financial instability can ensue, carrying with it, as I mentioned at the outset, the threat of a serious decline in economic activity through a virtual cessation of lending.\n\nBroadly speaking, at least four types of shocks can lead to financial instability.  The first type, and the one that has been the catalyst of many past crises, is a sharp increase in interest rates.  Higher interest rates increase the likelihood that borrowers will be poor credit risks because those taking on more risk are the most willing to pay the higher rates; not surprisingly, lenders have many times responded to sharp increases in interest rates by cutting back the supply of loans, which results in credit rationing.\n\nThe other types of shocks are a sudden increase in uncertainty, a deterioration of corporate and household balance sheets, and weakened financial intermediaries.  Some of these have clearly manifested themselves in the episode that has affected global financial markets in recent months.  In general, an increase in uncertainty makes it harder to obtain information about borrowers, thereby deterring lending, and may arise from the failure of a large financial institution, a market crash, a recession, or, as in the latest episode, an inability to value complex financial products.  Weak borrower balance sheets are also a deterrent to lending because they worsen credit risk and thus make the problems of adverse selection and moral hazard more prominent.  The harmful effects of balance sheet deterioration were evident in recent months, when concerns about borrower financial positions caused lenders to demand a substantial premium on loans and even to drastically cut the flow of credit.  Finally, problems in the banking sector may also be a catalyst for financial instability because financial intermediaries that find themselves with weakened financial positions typically respond by cutting their lending.\n\nA central bank can respond to an episode of financial instability by temporarily providing the liquidity that financial intermediaries are unwilling or unable to supply.  Central banks typically inject liquidity into the system via the banking sector, but the intent is clearly to have the liquidity spread from there.  From banks, liquidity typically finds its way to financial markets and to nonbank institutions that are unable to access securities markets.  Thus, injections of liquidity have the potential to directly address the causes of financial instability and therefore to counteract the pernicious effects of financial instability on broad macroeconomic conditions.  With this understanding, we can now turn to some history.\n\nThe Panic of 1907 and the Creation of the Federal Reserve\nDuring the National Banking Era, which began in 1865 and ended in 1914, when the Federal Reserve began operations, reserves were channeled to New York banks, which became providers of liquidity to the financial system through the New York Clearinghouse.  Nevertheless, the banking system periodically suffered panics, as the demand for liquidity often exceeded what New York institutions could provide.  These panics, in turn, caused large numbers of banks to fail or at least suspend the conversion of deposits into currency.  During these episodes, interest rates tended to spike and equity markets to decline, often sharply.3\n\nThe Panic of 1907, the subject of this conference, was the last panic of the National Banking Era and one of the most severe.  During the summer of 1907, U.S. financial institutions, especially those in New York, came under increasing pressure as interest rates crept up and U.S. gold reserves declined; those movements were due in part to increases in interest rates abroad as foreign central banks sought to maintain their gold reserves.4  The panic started in October with a run on the Knickerbocker Trust Company, one of the largest financial institutions at the time.  The stock market crashed, and the resulting surge in demand for liquid assets by banks that had New York correspondents, as well as by the general public, caused a sharp rise in interest rates, the failure of a number of financial institutions, and widespread suspensions of convertibility of deposits into currency.  The subsequent banking crisis arguably turned a mild recession into a sharp, though fairly short, contraction.\n\nAs a result of the 1907 crisis, the Congress set up the National Monetary Commission to consider whether the federal government should more actively manage the nation's money supply.  The committee's recommendations led to the creation of the Federal Reserve.  Thus, an important impetus for the establishment of the U.S. central bank was the desire to prevent panics such as the one in 1907, whose detrimental effects on the rest of the economy were not lost on policymakers.5\n\nAt the time, the belief was widespread that the inelasticity of the currency contributed to the weakness of the financial system.  The supply of high-powered money‑‑which included national bank notes, specie, U.S. notes, and silver certificates‑‑was relatively fixed (inelastic) in the short run.  Thus, even fairly moderate increases in demand for high-powered money led to higher interest rates.  In addition, some of the increases in the demand for money were not random; they were associated with the crop cycle and were thus seasonal.6  Just as no existing mechanism or institution could quickly supply the liquidity needed to respond to a rise in the demand for money, none could supply it to resolve the ensuing crisis.  The establishment of the Federal Reserve provided the nation with an institution that was able to expand the money supply to virtually eliminate seasonal fluctuations in interest rates and thus mitigate the stress such fluctuations might put on the financial system.  The policymakers involved in establishing the Federal Reserve also believed that the institution would be able to meet the needs of the financial system at times when there was an unanticipated scramble for liquidity.  The founders tended to focus on the ability to rediscount commercial loans at the discount window, which was seen as a way of providing an elastic currency that would eliminate crises or at least mitigate them.7\n\nThe Federal Reserve as a Liquidity Provider\nIn past research, I have examined the importance of liquidity provisions by central banks in overcoming the problem of asymmetric information, exacerbated by the increase in interest rates, and the uncertainty that generally accompany a financial crisis (Mishkin, 1997).  Increased liquidity can help restrain rising interest rates and, amid an environment of heightened uncertainty, enable the institutions best able to make judgments about the creditworthiness of prospective borrowers to provide credit.\n\nOver time, the Federal Reserve has used various methods to provide liquidity to the financial system.  At first, the ability to rediscount paper was seen as the main tool with which to provide liquidity.  At other times, the Federal Reserve has changed interest rate ceilings or used open market operations.  Since 2003, the operation of the discount window has altered significantly.  The new facility allows banks in sound financial condition to borrow at will at a rate above the target federal funds rate.  The facility can be used to provide liquidity to financial institutions during a crisis in which financial markets become dysfunctional as well as during other periods of financial tightness that are not the result of a systemic event.\n\nEven with the Federal Reserve System in place, periods of financial stress have arisen since World War II.  I will briefly review a few of the more recent ones and focus on the role of the Federal Reserve as a liquidity provider in mitigating them.  The first of these more recent episodes arose from the 1970 bankruptcy of the Penn Central Corporation, an important issuer of commercial paper.  Policymakers and market participants worried that the bankruptcy of such a major issuer would have a chilling effect on the commercial paper market, as uncertainties about the quality of other issuers would impede them from rolling over their commercial paper to obtain short-term financing.8  The Penn Central bankruptcy thus had the potential to send other companies into bankruptcy and possibly trigger a full-scale financial panic.  To forestall this possibility, the Federal Reserve provided liquidity to financial markets in two ways.  First, the Federal Reserve Banks let the depositories in their respective Districts know that the discount window was available to enable them to make loans to customers who could not roll over their commercial paper.   Second, the Federal Reserve (along with the Federal Deposit Insurance Corporation and the Federal Home Loan Bank Board) suspended the interest rate ceilings imposed by the Federal Reserve's Regulation Q on some large certificates of deposit, thus providing banks another means of financing increases in loans.\n\nThe second recent episode of financial instability was the stock market crash of 1987.  A significant concern during this episode was the possibility of a breakdown in the clearing and settlement systems in the equity and futures markets.  To keep those markets functioning in an orderly fashion, brokerage firms needed to obtain massive amounts of credit from banks to meet margin calls from the clearing and settlement institutions.  However, although brokerages were clearly in need of additional funds to finance their activities, banks were understandably nervous about the health of securities firms in the wake of the unprecedented market movements.9  In response, the Federal Reserve issued a statement affirming its readiness to serve as a source of liquidity to support the economic and financial system.  The Federal Reserve underscored its commitment to providing liquidity by conducting open market operations in a high-profile manner and earlier than usual.10  The Federal Reserve also communicated with market participants to support efforts to coordinate a market response (Greenspan, 1994).\n\nWhat is remarkable about this episode is the relatively limited amount of liquidity extended by the Federal Reserve:  In the immediate aftermath of the crash, it temporarily injected through open market operations about $12 billion, which at the time was a notable but not exceptional amount.  Because the Federal Reserve acted promptly (within a day) to prevent the financial system from seizing up, banks were not as reluctant to lend their funds to securities firms as they presumably would have been in the absence of a stabilizing action from the central bank.  Thus, the Fed did not need to lend directly to the banks to encourage them to lend to the securities firms that needed funds to clear their customers' accounts.  Confidence was restored, and the fear of crisis subsided quickly.\n\nThe ability of the Federal Reserve to provide liquidity was also vital following the terrorist attacks on September 11, 2001, which disrupted payment, settlement, and communication systems needed to transfer funds among financial institutions.  Unable to receive funds for their normal operations, many of these institutions turned to the central bank.  The Federal Reserve responded by injecting a considerable amount of liquidity into the system, both through discount window loans (which soared from around $200 million in the days immediately before the attacks to a maximum of $46 billion) and repurchase agreements (which jumped from $25 billion to a peak of more than $80 billion).  Further, the Federal Reserve helped facilitate the provision of dollar liquidity abroad by arranging swap lines with foreign central banks.  Thanks to these steps, credit continued to flow through the system during the episode, and serious macroeconomic consequences were avoided.\n\nThe final episode of financial instability I want to discuss is the market turbulence of the past few months.  Concerns originating in the subprime-mortgage sector sparked a pullback in risk-taking in a variety of markets.  As investors sought safety and liquidity, the issuance and trading of structured financial products slowed sharply or, in some cases, completely ceased; some asset-backed commercial paper could not be rolled over; yields in the Treasury bill market fluctuated considerably; and spreads in interbank funding markets rose to unusually high levels.  To prevent these problems from turning into a severe credit crunch for households and corporations alike, the Federal Reserve used several tools to inject liquidity into the system.  In particular, on August 10, the Federal Reserve announced that it would conduct open market operations necessary to promote trading in the interbank market at close to the target federal funds rate and noted that the discount window was available as a source of funding.  On August 17, the Federal Reserve Board approved a reduction of 50 basis points in the discount window's primary-credit rate and announced changes in practices to allow the provision of term financing to depository institutions in solid financial condition for as long as thirty days.  Also, to help address the heightened safe-haven demands for Treasury securities, the Federal Reserve announced a temporary reduction‑‑from 100 basis points to 50 basis points‑‑in the minimum fee for borrowing securities from the System Open Market Account.  Although market functioning has certainly not yet returned to normal, and while it is still too early to judge their ultimate success, these actions, along with the policy easing decided at the September FOMC meeting, have helped improve conditions in several short-term funding markets and instill confidence in investors that liquidity would be available if needed.\n\nPolicy Issues\nAs I have just recounted, the Federal Reserve has been able to reduce the negative macroeconomic consequences of bouts of financial instability by providing liquidity to markets and institutions at times of need.  The provision of liquidity should thus be seen as a key tool for the Federal Reserve in its quest to achieve its statutory objectives of maximum sustainable employment and price stability.  The episodes I reviewed here also suggest that the more quickly liquidity can be provided when financial instability occurs, the more effective it may be.  As I mentioned earlier, the Federal Reserve's quick response to the stock market crash of 1987 is noteworthy because it meant that only a small amount of liquidity, and no direct lending, was needed to restore normal market conditions.\n\nAlthough the provision of liquidity is undoubtedly a useful tool, it is not without potential costs.  Improperly managed, it can lead to increased incentives for banks to take on excessive risk‑‑that is, it can create another form of moral hazard.  That additional moral hazard will worsen, rather than enhance, the stability of the financial system.  The risk of moral hazard increases if the central bank is perceived to be lending frequently to insolvent financial institutions.  That concern, however, is addressed in the United States by the Federal Deposit Insurance Corporation Improvement Act of 1991, which establishes certain constraints on the Federal Reserve's ability to lend to troubled institutions, and by the guiding principle for the operation of the Federal Reserve's primary-credit facility, namely, to lend only to sound institutions and on good collateral.11  The need to limit moral hazard by not lending to insolvent institutions indicates that central banks must have information sufficient to determine whether an institution with access to the discount window is indeed healthy.  That consideration is one reason that central banks benefit from having some supervisory responsibility for institutions with access to the discount window (Mishkin, 1994; Bernanke, 2007).\n\nMoral hazard could also arise when a central bank lends in response to liquidity problems, but I would argue that the risk of that happening might be lower than in the case of lending to troubled institutions.  I have in mind situations in which markets become impaired for exogenous reasons.  In those circumstances‑‑when financial institutions that are otherwise perfectly solid are at risk of failure because market infrastructures are disrupted or, more generally, when financial instability originates outside the banking sector‑‑an intervention by the Federal Reserve would certainly be beneficial, and the creation of perverse incentives would probably be limited.\n\nThe traditional recommendation for the prevention of financial crises goes back to Thornton (1802) and Bagehot (1873), who argued that, during a panic, the central bank should be a lender of last resort that lends freely at a penalty rate.  Charging a penalty rate is another way to limit the possible moral hazard arising from liquidity provisions because it gives institutions an incentive to maintain liquidity sufficient to avoid borrowing at a high rate.  Indeed, the Federal Reserve's standing facility charges an interest rate above the target federal funds rate for exactly that reason (Madigan and Nelson, 2002).\n\nThe Thornton-Bagehot criterion is an important principle, but central banks must also determine the appropriate penalty rate to apply under the circumstances.  For example, when financial instability originates outside of the banking sector, liquidity made available to banks may allow them to channel funds to the part of the financial system that has seized up, thereby helping the recovery of that sector.  Indeed, that mechanism seems to have been at work during the Penn Central bankruptcy episode.  However, if banks must pay a substantial penalty rate to acquire the extra liquidity, they may find it unprofitable to extend loans to the troubled sector.  In such circumstances, the central bank may judge that a lower penalty rate for borrowing at the discount window is warranted for a time.  Lowering the discount rate may also help underscore the central bank's intent to provide adequate liquidity to promote the functioning of financial markets.  The Federal Reserve took just such an action on August 17 of this year, when it lowered the rate for primary credit by 50 basis points.\n\nWhen even depository institutions in sound financial condition are constrained in their access to funding because of information problems or operational disruptions, open market operations may be incapable of providing reserves to the specific banks in need of funding.  Having a discount facility at its disposal provides a central bank with a more targeted tool for coping with financial disturbances without promoting inflationary tendencies.\n\nBut wouldn't lending at a reduced rate exacerbate moral hazard?  That seems to me to be unlikely so long as two fundamental principles are observed.  First, any such lending should be temporary and implemented only in the highly unusual circumstance in which systemic risk is clearly present; that restriction should prevent market participants from expecting such lending to become a normal part of doing business (Brimmer, 1989).  Second, commercial banks‑‑to the extent that they lend out the funds obtained from the central bank‑‑should remain responsible for their own credit judgments, even in extreme situations.\n\nConclusion\nThe Congress has given the Federal Reserve a dual mandate to achieve both price stability and maximum sustainable employment.  The Federal Reserve's role as a provider of liquidity to cope with episodes of financial instability has been, and will continue to be, critical to its success in achieving this mandate.  How best to perform that role is something that both academics and policymakers will continue to think about for a long time.\n\nReferences\nAldrich, Nelson (1908).  Congressional Record, Feb. 10, p. 1755.  Cited in Robert L. Owen (1919), The Federal Reserve Act, New York: Century, pp. 29-30.\n\nAkerlof, George A. (1970).  \"The Market for 'Lemons':  Quality Uncertainty and the Market Mechanism,\"  Quarterly Journal of Economics, vol. 84 (August), pp. 488-500.\n\nBagehot, Walter (1873).  Lombard Street: A Description of the Money Market. London: King. Reprint (1962), Westport, Conn.:  Hyperion Press.\n\nBernanke, Ben S. (2007).  \"Central Banking and Bank Supervision in the United States,\" speech delivered at the Allied Social Science Association Annual Meeting, Chicago, January 5.\n\nBrimmer, Andrew F. (1989).  \"Distinguished Lecture on Economics in Government: Central Banking and Systemic Risks in Capital Markets,\" Journal of Economic Perspectives, vol. 3 (Spring), pp. 3-16.\n\nCalomiris, Charles W., and Gary Gorton (1991).  \"The Origins of Banking Panics: Models, Facts, and Bank Regulation,\" in R. Glenn Hubbard, ed., Financial Markets and Financial Crises.  Chicago: University of Chicago Press, pp. 109-75.\n\nCarlson, Mark (2007).  \"A Brief History of the 1987 Stock Market Crash with a Discussion of the Federal Reserve Response,\" Finance and Economics Discussion Series 2007-13.  Washington: Board of Governors of the Federal Reserve System, May.\n\nFriedman, Milton, and Anna J. Schwarz (1963).  A Monetary History of the United States 1867-1960.  Princeton: Princeton University Press.\n\nGreenspan, Alan (1994).  \"Statement and Comments of Alan Greenspan, Chairman of the Federal Reserve,\" in Banking Industry Regulatory Consolidation, U.S. Senate, Committee on Banking, Housing, and Urban Affairs, Senate Hearing, 103 Cong. 2 Session, March 2.  Washington: Government Printing Office.\n\nHamlin, Charles S. (1914).  \"The Federal Reserve System as Established and in Operation,\" speech delivered to the New York Chamber of Commerce, New York, December 3, in New York Times, \"Financial Panics Things of the Past,\" December 4.\n\nMadigan, Brian F., and William R. Nelson (2002).  \"Proposed Revision to the Federal Reserve's Discount Window Lending Programs,\" Federal Reserve Bulletin, vol. 88 (July), pp. 314-19.\n\nMaisel, Sherman J. (1973).  Managing the Dollar.  New York: Norton.\n\nMiron, Jeffrey A.  (1986).  \"Financial Panics, the Seasonality of the Nominal Interest Rate, and the Founding of the Fed,\" American Economic Review, vol. 76 (March), pp. 125-40.\n\nMishkin, Frederic S. (1991).  \"Asymmetric Information and Financial Crises: A Historical Perspective,\" in R. Glenn Hubbard, ed., Financial Markets and Financial Crises.  Chicago: University of Chicago Press, pp. 69-108.\n\nMishkin, Frederic S. (1994).  \"Preventing Financial Crises: An International Perspective,\" The Manchester School of Economic and Social Studies, vol. 62, pp. 1-40.\n\nMishkin, Frederic S. (1997).  \"The Causes and Propagation of Financial Instability: Lessons for Policymakers (145 KB PDF),\" presented at \"Maintaining Financial Stability in a Global Economy,\" a symposium sponsored by the Federal Reserve Bank of Kansas City, Jackson Hole, Wyo., August 28-30, pp. 55-96.\n\nOdell, Kerry A., and Marc D. Weidenmier (2004).  \"Real Shock, Monetary Aftershock: The 1906 San Francisco Earthquake and the Panic of 1907,\" Journal of Economic History, vol. 64 (December), pp. 1002-27.\n\nSprague, Oliver M. (1910).  History of Crises under the National Banking System.  Washington: National Monetary Commission.\n\nTallman, Ellis, and Jon Moen (1998).  \"Gold Shocks, Liquidity, and the United States Economy during the National Banking Era,\" Explorations in Economic History, vol. 35 (October), pp. 381-404.\n\nThornton, Henry (1802).  An Inquiry into the Nature and Effects of the Paper Credit of Great Britain.  London.  Reprint (1962), New York:  Kelley.\n\n\n\nFootnotes\n1. I thank Mark Carlson, William English, and Roberto Perli for their comments and assistance on this speech.  Note that my comments here reflect my own views and not necessarily those of others on the Board of Governors or the Federal Open Market Committee. Return to text\n\n\n\n2.  This outcome is a feature of the classic \"lemons problem\" first described by Akerlof (1970). Return to text\n\n3.  As discussed, for example, in Sprague (1910); Mishkin (1991); and Calomiris and Gorton (1991). Return to text\n\n4.  A description of what occurred in this panic is in Sprague (1910); Friedman and Schwarz (1963); Mishkin (1991); Tallman and Moen (1998); and Odell and Weidenmier (2004). Return to text\n\n5.  Following the panic of 1907, Senator Aldrich noted that \"the shrinkage in values of securities and property and the losses from injury to business, resulting from and incidental to the crisis, amounted to thousands of millions of dollars\" (Aldrich, 1908). Return to text\n\n6.  As discussed, for example, in Miron (1986). Return to text\n\n7.  According to Charles Hamlin, the first head of the Federal Reserve Board, \"[During periods of hoarding,] each bank retreats into its own citadel at the sound of danger and at a time when it should be drawing upon its reserves to help the business man of the community, it stays aloof, piling up reserves… such a state of affairs will never occur again under the Federal reserve system. The mobilization of reserves and the turning of commercial paper [loans] into a liquid investment, will enable every bank to draw down its reserves with confidence that it can replace them at will if it has proper commercial paper at its disposal\" (Hamlin, 1914). Return to text\n\n8.  A discussion of this episode is in Maisel (1973); Brimmer (1989); and Mishkin (1991). Return to text\n\n9.  Brimmer (1989) and Mishkin (1991) also discuss this episode. Return to text\n\n10.  Carlson (2007) has a more detailed account of the Federal Reserve response to that episode. Return to text\n\n11.  The Federal Reserve’s secondary-credit program, which applies to institutions not eligible for primary credit, also requires good-quality collateral. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/mishkin20071026a.htm",
        "title": "Financial Instability and the Federal Reserve as a Liquidity Provider",
        "date": "10/26/2007"
    },
    {
        "content": "October 22, 2007\n\nGovernor Randall S. Kroszner\n\nAt the Institute of International Bankers Annual Breakfast Dialogue, Washington, D.C.\n\nPrice Discovery\nWhen markets are functioning properly, one of the key roles that they perform is what economists refer to as \"price discovery.\"2  Essentially, price discovery is the process by which buyers and sellers’ preferences, as well as any other available market information, results in the \"discovery\" of a price that will balance supply and demand and provide signals to market participants about how most efficiently to allocate resources..  This market-determined price will, of course, be subject to change as new information becomes available, as preferences evolve, as expectations are revised, and as costs of production change.\n\nIn well-functioning markets, the price discovery process represents the efforts of market participants to use all available information to decide whether to buy or to sell or to abstain from buying and selling.  In efficient and competitive markets, participants will tend to undertake a certain amount of due diligence before making their decisions.  This means that prices do not just appear by themselves; a substantial amount of work is required by buyers and sellers for markets to produce prices that clear markets and provide useful signals to consumers and producers.  Indeed, this is one of the brilliant aspects of the market mechanism in that a number of participants, each pursuing their own interests and trying to maximize their own welfare and profits, determine a market-clearing price.  A core principle of economics is that markets are more competitive, and therefore more efficient, when accurate information is available to both buyers and sellers.  But for markets to work best, market participants must utilize available information, including analysis of costs and benefits of obtaining such information.  In the case of new and innovative products, there might be a particularly strong demand for information.  Then this information must be processed appropriately before decisions are made about whether to buy or sell.\n\nIn some instances, the price discovery process can break down and buyers and sellers are unable to discover any price at all--perhaps because of a lack of information or because of general uncertainty among market participants.  I would suggest that this is fundamentally what has occurred in some financial markets over the past several months.  This has certainly not been the case in all markets.  For example, while equity markets in the United States have experienced greater price volatility in recent months, and credit spreads have widened in markets for highly-rated, traditional debt instruments, I believe this has been a function of reassessing risk rather than a broader failure of the price discovery process itself.  Moreover, investment-grade corporations faced little trouble in issuing traditional debt instruments during the market turbulence in August and did so in significant volumes.\n\nIn some financial markets, however, the price discovery process appears to have actually broken down.  In particular, I am referring to markets for structured credit products (for example, collateralized loan obligations and collateralized debt obligations) that are often complex and opaque, as well as instruments that are linked to these structured products, such as asset-backed commercial paper.  Why did the price discovery process fail in these markets but not in others?  I would suggest that there are two principal related causes.\n\nFirst, some investors may not have done sufficient due diligence with regard to complex structured products.  Prior to the recent market disruptions, many buyers and sellers of complex structured products appear not to have demanded sufficient information from sellers, and simply accepted investment-grade ratings of these securities as a substitute for their own risk analysis.  When the problems in the subprime mortgage market began to emerge and delinquencies on subprime mortgages in pools backing these securities exceeded rating agency estimates, subsequently resulting in a number of downgrades, investors lost confidence in the quality of these ratings, and hence the quality of the information they had about these instruments, and pulled back from markets for structured products across the board.\n\nA second, related factor contributing to the breakdown in price discovery is the recognition by investors of complexity and lack of transparency, both in the instruments themselves and in the markets more broadly.  The complex structures of the innovative instruments, and the lack of transparency with regard to the underlying assets backing these instruments, made them more difficult and costly to value than many investors originally thought.  At the same time, many investors realized that it was difficult to identify where the risks were lodged.  This uncertainty, of course, is one of the trade-offs of a more market-intermediated finance system in which risks are more widely dispersed rather than concentrated in the banking system.  As problems in the subprime mortgage market became more apparent, investors became unwilling to purchase products that could have any exposure not only to subprime mortgages, but to housing-related assets and other structured products more generally.\n\nPut simply, investors suddenly realized that they were much less informed than they originally thought.  In these circumstances, it is not necessarily surprising that investors pulled back from purchasing certain instruments at any price.\n\nProspects for Market Recovery\nIn light of these factors, what is the prognosis for recovery in markets for complex structured credit instruments?  I would suggest that, while we have seen more normal price discovery activity slowly returning to some markets, the recovery may be a relatively gradual process, and these markets may not look the same when they re-emerge.  Both investors and sellers will need to take steps for the price discovery process to be re-established in these markets.\n\nIn observing the challenges to price discovery and the repricing of risk in many markets recently, I have been reminded of a Russian proverb that was made famous in the context of international relations but applies equally to investment decisions: \"Trust, but verify.\" Let me explain.\n\nAs I mentioned earlier, one of the reasons that the price discovery mechanism has broken down in some U.S. markets in recent months is that a number of investors failed to exercise due diligence and relied on rating agency assessments.  That is, there was a lot of trust but not much verification.  I would suggest that the value of independent due diligence on the part of investors is especially high for newer and more-complex products compared with more traditional, familiar, and less-complex products.\n\nReducing the chance of unanticipated losses may require significant effort on the part of investors looking to purchase complex structured products and the creators or sellers of those products.  To be able to better understand the risk profile of such instruments, some market participants will have to invest in three ways to revive the price discovery mechanism.  First, they will likely need to collect more detailed data.  In particular, investors will need to gather data more systematically to help them understand the nature and risks of the underlying assets and the structures of the instruments.  Second, investors will likely require enhanced systems to warehouse and model data related to these instruments to better understand their risk profile, especially under stress conditions.  Third, investors will likely need to ensure that they have the appropriate human capital expertise--that is, people--to interpret, understand, and act appropriately on the results of their modeling and analysis.  The investment in data, modeling, and assessment will take time so there may be an extended period before normal price discovery will return in markets for some existing products.\n\nIn turn, given the likely increase in the costs of producing and evaluating certain complex instruments, these actions and efforts may affect investors’ risk-reward calculus by increasing required returns--or the \"hurdle rates\"--on these investments.  Creators and sellers may respond by reducing complexity, improving quality of underlying assets or increasing transparency and disclosure.  In light of recent events, market innovation may result in new instruments that satisfy the needs of both buyers and sellers--instruments that, of course, should not just be accepted on their face but should be subject to proper due diligence.  In the end, investors will decide for themselves whether acquiring the data and expertise necessary to participate in certain markets is worth the cost.  As a result, it is likely that these markets and instruments will look different than they did prior to the recent market turmoil.\n\nLet me close by highlighting the role of the Federal Reserve over the past several months as a backstop source of liquidity in interbank funding markets.  As price discovery broke down in a variety of markets, financial institutions, as intermediaries and liquidity providers themselves in the affected markets, became protective of their liquid reserves and balance sheet capacity.  As a result, overnight and term interbank funding markets have come under pressure.  The Federal Reserve accordingly took a number of steps to try to alleviate these pressures.\n\nThe Fed’s initial action in early August was to increase liquidity in short-term money markets through larger open-market operations--the standard means by which it seeks to ensure that the federal funds rate is maintained at or close to the target rate set by the Federal Open Market Committee.  This extra provision of liquidity helped bring the funds rate down to its target early in the day; it also eased banks’ concerns about the availability of funding and thus assisted the functioning of the interbank market.  The vigorous provision of funds through open market operations succeeded in damping pressures in overnight funding markets.  Yet, markets for term interbank funding remained strained.\n\nOn August 17, the Federal Reserve Board took further action by cutting the discount rate--the rate at which it lends directly to banks--by 50 basis points, or half a percentage point.  The Fed also adjusted its usual practices to facilitate the provision of financing for as long as thirty days, renewable at the request of the borrower.  These actions also appear to have improved market functioning, though strains, particularly in term funding markets, persist even now.  Moreover, judging from forward curves in interbank and overnight indexed swaps markets, market participants expect pressures in term funding markets to persist for several quarters.\n\nI should emphasize that the purpose of these actions was not to insulate financial institutions from the consequences of their business decisions, but rather to facilitate the orderly function of markets more broadly in the face of risks to the overall economy.  I believe that this provision of liquidity has contributed, at least in part, to the recent improvements we have seen in the functioning of financial markets.\n\nImportantly, the Federal Open Market Committee’s most recent action, the 50 basis point cut in the target federal funds rate in September, was an attempt to help offset the potential effects of financial market turmoil on real economic activity.  The breakdown in the price discovery process can, after all, have real economic consequences that the Federal Reserve should, in my opinion, consider when fulfilling its statutorily mandated goals of maximum employment and price stability.\n\nConclusion\nIn the months ahead, the Federal Reserve will continue to monitor developments in the financial markets and act as needed to support the effective functioning of these markets and to foster sustainable economic growth and price stability.  In addition, we will be reviewing the events of the past several months to understand the likely causes and effects.\n\nThank you very much, and I look forward to a lively dialogue following my esteemed colleagues’ remarks.\n\nFootnotes\n\n1. These views are my own and do not necessarily reflect those of the Federal Reserve Board or the Federal Reserve System as a whole. Return to text\n\n2. See, for example, the work of Friedrich A. Hayek, including \"Competition as a Discovery Procedure\" in New Studies in Philosophy, Politics, Economics and the History of Ideas.  Chicago: University of Chicago Press, 1978. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20071022a.htm",
        "title": "Recent Events in Financial Markets",
        "date": "10/22/2007"
    },
    {
        "content": "October 20, 2007\n\nGovernor Frederic S. Mishkin\n\nAt the Business Cycles, International Transmission and Macroeconomic Policies Conference, HEC Montreal, Montreal, Canada\n\nIn discussing and thinking about the conduct of monetary policy, many central bankers focus on core inflation‑‑that is, a measure of inflation that excludes the rate of increase of prices for certain volatile components in price indexes.1   The Federal Reserve, for example, pays particular attention to the rate of growth of the core personal consumption expenditure (PCE) deflator, which excludes food and energy prices.  Indeed, in the presentation of its twice yearly Monetary Policy Report to the Congress, the Federal Reserve Board reports the projections of Federal Open Market Committee participants regarding core PCE inflation, not headline inflation, the latter of which includes all items in the price index.  Here in Canada, unlike in the United States, the central bank maintains an explicit inflation target.  The Bank of Canada states its target in terms of the headline consumer price index, and although this choice of inflation measure contrasts with the Federal Reserve's preferred index, the difference is not nearly as great as it appears on the surface.  In fact, the Bank of Canada monitors a number of inflation measures and uses core inflation as an \"operational guide\" in coming to its monetary policy decisions and discussing these decisions with the public.\n\nDoes it make sense for a central bank to concentrate on core inflation?  After all, households almost daily pay for energy and food items‑‑which are excluded from the most prominent measures of core inflation‑‑when they fill up their cars at gas stations or visit a grocery store.  Households, particularly less-affluent households, spend a major portion of their budgets on food and energy.  A focus on core inflation, which excludes these items, might be viewed as indicating that monetary policy makers are out of touch with what consumers really care about.  Wouldn't it be better for monetary policy authorities to focus on headline inflation so that they include food and energy in their monitoring of consumer price inflation?\n\nAs I will argue, this is not an either-or decision.  It does indeed make sense for central banks to emphasize headline inflation when determining the appropriate stance of monetary policy over the medium run, but policymakers also are right to emphasize core inflation when deciding how to adjust policy from meeting to meeting.  Why?  Because what central bankers are truly concerned with‑‑both for the purposes of internal deliberations and for communications with the public‑‑is the underlying rate of inflation going forward, and core inflation can be a useful proxy for that rate.  Thus, focusing on core inflation can help prevent a central bank from responding too strongly to transitory movements in inflation.\n\nWhy Monetary Policy Should Focus on Core Inflation\nBecause households care about the prices of all the items they buy, it clearly does not make sense to pretend that people do not eat or drive.  Thus, I have no qualm in stating that controlling headline inflation, not core inflation, is‑‑along with maintaining maximum sustainable employment‑‑the ultimate aim of monetary policy.  Nonetheless, I will argue that it is still useful for monetary policy makers to focus on core inflation when deciding how to respond to incoming economic news.\n\nAlthough monetary policy is capable of controlling overall inflation in the long run, it does not have the ability to control relative price movements such as those for food and energy.  When a cold snap freezes the Florida orange crop or a tropical storm hits the gasoline refineries along the Gulf Coast, monetary policy cannot reverse the resulting spikes in prices for fresh orange juice or for gasoline at the pump.  Temporary supply shocks such as these raise the prices of food and energy relative to other prices and can have substantial effects on inflation in the short run.  By including all items‑‑including particularly volatile items like food and energy‑‑headline measures of inflation are inherently noisy and often do not reflect changes in the underlying rate of inflation, the rate at which headline inflation is likely to settle and that monetary policy can affect.\n\nMeasures of core inflation attempt to strip out or smooth volatile changes in particular prices to distinguish the inflation signal from the transitory noise.  Thus, relative to changes in headline inflation measures, changes in core measures are much less likely to be reversed, provide a clearer picture of the underlying inflation pressures, and so serve as a better guide to where headline inflation itself is heading.  Of course, if a particular shock to noncore prices is not temporary but, rather, turns out to be more persistent, then the higher costs are likely to put some upward pressure on core prices.  Central bankers must always be aware of this risk.  However, research has shown that, over the past twenty-five years or more, headline inflation in the United States has tended to revert more strongly toward core inflation than core inflation has moved toward headline inflation.2   As that record suggests, core measures often are much better than headline indexes at providing a first approximation of the permanent changes to inflation.\n\nWhen headline inflation has an important transitory component, a focus on core measures can help avoid monetary policy mistakes.  If the monetary authorities react to headline inflation numbers, they run the risk of responding to merely temporary fluctuations in inflation.  We can think about this danger by considering a supply shock, that causes the relative price of energy to increase sharply, as in fact has happened over the past three years. Let us suppose that we start out with both headline and core inflation at acceptably low levels.\n\nFirst, let's consider a supply shock that temporarily raises the price of energy by a large amount.  In this case headline inflation will rise well above its underlying trend as the price of energy rises but will soon fall well below its underlying trend as the price of energy falls back to its initial level.  On average, inflation will remain unchanged without any monetary policy action.  But a tightening of monetary policy in reaction to the rise in headline inflation would lead to a decline in employment and inflation; and because of the long lags between monetary policy actions and changes in economic activity, that decline would occur some time down the road, when inflation would more likely be at or below its underlying trend.  The outcome of such a policy would be a more pronounced fall in inflation with a decline in employment.  Such a policy would be bad indeed because it would increase volatility in inflation and employment, which is the opposite of what a central bank should be trying to achieve as it seeks to promote price stability and maximum sustainable employment.\n\nBut what if a supply shock leads to a permanent shift in the relative price of energy?  Indeed, almost all commentators characterize the current oil price shock that way, in part because they see the rise of new economic powers like China as adding to the demand for energy in the foreseeable future.  Although in that case the rise in relative energy prices is not reversed, the rate of change in energy prices does not persist:  Headline inflation will rise with the increase in energy prices, but once energy prices reach their permanently high level, headline inflation will revert to its underlying trend rate.  Thus, as long as the permanent change in relative energy prices does not lead to a change in the underlying trend rate of inflation‑‑a crucial assumption‑‑then headline inflation will come back down again.  This is what we seem to have seen recently in the United States.  From a low near $30 per barrel in late 2003, the price of oil rose to $70 per barrel by the middle of 2006, and it has stayed high, with the current price more than $80.  That move increased headline PCE inflation to the 4 percent level for a time, but it has since retreated to around 2 percent (figure 1).\n\nMonetary policy clearly can do little about the first-round effects of a permanent rise in energy prices, which include both its direct impact on the energy component of overall consumer prices and the pass-through of higher energy costs into prices of non-energy goods and services.  But policy does have a critical role to play in determining the second-round effects associated with changes in the underlying trend rate of inflation.  Such second-round effects are likely to be quite limited as long as the rise in the relative price of energy does not lead to a rise in long-run inflation expectations, as has largely been the case in the recent period (figure 1).  However, the stability of expectations rests on the central bank's strong long-term commitment to providing a nominal anchor, which, as I have argued elsewhere (Mishkin, 2007), describes the situation in the United States.  With such a commitment firmly established, monetary policy then does not need to respond as much to the temporary rise in headline inflation to stabilize inflation over the longer run.  Under these circumstances, if monetary policy is tightened appreciably in the face of a surge in headline inflation, the policy would likely be excessively tight and lead to an unnecessary decline in employment.\n\nBecause the point about headline inflation is so important, I would like to illustrate it further with simulations of FRB/US, the model of the U.S. economy created and maintained by the staff at the Federal Reserve Board.  To keep the experiments as clean as possible, I assume that the economy begins at full employment and with both headline and core inflation at desired levels.  The economy is then assumed to experience a shock that raises the world price of oil about $30 per barrel over two years; the shock is assumed to slowly dissipate thereafter.  In each of two scenarios, a Taylor rule is assumed to govern the response of the federal funds rate; the only difference between the two is that in one scenario the funds rate responds to core PCE inflation, while in the other it responds to headline PCE inflation.3   Figure 2 illustrates the results of these two scenarios.  The federal funds rate jumps higher and faster when the central bank responds to headline inflation rather than to core inflation, as would be expected (top-left panel).  Likewise, responding to headline inflation pushes the unemployment rate markedly higher than otherwise in the early going (top-right panel), and produces an inflation rate that is slightly lower than otherwise, whether measured by core or headline indexes (bottom panels).  More important, even for a shock as persistent as this one, the policy response under headline inflation has to be unwound in the sense that the federal funds rate must drop substantially below baseline once the first-round effects of the shock drop out of the inflation data.  Responding to headline inflation is therefore inappropriate because it generates extensive variability in the unemployment rate‑‑variability that is much more subdued when policy responds to core inflation.4\n\nOf course, the pitfalls associated with reacting to realized headline inflation, as illustrated by this simulation, are well understood.  Accordingly, advocates of targeting headline inflation generally have in mind a strategy of responding to forecasts of headline inflation‑‑forecasts that try to take account of inflation movements that are likely to be transitory.  This is a further illustration of my basic point, namely, that it is important to distinguish between transitory and persistent inflation movements, and focusing on core inflation can help achieve that end.5\n\nThe focus on core instead of headline inflation‑‑and clear communication with the public about that focus‑‑can have another benefit:  It may help anchor inflation expectations when headline inflation increases temporarily but core inflation remains essentially unchanged.  If the public understands that the central bank is using core inflation in formulating monetary policy and trusts that the central bank is right to do so, the public will realize that the central bank does not need to respond aggressively to a surge in headline inflation to keep inflation under control.  And, with core inflation stable, the public will be less likely to think that the central bank has weakened its commitment to a strong nominal anchor when it does not tighten monetary policy to stabilize headline inflation.  The result is that inflation expectations are likely to remain anchored, which may lead to better outcomes not only on inflation but on employment as well, because the central bank will not have had to tighten monetary policy as much in response to the energy price shock.  As I noted earlier, figure 1 offers some support for this view:  It shows that the rise in headline inflation in the 2004-06 period, when core inflation remained quite stable, did not lead to an appreciable rise in long-run inflation expectations, even with a substantial decline in unemployment during this period.\n\nIs Inflation Excluding Food and Energy the Best Measure of Underlying Inflation?\nThe most popular core inflation measures used and published in the United States are quite straightforward:  They simply exclude changes in food and energy prices.  However, such simple \"exclusion\" measures can likely be improved upon.  For example, many of the food categories excluded in the standard measure of core inflation, such as food away from home (restaurants), are not highly volatile and so likely should remain in an optimal core inflation measure.  On the other hand, some items that are not excluded from the U.S. core measures, such as airline fares, tobacco, and apparel, are extremely volatile and thus are unlikely to be included in an optimal core inflation measure.\n\nThe Bank of Canada, for example, uses a more targeted approach to its core inflation indicator, the CPIX.  Of fifty-four consumer product categories, it excludes eight, which account for about 16 percent of the consumption basket (Macklem, 2001).  Most of the eight items are also excluded from the standard core measure in the United States.  However, Canada's CPIX also excludes intercity transportation and tobacco, which are included in the U.S. core measures.6\n\nAnother approach to estimating core inflation measures is statistical, in which certain prices are excluded at each point on the basis of statistical criteria.  Among these statistical measures are trimmed indexes, such as the trimmed mean and the weighted median consumer price indexes produced by the Cleveland Fed and the trimmed mean PCE price index produced by the Dallas Fed.  Each month, these measures throw out the components with the largest price changes, both positive and negative (though the Dallas measure is asymmetric).7\n\nOther approaches to estimating core inflation are theory-based measures employing common trends, unobserved components, or a particular model of the economy.8   However, these theory-based approaches tend to be rather complex and require faith that the model they are based on is the right one.\n\nEach of these measures of core inflation can be evaluated according to a variety of criteria, including the ability to forecast headline inflation over some period and the degree of correlation with alternative definitions of trend inflation.  According to some of these criteria, statistical and theory-based approaches are sometimes found to outperform simpler exclusion measures of core inflation, like the standard one excluding food and energy.  As you can tell, this is an active area of research, but recent research done at the Federal Reserve Bank of New York on U.S. data finds that no one particular core measure, including the standard one, dominates the others:  The relative performance of different core measures varies depending on the choice of the price index, the sample period, and the criteria for evaluating their performance (Rich and Steindel, 2007).  Research on Canadian and U.K. data comes to similar conclusions (Hogan, Johnson, and Laflèche, 2001; Mankikar and Paisley, 2002).\n\nDoes the lack of empirical support for any one particular type of core measure suggest that our focus on the standard core measure should be abandoned?   I think not.  The simplicity and long history of the standard core measure that excludes food and energy gives it several major advantages.  Its simplicity makes it straightforward to explain and thus more understandable to the public‑‑assuming, of course, that we successfully communicate that we recognize the importance of food and energy items in people's consumption.  Its simplicity also makes its estimation very transparent.  That transparency and the fact that it has been around a long time‑‑hence the characterization as \"standard\"‑‑both prevent a central bank or a government authority from easily manipulating the measure to show good results.  These features of the standard core measure therefore make it a credible device to keep inflation expectations anchored when supply shocks occur:  If the standard measure remains stable, then a surge in headline inflation is less likely to unhinge inflation expectations, which as we have seen can lead to both better inflation and better employment performance.\n\nWhy Central Banks Should also Focus on Headline Inflation\nDespite the advantages of core measures of inflation, their use in conducting monetary policy has been criticized.9   Core measures of inflation attempt to remove the most volatile or transitory components of the inflation measure.  But, because the nature of price shocks may change over time, items that have been highly volatile in the past may not be so in the future; hence, any core measure will itself likely be subject to transitory shocks.  Unfortunately, as noted above, empirical research suggests that no one measure of core inflation will work in all situations.  Therefore, central banks do not focus solely on core inflation; rather, they devote considerable resources to understanding inflation developments in an effort to distinguish signal from noise in the incoming data.10\n\nAnother reason to keep watch over the broader inflation picture is that, if the rate of change in the price of an excluded item receives a permanent shock, then the headline inflation rate can deviate from the core measure for an extended period of time.  The longer this period of high headline inflation persists, the greater is the risk of second-round effects as the public begins to build this higher inflation into its expectations.  For example, from 1987 to 2002, energy price shocks appear to have been temporary in that energy prices were mean reverting; as a result, in this period, the average difference between core and headline PCE inflation was quite small.  Since 2002, however, the effect of energy price shocks have been more persistent, and headline inflation has averaged more than 1/2 percentage point above core inflation over this period.  Over the past year, energy prices have about leveled off on balance--or at least that was the case until a couple of weeks ago--and the rate of headline inflation for most of this year has come down close to that for core, one hopes alleviating the risk of second-round effects.  But the increases in oil prices in recent days provide another reminder that shocks can persist longer than one might have at first expected.\n\nA prolonged divergence between core and headline measures of inflation could complicate central bank communications with the public because core inflation would require some adjustment before it would provide a clear gauge of underlying inflation.  For example, the weighted median CPI inflation rate, one popular measure of core inflation, has tended to be a biased measure of headline inflation.  The weighted median is exactly equal to owners' equivalent rent nearly half the time because that component has an extremely large share of the consumption basket and fairly low volatility (Bryan and Pike, 1991; Hogan and others, 2001; and Bryan and Meyer, 2007).  However, the productivity growth rate in residential construction is not terribly high, and so this sector is likely to have below-average productivity growth and above-average price increases.  Indeed, the weighted median CPI exceeded overall CPI inflation on average by 1/4 percentage point per year over the decade from 1992 to 2002, a period when changes in energy prices were transitory.  Of course, to the extent that such a bias in a core inflation measure is stable or predictable, the central bank could easily take this into account in setting monetary policy.  But even in that case, differences in average rates of inflation between the core measure and overall inflation could complicate communication with the public.\n\nConclusion\nI have argued that a measure of core inflation that is easily understood and provides some greater signal about persistent movements in inflation than does headline inflation itself is extremely valuable for the conduct of monetary policy, and that is why the Federal Reserve pays so much attention to such measures.  However, I have also argued that core measures have their limitations.  A single core inflation measure cannot account for all types of shocks and can at times be misleading about what is happening to the underlying rate of overall inflation.  And if increases in headline inflation prove more persistent than initially expected, central bankers must be vigilant to ensure that they do not become embedded into expectations and thereby generate substantial second-round effects on inflation.  Finally, because price stability ultimately involves control of overall, headline inflation, which after all is the inflation measure that households really care about, central bankers should and do pay attention to headline inflation as well as to core inflation measures.  A core inflation measure should not be seen as a substitute for thorough and careful analysis of the forces that are driving our economy and the inflation process.\n\n\n\nReferences\nAoki, Kosuke (2001). \"Optimal Monetary Policy Responses to Relative Price Changes,\"   Journal of Monetary Economics, vol. 48 (August), pp. 55-80.\n\nArmour, Jamie (2006). \"An Evaluation of Core Inflation Measures,\"  Bank of Canada Working Paper 2006-10 (Ottawa: Bank of Canada, March).\n\nBagliano, Fabio C., and Claudio Morana (2003). \"Measuring U.S. Core Inflation: A Common Trends Approach,\"  Journal of Macroeconomics, vol. 25 (June), pp. 197-212.\n\nBean, Charles (2006). \"Commentary: Impact of Globalization on Monetary Policy (40 KB PDF),\" speech delivered at the Federal Reserve Bank of Kansas City 30th Annual Economic Symposium, Jackson Hole, Wyo., August 26, www.kansascityfed.org/publicat/Sympos/2006/sym06prg.htm.\n\nBernanke, Ben S. (2007). \"Inflation Expectations and Inflation Forecasting,\" speech delivered at the Monetary Economics Workshop of the National Bureau of Economic Research Summer Institute, Cambridge, Mass., July 10, www.federalreserve.gov/newsevents.\n\nBlinder, Alan S. (2006). \"Monetary Policy Today: Sixteen Questions and About Twelve Answers,\" in S. Fernandez de Lis and F. Restoy, eds., Central Banks in the 21st Century. Madrid: Banco de España, pp. 31-72.\n\nBodenstein, Martin, Christopher Erceg, and Luca Guerrieri (2007). \"Optimal Monetary Policy in a Model with Distinct Core and Headline Inflation Rates,\" unpublished paper, Board of Governors of the Federal Reserve System.\n\nBrischetto, Andrea, and Anthony Richards (2006). \"The Performance of Trimmed Mean Measures of Underlying Inflation,\"  Reserve Bank of Australia Research Discussion Paper 2006-10. Sydney: Reserve Bank of Australia, December, www.rba.gov.au/PublicationsAndResearch/RDP.\n\n_________ (2007). \"The Performance of Trimmed Mean Measures of Underlying Inflation,\" speech delivered at the Federal Reserve Bank of Dallas Conference on Price Measurement for Monetary Policy, May 24-25, www.dallasfed.org/news/research/2007/07price.cfm.\n\nBryan, Michael F., and Christopher J. Pike (1991). \"Median Price Changes: An Alternative Approach to Measuring Current Monetary Inflation (197 KB PDF),\" Federal Reserve Bank of Cleveland, Economic Commentary, December 1, www.clevelandfed.org/Research/commentary.\n\nBryan, Michael F., and Stephen G. Cecchetti (1994). \"Measuring Core Inflation,\" in N. Gregory Mankiw, ed., Studies in Business Cycles, vol. 29: Monetary Policy. Chicago: University of Chicago Press, pp. 195-215.\n\nBryan, Michael F., and Brent H. Meyer (2007). \"Methodological Adjustments to the Median and 16 Percent Trimmed-Mean CPI Estimators,\" Cleveland: Federal Reserve Bank of Cleveland, September, www.clevelandfed.org/research/inflation.\n\nCecchetti, Stephen G. (1997). \"Measuring Short-Run Inflation for Central Bankers (295 KB PDF),\" Federal Reserve Bank of St. Louis, Review, vol. 79 (May/June), pp. 143-55.\n\nClark, Todd E. (2001). \"Comparing Measures of Core Inflation,\" Federal Reserve Bank of Kansas City, Economic Review, vol. 86 (no. 2), pp. 5-31.\n\nClinton, Kevin (2006). \"Core Inflation at the Bank of Canada: A Critique (85 KB PDF),\"  Queen's Economics Department Working Paper No. 1077. Kingston, ON, Canada: Queen's University (May), http://qed.econ.queensu.ca/pub/papers.\n\nCogley, Timothy (2002). \"A Simple Adaptive Measure of Core Inflation,\"  Journal of Money, Credit and Banking, vol. 34 (February), pp. 94-113.\n\nCristadoro, Riccardo, Mario Forni, Lucrezia Reichlin, and Giovanni Veronese (2005). \"A Core Inflation Indicator for the Euro Area,\"  Journal of Money, Credit and Banking, vol. 37 (June), pp. 539-60.\n\nDolmas, Jim (2005). \"Trimmed Mean PCE Inflation (1.7 MB PDF),\" Working Paper Series 0506. Dallas: Federal Reserve Bank of Dallas, July, http://dallasfed.org/research/papers.\n\nDoménech, Rafael, and Victor Gómez (2006). \"Estimating Potential Output, Core Inflation, and the NAIRU as Latent Variables,\"  Journal of Business and Economic Statistics, vol. 24 (July), pp. 354-65.\n\nHogan, Seamus, Marianne Johnson, and Thérèse Laflèche (2001). \"Core Inflation,\"  Technical Report No. 89. Ottawa: Bank of Canada, January, www.bank-banque-canada.ca/en/res/tr.\n\nLaflèche, Thérèse, and Jamie Armour (2006). \"Evaluating Measures of Core Inflation,\"  Bank of Canada Review, 2006 (Summer), pp. 19-29.\n\nLaidler, David, and Shay Aba (2000). \"It's Time to Ignore Core Inflation (54 KB PDF)\"  C.D. Howe Institute, Backgrounder, vol. 2001 (November), pp. 1-8, www.cdhowe.org.\n\nMacklem, Tiff (2001). \"A New Measure of Core Inflation,\"  Bank of Canada Review,  2001 (Autumn), pp. 3-12.\n\nMankikar, Alan, and Jo Paisley (2002). \"What Do Measures of Core Inflation Really Tell Us? (102 KB PDF)\"  Bank of England Quarterly Bulletin, vol. 42 (Winter), pp. 373-83.\n\nMishkin, Frederic S. (2007). \"Inflation Dynamics,\" speech delivered at the Annual Macro Conference, Federal Reserve Bank of San Francisco, San Francisco, March 23, www.federalreserve.gov/newsevents.\n\nOrganisation for Economic Co-operation and Development (2005). \"Measuring and Assessing Underlying Inflation,\"  OECD Economic Outlook, vol. 2005 (June), pp. 270-90.\n\nQuah, Danny, and Shaun P. Vahey (1995). \"Measuring Core Inflation,\"  Economic Journal, vol. 105 (September), pp. 1130-44.\n\nRich, Robert, and Charles Steindel (2005). \"A Review of Core Inflation and an Evaluation of its Measures,\" Staff Report No. 236. Federal Reserve Bank of New York, December, www.newyorkfed.org/research/staff_reports.\n\n_________ (2007). \"A Review of Core Inflation and an Evaluation of its Measures,\" speech delivered at the Conference on Price Measurement for Monetary Policy, Federal Reserve Bank of Dallas, May 24-25.\n\nRobalo Marques, Carlos, Pedro Duarte Neves, and Luís Morais Sarmento (2003). \"Evaluating Core Inflation Indicators,\" Economic Modelling, vol. 20 (July), pp. 765-75.\n\nSmith, Julie K. (2004). \"Weighted Median Inflation: Is This Core Inflation?\" Journal of Money, Credit and Banking, vol. 36 (May), pp. 253-63.\n\nVelde, François R. (2006). \"An Alternative Measure of Inflation (591 KB PDF),\" Federal Reserve Bank of Chicago, Economic Perspectives, vol. 30 (no. 1), pp. 55-65.\n\nWoodford, Michael (2003).  Interest and Prices: Foundations of a Theory of Monetary Policy.  Princeton: Princeton University Press.\n\n\n\nFootnotes\n\n1.  I thank Alan Kackmeister, Jean-Philippe Laforte, David Lebow, Deb Lindner, and Robert Tetlow for their comments and assistance.  The speech reflects my own views and not necessarily those of others on the Federal Open Market Committee. Return to text\n\n2.   These tests allow for a nonzero constant differential between headline and core inflation.  The differential often turns out to be nontrivial and statistically significant.  Cogley (2002) uses published CPI over the period from 1967:Q2 to 1997:Q4 and finds that it may take about 8 quarters before there is substantial reversion of headline inflation to core inflation.  Clark (2001), using 12-month CPI inflation from 1967 to 2000 finds little reversion over the next 12 months but about 50 percent reversion over 24 months, though that level is still statistically insignificant.  In contrast, he finds substantial and statistically significant reversion at both the 12- and 24-month horizons using data from 1985 to 2000.  Rich and Steindel (2007) find significant reversion of headline CPI inflation to core CPI inflation over 12 quarters using methodologically consistent CPI data from 1978 to 2004.  An earlier version of their work (Rich and Steindel, 2005) showed similar results using PCE price inflation in the 1978-2004 and 1959-2004 periods.  Internal work conducted at the Federal Reserve using 15-year rolling-window regressions suggests that the reversion of headline PCE inflation to core PCE inflation has been much stronger in samples that start after the early 1980s and also that the reversion of core inflation to headline inflation has been much weaker in that period.\n\nResults for other countries are more ambiguous.  In recent years, Canada appears to have had a similar reversion of headline inflation to their version of core inflation, along with little reversion of core inflation to headline inflation (Laflèche and Armour, 2006; also, Armour, 2006; Hogan, Johnson, and Laflèche, 2001).    However, the OECD (2005) has found that many countries show little reversion of headline inflation to core inflation and that, in fact, core inflation often reverts to headline inflation. Return to text\n\n3.   The Taylor rule is written as follows:  where  is the four-quarter inflation rate, either core or headline,  is the inflation target, taken to be the baseline inflation rate, and  is the output gap. This specification means that the response coefficients on each gap variable is 1.  Return to text\n\n4.  These scenarios were constructed using a rule that assumes no prior knowledge of how long the oil price shock will last.  Research done by the staff at the Federal Reserve Board using other types of models also suggests that when the persistence of shocks is uncertain, using core inflation rather than headline inflation  in central bank reaction functions can improve policy outcomes (Bodenstein, Erceg, and Guerrieri, 2007). Return to text\n\n5.  Economic theory also suggests an additional argument for focusing on a core measure of inflation.  Because some prices are \"sticky,\" meaning they move sluggishly in response to shocks, higher inflation generates greater dispersion of relative prices and, with that, a misallocation of resources.  Responding to a core inflation measure, which puts more weight on sticky prices, can minimize this distortion and promote relative prices that better reflect the true underlying demand and supply conditions in the economy (for example, Aoki, 2001; and Woodford, 2003, chap. 6). Return to text\n\n6.  Canada’s CPIX also excludes mortgage interest costs, which are never included in measures of  U.S. consumer prices because the prices of owner occupied housing services are measured on a rental equivalence basis in the United States.  The other items excluded from the CPIX are fruits, vegetables, gasoline, natural gas, and fuel oil and other fuel. Return to text\n\n7.  Bryan and Cecchetti (1994); Robalo Marques, Duarte Neves, and Morais Sarmento (2003); Dolmas (2005); Brischetto and Richards (2007); Smith (2004). Return to text\n\n8.  Recent examples include: the common trends approach of Bagliano and Morana (2003), the VAR approach of Quah and Vahey (1995); the unobserved components approach of Doménech and Gomez (2006) and Velde (2006); and the factor-models approach of Cecchetti (1997) and Cristadoro and others (2005). Return to text\n\n9.  Laidler and Aba (2000); Clinton (2006); and Bean (2006) are critical of responding to core inflation;  Blinder (2006) argues on the other side. Return to text\n\n10.  More details on inflation forecasting at the Board are in Bernanke (2007). Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/mishkin20071020a.htm",
        "title": "Headline versus Core Inflation in the Conduct of Monetary Policy",
        "date": "10/20/2007"
    },
    {
        "content": "October 19, 2007\n\nChairman Ben S. Bernanke\n\nAt the 32nd Annual Economic Policy Conference, Federal Reserve Bank of St. Louis (via videoconference)\n\nBill Poole's career in the Federal Reserve System spans two decades separated by a quarter of a century.  From 1964 to 1974 Bill was an economist on the staff of the Board's Division of Research and Statistics.  He then left to join the economics faculty at Brown University, where he stayed for nearly twenty-five years.  Bill rejoined the Fed in 1998 as president of the Federal Reserve Bank of St. Louis, so he is now approaching the completion of his second decade in the System.\n\nAs it happens, each of Bill's two decades in the System was a time of considerable research and analysis on the issue of how economic uncertainty affects the making of monetary policy, a topic on which Bill has written and spoken many times.  I would like to compare the state of knowledge on this topic during Bill's first decade in the System with what we have learned during his most recent decade of service.  The exercise is interesting in its own right and has the added benefit of giving me the opportunity to highlight Bill's seminal contributions in this line of research.\n\nDevelopments during the First Period: 1964-74\nIn 1964, when Bill began his first stint in the Federal Reserve System, policymakers and researchers were becoming increasingly confident in the ability of monetary and fiscal policy to smooth the business cycle.  From the traditional Keynesian perspective, which was the dominant viewpoint of the time, monetary policy faced a long-term tradeoff between inflation and unemployment that it could exploit to keep unemployment low over an indefinitely long period at an acceptable cost in terms of inflation.  Moreover, improvements in econometric modeling and the importation of optimal-control methods from engineering were seen as having the potential to tame the business cycle.\n\nOf course, the prevailing optimism had its dissenters, notably Milton Friedman.  Friedman believed that the inherent complexity of the economy, the long and variable lags with which monetary policy operates, and the political and bureaucratic influences on central bank decisionmaking precluded policy from fine tuning the level of economic activity.  Friedman advocated the use of simple prescriptions for monetary policy--such as the k percent money growth rule--which he felt would work reasonably well on average while avoiding the pitfalls of attempting to fine-tune the economy in the face of pervasive uncertainty (Friedman, 1968).\n\nOther economists were more optimistic than Friedman about the potential benefits of activist policies.  Nevertheless, they recognized that the fundamental economic uncertainties faced by policymakers are a first-order problem and that improving the conduct of policy would require facing that problem head on.  During this decade, those researchers as well as sympathetic policymakers focused especially on three areas of economic uncertainty:  the current state of the economy, the structure of the economy (including the transmission mechanism of monetary policy), and the way in which private agents form expectations about future economic developments and policy actions.\n\nUncertainty about the current state of the economy is a chronic problem for policymakers.  At best, official data represent incomplete snapshots of various aspects of the economy, and even then they may be released with a substantial lag and be revised later.  Apart from issues of measurement, policymakers face enormous challenges in determining the sources of variation in the data.  For example, a given change in output could be the result of a change in aggregate demand, in aggregate supply, or in some combination of the two.\n\nAs most of my listeners know, Bill Poole tackled these issues in a landmark 1970 paper, which examined how uncertainty about the state of the economy affects the choice of the operating instrument for monetary policy (Poole, 1970).  In the simplest version of his model, Bill assumed that the central bank could choose to specify its monetary policy actions in terms of a particular level of a monetary aggregate or a particular value of a short-term nominal interest rate.  If the central bank has only partial information about disturbances to money demand and to aggregate demand, Bill showed that the optimal choice of policy instrument depends on the relative variances of the two types of shocks.  In particular, using the interest rate as the policy instrument is the better choice when aggregate demand is relatively stable but money demand is unstable, with money growth being the preferable policy instrument in the opposite case.\n\nBill was also a pioneer in formulating simple feedback rules that established a middle ground between the mechanical approach advocated by Friedman and the highly complex prescriptions of optimal-control methods.  For example, Bill wrote a Federal Reserve staff paper titled \"Rules-of-Thumb for Guiding Monetary Policy\" (Poole, 1971).  Because his econometric analysis of the available data indicated that money demand was more stable than aggregate demand, Bill formulated a simple rule that adjusted the money growth rate in response to the observed unemployment rate.  Bill was also practical in noting the pitfalls of mechanical adherence to any particular policy rule; in this study, for example, he emphasized that the proposed rule was not intended \"to be followed to the last decimal place or as one that is good for all time [but] . . . as a guide--or as a benchmark--against which current policy may be judged\" (p. 152).\n\nUncertainty about the structure of the economy also received attention during that decade.  For example, in his elegant 1967 paper, Bill Brainard showed that uncertainty about the effect of policy on the economy may imply that policy should respond more cautiously to shocks than would be the case if this uncertainty did not exist.  Brainard's analysis has often been cited as providing a theoretical basis for the gradual adjustment of policy rates of most central banks.  Alan Blinder has written that the Brainard result was \"never far from my mind when I occupied the Vice Chairman's office at the Federal Reserve.  In my view, . . . a little stodginess at the central bank is entirely appropriate\" (Blinder, 1998, p. 12).\n\nA key source of uncertainty became evident in the late 1960s and 1970s as a result of highly contentious debates about the formation of expectations by households and firms.  Friedman (1968) and Ned Phelps (1969) were the first to highlight the central importance of expectations formation, arguing that the private sector's expectations adjust in response to monetary policy and therefore preclude any long-run tradeoff between unemployment and inflation.  However, Friedman and Phelps retained the view that monetary policy could exert substantial effects on the real economy over the short to medium run.  In contrast, Robert Lucas and others reached more dramatic conclusions, arguing that only unpredictable movements in monetary policy can affect the real economy and concluding that policy has no capacity to smooth the business cycle (Lucas, 1972; Sargent and Wallace, 1975).  Although these studies highlighted the centrality of inflation expectations for the analysis of monetary policy, the profession did not succeed in reaching any consensus about how those expectations evolve, especially in an environment of ongoing structural change.\n\nDevelopments during the Second Period:  1998-2007\nResearch during the past ten years has been very fruitful in expanding the profession's understanding of the implications of uncertainty for the design and conduct of monetary policy.\n\nOn the issue of uncertainty about the state of the economy, Bill's work continues to provide fundamental insights regarding the choice of policy instrument.  Money demand relationships were relatively stable through the 1950s and 1960s, but, in the wake of dramatic innovations in banking and financial markets, short-term money-demand relationships became less predictable, at least in the United States.  As a result, consistent with the policy implication of Bill's 1970 model, the Federal Reserve (like most other central banks) today uses the overnight interbank rate as the principal operating target of monetary policy.  Bill's research also raised the possibility of specifying the operating target in other ways, for example, as an index of monetary or financial conditions; and it provided a framework for evaluating the usefulness of intermediate targets--such as core inflation or the growth of broad money--that are only indirectly controlled by policy.\n\nMore generally, the task of assessing the current state of the economy remains a formidable challenge.  Indeed, our appreciation of that challenge has been enhanced by recent research using real time data sets.1  For example, Athanasios Orphanides has shown that making such real-time assessments of the sustainable levels of economic activity and employment is considerably more difficult than estimating those levels retrospectively.  His 2002 study of U.S. monetary policy in the 1970s shows how mismeasurement of the sustainable level of economic activity can lead to serious policy mistakes.\n\nOn a more positive note, economists have made substantial progress over the past decade in developing new econometric methods for summarizing the information about the current state of the economy contained in a wide array of economic and financial market indicators (Svensson and Woodford, 2003).  Dynamic-factor models, for example, provide a systematic approach to extracting information from real-time data at very high frequencies.  These approaches have the potential to usefully supplement more informal observation and human judgment (Stock and Watson, 2002; Bernanke and Boivin, 2003; and Giannone, Reichlin, and Small, 2005).\n\nThe past decade has also witnessed significant progress in analyzing the policy implications of uncertainty regarding the structure of the economy.  New work addresses not only uncertainty about the values of specific parameters in a given model of the economy but also uncertainty about which of several competing models provides the best description of reality.  Some research has attacked those problems using Bayesian optimal-control methods (Brock, Durlauf, and West, 2003).  The approach requires the specification of an explicit objective function as well as of the investigator's prior probabilities over the set of plausible models and parameter values.  The Bayesian approach provides a useful benchmark for policy in an environment of well-defined sources of uncertainty about the structure of the economy, and the resulting policy prescriptions give relatively greater weight to outcomes that have a higher probability of being realized.  In contrast, other researchers, such as Lars Hansen and Thomas Sargent, have developed robust-control methods--adapted from the engineering literature--that are aimed at minimizing the consequences of worst-case scenarios, including those with only a low probability of being realized (Hansen and Sargent, 2007).\n\nAn important practical implication of all this recent literature is that Brainard's attenuation principle may not always hold.  For example, when the degree of structural inertia in the inflation process is uncertain, the optimal Bayesian policy tends to involve a more pronounced response to shocks than would be the case in the absence of uncertainty (Söderstrom, 2002).  The concern about worst-case scenarios emphasized by the robust-control approach may likewise lead to amplification rather than attenuation in the response of the optimal policy to shocks (Giannoni, 2002; Onatski and Stock, 2002; and Tetlow and von zur Muehlen, 2002).  Indeed, intuition suggests that stronger action by the central bank may be warranted to prevent particularly costly outcomes.\n\nAlthough Bayesian and robust-control methods provide insights into the nature of optimal policy, the corresponding policy recommendations can be complex and sensitive to the set of economic models being considered.  A promising alternative approach--reminiscent of the work that Bill Poole did in the 1960s--focuses on simple policy rules, such as the one proposed by John Taylor, and compares the performance of alternative rules across a range of possible models and sets of parameter values (Levin, Wieland, and Williams, 1999 and 2003).  That approach is motivated by the notion that the perfect should not be the enemy of the good; rather than trying to find policies that are optimal in the context of specific models, the central bank may be better served by adopting simple and predictable policies that produce reasonably good results in a variety of circumstances.\n\nGiven the centrality of inflation expectations for the design of monetary policy, a key development over the past decade has been the burgeoning literature on the formation of these expectations in the absence of full knowledge of the underlying structure of the economy.2  For example, considerations of how the public learns about the economy and the objectives of the central bank can affect the form of the optimal monetary policy (Gaspar, Smets, and Vestin, 2006; Orphanides and Williams, 2007).  Furthermore, when the public is unsure about the central bank's objectives, even greater benefits may accompany achieving a stable inflation rate, as doing so may help anchor the public's inflation expectations.  These studies also show why central bank communications is a key component of monetary policy; in a world of uncertainty, informing the public about the central bank's objectives, plans, and outlook can affect behavior and macroeconomic outcomes (Bernanke, 2004; and Orphanides and Williams, 2005).\n\nConclusion\nUncertainty--about the state of the economy, the economy's structure, and the inferences that the public will draw from policy actions or economic developments--is a pervasive feature of monetary policy making.  The contributions of Bill Poole have helped refine our understanding of how to conduct policy in an uncertain environment.  Notably, we now appreciate that policy decisions under uncertainty must take into account a range of possible scenarios about the state or structure of the economy, and those policy decisions may look quite different from those that would be optimal under certainty.  For example, policy actions may be attenuated or augmented relative to the \"no-uncertainty benchmark,\" depending on one's judgments about the possible outcomes and the costs associated with those outcomes.  The fact that the public is uncertain about and must learn about the economy and policy provides a reason for the central bank to strive for predictability and transparency, avoid overreacting to current economic information, and recognize the challenges of making real-time assessments of the sustainable level of real economic activity and employment.  Most fundamentally, our discussions of the pervasive uncertainty that we face as policymakers is a powerful reminder of the need for humility about our ability to forecast and manage the future course of the economy.\n\nReferences\nBernanke, Ben S. (2004).  \"Fedspeak,\" speech delivered at the Meetings of the American Economic Association, San Diego, January 3, www.federalreserve.gov/boarddocs/speeches/2004/200401032/default.htm.\n\n_________ (2007). \"Inflation Expectations and Inflation Forecasting,\" speech delivered at the Monetary Economics Workshop of the National Bureau of Economic Research Summer Institute, Cambridge, Mass., July 10, www.federalreserve.gov/newsevents/speech/bernanke20070710a.htm.\n\nBernanke, Ben S., and Jean Boivin (2003). \"Monetary Policy in a Data-Rich Environment,\"  Journal of Monetary Economics, vol. 50 (April), pp. 525-46.\n\nBlinder, Alan S. (1998). Central Banking in Theory and Practice. Cambridge, Mass.:  MIT Press.\n\nBrainard, William C. (1967). \"Uncertainty and the Effectiveness of Policy,\" American Economic Review, vol. 57 (May, Papers and Proceedings), pp. 411-25.\n\nBrock, William A., Steven N. Durlauf, and Kenneth D. West (2003). \"Policy Analysis in Uncertain Economic Environments,\" Brookings Papers on Economic Activity, vol. 2003 (no. 1), pp. 235-322.\n\nFaust, Jon, and Jonathan H. Wright (2007).  \"Comparing Greenbook and Reduced Form Forecasts Using a Large Realtime Dataset (259 KB PDF),\" paper presented at \"Real-Time Data Analysis and Methods in Economics,\" a conference held at the Federal Reserve Bank of Philadelphia, April 19-20, www.phil.frb.org/econ/conf/rtconference2007/papers/Paper-Wright.pdf.\n\nFriedman, Milton (1968). \"The Role of Monetary Policy.\" American Economic Review, vol. 58 (March), pp. 1-17.\n\nGaspar, Vitor, Frank Smets, and David Vestin (2006).  \"Adaptive Learning, Persistence, and Optimal Monetary Policy,\" Journal of the European Economic Association, vol. 4 (April-May), pp. 376-85.\n\nGiannone, Domenico, Lucrezia Reichlin, and David Small (2005). \"Nowcasting GDP and Inflation: The Real-Time Informational Content of Macroeconomic Data Releases,\" Finance and Economics Discussion Series 2005-42. Washington: Board of Governors of the Federal Reserve System, October, www.federalreserve.gov/pubs/feds/2005.\n\nGiannoni, Marc P. (2002). \"Does Model Uncertainty Justify Caution? Robust Optimal Monetary Policy in a Forward-Looking Model,\" Macroeconomic Dynamics, vol. 6 (February), pp. 111-44.\n\nHansen, Lars Peter, and Thomas J. Sargent (2007). Robustness. Princeton: Princeton University Press.\n\nLevin, Andrew, Volker Wieland, and John Williams (1999). \"Robustness of Simple Monetary Policy Rules under Model Uncertainty,\" in Taylor, John, ed., Monetary Policy Rules.  Chicago:  University of Chicago Press, pp. 263-99.\n\n_________ (2003). \"The Performance of Forecast-Based Monetary Policy Rules under Model Uncertainty,\" American Economic Review, vol. 93 (June), pp. 622-45.\n\nLucas, Robert E., Jr. (1972). \"Expectations and the Neutrality of Money,\" Journal of Economic Theory, vol. 4 (June), pp.103-24.\n\nOnatski, Alexei, and James H. Stock (2002). \"Robust Monetary Policy under Model Uncertainty in a Small Model of the U.S. Economy,\" Macroeconomic Dynamics, vol. 6 (March), pp. 85-110.\n\nOrphanides, Athanasios (2002). \"Monetary-Policy Rules and the Great Inflation,\" American Economic Review, vol. 92 (May, Papers and Proceedings), pp. 115-20.\n\nOrphanides, Athanasios, and John C. Williams (2005).  \"Inflation Scares and Forecast-based Monetary Policy,\" Review of Economic Dynamics, vol. 8 (April), pp. 498-527.\n\n_________ (2007).  \"Robust Monetary Policy with Imperfect Knowledge,\"  Journal of Monetary Economics, vol. 54 (July), pp. 1406-35.\n\nPhelps, Edmund S. (1969). \"The New Microeconomics in Inflation and Employment Theory,\" American Economic Review, vol. 59 (May, Papers and Proceedings), pp. 147-60.\n\nPoole, William (1970). \"Optimal Choice of Monetary Policy Instruments in a Simple Stochastic Macro Model,\" Quarterly Journal of Economics, vol. 84 (May), pp. 197-216.\n\n_________ (1971). \"Rules-of-Thumb for Guiding Monetary Policy,\" in Open Market Policies and Operating Procedures--Staff Studies.  Washington:  Board of Governors of the Federal Reserve System, pp. 135-89.\n\nSargent, Thomas J., and Neil Wallace (1975).  \"'Rational' Expectations, the Optimal Monetary Instrument, and the Optimal Money Supply Rule,\" Journal of Political Economy, vol. 83 (April), pp. 241-54.\n\nSöderstrom, Ulf (2002). \"Monetary Policy with Uncertain Parameters,\" Scandinavian Journal of Economics, vol. 104 (February), pp. 125-45.\n\nStock, James, and Mark Watson (2002). \"Forecasting Using Principal Components from a Large Number of Predictors,\" Journal of the American Statistical Association, vol. 97 (December), pp. 1167-79.\n\nSvensson, Lars E.O., and Michael Woodford (2003). \"Indicator Variables for Optimal Policy,\"  Journal of Monetary Economics, vol. 50 (April), pp. 691-720.\n\nTetlow, Robert, and Peter von zur Muehlen (2001). \"Robust Monetary Policy with Misspecified Models: Does Model Uncertainty Always Call for Attenuated Policy?\" Journal of Economic Dynamics and Control, vol. 25 (June), pp. 911-49.\n\nFootnotes\n\n1.  A recent example is Faust and Wright (2007). Return to text\n\n2.  Bernanke (2007) and the references therein. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20071019a.htm",
        "title": "Monetary Policy under Uncertainty",
        "date": "10/19/2007"
    },
    {
        "content": "October 15, 2007\n\nChairman Ben S. Bernanke\n\nAt the Economic Club of New York, New York, New York\n\nThe past several months have been an eventful period for the U.S. economy. In financial markets, sharpened concerns about credit quality induced a retrenchment by investors, leading in some cases to significant deterioration in market functioning. For some households and firms, credit became harder to obtain and, for those who could obtain it, more costly. Tightening credit conditions in turn threatened to intensify the ongoing correction in the housing market and to restrain economic growth. In response to these developments, the Federal Reserve has taken a number of measures to help ensure the normal functioning of financial markets and to promote sustainable economic growth and price stability. In my remarks this evening I will review recent events, discuss the Federal Reserve's responses to those events, and conclude with some comments on the economic outlook in light of recent developments. Although financial markets around the world have come under pressure in the past few months, I will focus my comments primarily on the United States. I will also have little to say this evening about the serious implications of rising rates of mortgage delinquency and foreclosure for troubled borrowers and their communities or about the Federal Reserve's responses to these important problems; I have discussed these issues several times in the past and will return to them in the future.\n\nThe Origins and Evolution of the Financial Turmoil\nOverall, U.S. economic performance so far this year has been reasonably good. The rate of economic expansion slowed somewhat in late 2006 and early 2007, but growth in the second quarter was solid and some of that momentum appears to have carried over into the third quarter. The pace of private-sector job creation has slowed this year, but the unemployment rate has moved up only a little from its recent lows. And, although energy prices have been volatile, indicators of the underlying inflation trend, such as core inflation, have moderated since the middle of last year.\n\nModerate growth in overall economic activity has continued despite a notable contraction in the housing sector that began in the second half of 2005. The housing correction has intensified this year as demand has declined further, inventories of unsold new homes have climbed relative to sales, and house prices have decelerated, with some areas of the country experiencing outright declines in home values. In response to weak demand and bloated inventories, homebuilders have sharply curtailed new construction. The decline in residential investment directly subtracted about 3/4 percentage point from the average pace of U.S. economic growth over the past year and a half.  In its regular reports to Congress, most recently in July, the Federal Reserve Board has highlighted as a downside risk the possibility that housing weakness might spill over to other parts of the economy--for example, by acting as a restraint on consumer spending.  Thus far, however, direct evidence of such spillovers onto the broader economy has been limited.\n\nThe housing correction has taken a more visible toll on the financial markets. In particular, since early this year, investors have become increasingly concerned about the credit quality of mortgages, especially subprime mortgages.  The rate of serious delinquencies has risen notably for subprime mortgages with adjustable rates, reaching nearly 16 percent in August, roughly triple the recent low in mid-2005.1  Subprime mortgages originated in late 2005 and 2006 have performed especially poorly, in part because of a deterioration in underwriting standards. Moreover, many recent-vintage subprime loans will experience their first interest-rate resets in coming quarters. With the softness in house prices likely to make refinancing more difficult, delinquencies on these mortgages are expected to rise further.\n\nAt one time, most mortgages were originated by depository institutions and held on their balance sheets. Today, however, mortgages are often bundled together into mortgage-backed securities or structured credit products, rated by credit rating agencies, and then sold to investors. As mortgage losses have mounted, investors have questioned the reliability of the credit ratings, especially those of structured products. Since many investors had not performed independent evaluations of these often-complex instruments, the loss of confidence in the credit ratings led to a sharp decline in the willingness of investors to purchase these products. Liquidity dried up, prices fell, and spreads widened. Since July, few securities backed by subprime mortgages have been issued.\n\nInvestors' reluctance to buy has not been confined to securities related to subprime mortgages. Notably, the secondary market for private-label securities backed by prime jumbo mortgages has also contracted, and issuance of such securities has dwindled.2  Even though default rates on such mortgages have remained very low, the experience with subprime mortgages has evidently made investors more sensitive to the risks associated with other housing-related assets as well.\n\nThe problems in the mortgage-related sector reverberated throughout the financial system and particularly in the market for asset-backed commercial paper (ABCP). In this market, various institutions have established special-purpose vehicles to issue commercial paper to help fund a variety of assets, including some private-label mortgage-backed securities, mortgages warehoused for securitization, and other long-maturity assets. Investors had typically viewed the commercial paper backed by these assets as quite safe and liquid, because of the quality of the collateral and because the paper is often supported by banks' commitments to provide lines of credit or to assume some credit risk. But the concerns about mortgage-backed securities and structured credit products (even those unrelated to mortgages) greatly reduced the willingness of investors to roll over ABCP, particularly at maturities of more than a few days.  The problems intensified in the second week of August after the announcement by a large overseas bank that it could not value the ABCP held by some of its money funds and was, as a result, suspending redemptions from those funds. Some commercial paper issuers invoked their right to extend the maturity of their paper, and a few issuers defaulted. In response to the heightening of perceived risks, investors fled to the safety and liquidity of Treasury bills, sparking a plunge in bill rates and a sharp widening in spreads on ABCP.\n\nThe retreat by investors from structured investment products also affected business finance. In particular, issuance of collateralized loan obligations (CLOs) and collateralized debt obligations (CDOs), which in turn had been major buyers of leveraged syndicated loans, fell off significantly during the summer. Demand for leveraged loans slowed sharply, reducing credit access for private equity firms and other borrowers seeking to finance leveraged buyouts (LBOs).\n\nConcerns about liquidity and credit risk surfaced even in markets in which securitization plays a much smaller role. For example, spreads on lower-tier unsecured commercial paper jumped and issuance was limited to very short maturities. In corporate bond markets, issuance of speculative-grade bonds dropped off sharply as risk spreads widened. And although equity prices have moved up on balance since late spring, swings in prices have been large; indeed, the expected stock-price volatilities implicit in options prices roughly doubled during the summer before falling back more recently.\n\nAs the strains in financial markets intensified, many of the largest banks became concerned about the possibility that they might face large draws on their liquidity and difficult-to-forecast expansions of their balance sheets. They recognized that they might have to provide backup funding to programs that were no longer able to issue ABCP. Moreover, in the absence of an active syndication market for the leveraged loans they had committed to underwrite and without a well-functioning securitization market for the nonconforming mortgages they had issued, many large banks might be forced to hold those assets on their books rather than sell them to investors as planned. In these circumstances of heightened volatility and diminished market functioning, banks also became more concerned about the possible risk exposures of their counterparties and other potential contingent liabilities.\n\nThese concerns prompted banks to become protective of their liquidity and balance sheet capacity and thus to become markedly less willing to provide funding to others, including other banks. As a result, both overnight and term interbank funding markets came under considerable pressure. Interbank lending rates rose notably, and the liquidity in these markets diminished. A number of the U.S. ABCP programs that had difficulty rolling over paper were sponsored by or had backup funding arrangements with European banks. As a result, some of these banks faced potentially large needs for dollar funding, and their efforts to manage their liquidity likely contributed to the pressures in global money and foreign exchange swap markets.\n\nThe U.S. subprime mortgage market is small relative to the enormous scale of global financial markets. So why was the impact of subprime developments on the markets apparently so large?  To some extent, the outsized effects of the subprime mortgage problems on financial markets may have reflected broader concerns that problems in the U.S. housing market might restrain overall economic growth. But the developments in subprime were perhaps more a trigger than a fundamental cause of the financial turmoil. The episode led investors to become more uncertain about valuations of a range of complex or opaque structured credit products, not just those backed by subprime mortgages. They also reacted to market developments by increasing their assessment of the risks associated with a number of assets and, to some degree, by reducing their willingness to take on risk more generally. To be sure, these developments may well lead to a healthier financial system in the medium to long term:  Increased investor scrutiny of structured credit products is likely to lead to greater transparency in these products and more rigor in the credit-rating process. And greater caution on the part of investors seems appropriate given the very narrow spreads and the loosening in some underwriting standards seen before the recent episode began. In the shorter term, however, these developments do imply a greater measure of financial restraint on economic growth as credit becomes more expensive and difficult to obtain.\n\nThe Federal Reserve's Response to the Financial Turmoil\nFortunately, the financial system entered the episode of the past few months with strong capital positions and a robust infrastructure. The banking system is healthy.   Despite a few notable failures, hedge funds overall seem to have held up well, and their counterparties have not sustained material losses. The clearing and settlement infrastructure generally worked well despite trading volumes that were extremely high in some cases. Nevertheless, the market strains were serious, as I have discussed, and they posed risks to the broader economy. The Federal Reserve accordingly took a number of steps to help markets return to more orderly functioning.\n\nThe Federal Reserve's initial action was to increase liquidity in short-term money markets through larger open market operations--the standard means by which it seeks to ensure that the federal funds rate stays at or near the target rate set by the Federal Open Market Committee (FOMC).  A number of other central banks took similar steps. One source of pressure in the overnight market was the demand for dollar funding by European banks to which I alluded earlier. As Europe is in the latter part of its trading day when U.S. markets open, this extra demand for dollars at times led the federal funds rate to open well above the target. The extra provision of liquidity by the Fed helped counter the resulting pressure on the funds rate early in the day; it also eased banks' concerns about the availability of funding and thus assisted the functioning of the interbank market.  To be clear, an open market operation can provide market participants with increased liquidity; but the intervention does not directly increase participants' capital or allow them to shed risk. In essence, these operations are short-term loans collateralized by government securities.\n\nThe vigorous provision of funds through open market operations succeeded in damping pressures in overnight funding markets. Yet markets for term funding, including commercial paper markets as well as the interbank markets, remained strained, and signs of broader financial stress persisted. On August 17, the Fed took further action when the Federal Reserve Board cut the discount rate--the rate at which it lends directly to banks--by 50 basis points, or 1/2 percentage point. The Fed also adjusted its usual practices to facilitate the provision of financing for as long as thirty days, renewable at the request of the borrower.\n\nLoans through the discount window differ from open market operations in that they can be made directly to specific banks with strong demands for liquidity.  (In contrast, open market operations are arranged with a limited set of dealers of government securities.)  In addition, whereas open market operations typically involve lending against government securities, loans through the discount window can be made against a much wider range of collateral, including mortgages and mortgage-backed securities.  As with open market operations, however, Fed lending through the discount window provides banks with liquidity, not risk capital. In particular, the strong collateralization accompanying discount window credit eliminates essentially all risk for the Federal Reserve System and the taxpayer. Nonetheless, the availability of the discount window is potentially significant for banks, as it gives them greater confidence that they can obtain additional liquidity as necessary. Access to a backstop source of liquidity in turn reduces the incentives of banks to limit the credit they provide to their customers and counterparties. The Federal Reserve also took some other steps in response to strains in financial markets, including reducing the fee that it charges for lending Treasury securities from its portfolio, thus helping to meet the heavy demands in the market for those securities.\n\nThe Federal Reserve's actions to ease the liquidity strains in financial markets were similar to actions that central banks have taken many times in the past. Promoting financial stability and the orderly functioning of financial markets is a key function of central banks. Indeed, a principal motivation for the founding of the Federal Reserve nearly a century ago was the expectation that it would reduce the incidence of financial crises by providing liquidity as needed.\n\nIn its supervisory role, the Federal Reserve--like other bank regulators--attempts to ensure that individual banks maintain adequate liquidity on hand and make provision to raise additional funds quickly when the need arises. We must be wary of a subtle fallacy of composition, however. Even if each market participant holds a significant reserve of what--in normal times, at least--would be considered highly liquid assets, for the system as a whole the only truly liquid assets are cash and its equivalents. The quantity of cash assets in the system at a point in time is, in turn, essentially fixed, being determined directly or indirectly by the central bank. Thus, whenever an investor sells less liquid assets to raise cash, the cash holdings of other market participants are reduced by an equal amount. Consequently, in highly stressed financial conditions, when the marketwide demand for liquidity rises sharply, one of two things must happen:  Either the central bank provides the liquidity demanded by lending against good collateral, or forced sales of illiquid assets will drive the prices of those assets well below their longer-term fundamental values, raising the risk of widespread insolvency and intensifying the crisis. If the crisis becomes sufficiently severe, history suggests that damage to the broader economy is likely to follow.\n\nIn his classic 1873 treatise, Lombard Street, Walter Bagehot famously articulated the need for central banks to be prepared to lend freely against good collateral (what he called \"good banking security\") but at a penalty rate.3  A panic, said Bagehot, is a \"species of neuralgia\" and as such must not be starved (p. 25). Of course, judgment is required to assess whether a particular set of market conditions is severe enough to warrant extraordinary injections of liquidity by the central bank; a too-aggressive intervention could unduly reduce the incentives of market participants to insure against more-normal liquidity risks. In the steps it took, the Federal Reserve strove to reach a middle ground, signaling its willingness and ability to provide liquidity to markets as needed without significantly distorting the incentives of individual banks and other market participants to manage their liquidity prudently.\n\nThe Federal Reserve's efforts to provide liquidity appear to have been helpful on the whole. To be sure, the volume of loans to banks made through the discount window, though it increased for a time, has been modest. However, collateral placed by banks at the discount window in anticipation of possible borrowing rose sharply during August and September, suggesting that some banks viewed the discount window as a potentially valuable option. On the other hand, no amount of liquidity provision by the central bank can be expected to solve the problems regarding the valuation of complex securitized assets or to reverse the credit losses on subprime mortgages. These underlying difficulties will be resolved only over time by financial markets.\n\nSince mid-August the functioning of financial markets has improved to some degree, supported not only by liquidity provision but also by the monetary policy action taken in September, to which I will return in a moment. Interest rate spreads on ABCP have fallen by more than half from their recent peaks, and overall commercial paper outstanding has edged up this month after declining sharply over August and September. Interbank term funding markets have improved modestly, though spreads there remain unusually wide. Some progress has been made in bringing pending LBO-related loans to market, albeit at discounts and with tightened terms. Risk spreads in corporate bond markets have narrowed somewhat, the issuance of speculative-grade bonds has restarted, and investment-grade issuance has been strong. Volatility in many asset markets has declined toward more-normal levels. Perhaps most important, in many markets investors are showing an increased capacity and willingness to differentiate among assets of varying quality.\n\nIn contrast, despite a few encouraging signs, conditions in mortgage markets remain difficult. The markets for securitized nonprime (that is, subprime and so-called alt-A) loans are showing little activity, securitizations of prime jumbo mortgages reportedly have increased only slightly from low levels, and the spread between the interest rates on nonconforming and conforming mortgages remains elevated. These continued problems suggest that investors will need more time to gather information and reevaluate risks before they are willing to reenter these markets.\n\nMonetary Policy and the Economic Outlook\nThe Federal Reserve's efforts to support the normal functioning of financial markets have as their ultimate objective the stability and efficiency of the broader economy. In addition, of course, the Federal Reserve can adjust the stance of monetary policy by changing its target for the federal funds rate. The FOMC manages monetary policy to further its dual mandate to promote maximum sustainable employment and price stability.\n\nThe turmoil in financial markets significantly affected the Committee's outlook for the broader economy. Indeed, in a statement issued simultaneously with the Federal Reserve Board's August 17 announcement of the cut in the discount rate, the FOMC noted that the downside risks to growth had increased appreciably. However, to allow time to gather and evaluate incoming information, possible policy action was deferred until the Committee's next regularly scheduled meeting on September 18.\n\nA key issue at that meeting was the extent to which the market disturbances had affected the outlook for the housing sector.  Financial markets overall had improved somewhat, but tighter terms and standards in the mortgage market--particularly in the nonprime and jumbo segments--appeared likely to intensify the correction in housing significantly, with adverse implications for construction activity and house prices. Indeed, incoming housing data had continued to soften even before the advent of the stress in financial markets. A further sharp contraction in residential construction seemed likely to hold down overall economic growth in the fourth quarter and in early 2008.\n\nAs they had at earlier meetings, the participants in the September meeting evaluated the potential effects of housing-market developments on other parts of the economy. They agreed that significant spillovers to household and business spending were not yet evident. For example, auto sales had picked up in August from the low levels of earlier in the summer; and business investment did not appear to have been seriously affected by financial market developments, as highly rated firms continued to enjoy good access to credit. Strong growth abroad was also viewed as supporting U.S. exports and domestic production. And as I have noted, the available evidence suggested that overall economic growth in the third quarter remained moderate.\n\nHowever, downside risks to both household and business spending had clearly increased over the period since the Committee's previous meeting. Notably, the weak housing market, somewhat downbeat consumer sentiment, and slower growth in private-sector employment increased the likelihood that consumption spending would slow in coming quarters. Participants at the September meeting also reported somewhat greater caution in the outlooks of their business contacts. Financial market conditions were expected to improve slowly at best; and even if conditions began to normalize, credit would likely remain noticeably tighter for many borrowers than had been the case during the summer. Furthermore, any weakening in the economy could itself have a negative effect on still-fragile credit markets, possibly leading credit conditions to tighten further.\n\nRegarding the other half of its mandate, to promote price stability, the Committee noted some improvement over the past year in measures of the trend component of inflation, such as core inflation. Moreover, slower growth in aggregate demand would help to ease pressure on resources. But inflation risks remained, including still-high levels of resource utilization and elevated prices for oil and other commodities. The Committee agreed that continued close attention to inflation developments was warranted. Overall, given the great difficulty of knowing how financial conditions would evolve or the extent of their effect on the economy, Committee members judged the level of uncertainty in the outlook to be unusually high.\n\nAs you know, the Committee chose to cut its target for the federal funds rate by 50 basis points at the September meeting. This action was intended to help offset the tightening of credit conditions resulting from the financial turmoil. Risk-management considerations also played a role in the decision, given the possibility that the housing correction and tighter credit could presage a broader weakening in economic conditions that would be difficult to arrest. By doing more sooner, policy might be able to forestall some part of the potential adverse effects of the disruptions in financial markets. As most of the meeting participants saw growth likely to run below trend for a while and with the incoming inflation data on the favorable side, the risks to inflation from this action seemed acceptable, especially as the Committee was prepared to reverse the policy easing if inflation pressures proved stronger than expected.\n\nSince the September meeting, the incoming data have borne out the Committee's expectations of further weakening in the housing market, as sales have fallen further and new residential construction has continued to decline rapidly. The further contraction in housing is likely to be a significant drag on growth in the current quarter and through early next year. However, it remains too early to assess the extent to which household and business spending will be affected by the weakness in housing and the tightening in credit conditions. We will be following indicators of household and business spending closely as we update our outlook for near-term growth. The evolution of employment and labor income also will bear watching, as gains in real income support consumer spending even if the weakness in house prices adversely affects homeowners' equity. The labor market has shown some signs of cooling, but these are quite tentative so far, and real income is still growing at a solid pace.\n\nOn the inflation side, prices of crude oil and other commodities have increased somewhat in recent weeks, and the foreign exchange value of the dollar has weakened. However, overall, the limited data that we have received since the September FOMC meeting are consistent with continued moderate increases in consumer prices. As the Committee noted in its post-meeting statement, we will continue to monitor inflation developments carefully.\n\nIt does seem that, together with our earlier actions to enhance liquidity, the September policy action has served to reduce some of the pressure in financial markets, although considerable strains remain. From the perspective of the near-term economic outlook, the improved functioning of financial markets is a positive development in that it increases the likelihood of achieving moderate growth with price stability. However, in such situations, one must also take seriously the possibility that policy actions that have the effect of reducing stress in financial markets may also promote excessive risk-taking and thus increase the probability of future crises.  As I indicated in earlier remarks, it is not the responsibility of the Federal Reserve--nor would it be appropriate--to protect lenders and investors from the consequences of their financial decisions. But developments in financial markets can have broad economic effects felt by many outside the markets, and the Federal Reserve must take those effects into account when determining policy. In particular, as I have emphasized, the Federal Reserve has a mandate from the Congress to promote maximum employment and stable prices, and its monetary policy actions will be chosen so as to best meet that mandate.\n\nIndeed, although the Federal Reserve can seek to provide a more stable economic background that will benefit both investors and non-investors, the truth is that it can hardly insulate investors from risk, even if it wished to do so. Developments over the past few months reinforce this point. Those who made bad investment decisions lost money. In particular, investors in subprime mortgages have sustained significant losses, and many of the mortgage companies that made those loans have failed. Moreover, market participants are learning and adjusting--for example, by insisting on better mortgage underwriting and by performing better due diligence on structured credit products. Rather than becoming more crisis-prone, the financial system is likely to emerge from this episode healthier and more stable than before.\n\nConclusion\nI have sought this evening to put recent financial market developments in context and to explain the thinking behind the steps taken by the Federal Reserve.  This has been a challenging period. Conditions in financial markets have shown some improvement since the worst of the storm in mid-August, but a full recovery of market functioning is likely to take time, and we may well see some setbacks. In particular, investors are continuing to reassess the risks they face and have not yet fully regained confidence in their ability to accurately price certain types of securities. The ultimate implications of financial developments for the cost and availability of credit, and thus for the broader economy, remain uncertain.\n\nIn coming months, the Federal Reserve, together with other agencies both here and abroad, will perform comprehensive reviews of recent events to better understand the episode and to draw lessons for the future. For now, the Federal Reserve will continue to watch the situation closely and will act as needed to support efficient market functioning and to foster sustainable economic growth and price stability.\n\nFootnotes\n\n1. Estimates of delinquencies are based on data from First American LoanPerformance. Return to text\n\n2. Jumbo mortgages are those mortgages for which the principal value does not conform to the limit set annually by Fannie Mae and Freddie Mac for loans they will purchase; the amount for 2007 is $417,000. Jumbo loans are thus a type of  \"nonconforming\" loan. Prime loans are those made to borrowers with good credit records. Return to text\n\n3. Walter Bagehot (1962 reprint), Lombard Street (Westport, Conn.: Hyperion Press). Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20071015a.htm",
        "title": "The Recent Financial Turmoil and its Economic and Policy Consequences",
        "date": "10/15/2007"
    },
    {
        "content": "October 12, 2007\n\nChairman Ben S. Bernanke\n\nTo the Conference on John Taylor's Contributions to Monetary Theory and Policy, Federal Reserve Bank of Dallas, Dallas, Texas(via videoconference)\n\nIt is a privilege for me to open this conference dedicated to our colleague and friend, John Taylor. John's influence on monetary theory and policy has been profound indeed. That influence has been manifest in undergraduate lecture halls and graduate seminar rooms, in the best research journals, and in the highest ranks of government. His ability to crystallize important analytical insights and apply them to policy issues is unsurpassed. Indeed, in a speech a few years ago, I noted three concepts named after John that are central to understanding our macroeconomic experience of the past three decades--the Taylor curve, the Taylor rule, and the Taylor principle (Bernanke, 2004). I'd like to take a few minutes to review John's career and impressive body of work.\n\nAfter receiving his Ph.D. from Stanford nearly thirty-five years ago, John began his career as an assistant professor at Columbia University. Even in those early years, John revealed his interest in applying the analytical tools of economics to practical policy issues. He took a leave of absence from academia in 1976-77 to serve on the staff of the Council of Economic Advisers. I suspect that the circumstances of the mid-1970s intensified John's motivation to help improve economic performance through sound policymaking.\n\nDuring the late 1970s and early 1980s, John published a number of highly influential papers, including: \"Conditions for Unique Solutions in Stochastic Macroeconomic Models with Rational Expectations,\" \"Estimation and Control of a Macroeconomic Model with Rational Expectations,\" \"Aggregate Dynamics and Staggered Contracts,\" and \"Solution and Maximum Likelihood Estimation of Dynamic Nonlinear Rational Expectations Models.\" (As you can tell, John has always had a penchant for catchy titles.) Beyond its important technical contributions, this work showed that the insights and methods of the rational expectations revolution could be applied to models with sticky wages and prices. That observation has proved enormously influential in subsequent policy research.\n\nThe rational expectations revolution helped to kill the idea of a long-run tradeoff between the levels of inflation and unemployment. However, John's analysis showed that, in the presence of aggregate supply shocks, attempts by monetary policymakers to reduce the volatility of inflation over time could be associated with higher volatility in unemployment, and vice versa. John's visual depiction of this policy tradeoff has come to be known as the Taylor curve. Interestingly, John's work anticipated the possibility that improvements in the conduct of monetary policy or changes in the structure of the economy could result in a shift of the Taylor curve--that is, a change in the ability of policy to smooth both inflation and employment. And indeed, what economists have dubbed the Great Moderation--a simultaneous reduction in the volatility of inflation and the volatility of real economic activity--has occurred in the United States and in many other economies over the past quarter-century.\n\nOver the course of the 1980s, John continued his work on rational expectations issues and monetary policy and macroeconomics more generally. He also began to broaden his focus to matters of international economics. He developed a multi-country rational expectations econometric model--a truly ambitious undertaking, especially in light of the limited computing capabilities of the era. By this point in his career, John had firmly established his reputation as a leader in the profession.\n\nIn 1989, John became a member of the first President Bush's Council of Economic Advisers. During the next four years, he played a key role in shaping the Administration's positions on macroeconomic, fiscal, international finance, and trade issues. The U.S. economy was entering a difficult period at that point. Among other problems, significant pressures on bank balance sheets were beginning to emerge that would damp economic growth for the next several years. While dealing with the serious economic issues of the time, John and the other members of the Council simultaneously produced an impressive manifesto for policymaking. They developed a rules-based approach to the conduct of monetary and fiscal policy and published it in 1990 in the Economic Report of the President.\n\nThat essay laid the foundation for what is perhaps John's most well-known contribution to economics--the simple description of the determinants of monetary policy that eventually became known as the Taylor rule.1 John's analysis triggered an avalanche of studies examining the stabilization properties of Taylor rules in the context of macroeconomic models. Other work investigated the ability of variants of the Taylor rule to describe empirically the actual course of monetary policy in the United States and in other economies.\n\nThe Taylor rule also embeds a basic principle of sound monetary policy that has subsequently been referred to as the Taylor principle.2 According to this principle, when a shock causes a shift in the inflation rate, the central bank must adjust the nominal interest rate by more than one-for-one. This ensures that the real interest rate moves in the right direction to restore price stability. The Taylor principle provides essential guidance for central banks on how to anchor long-run inflation expectations and foster stable growth and low inflation.\n\nEver since its inception, John has emphasized that the Taylor rule should not be applied mechanistically. The world is far too complicated for that. But he has argued that such rules can serve as useful benchmarks for the practical conduct of monetary policy. In fact, policymakers at the Federal Reserve and many other central banks do routinely consult various policy rules as they make judgments on the appropriate stance of monetary policy.\n\nAfter a decade back at Stanford, John was called again to Washington by President Bush--this time the current President Bush. He served as Under Secretary for International Affairs at the U.S. Treasury from 2001 through 2005. Our economy faced severe challenges during that period--the terrorist attacks of September 11, a recession and the threat of deflation, corporate governance scandals, and economic issues posed by the conflicts in Afghanistan and Iraq. Suffice it to say, John earned his stripes as the leader of the \"Global Financial Warriors.\" As detailed in his book of the same title, John worked extensively on the financial reconstruction of Iraq and on the development of financial tools for fighting terrorism. The Treasury Department recognized his efforts in 2004 with its Distinguished Service Award and in 2005 with the Alexander Hamilton Award for leadership in international finance.\n\nAfter his stint at Treasury, John returned once again to the less tumultuous life of a professor, ensconced in his offices at Stanford University and the Hoover Institution. I'm sure that, in between teaching and guiding the work of graduate students, he will continue to offer insightful commentary on monetary policy and other economic issues. And doubtless he will also continue to do pathbreaking research. Indeed, with our appetites whetted by the Taylor rule, principle, and curve, we now look forward to the Taylor dictum, the Taylor hyperbola, and maybe even the Taylor conundrum.\n\nBefore closing my remarks, I would like to express my appreciation to President Fisher and the Federal Reserve Bank of Dallas for hosting this conference. It is a terrific opportunity for celebrating John's contributions to monetary theory and policy. The papers on the program testify to the breadth and depth of John's work. I wish I could be there with you today to listen to the presentations and participate in the discussions. However, I know that you will have a productive and collegial gathering over the next two days. And I wish all of you--but especially my friend and colleague John Taylor--all the best in your endeavors.\n\nReferences\n\nBernanke, Ben S. (2004). \"The Great Moderation.\" Remarks at the meetings of the Eastern Economic Association, February 20, Washington, D.C. http://www.federalreserve.gov/boarddocs/speeches/2004/20040220/default.htm\n\nBryant, R., Hooper, P. and Mann, C., (1993). Evaluating Policy Regimes: New Research in Empirical Macroeconomics. Washington, D.C.: Brookings Institution.\n\nWoodford, M., (2001). \"The Taylor Rule and Optimal Monetary Policy.\"  American Economic Review, 91(2), pp. 232-237.\n\nFootnotes\n\n1. The number \"two\" and its inverse, one-half, played a key role in this rule:  The benchmark setting for the federal funds rate would be 2 percent, plus the current rate of inflation, plus one-half times the gap between current inflation and 2 percent, plus one-half times the output gap. Return to text\n\n\n\n2. This principle originally became apparent through numerical simulations of macroeconomic models with rational expectations (including Taylor's multicountry model); refer also to Bryant, Hooper, and Mann (1993). The phrase \"Taylor principle\" was introduced by Woodford (2001), who demonstrated this principle analytically in a stylized New Keynesian model.  Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20071012a.htm",
        "title": "Opening remarks",
        "date": "10/12/2007"
    },
    {
        "content": "October 12, 2007\n\nVice Chairman Donald L. Kohn\n\nAt the Conference on John Taylor's Contributions to Monetary Theory and Policy, Federal Reserve Bank of Dallas, Dallas, Texas\n\nThe Role of Simple Rules in Monetary Policymaking\nIt is a pleasure and an honor to speak at this conference honoring John Taylor and his contributions to monetary theory and policy.  As you have already heard from Chairman Bernanke and the other speakers today, John has made a number of seminal contributions to the field of macroeconomics.  What has distinguished John's work, in my view, is that he takes policymaking in the real world seriously.1\n\nTaking policymaking seriously involves understanding the constraints imposed on our decisions by partial information and incomplete knowledge of economic relationships.  It also implies the use of empirically valid models that acknowledge the efforts of households and businesses to anticipate the future and maximize their welfare over time.  In the late 1980s and early 1990s, macroeconomics was focused mainly on real business cycles and endogenous growth theory.  During this period, John was one of a very small number of academic economists who continued to pursue research aimed at informing the conduct of monetary policy.  John's Carnegie Rochester conference paper published in 1993 is an excellent example of this research.\n\nImportantly, John's legacy to the Federal Reserve has not been confined to enhancing our understanding of monetary policy.  In addition, he has turned out legions of students who have followed in his footsteps in their interest in policy.  Many of them have spent time in the Federal Reserve, producing a rich array of contributions to policymaking and research.\n\nJohn and I have spent countless hours discussing how the Federal Reserve arrives at decisions about monetary policy and how it should arrive at decisions.  Those conversations began in earnest in the late 1980s, when John was on the Council of Economic Advisers, and they have continued to the present day.  They have occurred not only in offices and classrooms in Washington and Stanford and at numerous conferences around the globe, but also around dinner tables in Washington and Palo Alto and on hiking trails from Vermont to Wyoming.  Those conversations made me a better policy adviser and then policymaker, and they have had the added and very special bonus of allowing Gail and me to count John and Allyn among our friends.  I can't think of a better way to honor John's contributions than to continue that discussion around the dinner tables of Dallas by reflecting on the role of simple rules in informing policymaking.\n\nThree Benefits of Simple Rules in Monetary Policymaking\nIn his Carnegie Rochester conference paper, John considered a simple policy rule under which the nominal federal funds rate is adjusted in response to both the gap between real and trend gross domestic product (GDP) and the gap between the inflation rate and policymakers' target.  Based on data for the previous few years, John calibrated the long-run target for inflation and the two parameters that determine the responsiveness of the federal funds rate to the two gaps.  The equilibrium real interest rate was based on a longer history of actual real interest rates.  In the handout, Figure 1A depicts the actual nominal funds rate and the Taylor rule prescriptions between 1987 and 1992, as presented in John's paper.  Despite its simplicity, this policy rule fits the data remarkably well; it described a period of generally successful policymaking; and it adhered to the Taylor principle of adjusting the nominal rate more than one-for-one with changes in the inflation rate, so it provided a plausible template for future success.  It is no wonder that John has been such a dedicated salesman and that his efforts have been so well received in academia and policy councils.\n\nFollowing John's seminal contribution, many other economists have engaged in research on similar policy rules and, together with John, have identified several benefits of such rules in conducting monetary policy.  I will elaborate on three of them.\n\nThe first benefit of looking at a simple rule like John's is that it can provide a useful benchmark for policymakers.  It relates policy setting systematically to the state of the economy in a way that, over time, will produce reasonably good outcomes on average.  Importantly, the emphasis is on levels and gaps, not growth rates, as inputs to the policy process.  This emphasis can be a problem when a level, say of potential GDP, is in question, but in many respects it is also a virtue.  For the United States, the two gaps relate directly to the legislative mandate of the Federal Reserve to achieve stable prices and maximum employment.  Moreover, those two gaps fit directly into most modern macroeconomic theories, which tell us something about their relationship and how that relationship can be affected by the type of shock hitting the economy.\n\nModel uncertainties make the simplicity of the rule particularly important for the policymaker because research suggests that the prescriptions from simple rules can be more robust than optimal-control policies.  Optimal-control policies can depend critically on the exact specification of the model, and clearly there is no consensus about which model best describes the U.S. economy.\n\nFederal Reserve policymakers are shown several versions of Taylor rules in the material we receive before each meeting of the Federal Open Market Committee (FOMC).  I always look at those charts and tables and ask myself whether I am comfortable with any significant deviation of my policy prescription from those of the rules.\n\nA second benefit of simple rules is that they help financial market participants form a baseline for expectations regarding the future course of monetary policy.  Even if the actual policy process is far more sophisticated than any simple rule could completely describe, the rule often provides a reasonably good approximation of what policymakers decide and a framework for thinking about policy actions.  Indeed, many financial market participants have used the Taylor rule to understand U.S. monetary policy over the past fifteen years.  Investors and other market participants are going to form expectations about policy and act on those expectations.  The more accurate and informed those expectations are, the more likely are their actions to reinforce the intended effects of policy.\n\nA third benefit is that simple rules can be helpful in the central bank's communication with the general public.  Such an understanding is important for the transmission mechanism of monetary policy.  Giving the public some sense of how the central bank sees the output and inflation gaps and how they are expected to evolve will help it understand the central bank's objectives and how policymakers are likely to respond to surprises in incoming data.\n\nFour Limitations of Simple Rules\nSimple rules have limitations, of course, as benchmarks for monetary policy.  To quote from John's Carnegie Rochester paper, \"a policy rule can be implemented and operated more informally by policymakers who recognize the general instrument responses that underlie the policy rule, but who also recognize that operating the rule requires judgment and cannot be done by computer\" (p. 198).  In that context, four limitations of simple rules are important.\n\nThe first limitation is that the use of a Taylor rule requires that a single measure of inflation be used to obtain the rule prescriptions.  The price index used by John in the Carnegie Rochester paper was the GDP price deflator.  Other researchers have used the inflation measure based on the consumer price index (CPI).  Over the past fifteen years, the Federal Reserve has emphasized the inflation rate as measured by changes in the price index for personal consumption expenditures (PCE).  Many researchers have also explored the use of core price indexes, which exclude the volatile food and energy components, as better predictors of future inflation or as more robust indicators of the sticky prices that some theories say should be the targets of policy.   To be sure, over long periods, most of these measures behave very similarly.  But policy is made in the here and now, and the various indexes can diverge significantly for long stretches, potentially providing different signals for the appropriate course of monetary policy.\n\nSecond, the implementation of the Taylor rule and other related rules requires determining the level of the equilibrium real interest rate and the level of potential output; neither of them are observable variables, and both must be inferred from other information.  John used   2 percent as a rough guess as to the real federal funds rate that would be consistent with the economy producing at its potential.  But the equilibrium level of the real federal funds rate probably varies over time because it depends on factors such as the growth rate of potential output, fiscal policy, and the willingness of savers to supply credit to households and businesses.  Inaccurate estimates of this rate will mislead policymakers about the policy stance required to achieve full employment.  In a similar vein, real-time estimates of potential output can be derived in a number of ways and--as shown by Orphanides (2003) and others--they are subject to large and persistent errors.  If policymakers inadvertently rely on flawed estimates, they will encounter persistent problems in achieving their inflation objective.\n\nThe third limitation of using simple rules for monetary policymaking stems from the fact that, by their nature, simple rules involve only a small number of variables.  However, the state of a complex economy like that of the United States cannot be fully captured by any small set of summary statistics.  Moreover, policy is best made looking forward, that is, on the basis of projections of how inflation and economic activity may evolve.  Lagged or current values of the small set of variables used in a given simple rule may not provide a sufficient guide to future economic developments, especially in periods of rapid or unusual change.  For these reasons, central banks monitor a wide range of indicators in conducting monetary policy.  In his Carnegie Rochester paper, John mentioned the stock market crash of October 1987 as an example of how other variables can and should influence the course of monetary policy in some situations.\n\nThe final limitation I want to highlight is that simple policy rules may not capture risk-management considerations.  In some circumstances, the risks to the outlook or the perceived costs of missing an objective on a particular side may be sufficiently skewed that policymakers will choose to respond by adjusting policy in a way that would not be justified solely by the current state of the economy or the modal outlook for output and inflation gaps.\n\nPolicy Rules around 2003\nSome of the ambiguities and potential pitfalls in the use of simple policy rules are highlighted by considering their prescriptions for a period earlier in this decade.  Turning to Figure 1B, the solid line indicates the actual federal funds rate between the first quarter of 1993 and the second quarter of 2007, and the dashed line shows the prescriptions of the Taylor rule using the same methodology that John used in his Jackson Hole remarks this year.2  For the earlier part of the sample, the prescription from this simple rule tracks the actual funds rate relatively well.  As John pointed out, a notable deviation happened beginning in 2002, and I would like to discuss that period to illustrate the limitations I noted earlier.\n\nInflation Measure\nThe first limitation is related to the measure used for the inflation variable included in the rules.  The rule prescriptions depicted by the dashed line in Figure 1B are based on the headline CPI.  But as you know, the FOMC often looks at core inflation, stripping out the effects of energy and food prices, as a better indicator of future price behavior.  The dotted line represents the rule prescriptions based on the chain-weighted core CPI, which the Bureau of Labor Statistics has produced since 2000. Using this measure lowers the prescribed funds rate by about 2 percentage points during 2003, bringing the rule prescriptions much closer to the actual path of policy.  The reason for the improvement is evident from Figure 2A, on the other side of the handout:  Even though the headline and core CPI measures were broadly similar in the mid- to late 1990s, these measures diverged substantially between 2003 and 2005.\n\nPotential Output\nThe second limitation relates to the challenge of judging the level of potential output in real time.  To illustrate this point, Figure 2B plots three measures of the output gap.  The solid line is the real-time estimate by the Congressional Budget Office (CBO) that was used in the Taylor rule prescriptions in Figure 1B, while the dashed line depicts the CBO's ex post estimate of the output gap as of the third quarter of 2007.  Back in 2003, the CBO estimated that output at that time was below potential by only 1 percent.  With the benefit of four more years of data, the CBO currently estimates that the output gap for the first half of 2003 was considerably wider--about 3 percent.  In addition, the dotted line represents an alternative measure of resource utilization derived from the unemployment rate and an estimate of the natural rate of unemployment (NAIRU) taken from the Board staff's FRB/US model.  In fact, the unemployment rate was rising through the middle of 2003, so the FOMC had every reason to believe that the output gap was widening at that time.  Using this unemployment-based measure rather than the real-time CBO measure would reduce the prescriptions of simple policy rules by roughly 1/2 percentage point in early 2003.\n\nOther Variables\nThe third limitation in my list was that the small set of economic measures included in simple rules may not fully reflect the state of the economy.  Around 2003, financial market conditions may not have been adequately summarized by the assumed 2 percent equilibrium federal funds rate.  Accounting scandals caused economic agents to lose confidence in published financial statements and in bond ratings.  The result was higher uncertainty about the financial health of firms, and credit spreads widened substantially.  Figure 2C shows that risk spreads on corporate bonds were elevated in this period.  Other things equal, such spreads would reduce the federal funds rate needed to achieve full employment, perhaps explaining a portion of the gap between the actual federal funds rate and the outcome from the policy rule during this period.\n\nRisk Management\nThe last item on my list of limitations was that simple rules do not take account of risk-management considerations.  As shown in Figure 2A, the core CPI inflation rate for 2003 was falling toward 1 percent.  The real-time reading of the core PCE inflation rate (not shown) was on average even lower than the comparable CPI figure.  Given these rates, the possibility of deflation could not be ruled out.  We had carefully analyzed the Japanese experience of the early 1990s; our conclusion was that aggressively moving against the risk of deflation would pay dividends by reducing the odds on needing to deal with the zero bound on nominal interest rates should the economy be hit with another negative shock.  This factor is not captured by simple policy rules.\n\nA Final Note\nI have offered this analysis in the spirit of so many of the discussions I have had with John.  His framework has been enormously important to policymaking in the Federal Reserve, and it has yielded many benefits.  Nevertheless, it's important to keep in mind that some significant practical limitations also are associated with the application of such rules in real time.  In other words, it's not so simple to use simple rules!\n\nReferences\nOrphanides, Athanasios (2003).  \"The Quest for Prosperity without Inflation,\"  Journal of Monetary Economics, vol. 50 (April), pp. 633-63.\n\nPoole, William (2007).  \"Understanding the Fed (210 KB PDF),\" Federal Reserve Bank of St. Louis, Review, vol. 89 (January/February), pp. 3-14, http://research.stlouisfed.org/publications/review/past/2007.\n\nTaylor, John B. (1993).  \"Discretion versus Policy Rules in Practice,\"  Carnegie-Rochester Conference Series on Public Policy, vol. 39, pp. 195-214, http://econpapers.repec.org/article/eeecrcspp/default1993.htm.\n\n_________ (2007).  \"Housing and Monetary Policy (244 KB PDF),\" speech delivered at \"Housing, Housing Finance, and Monetary Policy,\" a symposium sponsored by the Federal Reserve Bank of Kansas City, held in Jackson Hole, Wyo., August 30-September 1, www.kansascityfed.org/publicat/sympos/2007/pdf/2007.09.04.Taylor.pdf.\n\nFootnotes\n\n1. I am sure my colleagues join me in honoring John.  However, my thoughts on policy rules are my own and not necessarily those of my colleagues on the Federal Open Market Committee.  Jinill Kim and Andrew Levin, of the Board's staff, contributed to the preparation of these remarks. Return to text\n\n2.  Following John, the rule specification and the data used for the prescriptions closely follow the implementation of the Taylor rule in Bill Poole's speech in August 2006 (Poole, 2007).  The inflation measure used for this rule is the four-quarter average headline CPI inflation rate, with the benchmark value set to 2 percent.  Through 2001, the gap between real GDP and its potential is the value measured in real time by the staff of the Board of Governors.  Because subsequent staff estimates of the output gap are not yet publicly available, the rule prescriptions for the post-2001 period are computed with the real-time output gap as constructed by the Congressional Budget Office. Return to text",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20071012a.htm",
        "title": "John Taylor Rules",
        "date": "10/12/2007"
    },
    {
        "content": "October 11, 2007\n\nGovernor Randall S. Kroszner\n\nAt the National Bankers Association 80th Annual Convention, Durham, North Carolina\n\nIt is a pleasure to take part in this event. I commend our hosts for bringing together such a broad range of participants and for developing an agenda that addresses issues of great significance to the banking community. My goal today is to offer a broad perspective of the Federal Reserve's various responsibilities and complementary roles.\n\nThe roles of the Federal Reserve may seem somewhat obscure and, to many, even disconnected. I would venture a guess that as bankers, you think first of examiners when you hear \"Federal Reserve\"--and perhaps do so with a groan. But as you know, bank supervision is only one of the Federal Reserve's functions. In my remarks today, I will speak about some of the Federal Reserve's responsibilities and discuss how they relate to markets, financial institutions, and consumers.\n\nThe Central Bank and Monetary Policy\nAs the nation's central bank, the Federal Reserve has the unique power in the U.S. financial system to create money, giving it the ability to conduct monetary policy for the U.S. economy. That power also enables the Federal Reserve to provide liquidity--a capability that is particularly important when the financial system is under stress. Indeed, the Federal Reserve was established in 1913 as a means of addressing the periodic financial panics and accompanying economic downturns that had afflicted the nation at various times.\n\nThe Congress has given the Federal Reserve specific objectives for the conduct of monetary policy--in particular, to conduct monetary policy in a way that promotes the long-run objectives of maximum sustainable employment and stable prices. Monetary policy is implemented primarily through open market operations--that is, the purchase and sale of securities in the open market--which gives the Federal Reserve the ability to control short-term interest rates. The Federal Open Market Committee (FOMC) oversees the conduct of open market operations and formulates monetary policy by setting an operating target for the federal funds rate that is judged to be consistent with fostering the Federal Reserve's long-run objectives.\n\nAs part and parcel of its pursuit of maximum sustainable employment and stable prices, the Federal Reserve has a broad responsibility to foster financial stability. I should hasten to add that financial stability does not imply that asset prices will be stable at all times or that investors will be protected from significant losses. Rather, financial stability is fundamentally about the efficient functioning of financial markets in channeling credit and financial resources to the most productive ends and allowing investors to effectively manage risks.\n\nDuring periods of financial distress, the Federal Reserve can promote financial stability by providing liquidity through both open market operations and the discount window. It can provide liquidity through open market operations by purchasing assets in the open market in exchange for newly created reserves. It can also address the liquidity needs of solvent but temporarily illiquid banks by lending to them through the discount window. Such lending can allow banks to meet payment obligations; to avoid \"fire sales\" of assets, which can disrupt financial markets; and to continue to lend to customers without tightening terms or credit standards, which can limit economic growth. It is important to note that the ability and willingness of the central bank to provide liquidity may be useful in reducing market uncertainty and bolstering investor confidence, even if the actual amount of liquidity provided turns out to be rather modest. For these reasons, the ability to provide liquidity is a critical tool of a central bank.\n\nThis brief overview of the role of the Federal Reserve in fostering financial stability is instructive in considering the policy actions undertaken in response to recent disruptions and strains in financial markets. The Federal Reserve used a range of policy tools--including adjustments in short-term interest rates and the provision of liquidity--during this time of market unrest to counter the potential ill effects on the economy. On August 10, the Federal Reserve issued a statement announcing that it was providing liquidity to facilitate the orderly functioning of financial markets and would provide reserves as necessary to promote trading in the federal funds market at rates close to the target rate of 5-1/4 percent. The announcement also noted that the discount window was available as a source of funding.\n\nOn August 17, the FOMC issued a statement noting that financial market conditions had deteriorated and that tighter credit conditions and increased uncertainty had the potential to dampen economic growth. The FOMC said that it judged the downside risks to growth to have increased appreciably and stated that it was prepared to act as needed to mitigate adverse effects on the economy arising from the disruptions in financial markets. At the same time, the Board of Governors announced that it had approved a 50 basis point reduction in the primary credit rate at the discount window, to 5-3/4 percent, to promote the restoration of orderly conditions in financial markets. Given the turbulence in the short-term funding market, the Board also approved a change in the administration of the discount window that would allow the provision of term financing for as long as thirty days, renewable by the borrower. And on August 21, the Federal Reserve Bank of New York announced some temporary changes to the terms and conditions of the System Open Market Account securities lending program, including a reduction in the minimum fee, in an effort to facilitate lending of securities by the Fed to address heightened safe-haven demands for Treasury securities.\n\nThese actions appeared helpful in mitigating some of the strains in financial markets, but financial market functioning had not returned to normal by the time of the September FOMC meeting. At that time, we judged that a 50 basis point lowering of the target federal funds rate was appropriate to offset the effects of tighter financial conditions on the economic outlook and reduce the risks that a further tightening in credit conditions could impact the housing market and lead to significant broader weakness in output and employment.\n\nBank Supervision\nWhile the Federal Reserve's role in conducting monetary policy and providing liquidity garners a great deal of media attention, its role in bank supervision and regulation is no less important. The Federal Reserve shares responsibility for regulating and supervising the safety and soundness of the U.S. banking system with a number of federal and state government agencies. The Federal Reserve supervises state-chartered banks that are members of the Federal Reserve System, the U.S. operations of foreign banks, and, in some cases, the foreign operations of U.S. banks. In addition, it is the umbrella supervisor of all bank holding companies and financial holding companies. Although the banks and nonbank subsidiaries of these entities are often supervised by other agencies, the Federal Reserve coordinates with the primary supervisors in assessing the overall financial condition of the consolidated organization.\n\nThe scope of the Federal Reserve's supervisory activities includes regular examinations of depository institutions and inspections of holding companies to assess the safety and soundness of these entities and to ensure that they comply with relevant banking laws and regulations.\n\nIn addition, the Federal Reserve contributes to the development of policies regarding banking supervision and regulation, collaborating with the other agencies to issue rules and guidance that help ensure that federally supervised banking entities are adhering to the high prudential standards that are essential to maintain a stable banking system. The Federal Reserve plays an instrumental role in developing policies that affect both domestic and international banking organizations. One visible example of such collaboration is the international effort known as Basel I and II that ultimately resulted in a framework that established minimum regulatory capital requirements, increased supervisory attention to banking institutions' capital adequacy and risk-management techniques, and enhanced public disclosure of banking organizations' risk exposures. These standards promote safety and soundness and reduce competitive inequi­ties among banking organizations operating within an increasingly global market.\n\nThe full range of its supervisory activities gives the Federal Reserve ongoing access to critical information about the banking system that is pertinent to our assessment of the state of the overall economy and has proved highly valuable during the most recent market stress. With data on financial institutions' managerial, operational, and risk-management systems, the Federal Reserve has information about the overall condition of the banking institutions it supervises as well as insight into developments in the broader financial markets. Such knowledge allows for a broad and deep understanding of developments in financial markets and financial institutions. These supervisory activities provide the Federal Reserve with access to real-time information that facilitates the formulation of policy responses during periods of financial stress.\n\nFor example, we are all well aware that the subprime mortgage market has presented supervisory concerns. The Federal Reserve has collaborated with various agencies to address these concerns, working to overcome challenges presented by the rapid pace of financial innovation and the complex regulatory scheme of the mortgage market. The federal supervisory agencies first issued guidance on subprime lending in 1999 and expanded it in 2001 to emphasize that lending standards should include well-defined underwriting parameters, such as acceptable loan-to-value and debt-to-income ratios and minimum acceptable credit scores. Further, it advises institutions actively involved in the securitization and sale of subprime loans to develop contingency plans that include alternate funding sources and measures for raising additional capital if necessary.\n\nAs the mortgage industry continued to expand its product offerings, the Federal Reserve and the other federal agencies observed in 2005 that lenders were increasingly offering nontraditional, or \"exotic,\" mortgage loans, which defer the repayment of principal and, sometimes, interest. Of particular concern with these types of loans were the lack of principal amortization and the potential for negative amortization. Moreover, the easing of underwriting standards and the marketing of these products to a wider spectrum of borrowers held the potential to create larger risks. In 2006, the Federal Reserve and the other banking agencies issued guidance on nontraditional mortgage products to address these concerns. This guidance underscores the sound underwriting procedures, portfolio-risk management strategies, and consumer protection practices that institutions should follow to prudently originate and manage nontraditional mortgage loans. A major aspect of this guidance is the recommendation that a lender's analysis of repayment capacity include an evaluation of the borrower's ability to repay the debt by final maturity at the fully indexed rate, assuming a fully amortizing repayment schedule.\n\nContinuing concerns about the subprime market led the federal regulators to issue further guidance this year regarding adjustable-rate mortgages (ARMs). Of particular concern were those ARM products that allow for low initial payments based on a fixed introductory rate that expires after a short period and then adjust to a variable rate, plus a margin, for the remaining term of the loan. These products may result in payment shock to some borrowers, heightening risks to both lenders and borrowers.\n\nConsumer Protection in Financial Services\nAs you can see, the scrutiny of safety and soundness issues in the subprime mortgage market also raised concerns relating to consumer protection--another area of significant responsibility for the Federal Reserve. Toward this end, the Federal Reserve has primary rule-writing authority for many consumer protection laws. It takes two fundamental approaches to consumer protection: one focuses on the provision of information, and the other involves the development and enforcement of rules against abusive and unfair practices.\n\nI will speak first about the importance of providing consumers with pertinent and accurate information. Clearly, information is critical to the effective functioning of markets. A core principle of economics is that markets are more competitive, and therefore more efficient, when accurate information is available to both consumers and suppliers. When information on alternatives is readily available, product offerings must meet customers' demands and offering prices must reflect those of market competitors. If consumers are well informed, they are in a better position to make decisions that are in their best interest. Information helps and empowers individual consumers by improving their ability to compare products and choose those that will help them meet their personal goals, and this informed comparison shopping enhances competition. As a result, a significant component of the rule-writing process involves crafting disclosure requirements that provide consumers with consistent and relevant information about the terms and fees of financial products.\n\nThe Federal Reserve is keenly aware, however, that information alone may not always be sufficient to combat abusive practices--which brings me to the importance of rules and the enforcement of consumer protection measures. Indeed, the consumer financial services laws implemented by the Federal Reserve contain a number of substantive protections, reflecting carefully considered legislative judgments that certain practices should be restricted or prohibited. Enforcement of these rules is carried out through the Federal Reserve's supervisory oversight for compliance with consumer protection laws and regulations.\n\nAs consumer protection issues have emerged in relation to subprime mortgage lending, the Federal Reserve has sought to address concerns by using its various supervisory and rule-writing tools. In doing so, it is extremely important to strike the right balance by seeking to protect consumers from abusive lending practices without restricting credit from responsible lenders to borrowers with shorter or lower-rated credit histories. The guidance pieces on subprime and nontraditional mortgages that I mentioned earlier address many issues relating to consumer protection, in addition to safety and soundness. We are also undertaking other actions to address concerns related to the flow of information to consumers and enforcement of consumer protections.\n\nOne concern is the need to improve the mortgage information that consumers receive. As I mentioned earlier, effective disclosure empowers consumers and enhances competition. But to be effective, disclosures must give consumers information that they can readily understand at a time when the information is relevant. I anticipate that to that end, the Federal Reserve will propose improvements to the rules governing the disclosure of mortgage loan terms and conditions and the timing of those disclosures. We will soon begin an extensive consumer testing process in order to ensure that the new draft disclosures we propose will be comprehensible and useful to borrowers. To further improve consumers' access to meaningful information, the Federal Reserve also plans to propose rule changes to address misleading practices in mortgage loan advertisements and solicitations.\n\nIn addition to providing consumers with better information, the Federal Reserve plans to exercise its rule-making authority under the Home Ownership and Equity Protection Act (HOEPA) to address unfair or deceptive mortgage lending practices. We plan to propose rules by the end of this year that would apply to subprime loans offered by all mortgage lenders. The practices that may be addressed involve prepayment penalties, stated-income lending, failure to require escrows for taxes and insurance, and making loans without regard to the borrower's ability to repay. In the many Federal Reserve efforts to gather input from a variety of stakeholders, including a full-day hearing I chaired in June that yielded valuable insight from both the industry and consumer groups, these areas have been highlighted as practices for which there is a potential for abuse.\n\nWith respect to supervision of consumer protection in subprime lending, I would like to highlight an important collaboration with other federal and state agencies to expand consumer compliance reviews to include selected non-depository mortgage lenders. As I mentioned earlier, the regulatory scheme for the mortgage industry has become extremely complex as the breadth and depth of this market has grown over the past decade and the role of nonbank mortgage lenders, particularly in the subprime market, has increased. In fact, data collected under the Home Mortgage Disclosure Act show that independent mortgage companies--those that are not depository institutions, subsidiaries of depository institutions, or holding company affiliates--made about 46 percent of higher-priced first-lien mortgages in 2006.1 In addition, there has been an increased presence of mortgage brokers, often independent entities who take loan applications and shop them to depository institutions or other lenders. The increased fragmentation of the mortgage process, from marketing of products to servicing of loans after origination, has created a number of challenges in monitoring practices of various nonbank market players. These market developments have resulted in the oversight of mortgage lending extending beyond the federal banking agencies, and this underscores the importance of collaborating with the state banking agencies and other organizations to address concerns in the subprime mortgage market.\n\nVarious outreach and research efforts have revealed the value of increasing the scrutiny of these lenders, to deepen our understanding of their policies and practices. To this end--and to take advantage of the dual-supervisory roles of federal and state agencies--we have launched a cooperative pilot project to conduct reviews of non-depository lenders with significant subprime mortgage operations. The reviews will evaluate the companies' underwriting standards and senior-management oversight of risk-management strategies for ensuring compliance with consumer protection laws and regulations. Our partners in this initiative are the Office of Thrift Supervision, the Federal Trade Commission, and state agencies represented by the Conference of State Banking Supervisors (CSBS) and the American Association of Residential Mortgage Regulators.\n\nAs part of this project, the Federal Reserve will review nonbank subsidiaries of bank holding companies for compliance with a number of regulations, including TILA, HOEPA, the Equal Credit Opportunity Act, the Real Estate Settlement Procedures Act, and the Home Mortgage Disclosure Act. The other partners in the project will conduct similar reviews of non-depository subsidiaries of thrift holding companies, independent mortgage lending companies, and mortgage brokers doing business with these entities. The partner agencies intend to share information about their findings with each other, review the lessons learned, and seek additional ways to cooperate to ensure effective and consistent supervision of these entities. At the conclusion of the reviews, the agencies will analyze the results and determine whether to continue the project and, if so, how to focus future reviews.\n\nFinally, I would like to say a few words about what the Federal Reserve is doing to help borrowers who may be facing difficulty paying their mortgages. Undoubtedly, foreclosures can have a devastating impact on borrowers, lenders, families, and communities. To the extent possible, efforts should be made to avoid foreclosure. Toward that end, the Federal Reserve along with the other agencies have encouraged lenders and servicers to identify opportunities to work with borrowers confronting mortgage delinquency or foreclosure. For example, lenders and servicers may be able to assist troubled borrowers by modifying the loan, deferring payments, extending the loan maturities, converting an adjustable-rate mortgage to a fixed-rate or fully-indexed loan, or capitalizing delinquent amounts. The best outcome is a loss mitigation strategy that results in a mortgage obligation that the borrower can meet in a sustained manner. The use of these and other loss mitigation techniques is consistent with guidance that emphasizes the importance of prudent underwriting practices to help ensure that borrowers can meet the terms of their mortgage obligation and maintain homeownership.\n\nConclusion\nIn closing, I would like to reiterate that the Federal Reserve is committed to using its broad range of policy and supervisory tools to support efficient and fair markets. I have touched on the ways the Federal Reserve has used some of its tools and described the various efforts we have under way. I assure you that the Federal Reserve will continue to use these tools, as well as to examine various other authorities, to respond in a deliberative and balanced manner that is in the interests of markets, financial institutions, and consumers.\n\nFootnotes\n\n1.  Robert B. Avery, Kenneth P. Brevoort, and Glenn B. Canner, \"The 2006 HMDA Data (1.31 MB PDF),\" The Federal Reserve Bulletin, September 2007, http://www.federalreserve.gov/pubs/bulletin/2007/pdf/hmda06draft.pdf. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20071011a.htm",
        "title": "Markets, Financial Institutions, and Consumers: The Roles of the Federal Reserve",
        "date": "10/11/2007"
    },
    {
        "content": "October 05, 2007\n\nVice Chairman Donald L. Kohn\n\nAt the Greater Philadelphia Chamber of Commerce 207th Annual Meeting, Philadelphia, Pennsylvania\n\nThe economic picture has evolved rapidly over the past few months, and on September 18, the Federal Reserve eased monetary policy, cutting its target for the federal funds rate 1/2 percentage point.  I thought it might be useful this morning to give you my take on why this action was necessary and my sense of what might lie ahead for the economy.  I need to emphasize that these are my views and are not necessarily those of my colleagues on the Federal Open Market Committee (FOMC).1\n\nA brief recap of where the economy has been will be helpful in understanding where we are now and where we might be headed.  The economy has been growing at about a 2 percent pace since the middle of 2006 despite being held back by a weakening housing market.  Job gains have averaged about 125,000 per month over the past year or so, and the unemployment rate has hovered around 4-1/2 percent, a fairly low rate historically for the U.S. economy.  When the members of the Federal Reserve Board and the presidents of Federal Reserve Banks made their semi-annual economic projections this past July, most of us saw growth strengthening a little next year as the drag from the housing adjustment abated, with the unemployment rate perhaps drifting up just a little.  Inflation had been buffeted by large swings in food and energy prices, but underlying inflation rates had edged down since the summer of 2006; on balance, we expected relatively low inflation ahead, although we were concerned that tight labor and product markets could lead to further price pressures.\n\nWhen the FOMC met in early August, this basic picture had not changed much.  The housing sector had continued to weaken; indeed, with subprime mortgage markets increasingly impaired and problems becoming evident in some other segments of mortgage markets, the contraction in residential construction seemed likely to be even deeper than we had previously anticipated.  And increased resistance on the part of investors to some of the terms on the loans and bonds that were backing leveraged buyouts was a sign that a rethinking of risk exposures might be spreading beyond mortgage markets.  But output overall still appeared to be expanding at a moderate pace; jobs and incomes, although rising less rapidly than before, were still advancing reasonably well, supporting sustained expansion in household spending; and businesses had successfully worked off the overhang of inventories that had emerged at the end of 2006.  In addition, strong demand from abroad was providing considerable support to our exports.  The FOMC recognized that less accommodative conditions in some financial markets posed a threat to the strength of future economic growth.  Nonetheless, our primary concern remained the upside risks to inflation.\n\nShortly after the August FOMC meeting, however, financial market conditions deteriorated considerably further, following events that shook investor confidence, particularly in complex structured credit products.  The disruptions to nonprime mortgage markets became more severe and problems even extended to high-quality loans, as rates for prime jumbo mortgages jumped after the secondary markets for them shut down.  Importantly, the disruptions also spread beyond the mortgage markets.  Most notably, investors' concerns about exposures to subprime mortgage credit risk caused them to shun commercial paper that might be backed by such assets, in both Europe and the United States.  This aversion, in turn, meant that commercial banks that had written backup liquidity lines for commercial paper programs or had other connections with these programs might have to make good on their actual or implied support by extending credit.  With leveraged buyout credit and some mortgage originations also possibly staying on the balance sheet unexpectedly, the banks faced substantial, but uncertain, calls on their liquidity and capital.  All this uncertainty led the banks and other short-term lenders to turn very cautious; interest rates on bank deposits and other sources of credit beyond just a few days rose steeply, funding in money markets became concentrated in the very short term, and concerned and uncertain lenders generally became much less willing to extend the credit needed for liquid and efficient financial markets.\n\nWhy the financial markets behaved as they have is a complex story that we will be sorting out for a while.  For our purposes this morning, I will concentrate on the consequences for household and business borrowers and for the economy.  In particular, I expect that the financial market turmoil of the past few months will leave an imprint on the cost and availability of credit to many household and business borrowers.  The greatest effects have been on credit related to residential real estate; in addition to the problems with nonprime and jumbo first mortgages, second mortgages and home equity lines of credit that many households have been using to finance purchases of household durables and other consumer goods and services probably will become more expensive and less available as well.  Banks are likely to be especially cautious about making new loans and financing commitments while substantial uncertainty about the quality of loans and the extent of demands from previous commitments persists.  So it would not be surprising to see less-generous credit for a wide variety of loans to business and households.  And, the rates for some loans are tied directly to elevated libor or other rates in term funding markets.\n\nMore generally, credit will probably not be as easily available and as inexpensive for many borrowers as it was a few months ago, even after market functioning improves.  In several segments of the financial markets, compensation for taking on risk had for some time seemed too low to be sustainable.  In addition, more credit is likely to flow through banks, and leverage in the nonbank sectors of the financial markets will be lower.  The higher levels of capital relative to assets should form the basis for a more stable system, but the spread of lending rates over the cost of funds will need to rise so that capital can earn a competitive rate of return.\n\nThe deterioration in actual and expected financial market conditions between the August and September FOMC meetings changed our view of appropriate monetary policy.  Whatever policy path previously seemed appropriate to support sustainable growth and price stability looked too high once credit conditions tightened substantially.  Our policy easing was aimed at helping to offset the effects of those tighter credit conditions and thereby to encourage moderate economic growth over time.  It was not intended to, nor should it, short circuit a more realistic pricing of risk and the gains and losses that the repricing will entail for market participants.\n\nMany people had expected the Federal Reserve to follow a gradual path of rate reductions in response to financial market developments--say, 25 basis points in September and another 25 basis points in October.  Such a path would be in keeping with how we have often approached our policy choices, as it has the advantage of allowing us to calibrate our policy as we see how the economic situation is evolving and responding to earlier policy moves.  However, given the circumstances at the time of the September FOMC meeting, there were strong arguments in favor of the larger action of a 50 basis point decrease in the federal funds rate.  For one thing, it seemed that a decrease of that size could well be necessary to promote moderate growth.  We had been holding the federal funds rate at 5-1/4 percent, well above the expected rate of inflation, in part to compensate for what had been very narrow yield spreads and readily available credit.  We did not know how quickly markets would recover, the extent to which credit terms and standards would be tightened, or precisely how households and businesses would respond to recent or forthcoming financial developments.  But, pending further evidence, a 50 basis point easing was not an unreasonable first approximation of what might be required to keep the economy on a sustainable growth path.\n\nIn addition, I thought that economic performance would be better served by the Federal Reserve taking its chances on responding too much, or too rapidly, to the turmoil in financial markets rather than acting too little, or too slowly.  Sluggish or inadequate easing risked a weaker real economy that might cause lenders to pull back even more, leading to a deteriorating situation that could prove difficult to reverse.  With the news on inflation relatively favorable of late and with inflation expectations seemingly well anchored, I believed that we would be able to offset the cut in the federal funds rate--if it turned out to be larger than needed--in time to preserve price stability.\n\nSince the September FOMC meeting, we have seen some signs of improvement in some markets that were severely disrupted.  For example, investors appear to be differentiating more among risk characteristics of asset-backed commercial paper programs; term funding has become a little more readily available to banks and commercial paper issuers; and the run-off in commercial paper outstandings has slowed.  But spreads in these markets are still quite high by historical standards and funding maturities are very short, leaving many markets vulnerable to unpleasant surprises.  In mortgage markets, spreads for rates on jumbo prime mortgage loans over those on conforming, agency eligible, loans have come down a bit, but are still elevated.  Indeed, it may be a while before market participants regain enough confidence to price and trade certain types of assets and more normal liquidity conditions are restored.\n\nOur policy action will not be able to avert all of the weakness in the economy that may be in train for the next several months.  Monetary policy works with a lag, and the effects of our easing action will have their maximum effect only after several quarters.  In particular, housing markets are likely to remain depressed in coming months as housing demand is restrained by the difficulty in obtaining mortgages and perhaps also by spreading expectations on the part of buyers that house prices will fall, as they already have in a number of markets.  And, although builders have reduced housing starts sharply, they have made very little progress in reducing the number of unsold new homes on the market.  As a result, even absent a further deterioration in sales, residential construction would probably decline further in the months ahead, imparting a significant drag on overall growth in real gross domestic product.\n\nBeyond housing, it is too early to tell what effect financial market turmoil is having on household and business spending, though very preliminary and partial information suggest that thus far the effects seem to be limited.  Moreover, the available data indicate that the economy entered this period still expanding at a moderate pace.  For example, consumption held up well this summer supported by solid growth in real incomes.  And, the recent data on orders and shipments of capital goods and on nonresidential construction indicated further growth in capital outlays in August.  That said, credit availability is likely to be tighter than before, consumer confidence is down, and businesses will probably be a little more cautious for a while, suggesting that these components of aggregate demand could become more subdued in coming months.\n\nOver time, however, I anticipate that the economy will move back onto a moderate growth track.  The housing market should gradually recover as the cutback in production and lower prices help reduce the inventory overhang.  And, as it does, the drag on growth from the declines in residential construction will abate, providing a boost to overall economic activity.  To be sure, households are likely to start to save more out of their current incomes as they come to realize that they cannot count on a rise in the value of their real estate to build their retirement nest eggs.  However, households have been surprisingly resilient to recent economic shocks, and any rise in the saving rate probably would be gradual.  More generally, consumer spending should continue to be supported by ongoing growth in employment and income.  In the business sector, balance sheets are in good shape, and most firms are not likely to face an appreciable tightening of credit availability.  As a result, I anticipate that they will expand their investment spending to keep pace with rising household demands and with strength in export markets.  In sum, once we get through the near-term weakness caused by the extra downleg from the housing contraction and any spillover from tighter credit conditions, I am looking for moderate growth with high levels of employment.\n\nBut you should view these forecasts even more skeptically than usual.  The FOMC emphasized the considerable uncertainty in the outlook.  As I noted earlier, we do not know how financial markets will evolve, and we do not know how households and businesses will respond to financial developments.  Naturally, these types of uncertainties are greatest when markets are behaving abnormally.  The recovery from the problems of the early 1990s was prolonged because banks had to rebuild capital; the rebound from the market crisis of 1998 was swifter, helped along by higher productivity growth and the rise in the stock market that accompanied the optimism about high-tech profits.  We will need to be nimble in adjusting policy to promote growth and price stability.\n\nOf course, we would not have eased policy if the outlook for inflation had not been favorable.  The recent data on consumer price inflation have been encouraging.  Movements in energy prices have created volatility in overall inflation, but over the past twelve months both core and total prices for personal consumption expenditures rose 1.8 percent.  Moreover, the near-term weakness in the economy should intensify competitive conditions in markets and reduce potential pressures on costs and prices.  And, it will be critical for inflation expectations to remain well contained.\n\nThat said, I do not want to minimize the upside risks to inflation either.  Rates of resource utilization are still relatively high, and the slower rates of productivity growth over the past two years, coupled with a pickup in compensation growth, have led to a noticeable acceleration in unit labor costs.  Moreover, the decline in the exchange value of the dollar has put upward pressure on prices of imported goods, which have both direct and indirect effects on overall consumer prices.  As a result, and as the FOMC noted in our recent statement, we will need to monitor inflation developments carefully.\n\nAllowing inflation to rise would not be in the public interest and would be contrary to our legislative mandate for stable prices and maximum employment.  Maintaining an environment of low and stable inflation facilitates planning, saving, and capital investment by households and businesses and thus is a prerequisite for allowing the economy to realize its potential.  I assure you that we on the FOMC will continue to monitor economic developments closely and will act as necessary to promote both price stability and sustainable economic growth.\n\nFootnotes\n\n1.  Wendy Dunn and William Wascher, of the Board’s staff, contributed to the preparation of these remarks. Return to text",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20071004a.htm",
        "title": "Economic Outlook",
        "date": "10/5/2007"
    },
    {
        "content": "October 05, 2007\n\nGovernor Kevin Warsh\n\nAt the New York State Economics Association 60th Annual Conference, Loudonville, New York\n\nThank you to the New York State Economics Association and our hosts here at Siena College for inviting me to participate in the conference.  They say you can never go home again.  Well, I am testing that theory and am pleased to be back in upstate New York for the second time in as many weeks.  In my remarks at the School of Business at the University at Albany, I argued that the conditions causing the turmoil in the financial markets were long in the making and that these causes should not be conflated with the particular troubles in the mortgage markets.  I also posited that the financial market conditions may have proven to be overly ebullient, masking troubles that may have sown the seeds of financial distress.1  This evening, I will underscore the responsibility of the Federal Reserve during periods of financial market turmoil and offer some perspective on the current state of financial markets.\n\nThe Gathering Storm\nSeveral months ago, many large, global commercial and investment banks appeared on pace to post another record year of corporate profits.  Underwriting and M&A activities were robust.  Sales and trading revenues were bolstered by the acceleration of financial innovation.  Principal investing appeared to be an increasingly accepted industry practice alongside traditional advisory business.  Private pools of capital were growing strikingly.  Public and private pension funds were reportedly increasing capital allocations to alternative investments.  And, thanks in part to accommodative credit markets, the golden age of private equity appeared upon us.  Finance companies and other nondepository financial institutions were increasingly able to thrive, proving to be formidable competitors for traditional banks and thrifts.  In sum, market functioning appeared robust, and risks underlying various assets were seemingly dispersed among a range of sovereignties, financial intermediaries, and investors.\n\nDuring this period of seemingly benign economic conditions, most market participants appeared more focused on the dynamics of the new financial architecture than on the policy judgments of central bankers.  Surely, market participants did not presume that the Federal Reserve was a mere spectator to market developments.   Nonetheless, discussions of the Fed and financial stability may have seemed somehow anachronistic with the new paradigm sweeping financial markets.\n\nHow quickly times change.  As you well know, by mid-August, volatility spiked in many markets.  Risk premiums widened significantly.  Term premiums reappeared with force.  Signs of illiquidity were evident in a number of important markets.  And clarion calls for the Fed to bring stability to financial markets were loud.  Almost overnight, the role of the Federal Reserve and other central banks in fostering financial stability found its way to the front pages of major media.  So, let me discuss the responsibilities of the Federal Reserve in promoting stable financial conditions.  Although our policy tools are powerful, and our judgments are informed, our pronouncements are not made in isolation.  The roles and responsibilities of other public agents, domestic and abroad, and private market participants are particularly critical during times of financial turmoil.  We are, after all, central bankers, not central planners.\n\nResponsibilities of the Federal Reserve\nSo what is the role of a central bank like the Federal Reserve in fostering financial stability?2  Historically, episodes of financial instability and the sharp economic downturns that sometimes ensued were a driving force in the creation of the Federal Reserve itself.  After earlier, sporadic, and ultimately less-than-successful attempts to create a central bank of the United States, the U.S. financial system found itself lacking an effective means to address the periodic financial crises that occurred in the second half of the nineteenth and in the early twentieth century.3  Against this backdrop, the Congress authored the Federal Reserve Act in 1913, creating the Federal Reserve System.  It is worth emphasizing that the Federal Reserve's concern with financial stability stems largely from the adverse implications of financial instability for overall economic performance.4  The Fed's interest in promoting financial stability is thus intimately connected with its macroeconomic objectives:  maximum sustainable employment and price stability.\n\n\n\nFrom the founding of the Federal Reserve to the present, a key question confronts policymakers and market participants alike:  What is financial stability?  Perhaps it is better to address what it is not.  In my view, financial stability does not demand a state of lessened financial market movements, a state of muted volatility.  More often than not, financial markets process new information efficiently:  If some unexpected news arrives, markets adjust, sometimes even sharply, and they should.  These types of movements are healthy, even necessary.  They serve to quickly bring prices in line with underlying fundamentals.  And markets that move quickly and adroitly do not necessarily produce unstable financial conditions.  Nor should those who take up the cause of ensuring financial stability protect individual investors or financial institutions from substantial losses or insolvency.  To the contrary, a healthy and well-functioning financial system will tend to reward well-managed risk-taking and punish imprudence.\n\nI am inclined to interpret the Federal Reserve's interest in promoting financial stability as a desire to foster conditions that favor sustainable growth and stable prices.  In this sense, financial stability concerns may rightly shape policymakers' views about the modal outlook for the economy as well as the risks surrounding this outlook.  Financial instability may thus be characterized as a situation in which the financial system becomes incapable of efficiently allocating resources at market-clearing prices across the economy.5  If financial markets become dysfunctional, financial intermediaries' flexibility may be impaired, and investors may become uncertain about their prospects.  And if this situation were to persist, overall macroeconomic performance could be threatened.  In an earlier period of financial turmoil, this phenomenon was termed \"fear-induced disengagement.\"6\n\nAssessing Financial Stability\nThe Federal Reserve is well positioned to monitor developments in financial markets and assess the quality of market functioning.  We have access to a wide range of financial and economic data and extensive contacts with market participants.  Particularly in times of financial distress, we must draw on a full range of market indicators.  We also glean important information by virtue of our responsibilities as a banking regulator and payment system operator.  And although such a dashboard of key information is exceedingly useful, it should not be confused with a crystal ball.  For even if our understanding of the financial markets was somehow perfect, the transmission mechanism between financial markets and the real economy is only partially understood.  Like private market participants, the Federal Reserve is in the business of making policy judgments amid uncertainty and must assess the prospects for the real economy with considerable humility.\n\nWhat indicators might help us assess the economic and financial situation?   We look to prices at which investors are willing to provide capital by reviewing risk premiums across a range of asset markets.  As an example, we monitor corporate credit spreads from bond, loan, and credit derivative markets, and we follow closely the evolution of pricing in mortgage markets.  We also look to the terms by which market participants are willing to lock up funds over various time horizons by reviewing term premiums embedded in financial market prices.    And we constantly revisit investors' willingness to conduct business with financial intermediaries to assess counterparty credit risk.  In this respect, market-based indicators are certainly informative, as are measures of current exposures of financial institutions obtained through the supervisory process.  No central bank financial market dashboard is complete, however, if it does not give considerable weight to measures of price stability.  As a result, we constantly review inflation expectations, as measured by spot and forward TIPS spreads, surveys, commodity prices, and foreign exchange values.\n\nVolume indicators are often particularly useful in assessing market functioning in times of market turmoil.  Volume indicators impart knowledge about the depth and extent of trading and the willingness of financial intermediaries to serve as market makers.  By reviewing the sizes of issuance of various financial instruments as well as trade volumes in a number of markets, we try to assess the relative strength and resilience of markets.  And we try to assess the reliability of prices by reviewing information on bid-ask spreads and quote sizes where available.  Of course, these various price and volume indicators are not easy to disentangle, necessitating that our judgments on the state of market functioning customarily be provisional.\n\nThroughout the turbulence of the past few months, we have followed a number of indicators that pointed to strains in several markets.  For example, we saw spreads on subprime residential mortgage-backed securities soar and securitization volumes slow to a trickle as market participants became concerned about their ability to value those products amid mounting delinquency rates and defaults in the sector.  Investors' lack of confidence in valuations was also apparent in other securitized products, as evidenced by higher spreads and lower issuance in markets for collateralized debt obligations and collateralized loan obligations.  The asset-backed and the lower-grade unsecured commercial paper markets also came under pressure; difficulties spread to other money markets, and term spreads in interbank funding markets climbed much above historical levels.\n\nIt is also true, however, that some financial indicators provided some reassurance during this period.  Important parts of the financial system continued to function well, especially in markets where less-complex financial products are traded and where investors are less reliant on the role of the rating agencies.  For example, although equity markets were quite volatile at times, trading was generally not impaired, and investors were able to buy and sell stocks at market-prevailing prices, even at the times of greatest turbulence.  The markets for longer-term Treasury securities and investment-grade corporate bonds generally continued to function well, albeit at new market-clearing prices.  It is notable that the level of fails-to-deliver in Treasury trades did not spike amid the market turmoil despite very intense safe-haven demands for Treasury securities at times.  Of great importance, clearing and settlement systems proved to be extremely resilient throughout the episode, even if most post-trade infrastructure providers experienced record transaction volumes.  To be sure, mortgage lenders came under severe stress, and several large commercial and investment banks were subjected to strains brought about by higher contingent liabilities and various other commitments.  Still, at least by number, most financial institutions remained \"open for business,\" willing to lend, albeit at tighter terms.  And although many hedge funds posted meaningful losses, on balance, they appear to have performed a useful function during this period of considerable tumult.\n\nAlthough these positive signs are acknowledged, financial conditions were clearly stressed in recent months.  When markets do not clear and some large financial institutions withdraw from risk-taking, it is prudent for a central bank to take account of the impaired nature of market functioning.  The specter of financial instability is heightened, and the prospect of harm to the overall economy is difficult to dismiss.  The Federal Reserve responded to these developments by providing reserves to the banking system; it announced a cut in the discount rate of 50 basis points and adjustments to discount window practices to facilitate the provision of term funding.  In the current episode, the disruptions in the structured finance, mortgage, leveraged loan, commercial paper, and interbank term funding markets made credit considerably less available for many households and businesses and thus, ultimately, represented a risk to the performance of our macroeconomy.  As a result, the Federal Reserve took action to help forestall this risk, including the 50 basis points cut in the target federal funds rate on September 18.\n\nRecent Financial Market Developments\nIt is premature to judge the ultimate effects of our policy actions on financial conditions, let alone on macroeconomic performance.  Our dashboard of financial indicators, however, points to some encouraging signs, suggesting that financial conditions might be normalizing somewhat.  In particular, I am encouraged by the price differentiation in certain markets based upon company-specific and asset-specific assessments of fundamental value.  Although prices in several markets were no doubt affected by distortions around the quarter-end, some term spreads in the interbank market appear to have reversed a portion of their earlier increases, as have spreads in some parts of the commercial paper market.  On the basis of our most recent data, it seems that the runoff in outstanding commercial paper may be slowing.  Similarly, there are some signs of stabilization in the leveraged loan market.  Banks have been able to sell substantial parts of large deals to investors in recent weeks, and some collateralized loan obligations are coming to market.  Issuance of speculative-grade bonds resumed somewhat of late.  Still, the functioning of several markets continues to be strained, a condition which I would expect to continue for awhile.   Consequently, my colleagues and I on the FOMC will continue to assess the effects that these and other developments could have on the prospects for the economy.  We will rely not only upon economic modeling, but also real-time, forward-looking indicators to help inform our policy judgments.\n\nFootnotes\n\nNote. The views expressed herein are my own and do not necessarily reflect the views of other members of the Board of Governors or of the Federal Open Market Committee.  I thank Board staff members James Clouse and Roberto Perli for their valuable contributions to these remarks.\n\n1. Warsh, Kevin (2007), \"Financial Market Developments,\" speech delivered at the University at Albany, State University of New York, School of Business, Albany, NY, September 21, www.federalreserve.gov/newsevents. Return to text\n\n2.  The Federal Reserve is by no means the only institution in the United States that is concerned with the stability and functioning of the financial system.  Indeed, the Federal Reserve works closely with a number of other U.S. government agencies on both a bilateral basis and jointly through the President's Working Group on Financial Markets to enhance the integrity, efficiency, orderliness, and competitiveness of financial markets and to maintain investor confidence.  In addition, the Federal Reserve participates in a number of important international groups, such as the Financial Stability Forum, the Basel Committee on Banking Supervision, the Committee on the Global Financial System, and the Committee on Payments and Settlement Systems, to name just a few.  Indeed, in today's tightly integrated international financial markets, fostering financial stability requires a global perspective. Return to text\n\n3.  The First Bank of the United States was created in 1791 and lasted until 1811.  The Second Bank of the United States operated from 1816 to 1836. Return to text\n\n4.  Originally, the preamble to the Federal Reserve Act of 1913 stated that the Federal Reserve System was created \"[T]o furnish an elastic currency, to afford means of rediscounting commercial paper, to establish a more effective supervision of banking in the United States, and for other purposes.\"  Macroeconomic objectives were explicitly introduced later, with the 1977 amendment of the Federal Reserve Act, which stated, \"The Board of Governors of the Federal Reserve System and the Federal Open Market Committee shall maintain long-run growth of the monetary and credit aggregates commensurate with the economy's long run potential to increase production, so as to promote effectively the goals of maximum employment, stable prices, and moderate long-term interest rates.\" Return to text\n\n5.  We should also recognize that financial instability is symmetric and could arise equally when credit flows too freely or at prices that are too low. Return to text\n\n6.   Those were the words used by then-Chairman Alan Greenspan in his comments on the 1998 financial crisis. Alan Greenspan (2000), \"Technology and Financial Services,\" speech delivered before the Journal of Financial Services Research and the American Enterprise Institute Conference, in Honor of Anna Schwartz, Washington, DC, April 14, www.federalreserve.gov/newsevents.htm. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/warsh20071005a.htm",
        "title": "Financial Stability and the Federal Reserve",
        "date": "10/5/2007"
    },
    {
        "content": "September 28, 2007\n\nGovernor Frederic S. Mishkin\n\nAt the Tenth Annual International Banking Conference, Federal Reserve Bank of Chicago, Chicago, Illinois\n\nAfter the calm of the past several years, the events of this summer are a strong reminder that our increasingly globalized and sophisticated markets are still vulnerable to systemic risk. When we speak of systemic risk, we mean the risk of a sudden, usually unexpected, disruption of information flows in financial markets that prevents them from channeling funds to those who have the most productive profit opportunities. We have seen how systemic risk, when it becomes especially severe, can result in financial crises--the seizing up of financial markets--which can have potentially important economic consequences. We have also seen how governments, in their role as providers of emergency liquidity, can intervene to help put the financial system back on its feet and prevent a financial crisis from spinning out of control.1\n\nIn mature industrial economies, domestic central banks have the credibility and the resources to play this role. Around the world, central banks have injected liquidity and signaled that credit would be available to those institutions and markets that need it. At other times, as well, the Federal Reserve has acted successfully to prevent potentially devastating financial seizures: notably, after the stock market crash of October 19, 1987, and after the terrorist attacks of September 11, 2001.\n\nGiven the current focus on systemic risk, I would like to talk about an issue that I wrote about extensively before coming to the Board of Governors: financial instability in emerging-market countries. (Please note that my comments here reflect my own views and not necessarily those of the Board of Governors or the Federal Reserve System.) The need for emergency liquidity assistance in times of financial instability is just as strong, and arguably stronger, in emerging-market countries, in part because their less-developed financial markets, weaker institutions, and lack of easily available information often make these countries especially vulnerable to systemic risk. Such risk can be elevated and financial instability triggered by several factors: shocks related to weak domestic institutions and policies, swings in world commodity prices, contagion from other emerging markets, and, turmoil originating in the industrial countries.2\n\nDeveloping economies have made great strides over the past decade to improve economic fundamentals and policymaking, such as strengthening the independence and credibility of their central banks. Many of these countries have reaped the rewards of their labors during the most recent period of market turmoil, as volatility in their domestic financial markets was reasonably contained. However, room for improvement remains. As market participants have become more discriminating in recent years, emerging-market countries with weaker fundamentals and weaker institutions for the most part have been hit relatively harder. More broadly, these events should serve notice that no country is impervious to crises and that the need for a lender of last resort remains strong. To be clear, by lender of last resort, I mean short-term lending on good collateral to sound institutions, when financial markets temporarily seize up. I do not mean rescuing financial market participants from the consequences of their bad decisions by lending to unsound institutions with little capital, thereby postponing the recognition of insolvency.\n\nDespite the need for a lender of last resort, central banks in emerging-market countries, unlike those in advanced countries, often cannot undertake this role. Many emerging-market countries have histories of high inflation and of large fiscal deficits that have generally been accommodated by the monetary authority. This legacy has led to a lack of confidence in the domestic currency, which makes emerging-market economies different from advanced economies in two ways. First, emerging-market economies often have much of their debt denominated in foreign currency. Second, the credibility of central banks in these countries to keep inflation under control is low. Accordingly, an injection of liquidity in the form of domestic currency can actually make the financial crisis worse by raising inflation fears and thus causing the domestic currency to depreciate. Given a debt structure characterized by liabilities denominated in foreign currency, this depreciation causes the domestic-currency value of the liabilities to rise, induces a deterioration of balance sheets, and thus causes a severe economic contraction. Moreover, a run on the domestic currency will likely be associated with a spike in nominal domestic-currency interest rates--just the opposite of what the injection of liquidity was intended to achieve--which will further damage economic prospects.\n\nTherefore, if liquidity is to be provided during a financial crisis in an emerging-market economy, it generally should be in the form of foreign, not domestic currency. But, if a domestic central bank lacks the foreign reserves to conduct emergency liquidity assistance in foreign currency to stop a financial crisis or promote a recovery when one occurs, can another institution come to the rescue? The answer is yes, and it is often best if the assistance comes not from within the country, but from without. Liquidity provided by foreign sources can help emerging-market countries cope with financial crises without many of the undesirable consequences that can result from the provision of domestic-currency liquidity by the domestic central bank. roperly managed, and in conjunction with steps to address the sources of the crisis, foreign liquidity assistance does not lead to increased inflation, higher interest rates, and an excessive depreciation of the domestic currency. Instead, it gives the government international reserves which can then be used to stabilize the value of the domestic currency and support domestic financial markets and institutions. Indeed, foreign liquidity assistance should also help lower interest rates (and thus improve firms' and households' cash flow). The resulting strengthening of domestic balance sheets helps undo the asymmetric information problems created by a financial crisis.\n\nThe need for providing liquidity has once again become the primary focus of governments around the world. Today I would like to review the principles that should govern such lending and then examine some key issues concerning the activities of an international lender of last resort, including which institutions could play this role.\n\nHow Should a Lender of Last Resort Operate?\nOur understanding of the sources of systemic risk immediately suggests three general principles for operating as an effective lender of last resort: (1) restore confidence in the financial system by quickly providing liquidity, (2) limit moral hazard by encouraging adequate prudential supervision, and (3) act as a lender of last resort infrequently.\n\nRestore Confidence in the Financial System by Quickly Providing Liquidity\nWhen a systemic financial crisis occurs, the emergency lender's most crucial task is to restore confidence in the financial system. Without confidence, participants will pull out of financial markets, which will then be unable to channel funds to productive investment opportunities. Confidence is essential to an efficiently operating financial system, and it is also necessary for promoting recovery from, or forestalling, a financial crisis. romoting and restoring confidence are easier said than done, however, and require several measures.\n\nOne such measure is to quickly provide ample liquidity so that markets can operate effectively. Speed is critical. Experience shows that the faster the lending, the lower the amount of lending necessary.3\n\nTo illustrate the benefits of acting quickly, I will use a canonical example, the Federal Reserve's operations in the aftermath of the stock market crash in October 1987. What is remarkable about this episode is that the Federal Reserve did not need to lend directly to the banks to encourage them to lend to the securities firms that needed funds to clear their customers' accounts. ecause the Federal Reserve acted promptly (within a day) and reassured banks that the financial system would not seize up, banks knew that lending to securities firms would be profitable. They saw that making these loans immediately was in their interest, even if they did not borrow from the Federal Reserve. anks thus began lending freely to securities firms, and, as a result, confidence was restored and the fear of crisis diminished almost immediately. The Federal Reserve did not have to increase its lending to the banking system at all, and the actual amount of liquidity that it injected into the banking system through open-market operations in the immediate aftermath of the crash was around $12 billion, which at the time was notable but not exceptional. And the Federal Reserve was able to remove this liquidity almost immediately, within weeks of the crash.\n\nThe resolution of, and recovery from, a financial crisis require a restoration of the balance sheets of financial and nonfinancial firms. This restoration, in turn, requires several steps: the closing down of insolvent financial institutions, the injection of public funds so that healthy financial institutions can buy up the assets of insolvent institutions, and the establishment of a well-functioning bankruptcy law that enables the balance sheets of nonfinancial firms to be cleaned up quickly so that the firms can regain access to the credit markets.\n\nCrucial to a country's successful resolution of a financial crisis is a commitment to necessary reforms and a refusal to go halfway. Allowing weak financial institutions or practices to continue may encourage excessive risk-taking because participants have little to lose. ecause the continued presence of excessive risk diminishes confidence in the future health of the financial system, insolvent financial institutions must be shut down.\n\nLimit Moral Hazard by Encouraging Adequate Prudential Supervision\nThe funds provided by lenders of last resort may be used indirectly to protect depositors and other creditors of banking institutions from losses. This safety net means that depositors and other creditors have little incentive to monitor these banking institutions and withdraw their deposits if the institutions are taking on too much risk. As a result, in the absence of a strong system of bank supervision, banking institutions are encouraged to take on exposures that heighten systemic risk.\n\nTo limit the moral hazard problem created by their acting as lenders of last resort, governments and institutions must make improved financial-sector supervision and regulation a high priority. The usual elements of a well-functioning prudential regulatory and supervisory system are adequate disclosure and capital requirements, limits on currency mismatch and connected lending, prompt corrective action, careful monitoring of an institution's risk-management procedures, close supervision of financial institutions to enforce compliance with regulations, and sufficient resources and accountability for supervisors. Often, however, strong political forces resist putting these kinds of measures into place. This resistance has been a problem in industrialized countries (it was, for example, an important factor in the U.S. savings and loan debacle of the 1980s),4 but the problem is far worse in many emerging-market countries. The political will to adequately regulate and supervise financial institutions can be weak because powerful special interests have prevented such oversight and because the underlying legal and political framework has often been too frail to counteract the special interests.\n\nAnother important element of financial regulation is that the owners, if not also the managers, of insolvent institutions should suffer significant losses in the event of insolvency. In emerging-market countries (and sometimes in advanced countries, a prominent example of which is Japan during the 1990s), governments have often provided insolvent institutions with funds to keep them from failing and left the existing owners and managers in charge. ailing out the owners and managers in this way worsens the moral hazard problem. Knowing that a bailout will occur, they have incentives to take on huge risks because they have so little to lose. Furthermore, in some cases, the owners and managers of these institutions have been able to take the rescue funds for their own personal gain and send them out of the country before the institutions fail.\n\nAct as a Lender of Last Resort Infrequently\nBesides encouraging and promoting the adoption of prudential regulatory and supervisory measures to limit moral hazard, governments and institutions should act as lenders of last resort only when absolutely necessary, as doing so involves a tradeoff between the benefit of preventing a financial crisis and the cost of the moral hazard it creates, which increases systemic risk. Recognizing that the decision to act as a lender of last resort is often very difficult, lenders should refrain from providing funds to markets or institutions not in crisis or to those that are truly insolvent because of an unsustainable amount of debt.5 Furthermore, once a crisis is over, the liquidity that has been injected into the financial system must be removed so that asset prices represent the appropriate market-determined value.\n\nCurrent Challenges for an International Lender of Last Resort\nAs discussed earlier, for certain types of crises in emerging markets an international lender of last resort is necessary. However, the dramatic improvement of the policy and financial environment around the world over the past several years has left many wondering whether such crises are things of the past. For emerging-market economies, the most prominent international institution to act as a lender of last resort has been the International Monetary Fund (IMF). However, demand for IMF lending has dropped more than 80 percent since 2005 as emergency lending has almost ceased and most borrowers have repaid their loans. Such developments have led some to speculate that an international lender of last resort is no longer needed.\n\nHowever, it would be naïve to think that we will never again see situations where an international lender will be indispensable. The past few years have been unusual ones, providing ideal conditions for strong growth in emerging markets. In particular, growth in industrial countries has been solid, borrowing costs have been very low, and commodity prices have been high, not just for fuel but also for many of the primary metals and agricultural products that are produced in developing countries. Many countries have taken advantage of these developments to pay down debt and consolidate fiscal balances.\n\nNevertheless, concerns remain. Numerous economies are vulnerable to changes in commodity prices or slower world demand. This is particularly true for countries that have not improved their financial and regulatory infrastructure and that have adopted policies that stifle investment. In some countries, corporate and household debt levels have increased greatly. For example, one troubling development in the past few years has been the sharp rise of home mortgage lending in foreign currencies, particularly in eastern Europe. This development threatens to unwind the progress made in reducing currency mismatches by shifting the locus of the mismatch from the government or financial sector to the household sector, in which market participants are less well equipped to understand the risks they are taking on.\n\nAnd, more generally, we are increasingly realizing that globalization and the growth of markets have led to complex and occasionally surprising interconnections among markets and economies. Individual countries and regional institutions can track these developments to some extent, but the need to have institutions devoted to international monetary and financial stability on a global level has perhaps never been greater.\n\nGiven a need for lenders of last resort, the question remains, what institutions will best fill that role? The answer is that it is likely to be best filled by a combination of institutions. In some cases, as we have just seen, individual countries, particularly the large industrial countries, will be able to provide liquidity to markets that are domestically based but global in their linkages. To a much larger extent than in the recent past, countries are also working to insure themselves through the accumulation of foreign currency reserves. In the past few weeks, we have seen such reserves being used in the industrial and developing worlds to dampen volatility in exchange rates. Also, talk of regional arrangements such as the Chiang Mai initiative for currency cooperation in Asia, has been increasing. Finally, the IMF remains the premier institution overseeing international monetary and financial stability and crisis lending.\n\nNone of these options are perfect by themselves. Although central banks of large industrial countries have tremendous resources, their primary focus is domestic monetary policy and they have little mandate for involvement in crises without systemic implications for their countries. It is a positive development that countries with significant exposure to foreign currency risk are more and more able to insure themselves with reserves. However, there are costs associated with such reserve accumulation and there is also a danger that, under the guise of \"insurance,\" countries will engage in activities--including intervention to keep their currencies weak--that are increasingly distorting global capital and trade flows. In terms of regional arrangements, the trend toward rising international cooperation and coordination can have benefits. But regional institutions are typically small and untested, and so their actions may risk undermining more-global efforts. Moreover, their lending may violate the principles I discussed above. In many cases, the IMF is likely the best institution to provide liquidity--it has long experience in this role, significant expertise, and the ability to distribute funds quickly. However, IMF funds may be insufficient when the crisis countries--and associated capital markets--are large. In the mid-1990s and early 2000s, for example, the IMF worked in combination with other lenders in the cases of Korea and Mexico.\n\nRegardless of the institution providing emergency liquidity, several challenges must be addressed if that function is to remain effective. One such challenge, which recalls principle number one for operating as a lender of last resort, is the growing need to respond quickly as financial crises evolve. As shown by the events of the past several months, in a world of instantaneous communication and fully integrated financial markets, disruptions in such markets can materialize and spread very rapidly, thereby placing a premium on the ready analysis of developments and quick disbursement of funds. Moreover, an international lender of last resort will be challenged to substantively address liquidity problems in an environment in which gross international flows of capital are increasingly large and threaten to dwarf the resources that can be mustered by the international facility. In many cases, the IMF's funds will be sufficient--as of July, the institution had almost $200 billion in resources available for new financial commitments in the coming year. But in cases involving the largest countries and capital markets, the IMF has played, and must continue to play, the role of coordinator of funds from a variety of sources (a role it adopted most noticeably in the Mexican and Korean crises) or that of a catalyst to restore confidence (as in Brazil in 2002).\n\nA second key challenge for an international lender of last resort remains the need to limit moral hazard by encouraging adequate prudential supervision--principle number two discussed earlier. To address this concern, the official international community has promoted such efforts as the establishment of the Financial Sector Assessment Program (FSAP), the preparation of Reports on the Observance of Standards and Codes (ROSCs), and the publication of Financial Soundness Indicators. In particular, the FSAP and ROSC initiatives, which are conducted jointly with the World Bank, consist of detailed public examinations of the financial sectors of member countries and of the countries' adherence to best practices in data dissemination, policy transparency, legal systems, corporate governance, and in combating money laundering and terrorist financing. In combination with the FSAP, the ROSC program has greatly increased the pressure on emerging markets to adopt reforms to improve economic and financial stability and limit moral hazard. This surveillance should enhance the effectiveness of lending regardless of which institution provides the emergency liquidity.\n\nA third important challenge, reflecting principle number three for operating as an international lender of last resort, is to bolster the ability to say no to countries and, in cases of insolvency, to facilitate the involvement of governments and the private sector. Several years ago the IMF adopted criteria that countries must meet to receive sizable loans. These criteria included rigorous analysis indicating that a country's financial difficulties reflected a crisis of liquidity rather than of solvency, a high likelihood of a quick return to borrowing from private markets, and a strong probability that the stabilization program would be successful. It is less clear what safeguards regional institutions are adopting to enable them to say no to members when the lending is not justified. Moreover, in countries where reserves are plentiful, it may tempting to lend to insolvent institutions and to avoid the difficult reforms necessary to address the underlying weaknesses.\n\nDistinguishing between illiquidity and insolvency, though critical to being an effective lender of last resort, is exceedingly difficult. The difference hinges on many assumptions about future economic conditions, including global and domestic demand, interest rates, commodity prices, exchange rates, and so on, as well as the behavior of market participants, policymakers, and consumers. Moreover the determination is not a static one. Institutions and markets that are initially illiquid can quickly become insolvent without the appropriate funds. The distinction may be even trickier in the case of sovereign insolvency. One could argue that governments have at their disposal an even greater range of possible policy responses to crises than do firms or markets, and so they face a greater range of potential outcomes.\n\nAround the world over the past few weeks, central bankers, market participants, academics, and the media have been wrestling with the question of what it means to be an effective lender of last resort. Appropriately providing liquidity while limiting the risk of moral hazard has always been a challenge. Within their own countries, policymakers worldwide must wrestle with the best way to design institutions and, in times of crisis, support the stability of financial systems, in both the short and long runs. This approach must also be taken internationally. We have been fortunate that global economic conditions have been strong. However, it would be a grave mistake to assume that the world no longer needs a lender of last resort. In addition to promoting vigilance and crisis prevention, we should continue to strengthen our international institutions to enable them to provide liquidity quickly, appropriately, and in a way that encourages reform and good policymaking.\n\nFootnotes\n\n1.  I thank Joseph Gagnon, Steven Kamin, and Beth Anne Wilson for their assistance on this speech. Return to text\n\n2.  My views on the factors that produce systemic risk and financial instability are in Frederic S. Mishkin, \"The Causes and Propagation of Financial Instability: Lessons for Policymakers (145 KB PDF),\" Maintaining Financial Stability in a Global Economy (Federal Reserve Bank of Kansas City, Kansas City, MO., 1997): 55-96 and Frederic S. Mishkin, The Next Great Globalization: How Disadvantaged Nations Can Harness Their Financial Systems to Get Rich, 2006, (Princeton, NJ; Princeton University Press). For a survey on contagion, see Graciela L. Kaminsky et al., \"The Unholy Trinity of Financial Contagion,\" Journal of Economic Perspectives, Fall 2003, Vol 17, No. 4, pp. 51-74.  Return to text\n\n3.  Frederic S. Mishkin, \"Asymmetric Information and Financial Crises: A Historical Perspective,\" in R. Glenn Hubbard, ed., Financial Markets and Financial Crises (Chicago: University of Chicago Press, 1991), 69-108. Return to text\n\n4.  For example, see Edward J. Kane, The S&L Insurance Mess: How Did It Happen? (Washington, D.C.: Urban Institute Press, 1989). Return to text\n\n5.  Morris Goldstein, \"The International Financial Architecture,\" in C. Fred Bergston, ed., The United States and the World Economy: Foreign Economic Policy for the Next Decade (Washington, D.C.: Institute for International Economics, 2005), 373-407. Goldstein argues that surveillance by the International Monetary Fund needs to focus more on debt sustainability. Return to text\n\n",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/mishkin20070928a.htm",
        "title": "Systemic Risk and the International Lender of Last Resort",
        "date": "9/28/2007"
    },
    {
        "content": "September 27, 2007\n\nGovernor Frederic S. Mishkin\n\nAt the Domestic Prices in an Integrated World Economy Conference, Board of Governors of the Federal Reserve System, Washington, D.C.\n\nIn recent years, globalization has become one of the hottest topics, not only for the general public but also for central bankers.1  Some commentators have gone so far as to claim that greater openness of economies to flows of goods, services, capital, and businesses from other nations invalidate traditional economic models of inflation, which take little account of globalization.\n\nIn the long run, monetary policy strives to achieve price stability, which contributes to maximum sustainable employment and economic growth.  In the shorter run, we at the Federal Reserve aim to achieve our dual mandate of not only stabilizing prices but also reducing the volatility of output and employment around their maximum sustainable levels.  Globalization affects the ability of monetary policy makers to stabilize prices and output in two ways: (1) through its effects on the behavior of inflation and output and (2) through its effects on the ways in which monetary policy influences inflation and output--that is, on the monetary transmission mechanism.\n\nI will look at each in turn and will then use the analysis to address another important issue for monetary policy makers:  Has globalization been a key driver of improvements in inflation performance and the decline in inflation that we have been seeing throughout the world?\n\n(Please note that my comments here reflect my own views and not necessarily those of others on the Board of Governors or the Federal Open Market Committee.)\n\nGlobalization and Inflation\nWe should never forget Milton Friedman's adage that \"inflation is always and everywhere a monetary phenomenon.\"  In the long run, as long as a central bank has an independent monetary policy‑‑that is, it is not locked into a fixed-exchange-rate regime in which its hands are tied‑‑the rate of inflation is determined by monetary policy.  Globalization, however, can have an effect on the incentives for central banks to control inflation and, more directly, on inflation developments in the short and medium runs.\n\nKenneth Rogoff (2003) argues that globalization has led to greater price flexibility, which has reduced the ability of central banks to use inflation surprises to boost output.  In other words, the Phillips curve will steepen, making more stark the short-run tradeoff between unemployment and inflation.  As a result, central banks will be less tempted to try to exploit the short-run tradeoff between inflation and unemployment, as in the Barro-Gordon (1983) model, and so will be less likely to pursue overly expansionary monetary policy that leads to higher inflation.  A major problem with Rogoff's argument is that instead of steepening with the growth of globalization in recent years, the Phillips curve has become flatter, not only in the United States but also in many other countries throughout the world (Borio and Filardo, 2007; International Monetary Fund, 2006; Ihrig and others, 2007; Pain, Koske, and Sollie, 2006).  Therefore, even though Rogoff's argument is reasonable from a theoretical viewpoint, it is hard to make the case that it is important in the current economic environment.\n\nGlobalization, because it makes markets more competitive, also has the potential to spur productivity growth.  Higher productivity growth can lead to a reduction in inflation because it directly lowers prices if monetary policy does not become more expansionary.  In addition, such growth makes it easier for the monetary authorities to allow inflation to fall because output growth will continue to be rapid when inflation is declining.  This may have been the situation in the United States in the late 1990s, when productivity growth surged and inflation declined.  The rise in productivity growth during this period in the United States, however, did not seem to spill over to other industrial countries, a result that cast doubt on whether globalization has indeed accelerated the transmission of productivity growth across national borders.\n\nBecause globalization increases competition, it can also reduce markups (price over costs), and this reduction may lead to lower relative prices, as is argued by Chen, Imbs, and Scott (2007).  However, lower markups and price levels should have only transitory effects on inflation.  Furthermore, the prediction of lower markups from globalization seems to conflict with the high corporate profit rates that we are currently observing around the world.\n\nThese effects from the greater price flexibility and increased competition in domestic markets that have arisen from globalization, while theoretically plausible, are often at variance with salient features of the world economy, and so they do not explain why inflation has declined in recent years.  However, another very dramatic feature of globalization is that it has brought more than a billion new workers into the global economic system from China and India.  Some observers claim that through its sales of low-cost goods, developing Asia‑‑and especially China‑‑has been \"exporting deflation\" and will continue to do so until wages in these countries rise.  Although this effect, too, is plausible, research suggests that its importance should not be exaggerated.\n\nIf China were truly exporting deflation, the effect should be evident primarily in the behavior of import prices.  Research at the Federal Reserve Board by Kamin, Marazzi, and Schindler (2006) estimates that purchases of manufactures from China have lowered U.S. import price inflation roughly 1 percentage point annually over the past decade, a decline that has led to a short-run lowering of consumer price inflation of about one-tenth of 1 percentage point and a somewhat larger effect over the longer term.  Research at the Organisation for Economic Co-operation and Development (OECD) by Pain, Koske, and Sollie (2006) arrives at a similar estimate of the effect of trade in manufactures with developing countries on U.S. inflation:  negative 0.2 to negative 0.3 percentage point.\n\nAt the same time, the additional demand for primary commodities by developing Asia, especially China and India, is putting upward pressure on global prices of these goods.  During 2004-06, the region accounted for about 40 percent of the global growth in demand for oil and more than 70 percent of the growth in demand for copper and zinc.  The global commodities boom, which has been stimulated by the emergence of the Chinese and Indian economies on the international scene, has been an important offset to the deflationary effects from their low-priced goods and services.\n\nAnalysis by the Board's staff, which weights the negative effect on U.S. inflation from cheaper manufactures against the positive effect from higher commodity prices, finds that the net effect in recent years could go either way but in any case is probably quite small.  Looking back before the recent boom in commodities prices, the staff's best estimate is that China's and India's entrance on the global trading scene has had a small negative effect on inflation.  Research at the OECD (Pain, Koske, and Sollie, 2006) reaches similar conclusions, finding a net effect of 0 to negative 1/4 percentage point for the United States, the euro area, and the OECD economies in aggregate.\n\nLaurence Ball (2006) makes an important point that cheaper imports from places like China lower relative prices for imported goods but ultimately do not affect inflation, which is the change in overall prices.  Following the argument made by Milton Friedman (Friedman, 1974), Ball points out that for every decline in the relative price of one good or service in a price index like the consumer price index (CPI), there is by definition a rise in the relative price of some other good or service.  Any pattern of relative-price changes is compatible with a particular level of the inflation rate.  What determines the overall inflation rate is not relative prices for one category of goods and services but rather the balance between overall demand and supply in the economy, which ultimately is influenced by monetary policy.  Ball therefore takes the view that cheap imports from China and other low-wage economies should not affect inflation.\n\nI would not go as far as Ball does for the reason that changes in relative prices in an important category of goods and services could affect inflation for a considerable period of time.  Nevertheless, his argument, as well as the research cited earlier, indicates that many of the exaggerated claims that globalization has been an important factor in lowering inflation in recent years just do not hold up.\n\nGlobalization and Output\nThe increasing integration of the global economy can have several effects on output.  It is thus of concern to monetary policy makers because it can affect both output volatility and our forecasts of the economy.\n\nGlobalization may stabilize output by enabling producers to service a diversified global market rather than just the domestic market.  Research at the Federal Reserve Board (Ihrig and others, 2007) documents that net exports tend to be negatively correlated with domestic demand and thus stabilize output; other research (Guerrieri, Gust, and López-Salido, 2007) finds that shocks to domestic demand move output less in more open economies.  In the opposite direction, greater trade integration‑‑including greater trade in services (Markusen, 2007)‑‑could raise output volatility as countries become more vulnerable to foreign shocks.  There is indeed some evidence that this situation has occurred in Mexico (Bergin, Feenstra, and Hanson, 2007).\n\nAs with the effects of greater trade integration on the volatility of output, the effects of financial globalization can go both ways.  Increasing global diversification lowers the likelihood that financial shocks will be concentrated in individual economies and thus lead to economic downturns.  Furthermore, as I have emphasized in my writings (Mishkin, 2006a), financial globalization can help promote institutional reforms that can make the financial system more stable, thereby contributing to more output stability.  However, as I have also emphasized in my work (Mishkin, 2006a, chap. 4), financial globalization makes it easier for capital inflows to fuel excessive risk-taking on the part of financial institutions and allows financial shocks to be transmitted more readily across borders.\n\nOn balance, my sense is that economic globalization has the potential to be stabilizing for individual economies as both real and financial shocks are spread more evenly across larger numbers of economic agents.  One might even speculate that globalization has contributed to the so-called Great Moderation, the decline in output variability in countries like the United States over the past twenty years, and this hypothesis should be a topic for future research.  The bottom line, however, is that it is not at all clear whether globalization increases or reduces output volatility.\n\nThat said, as I have emphasized in my speeches and writings (Mishkin, 2006a, b; Mishkin, 2007b), I strongly believe that globalization is and has been a key factor in promoting economic growth.  Globalization not only promotes a more competitive economic environment--which forces business to innovate--but it also creates strong incentives for institutional reform to make markets work better.  Globalization in recent years has not only enabled hundreds of millions of  people in countries like China and India to escape abject poverty (income of less than $1 per day) but has also helped economies like ours in the United States to be highly dynamic, which is essential to our future economic well-being.\n\nGlobalization and the Monetary Transmission Mechanism\nFour questions naturally arise when we consider whether globalization has changed the monetary transmission mechanism, that is, how monetary policy influences inflation and output:  (1) Has globalization led to a decline in the sensitivity of inflation to domestic output gaps (the difference between actual and potential output) and thus to domestic monetary policy?  In other words, has globalization made the Phillips curve flatter?  (2) Are foreign output gaps playing a more prominent role in the domestic inflation process, so that domestic monetary policy has more difficulty stabilizing inflation?  (3) Can domestic monetary policy still control domestic interest rates and so stabilize both inflation and output?  (4) Are there other ways, besides possible influences on inflation and interest rates, in which globalization may have affected the transmission mechanism of monetary policy?\n\nHas globalization led to a decline in the sensitivity of inflation to domestic output gaps (the difference between actual and potential output) and thus to domestic monetary policy?\n\nIn recent years, we have clearly witnessed a decline in the sensitivity of inflation to the domestic output gap (a flattening of the Phillips curve) in the United States and other advanced countries (Borio and Filardo, 2007; International Monetary Fund, 2006; Ihrig and others, 2007; Pain, Koske, and Sollie, 2006).  Globalization might make inflation less responsive to rising domestic resource utilization because households and businesses can go outside the country to buy goods and services, so there will be less pressure for domestic prices to rise.  Another way of thinking about this point is to recognize that globalization might reduce the likelihood of having supply bottlenecks as domestic resource utilization rises.  Although this story is a plausible one, research at the Federal Reserve Board and elsewhere finds no evidence that the flattening of the Phillips curve reflects the process of increasing trade integration (Ihrig  and others, 2007; Ball, 2006; Wynne and Kersting, 2007).2\n\nRather than globalization being an important factor leading to flatter Phillips curves, I would argue (as in Mishkin, 2007a) that flatter Phillips curves are the direct result of better monetary policy that has anchored inflation expectations.  Because monetary authorities are focusing more on establishing a stronger nominal anchor, a rise in resource utilization will not lead to a rise in expected inflation.  Instead, households and businesses will expect monetary authorities to take the necessary steps to ensure that the economy will not overheat, and, as a result, they will not push for higher prices and wages.  Not only is this explanation for decreased sensitivity of inflation to output gaps more consistent with empirical evidence (see Mishkin, 2007a; Roberts, 2006), but it is more consistent with the timing of when Phillips curves became flatter.  In the United States, Phillips curves started to flatten in the 1980s, well before the recent surge of globalization but just after inflation expectations started to become anchored.\n\nAre foreign output gaps playing a more prominent role in the domestic inflation process, so that domestic monetary policy has more difficulty stabilizing inflation?\n\nAs economies have become more open, foreign factors may have become more important in the determination of domestic inflation.  Research at the Bank for International Settlements (Borio and Filardo, 2007) seems to provide evidence that foreign resource slack has superseded domestic slack as a key determinant of domestic inflation.  However, research at the Federal Reserve Board (Ihrig and others, 2007) and at the OECD (Pain, Koske, and Sollie, 2006) point out that the specification of the Phillips curve in Borio and Filardo (2007) is problematic.  Their key findings are not robust to alternative specifications.  Moreover, their specification leads to econometric difficulties because it causes serial correlation of the residuals that is not corrected for.\n\nUsing more-conventional specifications of Phillips curves, the research at the Federal Reserve Board (Ihrig and others, 2007) and the OECD (Pain, Koske, and Sollie, 2006), as well as Ball (2006), finds that foreign output gaps are not important determinants of domestic inflation.  However, foreign factors could more plausibly play a role in the inflation process through import prices.  As economies become more open and imports play a bigger role in the economy, consumer prices may become more sensitive to import prices.  Indeed, CPI inflation does appear to have become more sensitive to import prices over time, both in the United States and in other OECD countries (Pain, Koske, and Sollie, 2006).3\n\nCan domestic monetary policy still control domestic interest rates and so stabilize both inflation and output?\n\nIn principle, the increasing global integration of financial markets, by reducing the scope for individual central banks to control domestic interest rates, could hamper the ability of monetary policy to stabilize prices and economic activity.  Indeed, some evidence appears to suggest that foreign factors influence interest rates; for example, the global saving glut does seem to have led to somewhat lower long-term interest rates by reducing term premiums (Warnock and Warnock, 2006).  More generally, research points to important linkages between U.S. and foreign interest rates and other asset prices (Ehrmann, Fratzscher and Rigobon, 2005; Faust and others, 2007).   However, central banks still retain the ability to control short-term interest rates, which affect the domestic cost of credit and long-term interest rates, and so can continue to do their job of stabilizing inflation and output.\n\nAre there other ways, besides possible influences on inflation and interest rates, in which globalization may have affected the transmission mechanism of monetary policy?\n\nThe preceding discussion might lead to the conclusion that globalization has not had important effects, either on the behavior of inflation or the ability of monetary policy to affect the economy.  However, globalization might have an effect on the process of monetary transmission that does not operate through Phillips-curve-type mechanisms.\n\nOne of the key transmission channels of monetary policy is the exchange rate.  A tightening of monetary policy, for example, raises U.S. interest rates relative to those abroad, thereby inducing upward pressure on the foreign exchange value of the dollar.  An appreciation of the dollar, in turn, restrains exports (because the price of U.S. goods rises when measured in foreign currencies) and stimulates imports (because imports become cheaper in dollar terms).  The resulting decrease in net exports implies a reduction in aggregate demand.  In addition, an appreciation of the dollar that leads to a decline in import prices also helps restrain overall U.S. inflation.\n\nBy expanding the share of tradable goods and services in the economy, globalization might increase the role of the exchange rate as a transmission channel of monetary policy and could reduce the role of the interest rate channel.  The larger the share of imports and exports in the economy, the greater the change in net exports‑‑and, hence, in the contribution of net exports to gross domestic product (GDP) growth‑‑for a given change in the exchange rate.  In addition, the larger the share of imports in the economy, the larger should be the effect on overall CPI inflation of a given change in import prices when the exchange rate changes.4  (This effect is explicitly incorporated in Federal Reserve staff models of U.S. inflation, which weight import prices by the share of imports in the consumption basket.)\n\nBy the same token, the effect of the interest rate channel on overall economic activity may be diminished by greater trade integration as changes in domestic demand are offset by induced changes in imports.  Guerrieri, Gust, and López-Salido (2007), for example, find that shocks to domestic demand move output less in more-open economies because they lead to larger offsetting movements in the trade balance.  Supporting this result, Ihrig and others (2007) conclude that correlations between real GDP growth and real domestic demand growth have declined in recent decades in the United States and several other industrial economies.\n\nIn addition to increasing the sensitivity of the economy to changes in exchange rates, globalization may have increased the sensitivity of exchange rates to monetary policy.  Over the past few decades, as capital controls have been eliminated in most major economies and the levels of home bias in portfolio investment have declined, financial markets around the world have become more tightly integrated.  An implication of this financial globalization is that demand for domestic and foreign assets is likely to have become more sensitive to international differences in perceived rates of return.  Accordingly, monetary policy actions may now exert more influence on exchange rates than was the case when markets were less tightly integrated and assets of different countries were perceived to be less substitutable for each other.  This linkage between globalization and the effect of monetary policy on exchange rates is somewhat speculative but represents a worthwhile avenue for further research.\n\nWhy Has Inflation Declined in Recent Years?\nWhat does all the preceding analysis tell us about why we have had better inflation performance in recent years?  I don't know of anyone who would have predicted twenty years ago that inflation would be so low and stable in so many countries.  Has globalization been an important part of the story of inflation's remarkable decline in recent years?  In terms of direct effects, the discussion here provides a clear-cut answer:  No.  Inflation has come down in the old-fashioned way.  Tighter monetary policy and a commitment to price stability by central banks throughout the world have led to lower inflation and an anchoring of inflation expectations.  These policies have had huge benefits‑‑not only the achievement of low and stable inflation but also an improvement in the overall performance of the economy.\n\nGlobalization, however, may have helped reduce inflation in more-subtle ways.  By fostering increased interactions among central banks, academics, and the public in many different countries, globalization has helped spread a common culture that stresses the benefits of achieving price stability.  The resulting increased focus on price stability has been a key reason for the reduction of inflation worldwide.\n\nConclusion\nThe increasing integration of global product, labor, and financial markets has the potential to significantly alter the behavior of the economy, a development that could complicate the task of monetary policy.  In practice, however, the behavior of the U.S. and global economies does not appear to have radically changed in recent years.  The Federal Reserve and other central banks retain the ability to stabilize prices and output.  Nonetheless, central bankers must continue to monitor the evolution of the economy and the changes that may result from the ongoing globalization process.\n\nReferences\n\nBall, Laurence M. (2006). \"Has Globalization Changed Inflation?\"  NBER Working Paper Series 12687. Cambridge, Mass.: National Bureau of Economic Research, November, www.nber.org/papers.\n\nBarro, Robert J., and David B. Gordon (1983). \"Rules, Discretion, and Reputation in a Model of Monetary Policy,\"  Journal of Monetary Economics, vol. 12 (no. 1), pp. 101-22.\n\nBergin, Paul R., Robert C. Feenstra, and Gordon Hanson (2007). \"Outsourcing and Volatility (184 KB PDF),\"  paper presented at the Conference on Globalization and the Macroeconomy, European Central Bank, July 23-24, www.ecb.int/events/conferences/html/global_macro.en.html.\n\nBorio, Claudio E.V., and Andrew Filardo (2007). \"Globalisation and Inflation: New Cross-Country Evidence on the Global Determinants of Domestic Inflation,\" BIS Working Paper 227. Basel: Bank for International Settlements, May, www.bis.org/forum/research.htm.\n\nChen, Natalie, Jean Imbs, and Andrew Scott (2007). \"The Dynamics of Trade and Competition (556 KB PDF),\" paper presented at the Conference on Globalization and the Macroeconomy, European Central Bank, July 23-24, www.ecb.int/events/conferences/html/global_macro.en.html.\n\nEhrmann, Michael, Marcel Fratzscher, and Roberto Rigobon (2005). \"Stocks, Bonds, Money Markets, and Exchange Rates: Measuring International Financial Transmission (953 KB PDF),\" ECB Working Paper No. 452. Frankfurt: European Central Bank, March, www.ecb.int/pub/scientific/wps/date/html/wps2005.en.html.\n\nErceg, Christopher, Christopher Gust, and David López-Salido (2007). \"The Transmission of Domestic Shocks in Open Economies (406 KB PDF),\" paper presented at \"International Dimensions of Monetary Policy,\" a conference sponsored by the National Bureau of Economic Research Conference on Policy, held in Barcelona, June 11-13, www.nber.org/booksnew/gert07-1.\n\nFaust, Jon, John H. Rogers, Shing-Yi B. Wang, and Jonathan H. Wright (2007). \"The High-Frequency Response of Exchange Rates and Interest Rates to Macroeconomic Announcements,\" Journal of Monetary Economics, vol. 54 (May), pp. 1051-68.\n\nFriedman, Milton (1974). \"Perspectives on Inflation,\" Newsweek, June 24.\n\nGuerrieri, Luca, Christopher Gust, and David López-Salido (2007). \"International Competition and Inflation: A New Keynesian Perspective,\" unpublished paper, Board of Governors of the Federal Reserve System, August.\n\nIhrig, Jane, Steven B.  Kamin, Deborah Lindner, and Jaime Marquez (2007). \"Some Simple Tests of the Globalization and Inflation Hypothesis,\" International Finance Discussion Papers 891. Washington: Board of Governors of the Federal Reserve System, April, www.federalreserve.gov/pubs/ifdp.\n\nInternational Monetary Fund (2006). \"How Has Globalization Affected Inflation? (762 KB PDF)\" in Globalization and Inflation, IMF World Economic Outlook. Washington: International Monetary Fund, pp. 97-134, www.imf.org/pubs/ft/weo/2006/01.\n\nKamin, Steven B., Mario Marazzi, and John W. Schindler (2006). \"The Impact of Chinese Exports on Global Import Prices,\" Review of International Economics, vol. 14 (May), pp. 179-201.\n\nMarkusen, James R. (2007). \"Trade and Foreign Direct Investment in Business Services: A Modelling Approach (671 KB PDF),\" paper presented at the Conference on Globalization and the Macroeconomy, European Central Bank, July 23-24, www.ecb.int/events/conferences/html/global_macro.en.html.\n\nMishkin, Frederic S. (2006a). The Next Great Globalization: How Disadvantaged Nations Can Harness Their Financial Systems to Get Rich, Princeton: Princeton University Press.\n\n------------ (2006b). \"Globalization: A Force for Good?\" speech delivered at Baruch College, New York, N.Y., October 12, www.federalreserve.gov/newsevents.\n\n------------ (2007a). \"Inflation Dynamics,\" speech delivered at the Federal Reserve Bank of San Francisco, San Francisco, Calif., March 23, www.federalreserve.gov/newsevents.\n\n------------ (2007b). \"Globalization and Financial Development,\" speech delivered to the New Perspectives on Financial Development Conference, Washington, D.C., April 26, www.federalreserve.gov/newsevents.\n\nPain, Nigel, Isabell Koske, and Marte Sollie (2006). \"Globalisation and Inflation in the OECD Economies,\" OECD Economics Department Working Paper No. 524. Paris: Organisation for Economic Co-operation and Development, November, www.oecd.org/findDocument.\n\nRoberts, John M. (2006). \"Monetary Policy and Inflation Dynamics,\" International Journal of Central Banking, vol. 2 (September), pp. 193-230.\n\nRogoff, Kenneth S. (2003). \"Globalization and Global Disinflation (237 KB PDF),\" paper presented at \"Monetary Policy and Uncertainty: Adapting to a Changing Economy,\" a symposium sponsored by the Federal Reserve Bank of Kansas City, held in Jackson Hole, Wyo., August 28-30, www.kansascityfed.org/publicat/sympos/2003/pdf/Rogoff2003.pdf.\n\nWarnock, Francis E., and Veronica Cacdac Warnock (2006). \"International Capital Flows and U.S. Interest Rates,\" NBER Working Paper Series 12560. Cambridge, Mass.: National Bureau of Economic Research, October, www.nber.org/papers.\n\nWynne, Mark A., and Erasmus K. Kersting (2007). \"Openness and Inflation,\" Federal Reserve Bank of Dallas, Staff Papers, vol. 2007 (April), pp. 1-28, http://dallasfed.org/research/staff.\n\nFootnotes\n\n1.  I want to thank Steven Kamin and Jaime Marquez for their helpful comments and assistance on this speech. Return to text\n\n2.  International Monetary Fund (2006) does find some evidence supporting the hypothesis that globalization flattens the Phillips curve:  In a cross-country regression of inflation on, among other factors, the trade share multiplied by the output gap, the interaction term is found to have a significant negative coefficient.  In the simpler specification of Ihrig and others (2007), however, the effect of this interaction term is not significant. Return to text\n\n3.  International Monetary Fund (2006) finds indirect evidence supporting this channel‑‑namely, that inflation is affected by relative import prices multiplied by import shares in GDP and that import shares have been rising over time.  Ihrig and others (2007) document a rise in the sensitivity of U.S. inflation to import prices over time, although they do not identify such a trend in other countries. Return to text\n\n4.  A qualification of this point is that even as the share of imports in U.S. spending has risen, the pass-through of exchange rates into import prices has declined.  However, we do not know whether the decline in pass-through is merely transitory or will be sustained. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/mishkin20070927a.htm",
        "title": "Globalization, Macroeconomic Performance, and Monetary Policy",
        "date": "9/27/2007"
    },
    {
        "content": "September 24, 2007\n\nChairman Ben S. Bernanke\n\nAt the U.S. Chamber Education and Workforce Summit, Washington, D.C.\n\nWhen I travel around the country, meeting with students, business people, and others interested in the economy, I am occasionally asked for investment advice.  Usually (though not always) the question is posed in jest.  No one really expects me to tell them which three stocks they should buy.  However, I know the answer to the question and I will share it with you today:  Education is the best investment.\n\nHere at the U.S. Chamber Education and Workforce Summit, I don’t really need to convince you that, as an investment, education provides excellent returns, both for individuals and for society.  As executives accustomed to making hard cost-benefit decisions, you doubtless assign a high priority to the quality of your business’s workforce because you know that a key--perhaps the key--to your success is the capabilities of the people you employ.  To a significant extent, those capabilities are the product of education.  Here I am speaking not just of education acquired formally in classrooms before entering the workforce but also of lifelong learning that, yes, includes the formal classroom training that might first come to mind but that also includes early childhood programs, informal mentoring on the job, and mid-career retraining, to name a few examples.  And when I speak of capabilities, I mean not only the knowledge derived from education but also the values, skills, and personal traits acquired through education, which are as important as, and sometimes even more important than, the specific knowledge obtained.  These include such qualities as the ability to think critically, to communicate clearly and logically, and to see a project through from start to finish.\n\nToday, I would like to offer a broad overview of education and its importance to our economy from my perspective not only as an economist but also as a one-time school board member, the spouse of a teacher, and the parent of two young adults pursuing higher education.  Although the United States has long been a world leader in expanding educational opportunities, we have also long grappled with challenges, such as troubling high-school dropout rates, particularly for minority and immigrant youths, and frustratingly slow and uneven progress in raising test scores and other measures of educational achievement.  If we are to make progress in meeting these challenges, we must be willing to actively debate their causes and continually experiment and innovate to find solutions.\n\nThe Benefits of Education\nEducation imparts significant benefits both to our society and the individuals who pursue it.  Economists have long recognized that the skills of the workforce are an important source of economic growth.  Moreover, as the increase over time in the returns to education and skill is likely the single greatest cause of the long-term rise in economic inequality, policies that lead to broad investments in education and training can help reduce inequality while expanding economic opportunity (Bernanke, 2007).  But the benefits of education are more than economic.  A substantial body of evidence demonstrates that more-highly-educated individuals are happier on average, make better personal financial decisions, suffer fewer spells of unemployment, and enjoy better health.  Benefiting society as a whole, educated individuals are more likely to participate in civic affairs, volunteer their time to charities, and subscribe to personal values--such as tolerance and an appreciation of cultural differences--that are increasingly crucial for the healthy functioning of our diverse society (Glaeser, Ponzetto, and Shleifer, 2006; Dee, 2004).\n\nFrom a macroeconomic standpoint, education is important because it is so directly linked to productivity, which, in turn, is the critical determinant of the overall standard of living.  The Bureau of Labor Statistics estimates that, between 1987 and 2006, ongoing improvement in the education and experience of the U.S. workforce contributed 0.4 percentage point per year to the increase in nonfarm business labor productivity (U.S. Department of Labor, 2007), a significant amount.  These estimates are however conservative in that they hold fixed other sources of productivity growth, such as the accumulation of various forms of capital and the advance of technology; but workers’ skills certainly contribute indirectly to productivity growth by affecting these other factors as well.  For example, the state of technology is affected both by the creativity and knowledge of scientists and engineers engaged in formal research and development as well as by the efforts of skilled workers on the shop floor who find more efficient ways to accomplish a given task.  Managers who develop a new business plan or find new ways to use evolving technologies can also be thought of as adding to the “intangible,” or knowledge-based, capital of the firm, which by some estimates is comparable in importance to physical capital such as factories and equipment (Corrado, Hulten, and Sichel, 2006).\n\nFor individuals, the economic returns to education are substantial as well.  In 2006, the median weekly earnings of college graduates were 75 percent higher than the earnings of high-school graduates.  In turn, workers with a high-school degree earned 42 percent more than those without any diploma.1   These differentials are large and have been growing; indeed, they have roughly doubled in the past twenty-five years or so.  The source of the widening wage gap between the more-educated and less-educated is nothing more complicated than supply and demand.  The demand for more-educated workers has been increasing rapidly, partly because the much more widespread use of computers and other sophisticated information and communication technologies in the workplace has increased the reward for technical skills.  The supply of highly educated workers has also risen.  At the start of the 1980s, 22 percent of young adults aged 25 to 29 held a college degree or more; by last year, that fraction had moved up to 28.5 percent.2   Nevertheless, the supply of educated workers has not kept pace with demand, thus generating an increased salary premium for education.  Because the wages of those at the top of the educational ladder have increased the fastest, increasing our investment in education can benefit not only individuals and society but also might narrow income gaps.\n\nEducation and the Challenges Facing America Today\nThe educational challenges our society faces should be considered in the context of three broad trends: the retirement of the baby-boom generation, the inexorable advance of the technological frontier, and the ongoing globalization of economic activity.\n\nAs the baby boomers, now ranging in age from their late 40s to early 60s, leave the workforce, their places will be taken by the smaller cohort of workers born in the mid-to-late 1960s and early 1970s.  As a result, the U.S. workforce--as a matter of simple arithmetic--will increase more slowly and is likely to become less experienced on average (Jorgenson and others, 2007; Aaronson and Sullivan, 2002).  In a broader sense, the ratio of working people to retirees will decline, meaning that those still working will, in effect, be supporting relatively more non-working people.  This year, there are about five working-age people (20-64) for every person aged 65 and older; by 2030, the ratio will be about 3-to-1 (Bernanke, 2006).  The slower expansion of the labor force, all else equal, implies slower growth of potential output.  More schooling for more of the workforce could help cushion the impact of this demographic transition on economic growth by boosting productivity growth.\n\nContinuing advances in technology also put a premium on education.  Which jobs will be most affected by technology is difficult to predict, although some research suggests that sectors that now use information technology (IT) relatively less intensively, such as health-care and other service sectors, are likely to step up their use of software and IT services (Mann, 2003).  Regardless, better-educated workers are likely better prepared to adapt to new technologies as they develop (Doms, Dunne, and Troske, 1997).\n\nOngoing globalization of economic activity will also lead to continuing changes in the structure of the U.S. economy--including the composition of our output of both goods and services, and thus the structure of our labor force.  The world economy is benefiting from the expansion of trade and the rising productivity of countries abroad that are making great strides expanding both their infrastructure and the educational attainment of their workforces.  That can be good for them, and for us.  Importantly, our ability to reap the benefits of globalization will depend on the flexibility of our labor force to adapt to changes in job opportunities, in part by investing in the education and training necessary to meet the new demands (Bernanke, 2004).\n\nEducational Attainment and Achievement: Where Do We Stand?\nThe United States has a long tradition of recognizing the significant social and economic benefits of providing high-quality education for as many of its citizens as possible.  The United States led the world, first, in expanding access to high-school education and, then, in the post-World War II era, access to college (Goldin, 2001; Goldin and Katz, 1999).  By 1966, about half of the workforce aged 25 and older had completed high school and about 10 percent had completed college.  By 2006, more than 90 percent of adults in the labor force had a high-school education and more than 20 percent held at least a bachelor’s degree.  However, most of the progress over the past forty years occurred in the 1970s and early 1980s.  Since then, for example, the high-school graduation rate for 25-to-29-year-olds has not increased, and the college completion rate has risen only modestly (U.S. Department of Education, 2007b).\n\nOne trend is particularly disappointing:  Both high-school and college completion rates for minorities continue to lag.3   Over the past ten years, the high-school completion rate for whites aged 25 to 29 hovered above 93 percent, while the rate for blacks of the same age stayed near 87 percent; the rate for Hispanics, though trending up over the period, was only 63 percent last year.  The gaps in college completion are wider.  In recent years, more than one-third of whites aged 25 to 29 had at least a bachelor’s degree, compared with less than one-fifth of same-aged blacks and around 10 percent of Hispanics.\n\nAssessing where we stand in terms of educational achievement (how much students learn) is fraught with considerably greater difficulties than assessing attainment (how far students progress in their schooling).  And, the results of various metrics highlight both discouraging and encouraging elements.  The Department of Education’s National Assessment of Educational Progress shows the average reading levels of our high-school seniors have stagnated in recent years; however, our fourth graders continue to improve in reading, and both fourth and eighth graders have improved in math (U.S. Department of Education, National Center for Education Statistics, 2007b).  At the same time, some initial results from the adoption of state accountability standards suggest that they have had a positive effect on students’ test-score gains (Jacob, 2005; Hanushek and Raymond, 2004).  International comparisons of student achievement are even more difficult and present a mixed picture.4   Compared with students around the world, U.S. students still perform relatively well in reading and, in the lower grades, at math and science.  Older U.S. students, however, show less ability to apply math and science skills than their peers in other industrialized countries.\n\nLifelong Learning Can Help Us Meet Economic Challenges\nIn the past, the U.S. education system has responded to the needs of a changing economy, and I believe that as we address such challenges as the retirement of the baby-boom generation, advancing technology, and globalization, our education system will again make an important contribution to the adjustment process.  That means, of course, that we will have to grapple with difficult issues--how to boost educational attainment, particularly for minorities and immigrant youths; how to make more consistent and noticeable progress in raising academic achievement; and how to ensure that older workers have meaningful opportunities to refresh their skills.\n\nWhat can be done so that our educational system will continue to play a significant role in supporting economic change?  In broad terms, we must begin by recognizing that learning is a lifelong process and that we have opportunities to improve education at every point along the way.  Many of these opportunities lie outside the traditional route of a kindergarten-through-twelfth-grade education followed by four years of college.  I’d like to comment briefly on what economists have found about the benefits of educational investments at different points in the life cycle.\n\nEarly Childhood Education\nBuilding the foundation for lifelong learning from the earliest ages is crucial (Heckman, Stixrud, and Urzua, 2006).  Research suggests that the home environment is especially important and that children who start behind find catching up increasingly difficult (Heckman and Masterov, 2007).  Thus, the payoff from high-quality pre-school and home visitation programs is likely very high, especially for children born into poor or otherwise disadvantaged families.  Recent research--some sponsored by the Federal Reserve Bank of Minneapolis in collaboration with the University of Minnesota--has documented high returns from early childhood programs in terms of subsequent educational attainment and in lower rates of social problems, such as crime, teenage pregnancy, and welfare dependency (Burr and Grunewald, 2006).5   But early childhood education is only the beginning.  Positive results from programs such as Head Start dissipate without further high-quality schooling at the elementary and secondary levels (Garces, Thomas, and Currie, 2002).\n\nElementary and Secondary Schooling\nDeciding what and how to teach students from kindergarten through high school and then evaluating our schools’ effectiveness in preparing students for the workforce and a lifetime of learning is a daunting task.  I will make only a few observations on the goals we should keep in mind as we explore ways to improve learning at the elementary and secondary levels.\n\nFirst, we should encourage experimentation and innovation.  By my reading, the research on K-12 education has, to date, yielded no easy answers to the questions of how to raise academic achievement and how to ensure that students finish high school well prepared to move on to more advanced study.  A wide range of approaches has been and is being explored:  smaller class size, school choice, charter school programs, accountability standards, flexibility in teacher certification rules, better teacher pay, merit-based pay, year-round schooling--the list is long and probably will get longer.  The size and diversity of our country, together with the fact that state and local policymakers retain significant discretion over how to structure their educational systems, provides us a natural laboratory for assessing the effectiveness of alternative educational strategies.  I view the debate about what works and what doesn’t to be a crucial part of discovering cost-effective ways to improve our educational system.\n\nThe business community has an obvious interest in how well our schools prepare students for a future in the workforce and should actively participate in the debate.  But we all have a stake.  Students at the elementary and secondary levels are being prepared not just for work but for life.  Such skills and acquired traits as critical and creative thinking, social ability, persistence, and satisfaction in accomplishment make not only good employees but good citizens as well.  Exposure to the arts and culture and experience in serving the community can help support the development of these broader, harder-to-measure skills, alongside more readily measurable cognitive accomplishments in reading, math, and science.\n\nSecond, teacher quality is critical.  Studies show that student performance depends on putting high-quality teachers in the classroom and retaining them as long as possible (Aaronson, Barrow, and Sander, 2007; Rivkin, Hanushek, and Kain, 2005; Rockoff, 2004).  Indeed, many initiatives focus on linking students, especially disadvantaged students, with high-quality schools staffed by high-quality teachers.  High-quality teachers instill in their young students a desire to stay in school and seek more education later in life, and the evidence suggests that the quality of teaching might have the biggest impact on lower-ability students (Murnane and Steele, 2007; Clotfelter and others, 2006; Hanushek, Kain, and Rivkin, 2004).  Unfortunately, our most disadvantaged communities, the ones most troubled by high dropout rates, have difficulty attracting and keeping qualified teachers.\n\nWe must instill a desire among students to stay in school and to seek more education and training over their working lives.  Our elementary and secondary schools must provide students a strong foundation for a life of learning.  Although a wide range of remedial education programs exist, research suggests that they are more costly and less effective than a solid, sustained course of study through high school.  In particular, government training programs for disadvantaged youth have a rather disappointing reputation, particularly those that are less intensive and not well tied to labor market needs (Martin and Grubb, 2001; Heckman, LaLonde, and Smith, 1999).\n\nA number of possibilities for improving the education of disadvantaged students seem worth exploring.  For example, several experiments suggest that smaller schools and smaller classes may help disadvantaged students (although the benefits of such programs for the general student population remain controversial).  Supplemental education, including after-school and mentoring programs such as Big Brothers Big Sisters, have been shown to boost school attendance (Grossman and Tierney, 1998).  Increasing school time--either through longer school hours or summer school--also has found some support (Jacob and Lefgren, 2004).\n\nHigher Education\nIn many ways, higher education represents the strongest part of the U.S. educational system, as demonstrated by the fact that students from all parts of the world come here to study.  Our institutions of higher learning are extraordinarily varied, ranging from large public research universities to small liberal arts colleges to community colleges and vocational schools.\n\nThe main business of our institutions of higher education is, of course, undergraduate teaching.  But unlike some countries, we do not separate research and undergraduate education; our advanced, graduate-level research programs are housed in universities with strong undergraduate programs.  Thus, our colleges and universities are important sources of research and development (National Science Foundation, 2007; Litan, Mitchell, and Reedy, forthcoming).  More than half our basic research--the foundation for breakthroughs that create new industries--is conducted at universities.  Additionally, higher education has embraced the broader mission of translating research into new products and enterprises; our colleges and universities account for 15 percent of applied research and development (National Science Foundation, 2007).  The innovations that begin on campuses are diffused to businesses through patents, start-up companies, and consulting arrangements between faculty and industry.\n\nOne great challenge in higher education lies in making sure our high-school graduates are prepared for it and have access to it.  With college enrollment rates having leveled off in recent years, much debate surrounds how we can move more students into higher education and keep them in school until they graduate.  Researchers have demonstrated a strong relationship between family income and college attendance.  Since 1990, nearly 80 percent of high-school completers from high-income families (the top 20 percent of income) have enrolled in college the next fall.  The proportion of those from low-income families who enroll in college the following fall has been moving up gradually, but it remains much lower--just over 50 percent.6   This discrepancy holds even for students classified as high achievers:  A longitudinal study of eighth graders in 1988 found that only 29 percent of those scoring in the top fourth of the group in math--but who were from families with low social and economic status--had completed a bachelor’s degree or more by 2000, while three-fourths of those from families with high social and economic status finished their undergraduate degrees (Fox, Connelly, and Snyder, 2005).7   Surely, high tuition must be one barrier to attending and completing college (Card, 2001; Kane, 1994), but it is not the only barrier (Dynarski, 2005).  Low-income students, in particular, are more likely to come from school and family environments that do a poor job of preparing them for a successful transition to college (Carneiro and Heckman, 2003).  This suggests that supplemental programs to help under-prepared college students could improve eventual college completion rates; unfortunately, the research on the benefits of such programs is mixed, which reinforces the need to improve educational achievement in regular high school classes (Bettinger and Long, 2005; Angrist, Lang, and Oreopoulos, 2006).\n\nCommunity colleges have made a significant contribution to expanding educational opportunities.  Offering lower costs and more-flexible schedules, they now enroll almost one-half of U.S. undergraduates.  Attendance at one of these institutions is associated with higher wages, even if a degree is not completed.  Evidence suggests that each year of credit at a community college is worth almost as much, in terms of increased earnings potential, as a year at a four-year college.  The average student who entered, but did not complete, community college earns 9 percent to 13 percent more than the average for students who ended their education with high school.  Those who completed a two-year associate degree earn an even larger premium, 15 percent to 27 percent (Kane and Rouse, 1999).  And the earnings of graduates who started at two-year schools and transferred to four-year programs ultimately match those who begin their post-high-school education at four-year institutions (Gill and Leigh, 2003).  Community colleges play a constructive role not only for 18-to-22-year-olds but also for older adults, providing flexible programs for obtaining new skills, specialized training contracted for by individual businesses, remedial education, and adult enrichment.\n\nAdult Education\nToday we are increasingly recognizing that education need not, indeed should not, stop at the age of 22.  Economists have long argued that on-the-job training and learning-by-doing are significant components of the acquisition of human capital.  Research shows that the knowledge and experience gained over time through informal and formal learning on the job appear to pay off for workers and accrues particularly rapidly early in their careers (Altonji and Williams, 2005; Topel, 1991).  An extensive survey of firm-sponsored training a number of years ago found that 84 percent of employees received some kind of formal training while working for their current employer, and 96 percent received some type of informal training (Bureau of Labor Statistics, 1996).  With the advance of technology and the need to attract and retain skilled workers, I am certain that business-sponsored training will remain an important component of the management toolkit.\n\nUpgrading skills through continuing education and training outside the job is also important, particularly in an environment in which workers can face displacement from international competition or technological advance.  Recognizing this possibility, many workers continue to acquire formal education later in life than was once traditional.  For example, almost one-fifth of students at post-secondary institutions of all types are at least 35 years old (National Center for Education Statistics, 2007a).  And, for older workers looking to retool their skills, classroom instruction has been shown to be effective.  For example, classroom training for displaced workers is estimated to boost future wages as much as for students of the usual school age, although the overall return on investment for displaced workers is lower because they have fewer remaining working years than do new entrants to the labor force (Jacobsen, LaLonde, Sullivan, 2005).  Similarly, studies of a number of welfare-to-work programs have reported long-term gains for those who participated in intensive basic education and vocational training (Dyke and others, 2006; Hotz, Imbens, Klerman, 2006).  Such results suggest that well-designed programs to assist workers who lose their jobs can contribute to cushioning the effects of globalization and technological change.\n\nConclusion\nLet me close by reiterating that education--lifelong education for everyone, from toddlers to workers well advanced in their careers--is indeed an excellent investment for individuals and society as a whole.  Education fundamentally supports advances in productivity, upon which our ability to generate continuing improvement in our standard of living depends.  If we are to successfully navigate such challenges as the retirement of the baby-boom generation, advancing technology, and increasing globalization, we must work diligently to maintain the quality of our educational system where it is strong and strive to improve it where it is not.  In particular, we must find ways to move more of our students, especially minorities and students from disadvantaged backgrounds, into educational opportunities after high school.  To do that, we must continually experiment, innovate, and evaluate so that we can make rational decisions about what works and what doesn’t in education.  Because the quality of your workforces is so vital to the success of your businesses, you as business executives must participate fully in this process, along with other stakeholders--students, parents, teachers, and policymakers.  I’m encouraged that you are devoting so much energy and thought to this topic at this three-day conference.\n\nReferences\n\nAaronson, Daniel, Lisa Barrow, and William Sander (2007). “Teachers and Student Achievement in the Chicago Public High Schools,” Journal of Labor Economics, vol. 25 (no. 1), pp. 95-135.\n\nAaronson, Daniel, and Daniel G. Sullivan (2002). “Growth in Worker Quality,” Federal Reserve Bank of Chicago, Chicago Fed Letter, vol. 2002 (February), pp. 1-4, http://www.chicagofed.org/webpages/research/index.cfm.\n\nAltonji, Joseph G., and Nicolas Williams (2005). “Do Wages Rise with Job Seniority? A Reassessment (177 KB PDF) ,” Industrial and Labor Relations Review, vol. 58 (April), pp. 370-97.\n\nAngrist, Joshua D., Daniel Lang, and Philip Oreopoulos (2006). “Lead Them to Water and Pay Them to Drink: An Experiment with Services and Incentives for College Achievement,” NBER Working Paper Series 12790. Cambridge, Mass.: National Bureau of Economic Research, December, www.nber.org/new_archive.\n\nBernanke, Ben S. (2004). “Trade and Jobs,” Distinguished Speaker Series, speech delivered at the Fuqua School of Business, Duke University, Durham, N.C., March 30, www.federalreserve.gov/newsevents.\n\nBernanke, Ben S. (2006). “The Coming Demographic Transition: Will We Treat Future Generations Fairly?” speech delivered at the Washington Economic Club, Washington, October 4, www.federalreserve.gov/newsevents.\n\nBernanke, Ben S. (2007). “The Level and Distribution of Economic Well-Being,” speech delivered at the Greater Omaha Chamber of Commerce, Omaha, Neb., February 6, www.federalreserve.gov/newsevents.\n\nBettinger, Eric P. and Bridget Terry Long (2005). “Addressing the Needs of Under-Prepared Students in Higher Education: Does College Remediation Work?” NBER Working Paper Series 11325, Cambridge, Mass.: National Bureau of Economic Research, May, www.nber.org/new_archive.\n\nBurr, Jean, and Rob Grunewald (2006). “Lessons Learned: A Review of Early Childhood Development Studies (135 KB PDF),” Early Childhood Development, Staff Study, Minneapolis: Federal Reserve Bank of Minneapolis, April, http://minneapolisfed.org/research/studies/earlychild/lessonslearned.pdf/earlychild.\n\nCard, David (2001). “Estimating the Return to Schooling: Progress on Some Persistent Econometric Problems,” Econometrica, vol. 69 (September), pp. 1127-60.\n\nCarneiro, Pedro, and James J. Heckman (2003). “Human Capital Policy,” in James J. Heckman and Alan B. Krueger, eds., Inequality in America: What Role for Human Capital Policies? Cambridge, Mass.: MIT Press, pp. 77-239.\n\nClotfelter, Charles, Elizabeth Glennie, Helen Ladd, and Jacob Vigdor (2006). “Would Higher Salaries Keep Teachers in High-Poverty Schools? Evidence from a Policy Intervention in North Carolina,” NBER Working Paper Series 12285. Cambridge, Mass.: National Bureau of Economic Research, June, www.nber.org/new_archive.\n\nCorrado, Carol A., Charles R. Hulten, and Daniel E. Sichel (2006). “Intangible Capital and Economic Growth,” NBER Working Paper Series 11948. Cambridge, Mass.: National Bureau of Economic Research, January, www.nber.org/new_archive.\n\nDee, Thomas S. (2004). “Are There Civic Returns to Education?” Journal of Public Economics, vol. 88 (August), pp. 1697-720.\n\nDoms, Mark, Timothy Dunne, and Kenneth R. Troske (1997). “Workers, Wages, and Technology,” Quarterly Journal of Economics, vol. 112 (February), pp. 253-90.\n\nDyke, Andrew, Carolyn J. Heinrich, Peter R. Mueser, Kenneth R. Troske, and Kyung-Seong Jeon (2006). “The Effects of Welfare-to-Work Program Activities on Labor Market Outcomes,” Journal of Labor Economics, vol. 24 (July), pp. 567-607.\n\nDynarski, Susan (2005). \"Building the Stock of College-Educated Labor,” NBER Working Paper Series 11604. Cambridge, Mass.: National Bureau of Economic Research, September, www.nber.org/new_archive.\n\nFox, Mary Anne, Brooke A. Connolly, and Thomas D. Snyder (2005). Youth Indicators 2005: Trends in the Well-Being of American Youth (592 KB PDF), NCES 2005-050. Washington: Department of Education, Institute for Education Science, National Center for Education Statistics, July, http://nces.ed.gov/pubs2005/2005050.pdf.\n\nGarces, Eliana, Duncan Thomas, and Janet Currie (2002). “Longer-Term Effects of Head Start,” The American Economic Review, vol. 92 (September), pp. 999-1012.\n\nGill, Andrew M., and Duane E. Leigh (2003). “Do the Returns to Community Colleges Differ Between Academic and Vocational Programs?” Journal of Human Resources, vol. 38 (Winter), pp. 134-55.\n\nGlaeser, Edward L., Giacomo Ponzetto, and Andrei Shleifer (2006). “Why Does Democracy Need Education?” NBER Working Paper Series 12128. Cambridge, Mass.: National Bureau of Economic Research, March, www.nber.org/new_archive.\n\nGrossman, Jean Baldwin, and Joseph P. Tierney (1998). “Does Mentoring Work? An Impact Study of the Big Brothers Big Sisters Program,” Evaluation Review, vol. 22 (June), pp. 403-26.\n\nGoldin, Claudia (2001). “The Human-Capital Century and American Leadership: Virtues of the Past (220 KB PDF),” Journal of Economic History, vol. 61 (June), pp. 263-92.\n\nGoldin, Claudia, and Lawrence F. Katz (1999). “The Shaping of Higher Education: The Formative Years in the United States, 1890-1940,” Journal of Economic Perspectives, vol. 13 (Winter), pp. 37-62.\n\nHanushek Eric A., John F. Kain, and Steven G. Rivkin (2004). “Why Public Schools Lose Teachers,” Journal of Human Resources, vol. 39 (Spring), pp. 326-54.\n\nHanushek, Eric A., and Margaret E. Raymond (2004). “Does School Accountability Lead to Improved Student Performance?” NBER Working Paper Series 10591; Cambridge, Mass.: National Bureau of Economic Research, June, www.nber.org/new_archive.\n\nHeckman, James J., and Alan B. Krueger (2003). Inequality in America: What Role for Human Capital Policies? Cambridge, Mass.: MIT Press.\n\nHeckman, James, J., Robert J. LaLonde, and Jeffrey A. Smith (1999). “The Economics and Econometrics of Active Labor Market Programs (677 KB PDF),” in Orley C. Ashenfelter and David A. Card, eds., Handbook of Labor Economics, vol. 3A. Amsterdam: Elsevier Science and North-Holland, pp. 1865-2097.\n\nHeckman, James J., and Dimitriy V. Masterov (2007). “The Productivity Argument for Investing in Young Children,” NBER Working Paper Series 13016. Cambridge, Mass.: National Bureau of Economic Research, April, www.nber.org/new_archive.\n\nHeckman, James J., Jora Stixrud, and Sergio Urzua (2006). “The Effects of Cognitive and Noncognitive abilities on Labor Market Outcomes and Social Behavior,” Journal of Labor Economics, vol. 24 (July), pp. 411-82.\n\nHotz, V. Joseph, Guido W. Imbens, and Jacob Alex Klerman (2006). “Evaluating the Differential Effects of Alternative Welfare-to-Work Training Components: A Re-Analysis of the California GAIN Program,” NBER Working Paper Series 11939. Cambridge, Mass.: National Bureau of Economic Research, January, www.nber.org/new_archive.\n\nJacob, Brian A. (2005). “Accountability, Incentives, and Behavior: The Impact of High-Stakes Testing in the Chicago Public Schools,” Journal of Public Economics, vol. 89 (June), pp. 761-96.\n\nJacob, Brian A., and Lars Lefgren (2004). “Remedial Education and Student Achievement,” Review of Economics and Statistics, vol. 86 (February), pp. 226-44.\n\nJacobson, Louis, Robert J. LaLonde, and Daniel Sullivan (2005). “The Impact of Community College Retraining on Older Displaced Workers: Should We Teach Old Dogs New Tricks?” Industrial and Labor Relations Review, vol. 58 (April), pp. 398-415.\n\nJorgenson, Dale W., Richard J. Goettle, Mun S. Ho, Daniel T. Slesnick, and Peter J. Wilcoxen (2007). “U.S. Labor Supply and Demand in the Long Run,” prepared for “Labor Supply in the New Century (895 KB PDF),” the 52nd Economic Conference of the Federal Reserve Bank of Boston, Boston, Mass., June 18-20, www.bos.frb.org/economic/conf/conf52.\n\nKane, Thomas J. (1994). ”College Entry by Blacks Since 1970: The Role of College Costs, Family Background, and the Returns to Education,” Journal of Political Economy, vol. 102 (October), pp. 878-911.\n\nKane, Thomas J., and Cecilia Elena Rouse (1999). “The Community College: Educating Students at the Margin Between College and Work,” Journal of Economic Perspectives, vol. 13 (Winter), pp. 63-84.\n\nLitan, Robert E., Lesa Mitchell, and E.J. Reedy (forthcoming). “Commercializing University Innovations: A Better Way,” in Innovation Policy and the Economy, vol. 8. Cambridge, Mass.: MIT Press.\n\nMartin, John P., and David Grubb (2001). “What Works and For Whom? A Review of OECD Countries’ Experiences with Active Labour Market Policies,” Swedish Economic Policy Review, vol. 8 (Fall), pp. 9-56.\n\nMann, Catherine L. (2003). “Globalization of IT Services and White Collar Jobs: The Next Wave of Productivity Growth,” International Economics Policy Briefs PB03-11. Washington: Peterson Institute for International Economics, December, www.iie.com/publications/pubs.cfm.\n\nMurnane, Richard J., and Jennifer L. Steele (2007). “What Is The Problem? The Challenge of Providing Effective Teachers for All Children,” Future of Children, vol. 17 (Spring), pp. 15-43, www.futureofchildren.org.\n\nNational Science Foundation (2007). “U.S. R&D Increased 6.0% in 2006 According to the NSF Projections (55 KB PDF),” NSF Report 07-317. Arlington, Va.: Division of Science Resource Statistics, National Science Foundation, April, www.nsf.gov/statistics/infbrief/nsf07317.\n\nRivkin, Steven G., Eric A. Hanushek, and John F. Kain (2005). “Teachers, Schools, and Academic Achievement,” Econometrica, vol. 73 (March), pp. 417-58.\n\nRockoff, Jonah E. (2004). “The Impact of Individual Teachers on Student Achievement: Evidence from Panel Data,” American Economic Review, vol. 94 (May, Papers and Proceedings), pp. 247-52.\n\nRolnick, Art, and Rob Grunewald (2003). “Early Childhood Development: Economic Development with a High Public Return,” Federal Reserve Bank of Minneapolis, Fedgazette, vol. 2003 (March), http://minneapolisfed.org/pubs/fedgaz/03-03/earlychild.cfm.\n\nTopel, Robert (1991). “Specific Capital, Mobility, and Wages: Wages Rise with Job Seniority,” Journal of Political Economy, vol. 99 (February), pp. 145-76.\n\nU.S. Census Bureau and U.S. Department of Labor, Bureau of Labor Statistics (2007). Current Population Survey. Washington: Census Bureau, www.census.gov/cps.\n\nU.S. Department of Education, Institute of Education Sciences, National Center for Education Statistics (2007a). Digest of Education Statistics: 2006. Washington: Department of Education, http://nces.ed.gov/programs/digest/d06.\n\nU.S. Department of Education, Institute of Education Sciences, National Center for Education Statistics (2007b). The Condition of Education 2000-2007. Washington: Department of Education, http://nces.ed.gov/programs/coe.\n\nU.S. Department of Labor, Bureau of Labor Statistics (1996). “BLS Reports on the Amount of Formal and Informal Training Received by Employees,” press release, December 19, www.bls.gov/news.release/sept.nws.htm.\n\nU.S. Department of Labor, Bureau of Labor Statistics (2007). “Preliminary Multifactor Productivity Trends,” press release, May 24, www.bls.gov/news.release/prod3.nr0.htm.\n\n\n\n\n\n\n\nFootnotes\n\n1.  The data are weekly earnings of full-time wage and salary workers aged twenty-five and older and are derived from the Current Population Survey, published by the U.S. Bureau of Labor Statistics. Return to text\n\n2. The data are derived from the Current Population Survey, published by the U.S. Bureau of Labor Statistics. Return to text\n\n3. The data are derived from the Current Population Survey, March and Annual Social and Economic Supplement, 1971-2006, published by the U.S. Department of Commerce. Return to text\n\n4.  The results of two prominent international assessments--The Trends in International Mathematics and Science Study, conducted under the aegis of the International Association for the Evaluation of Educational Achievement, and the Organization for Economic Cooperation and Development’s Program for International Student Assessment--are summarized in the 2006 Digest of Education Statistics published by the U.S. Department of Education (2007a).  http://nces.ed.gov/programs/digest/d06/ch_6.asp Return to text\n\n5.  More information on the Early Childhood Research Collaborative and copies of its research papers can be obtained from the website of the Federal Reserve Bank of Minneapolis, www.earlychildhoodrc.org. Return to text\n\n6.  These data are derived from the annual October Supplement to the Current Population Survey.  They are summarized in the U.S. Department of Education’s Condition of Education 2007 available at http://nces.ed.gov/programs/coe/2007/section3/indicator 25.asp#info. Return to text\n\n7.  Socioeconomic status was measured by a composite score on parental education and occupations and family income.  The study also found that the proportion of low-scoring math students from high socioeconomic families who completed at least a bachelor’s degree was 30 percent versus only 3 percent for those from lower socioeconomic families.  For those in the middle quintiles of both scores and family characteristics, the proportion was 21 percent. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070924a.htm",
        "title": "Education and Economic Competitiveness",
        "date": "9/24/2007"
    },
    {
        "content": "September 21, 2007\n\nVice Chairman Donald L. Kohn\n\nAt Monetary Policy over Fifty Years, a conference to mark the fiftieth anniversary of the Deutsche Bundesbank, Frankfurt, Germany\n\nI appreciate this opportunity to speak on the occasion of the Deutsche Bundesbank's fiftieth anniversary by participating on this panel on \"Success and Failure of Monetary Policy since the 1950s.\"  I was reassured in my acceptance of Axel's invitation by David Laidler's survey paper, which found more successes than not over the past two decades.1\n\nA concise summary of this success is evident in the performance of consumer price inflation in the advanced economies.  Median inflation in that group (as defined by the International Monetary Fund) has held near 2 percent for this decade.  Indeed, in the IMF's latest World Economic Outlook, four out of five countries in this group are expected to post inflation rates between 1 percent and 3 percent this year.  That good performance has helped to anchor inflation expectations, which, in turn, generates many benefits.  Anchored inflation expectations damp the pass-through of supply-related price shocks.  They also permit central banks to respond more forcefully to output fluctuations.  Most significantly, the improved inflation performance has come with, not at the expense of, output stability.  Although a consensus has not formed on how much of the \"Great Moderation\" in the growth of real output can be attributed to monetary policy, everyone agrees that at least a portion of it can.\n\nDavid views these macroeconomic outcomes as a triumph of monetarism, but not because the formulaic policy prescription associated with that doctrine succeeded (or, for that matter, was even tried on a sustained basis).  Rather, the underlying tenets of monetarism ultimately seeped into the collective central banking unconscious and fostered better decisionmaking.  The beliefs that David identifies are threefold:  That market economies are inherently self-righting, that open economies perform best under flexible exchange rates, and that central bankers should focus on price stability as their long-run objective.\n\nDavid suggests that monetarism failed when its proponents got too prescriptive by advocating rigid rules for money growth.  Among the lessons he takes from the failed monetarist experiment are that central banking is an applied science and that our imperfect understanding of how economies and markets function implies that a good dose of humility is required--and I agree.  As evidence of that humility on my part, let me also agree with David that two important questions about the conduct of monetary policy have not yet been resolved.  This is unfortunate because these two questions are both longstanding sources of debate and central to current policy concerns.  First, what is the best way to pursue price stability, and, second, how should asset prices be taken into account in steering policy?\n\nIn many countries, though not my own, the answer to the first question has been that price stability should be pursued through the formal apparatus of an inflation target, which typically includes establishing an inflation goal by the government, setting metrics to evaluate central bank performance, and periodically communicating progress to the public.  Although correlation does not convey causation, the spread of such regimes has coincided with sustained low global inflation.  In addition, no adopter of an inflation target has subsequently abandoned it.\n\nBefore anyone jumps to the conclusion that Frankfurt is a stop on my road to Damascus, let this Saul state that for me the case remains open.  Inflation has come down worldwide, in countries without, as well as with, inflation targets.  Moreover, I share David's puzzlement about why an explicit inflation goal should make a substantial difference in performance given the paucity of evidence showing that choosing a target directly affects the level of the public's inflation expectations.  That said, I am relatively more persuaded that inflation targeting helps reduce the variance of inflation expectations.  Evidence has accumulated to suggest that stock prices, interest rates, and measures of inflation expectations seem to vary less in economies in which the central bank has an explicit long-run goal for inflation.\n\nI suspect that this better anchoring of expectations and the success of inflation targeting in many countries is attributable in part to aspects of the political economy that David identified.  A formal inflation target represents a national embrace of a goal, in which elected authorities recognize the primacy of price stability and publicly support--indeed, even require--the central bank's pursuit of that goal.  To the extent that elected authorities channel the desires of the electorate, a central bank directed to adopt an inflation target is being given a strong signal as to the goal's importance to the public at large.  This affirmation has often been reinforced by the granting of operational independence to the central bank to achieve that goal most effectively.  An important effect of such public acceptance of price stability is that it erodes the standing of those who would direct central bank action toward other ends.  In such an environment, workers, businesspeople, and investors can make plans with the expectation that nominal magnitudes will be predictable and so devote their attention to more productive matters.\n\nFor the European Central Bank, this framework was established by treaty.  In most other instances, the adoption of an inflation target involved laws and mutual understandings, not constitutional changes.  The early adopters of inflation targets were parliamentary democracies, which is not too surprising given that in such a system a single branch of government can enact laws and put them into effect.  With regard to an inflation goal, the parliament can erect the formal apparatus and the finance minister can serve as the government's point of contact with the central bank.\n\nThe system in the United States is different in that two independent branches of government are responsible for economic policy making, making agreement on a single goal problematic.  Moreover, those two branches have already spoken as to the appropriate aim of the nation's central bank:  The Congress, in a law the President signed, has given the Federal Reserve a dual mandate that directs us to foster maximum employment and stable prices over time.  This instruction is not an accident of history, in that, in the past, the Congress has shown no appetite to amend its legislation.  Nor is this instruction unreasonable, in that the dual mandate has come to be interpreted as assigning us the responsibility for attaining price stability in the long run, which will bring with it maximum employment, and of being mindful of resource utilization in the succession of short runs that make up the long run.  The dual mandate seems proper and fitting, given that economic costs are incurred both by having inflation stray from its long-run goal and by having output deviate from the economy's potential to produce; and it seems to produce results not too different in practice from those associated with central banks that are flexible inflation targeters.\n\nAs I said earlier, anchoring expectations has value, in that it makes planning easier, reduces resources spent on predicting and protecting against unexpected variations in nominal magnitudes, and grants a central bank greater scope to lean against fluctuations in output while keeping inflation contained.  The latter is particularly attractive given our dual mandate, in that better-anchored inflation expectations could produce the win-win outcome of improving the attainment of both goals.  For that reason, in its consideration of its communications strategies, the Federal Open Market Committee has been discussing whether mechanisms could be put in place that could better anchor inflation expectations in a manner consistent with the institutional framework of our dual mandate.\n\nThe second of David's open issues--whether central banks should lean against possible asset price bubbles--was the key topic in my discussion here eighteen months ago, at Otmar's festschrift.  My answer then is my answer now.  A central bank should focus on the outlook for the macroeconomy and generally relegate asset prices to the subordinate role of inputs to the forecast process.  I view this as the simple application of humility that David and I find so admirable.  Although economic theory provides no settled answers to any topic, its predictions are especially imprecise with regard to asset pricing, which has two implications for central bankers.  First, little confidence can be attached to the determination that an asset bubble exists except in the most extreme of circumstances.  Second, even less confidence can be attached to predictions of the effects of policy on asset prices, and in particular on any speculative element in those prices.  Moreover, monetary policy actions addressed at a perceived bubble in one sector may have undesirable effects on other asset prices and the economy more generally.\n\nAs a result, my preferred policy framework remains three pronged:  First, assign the single instrument of monetary policy to its macroeconomic objective; second, rely on regulation to erect a resilient financial structure; and, third, in the event that market judgments prove to be wrong and financial prices adjust sharply, apply the tool of monetary policy to the macroeconomic task at hand.  That task is not always easily captured by simple statistical regularities.  Relationships between financial markets and economic results are complex and nonlinear, especially when markets are not behaving normally.  When investors are ebullient, their expectations of outsized capital gains can feed on themselves and back on the economy.  On the way down, investors' loss of confidence, a reduction in credit availability, and a tightening of terms and conditions for credit have the potential to have pronounced effects on activity and inflation.\n\nThe world is, no doubt, different than when we gathered here eighteen months ago.  However, it is far too soon to pass judgment on what went wrong in the U.S. housing market and why.  I suspect that, when studies are done with cooler reflection, the causes of the swing in house prices will be seen as less a consequence of monetary policy and more a result of the emotions of excessive optimism followed by fear experienced every so often in the marketplace through the ages.  To some extent, too, the amplitude of the housing cycle was heightened by the newness of the subprime market, the fragmentation of regulatory oversight responsibility for that market, and the complexity and opacity of the newer instruments for transforming and distributing risk.  Low policy interest rates early in this decade helped feed the initial rise in house prices.  However, the worst excesses in the market probably occurred when short-term rates were already well on their way to more normal levels, but longer-term rates were held down by a variety of forces.  And similar, sometimes even sharper, trajectories of house prices have been witnessed in some economies in which the central banks said they were paying more attention to asset prices.\n\nThe action the Federal Open Market Committee took this Tuesday can be interpreted as the application of the third leg of my preferred policy triad, in that it was taken \"to forestall some of the adverse effects on the broader economy that might otherwise arise from the disruptions in financial markets and promote moderate growth over time.\"  In the past, such efforts to cushion the restraint induced by declines in asset prices have fueled the assertion that Federal Reserve policy is asymmetrical in its response to booms and busts in asset prices.  Such an asymmetry is said to have the potential to feed \"moral hazard\" in that investors would spend less effort evaluating underlying values as they were lulled by the protection they expected to be provided by monetary policy action.\n\nIn point of fact, Federal Reserve policy makers have not been asymmetrical in intent or in actions, in that we have always focused sharply on the macroeconomy.  Asset prices have mattered in the determination of policy because they have mattered for our outlook.  I am confident that the federal funds rate would not have been as high in 2000 if it had not been for the level of equity prices that year, nor would the federal funds rate have been as elevated in 2006 in the absence of the tight credit spreads, low term premiums, and the impetus from housing wealth.  And I doubt policy would have been eased this week if housing prices had continued their upward march.   In each instance, however, policy was motivated not by the desire to achieve any particular level of asset prices, but rather by the Federal Reserve's assessment of how changes in asset prices were affecting the forecast of growth and inflation.\n\nI would also caution that a symmetrical response to the macroeconomic outlook will need to reflect the inherent asymmetries in business cycles.  In the typical boom-bust cycle, asset prices tend to rise relatively gradually over a protracted period but fall sharply in a shorter stretch of time, which financial economists refer to as \"rising by the escalator and falling by the elevator.\"  Perhaps because those asset prices are important to spending, key macroeconomic indicators, such as the unemployment rate, exhibit a similar pattern.  It is not surprising then that a macro-focused monetary policy will leave an asymmetric footprint in the data.\n\nIn the end, my humble advice is to evaluate policymakers relative to the tasks the law has given them.  In my judgment, the record over the past twenty-five years of steady growth with two mild recessions and gradually declining inflation to a reasonably low level does not betray an asymmetry in our policy responses in the metric that counts--macroeconomic performance.\n\nFootnotes\n\n1.  Vincent Reinhart, of the Board's staff, contributed to the preparation of these remarks.  The views expressed are my own and not necessarily shared by my colleagues on the Board or the Federal Open Market Committee.  Return to text",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20070921a.htm",
        "title": "Success and Failure of Monetary Policy since the 1950s",
        "date": "9/21/2007"
    },
    {
        "content": "September 21, 2007\n\nGovernor Kevin Warsh\n\nTo the State University of New York at Albany’s School of Business, Albany, New York\n\nIt is good to be back in upstate New York.  Thank you to the School of Business of the University of Albany for inviting me.  I am particularly heartened to see old friends, new students, and prominent business leaders in attendance.  I played in the New York State Public High School Tennis Championships on the other side of this campus, so this is certainly familiar terrain.  However, I hope to perform better in my remarks today than I did with my racquet twenty years ago.\n\nI have been honored to serve in Washington, D.C. for the past 5-1/2 years.  My knowledge of the economy has deepened by working with my colleagues to put fiscal and monetary policy into practice.  Nonetheless, with each passing day, it is more obvious that I learned much of what I need to know about the real economy in my first eighteen years here in upstate New York.\n\nThe current period of heightened financial market volatility has drawn more attention than usual to the policy actions of the Federal Reserve.  Returning home at this time serves as a useful reminder that the decisions we make in Washington matter on the front lines of the real economy.  Our monetary tools, for example, affect the ability of aspiring homeowners to take out a first mortgage.  They also matter to retirees on fixed budgets, who are vulnerable to escalating prices, and to graduates of this and other universities looking for jobs.  Our supervision and regulatory policies, to cite another example, matter to businesses, large and small, looking to borrow from banks to expand their operations.\n\nOn the Federal Open Market Committee (FOMC), my colleagues and I seek to deploy our monetary policy tools to help keep the U.S. economy on an even keel.1  We try to provide the right mix of policy prescriptions, patience, and perspective to counter possible adverse developments.  In evaluating the state of the U.S. economy, its prospects, and the current stance of policy, we typically turn to such arcana from the economics textbooks as the term structure of interest rates, the shape of the Phillips curve, trends on the natural rate of unemployment, changing risk premiums, the exchange value of the dollar, and marginal propensities to consume as asset values change, to name just a few.\n\nWhile these indicators and relationships are important, many of the enduring teachings needed to evaluate the financial markets and U.S. economy (particularly in times of financial tumult) are well known without the technical jargon all across the country:  Trees don't grow to the sky.  There is no free lunch.  You shouldn't put all your eggs in one basket.  There is no substitute for doing your own homework.  And my favorite, that which can't go on forever usually doesn't.2\n\nMy intent today is not to suggest that the Federal Reserve is somehow omniscient regarding the path of the U.S. economy.  Nor is it my intent to suggest that the Federal Reserve's knowledge and tools are sufficiently surgical to steer the U.S. economy completely unscathed through the choppy waters of financial market turbulence.  And of course, I do not literally mean to suggest that common aphorisms provide an infallible compass to guide the economy past shocks of one sort or another.   Instead, my goal is to describe the Federal Reserve's monetary responsibilities, highlight the critical role of liquidity in financial markets, and discuss the recent financial market turmoil.  While the subprime-mortgage markets showed some of the earliest and most pronounced indications of weakness, I believe that problems afflicting the subprime-mortgage markets served more as the trigger than the fundamental cause of recent market turmoil and economic uncertainty.\n\nMonetary Responsibilities and Financial Markets\nBy way of background, allow me to highlight the Federal Reserve's dual mandate for monetary policy, as embodied in the Federal Reserve Act.  The Federal Reserve's statutory objectives are to institute policies that foster maximum employment and price stability.  To ensure that these objectives are consistent with each other and with strong, enduring economic performance over time, my colleagues, past and present, interpret \"maximum employment\" to mean maximum sustainable employment.  In pursuing this objective, the Federal Reserve is trying to foster an environment in which those who are looking for work can reasonably find it.  Similarly, we generally interpret our congressional mandate to ensure \"price stability\" to mean that inflation (the rate of price change for a broad range of products and services) is at sufficiently low and predictable levels so that it is not a factor in the economic planning of households and businesses.\n\nThe principal instruments of monetary policy conducted by the Federal Reserve--open market operations, the discount rate, and reserve requirements--do not operate in a vacuum.3  Rather, they operate dynamically in association with ever-changing financial market conditions to produce effects on the real economy.  Indeed, well-functioning financial markets are a precondition for a sustainable, prosperous economy.\n\nFinancial markets facilitate the flow of capital from individuals and institutions that have savings to individuals and institutions with investment opportunities that are deemed worthwhile.  When functioning properly, financial markets may also lower financing costs by allocating risks to suppliers of capital most willing and able to bear them.  Investments that build human and physical capital, in turn, generate economic growth and ultimately raise living standards.  In addition, financial markets should serve as a shock absorber of sorts for both individuals and businesses.  The capital cushions of financial intermediaries, for example, should help mitigate the impact of financial shocks on the overall economy.\n\nConversely, when financial markets function poorly, the capital allocation process I just described is impaired, and worthwhile investment projects may go unfunded.  In the extreme, savers refuse to part with their funds for capital investments at virtually any price.  Instead, they retreat to the shore for safety, waiting for calmer seas and cooler heads to prevail.  As I have noted previously, \"While policymakers and market participants know with certainty that these episodes will occur, [we] must be humble in [our] ability to predict the timing, scope, and duration of these periods of financial distress.\"4\n\nTo avoid these outcomes altogether, some believe that the Federal Reserve should treat financial stability itself as a goal.  Often that is seen to imply a preference by policymakers for the perpetuation of existing financial institutions and products.  That is not a view I share.  The level of economic activity would invariably be lower if financial stability alone were our guiding light; protecting incumbents at the expense of innovators would prove detrimental to the long-term vibrancy of the economy.  We should be extremely wary of protecting financial institutions and their various stakeholders from incurring losses.  Such actions distort asset prices and critically impair the efficiency of capital allocation.  The desire for well-functioning markets does not require us to insulate asset prices or individual financial institutions from the buffeting of the marketplace.\n\nLiquidity and Well-Functioning Financial Markets\nNow, let me briefly highlight a key attribute of well-functioning financial markets:  they function best when they attract sufficient liquidity.  In previous remarks, I advanced the notion that liquidity can be thought of as roughly comparable to investor confidence.5  Liquidity exists when investors are confident and willing to assume risks.   And liquidity persists when risks are quantifiable and investors are creditworthy.\n\nTo trace the origins of recent financial markets turmoil, let's recall a time when the environment was more benign and financial markets were flush with liquidity.  This does not require a long memory, as it aptly characterized our capital markets just four months ago.  In early June, I remarked that:\n\nThese financial market conditions were, in part, I argued, the consequence of a long period of remarkably supportive macroeconomic conditions, the acceleration in financial innovation, particularly the growth of structured finance products, and the continued export of the culture of capitalism to emerging-market countries.  Taken together, confidence fostered the continued propagation of new securities, new products, and new markets.  Not surprisingly, liquidity was ample.\n\nDid Success Sow the Seeds of Distress?\nSo, what could go wrong?  In times of abundant liquidity, investors that were no longer comfortable with their financial positions could readily sell their holdings.  Similarly, financial institutions could distribute the securities they originated with few constraints.  Like others who had grown increasingly watchful about the ebullience in the financial markets, I wondered whether the risks were being given their due:\n\nConfidence can be fleeting.  Confidence can beget complacency.  If, in liquid times, investors in structured products become complacent, they may not understand fully the value of the underlying assets.   High levels of confidence, perhaps even complacency, were also observable in the behavior of many financial intermediaries.  Many hedge funds, growing in size and scope, invested in less-liquid assets in search of higher expected returns.  Many commercial banks increased sponsorship of structured investment vehicles to invest in long-term securities, often financing them off-balance-sheet with short-term commercial paper.  Those financial intermediaries that recognized the risks of extrapolating high levels of liquidity indefinitely were threatened with eroding market share and less-impressive profit profiles.  They may have hoped that robust trading markets would allow them to exit positions ahead of a crowded trade.  But, to paraphrase an old Wall Street saw, they don't ring a bell when the markets are at the top or at the bottom.\n\nAs you know, liquidity conditions started to deteriorate by mid-July.  Subprime-mortgage markets suffered significantly from a rapid withdrawal of liquidity.  They were a particularly tempting target:  Many subprime mortgage products were newer, performance histories were shorter, prices were rising faster, securitization structures were more complex, disclosure was more opaque, and credit standards were weaker than most other asset classes.\n\nBut, were subprime credit problems the source of contagion causing broader reductions in liquidity and market functioning, as has become a common refrain?  Or did reductions in liquidity--and concomitant changes in investor sentiment--simply manifest themselves first in the subprime-mortgage markets?  If the latter is the case, then the true causes of recent financial tumult may well have preceded the turmoil in the subprime-mortgage markets altogether.  And policy prescriptions should be judged accordingly.\n\nSubprime Lending:  The Spark, Not the Cause\nThroughout the summer, delinquency rates for subprime adjustable-rate mortgages jumped as house prices decelerated and effective interest rates rose.  The rate of serious delinquencies for subprime mortgages with adjustable interest rates reached close to 15 percent in July.  Investors incurred large losses from forced sales of securities backed by subprime mortgages.  Credit-rating agencies downgraded numerous securities backed by subprime and alt-A mortgages.\n\nThe resulting investor skepticism about the accuracy of ratings, combined with mounting losses at mortgage lenders, caused investors to pull back from a broad range of structured products, even though unrelated to mortgages.  Financing for leveraged buyouts halted; demand for securities backed by syndicated loans evaporated.  Investors began to shun non-mortgage related asset-backed commercial paper.\n\nThese subsequent financial problems may not be a reflection of subprime contagion after all.  Instead, it may be that investors fundamentally lost confidence in their ability to value a broad range of assets, particularly those that rely on robust securitization and secondary markets.  Moreover, uncertainty about the ability of large financial institutions to fund their commitments eroded confidence in counterparties more generally.  Risk premiums and term premiums rose rapidly, and investors sought refuge.   The principles, products, and practices that served so many so well for so long seemed somehow ill-suited to the evolving financial architecture.\n\nMarkets that rely less on securitizations, and are more transparent, have fared better in recent days and weeks.  The stock market, while quite volatile, is about unchanged from mid-June levels.  In the corporate bond market, spreads have widened somewhat, but current levels are close to historic averages, and issuance of investment-grade bonds has been quite sizable.  Issuance of speculative-grade bonds has been sharply reduced, but aside from difficulties of LBO-related deals, this may reflect issuers' willingness to await improved conditions.\n\nNot unlike prior episodes, it seems increasingly apparent that many investors and financial intermediaries became so content with the benign economic conditions and robust financial markets that they tended to act with confidence greater than warranted by the fundamentals.  Indeed, some may have overly relied on credit ratings as sole gatekeepers for evaluating risks.  So perhaps, in some sense, markets can become too liquid.  In this case, markets may appear to function smoothly, but the risk-based pricing that lies at the heart of how financial markets efficiently allocate capital is impaired.  The gloss of confidence may cause a misallocation of resources, and investors and financial intermediaries can be sidelined for a time, undermining the normal functioning of market operations.\n\nConclusion\nSome months ago, I asked, \"What happens when liquidity falters?\"8  Highlighting the risks of a liquidity shock at some indeterminate point in the future is easier than ascertaining the consequences with precision.  Reduced liquidity conditions in markets today stem from a pullback in investors' willingness to take risks, which may have been triggered, but I argue not caused, by losses in subprime-mortgage markets.  Thus, a broader reassessment of risk positions appears at work, especially for products that are opaque or complex.  Investors who had relied on credit ratings alone are now confronted with having to perform their own credit and market valuations.  Some may now find they are not well-equipped to make these evaluations.  How quickly markets normalize may depend on the speed with which investors and counterparties gain comfort in their abilities to value assets.\n\nThe adjustment process by private investors has increased the risk that banks may increasingly be called upon as backup providers of funding.  The Federal Reserve responded to these developments by providing reserves to the banking system; it announced a cut in the discount rate of 50 basis points and adjustments to the Reserve Banks' usual discount window practices to facilitate the provision of term financing.  In addition, earlier this week, the FOMC lowered its target for the federal funds rate by 50 basis points.  The action was intended to help forestall some of the adverse effects on the broader economy that might arise from the disruptions in financial markets and to promote moderate growth over time.  Recent developments in financial markets, including impaired price discovery, have increased the uncertainty surrounding the economic outlook.  What originated as a liquidity shock could potentially give rise to increases in credit risk.  The Committee will continue to assess the effects of these and other developments on economic prospects and will act as needed to meet our dual mandate, fostering price stability and economic growth.\n\nFootnotes\n\n1. The views expressed herein are my own and do not necessarily reflect the views of other members of the Board of Governors or of the Federal Open Market Committee.  I am grateful for the assistance of Nellie Liang and Daniel Covitz of Board staff, who contributed to these remarks. Return to text\n\n2. That is a slight paraphrasing of Herbert Stein's self-styled \"Stein's Law\" (Herbert Stein, 1998, What I Think: Essays on Economics, Politics, and Life, Washington: AEI Press, p. 32). Return to text\n\n3. A fuller description of the tools available to Federal Reserve policymakers is in Laurence H. Meyer (1998), \"Come with Me to the FOMC,\" Gillis Lecture, speech delivered at Williamette University, Salem, Ore., April 2, www.federalreserve.gov/newsevents. Return to text\n\n4. Kevin Warsh (2007), \"Market Liquidity: Definitions and Implications,\" speech delivered at the Institute of International Bankers Annual Washington Conference, Washington, March 5, www.federalreserve.gov/newsevents.  Return to text\n\n5. Kevin Warsh (2007), \"Financial Intermediation and Complete Markets,\" speech delivered at the European Economics and Financial Centre, London, June 5, www.federalreserve.gov/newsevents. Return to text\n\n6. Warsh, \"Financial Intermediation and Complete Markets.\" Return to text\n\n7. Warsh, \"Financial Intermediation and Complete Markets.\" Return to text\n\n8. Warsh, \"Financial Intermediation and Complete Markets.\" Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/warsh20070921a.htm",
        "title": "Financial Market Developments",
        "date": "9/21/2007"
    },
    {
        "content": "September 11, 2007\n\nChairman Ben S. Bernanke\n\nAt the Bundesbank Lecture, Berlin, Germany\n\nIn a speech given in March 2005 (Bernanke, 2005), I discussed a number of important and interrelated developments in the global economy, including the substantial expansion of the current account deficit in the United States, the equally impressive rise in the current account surpluses of many emerging-market economies, and a worldwide decline in long-term real interest rates.  I argued that these developments could be explained, in part, by the emergence of a global saving glut, driven by the transformation of many emerging-market economies--notably, rapidly growing East Asian economies and oil-producing countries--from net borrowers to large net lenders on international capital markets.  Today I will review those developments and provide an update.  I will also consider policy implications and prospects for the future.\n\nA principal theme of my earlier remarks was that a satisfying explanation of the developments in the U.S. current account cannot focus on developments within the United States alone.  Rather, understanding these developments and evaluating potential policy responses require a global perspective.  I will continue to take that perspective in my remarks today and will emphasize in particular how changes in desired saving and investment in any given region, through their effects on global capital flows, may affect saving, investment, and the external balances of other countries around the world.\n\nThe Origins of the Global Saving Glut, 1996-2004\nI will begin by reviewing the origins and development of the global saving glut over the period 1996-2004, as discussed in my earlier speech, and will then turn to more-recent developments.\n\nAs is well known, the U.S. current account deficit expanded sharply in the latter part of the 1990s and the first half of the present decade.  In 1996, the U.S. deficit was $125 billion, or 1.6 percent of U.S. gross domestic product (GDP); by 2004, it had grown to $640 billion, or 5.5 percent of GDP.1  National income accounting identities imply that the current account deficit equals the excess of domestic investment in capital goods, including housing, over domestic saving, including the saving of households, firms, and governments.  The proximate cause of the increase in the U.S. external deficit was a decline in U.S. saving; between 1996 and 2004, the investment rate in the United States remained almost unchanged at about 19 percent of GDP, whereas the saving rate declined from 16-1/2 percent to slightly less than 14 percent of GDP.2  Domestic investment not funded by domestic saving must be financed by capital flows from abroad, and, indeed, the large increase in the U.S. current account deficit was matched by a similar expansion of net capital inflows.\n\nGlobally, national current account deficits and surpluses must balance out, as deficit countries can raise funds in international capital markets only to the extent that other (surplus) countries provide those funds.  Accordingly, it is not surprising that the widening of the U.S. current account deficit has been associated with increased current account surpluses in the rest of the world.\n\nWhat is surprising, however, in light of historical patterns, is that much of the increase in current account surpluses during this period took place in developing countries rather than in the industrial countries.3  The table shows current account balances for various countries and regions in selected years.  The aggregate current account balance of industrial countries other than the United States did increase between 1996 and 2004, by a bit less than $200 billion, much of that rise being accounted for by an increase in Japan's current account balance; the aggregate balance of the euro area rose only slightly.4  In comparison, the aggregate current account position of developing countries swung from a deficit of about $80 billion in 1996 to a surplus of roughly $300 billion in 2004, a net move toward surplus of $380 billion.\n\nIn the aggregate, the shift from deficit to surplus in the current account of the emerging-market world over this period largely reflected increased saving as a share of output rather than a decline in the rate of capital investment.  However, changes in saving and investment patterns varied by countries and regions.  For example, in the countries of developing Asia excluding China, most of the $150 billion swing toward external surplus between 1996 and 2004 was attributable to declines in domestic investment.  In China, rates of both saving and investment rose, but saving rates rose more, leading to an increase in that country's current account surplus of about $60 billion.\n\nOutside of developing Asia, oil exporters in the Middle East and the former Soviet Union were also important contributors to the large increase in emerging-market current account balances.  The combined current accounts of the two regions increased from a surplus of $20 billion in 1996 to a surplus of $162 billion in 2004, an increase of about $140 billion.  This rise largely reflected higher saving rates, as domestic consumption fell behind the surge in oil revenues.  Among other emerging-market economies, higher saving also accounted for an increase in the aggregate current account balance of Latin America.  Of course, as emerging-market countries switched from being net borrowers to being net lenders, they began to pay down their international debts and to acquire assets of industrial countries.\n\nI have noted the expansion of the U.S. current account deficit and the associated increases in current account surpluses abroad over the 1996-2004 period.  A third key development in that period was a sustained decline in long-term real interest rates in many parts of the world.  For example, the real yield on ten-year inflation-indexed U.S. Treasury securities averaged about 4 percent in 1999 but less than 2 percent in 2004.  The difference between the nominal long-term Treasury yield and the trailing twelve-month rate of consumer price inflation, another measure of the U.S. real interest rate, showed a similar pattern, falling from about 3.5 percent in 1996 to about 1.5 percent in 2004.  Similar movements were observed in other industrial countries:  In the United Kingdom, the real yields on inflation-indexed government bonds fell from an average of 3.6 percent in 1996 to just below 2 percent in 2004; in Canada, the analogous figures were 4.6 percent in 1996 and 2.3 percent in 2004.  Real interest rates measured as the difference between government bond yields and consumer inflation also fell in Germany, Sweden, and Switzerland.  However, in Japan, real interest rates remained low throughout the period.\n\nIn sum, considering the 1996-2004 period, we have three facts to explain:  (1) the substantial increase in the U.S. current account deficit, (2) the swing from moderate deficits to large surpluses in emerging-market countries, and (3) the significant decline in long-term real interest rates.  Many observers have focused on the expansion of the U.S. current account deficit in isolation and have argued that it is due largely to domestic factors, particularly declines in both public and private saving rates.  But accounting identities assure us that any movement in the current account must involve changes in realized saving rates relative to investment rates.  The question at issue, therefore, is whether the decline in the realized saving rate in the United States reflected a decline in desired saving or was instead a response to other, possibly external, economic developments.  Or, in textbook terms, did the fall in the realized saving rate in the United States reflect a shift in the demand for savings at any given interest rate (a shift in the saving schedule) or a decline in savings induced by a change in the interest rate (a movement along the saving schedule)?\n\nIn fact, there is no obvious reason why the desired saving rate in the United States should have fallen precipitously over the 1996-2004 period.5  Indeed, the federal budget deficit, an oft-cited source of the decline in U.S. saving, was actually in surplus during the 1998-2001 period even as the current account deficit was widening.  Moreover, a downward shift in the U.S. desired saving rate, all else being equal, should have led to greater pressure on economic resources and thus to increases, not decreases, in real interest rates.  As I will discuss later, from a normative viewpoint, we have good reasons to believe that the U.S. saving rate should be higher than it is.  Nonetheless, domestic factors alone do not seem to account for the large deterioration in the U.S. external balance.\n\nIn my earlier speech, I put forth an alternative explanation that is consistent with each of the three basic facts I listed earlier.  That explanation takes as a key driving force a large increase in net desired saving (that is, desired saving less desired domestic investment) in emerging-market and oil-producing economies, a change that transformed these countries from modest net demanders to substantial net suppliers of funds to international capital markets.  This large increase in the net supply of financial capital from sources outside the industrial countries is what, in my earlier remarks, I called the global saving glut.\n\nTo interpret the rise in net saving in emerging-market countries as causal, we need to identify factors in those countries that may have caused their desired saving to rise, or their desired investment to fall, or both.  In fact, several factors appear to have contributed to the increase in the supply of net saving from emerging-market countries.  First, the financial crises that hit many Asian economies in the 1990s led to significant declines in investment in those countries (in part because of reduced confidence in domestic financial institutions) and to changes in policies--including a resistance to currency appreciation, the determined accumulation of foreign exchange reserves, and fiscal consolidation--that had the effect of promoting current account surpluses.  Second, sharp increases in crude oil prices boosted oil exporters' incomes by more than those countries were able or willing to increase spending, thereby leading to higher saving and current account surpluses.  Finally, Chinese saving rates rose rapidly (by more even than investment rates); that rise in saving was, perhaps, a result of the strong growth in incomes in the midst of an underdeveloped financial sector and a weak social safety net that increases the motivation for precautionary saving.\n\nThe combined effect of these developments, I argued, raised desired saving relative to desired investment in the emerging markets, which in turn led to current account surpluses in those countries.  But for the world as a whole, total saving must equal investment, and the sum of national current account balances must be zero.  Accordingly, in the industrial economies, realized saving rates had to fall relative to investment, and current account deficits had to emerge as counterparts to the developing countries' surpluses.  This adjustment could be achieved only by declines in real interest rates (as well as increases in asset prices), as we observed.  The effects were particularly large in the United States, perhaps because high productivity growth and deep capital markets in that country were particularly attractive to foreign capital.  The global saving glut hypothesis is thus consistent with the three key facts I noted earlier.\n\nTo be sure, the global saving glut was not the only factor behind the decline in long-term real interest rates since the 1990s.  As I described in subsequent remarks (Bernanke, 2006), term premiums also declined during this period for reasons that are debated but may have included a perceived reduction in uncertainty regarding inflation and the real economy as well as increased demand for longer-term securities by various institutional investors, including pension funds and foreign central banks.  Changes in the global pattern of saving and investment surely played an important role in the decline in long-term rates, however.\n\nRecent Developments\nI turn now to a review of developments since I last spoke on these issues two and a half years ago.  In brief, external imbalances have become wider since 2004.  Both the geographical pattern of these imbalances and their sources in terms of saving and investment rates have changed a bit.  Nevertheless, the broad configuration that developed after 1996 still seems to be in place today.\n\nAs the table shows, the U.S. current account deficit has widened further in the past two years, from $640 billion in 2004 (5.5 percent of GDP) to $812 billion in 2006 (6.2 percent of GDP), although it fell a bit in the first quarter of this year, to $770 billion at an annual rate.  In an accounting sense, the increase in the U.S. deficit over this period reflects primarily an increase in the investment rate from about 19 percent of GDP in 2004 to 20 percent of GDP in 2006.  The U.S. national saving rate did not change significantly over that period.\n\nMeanwhile, the aggregate current account surplus of emerging-market economies expanded about $350 billion, from $297 billion in 2004 to $643 billion in 2006; almost all the increase was attributable to a higher aggregate rate of saving.  A significant portion of this further growth is due to China, whose current account surplus swelled an additional $180 billion, rising from 3.6 percent of national output in 2004 to 9.4 percent in 2006.  The increase in the Chinese surplus can be attributed primarily to an increase in the saving rate between 2004 and 2006.  The increase in China's saving rate could, in part, be a consequence of the rapid pace of growth in the country.  That is, with income growing very rapidly, but with consumer credit not readily available and precautionary motives for saving remaining strong, consumption is failing to catch up.6  Also contributing to high saving rates was the authorities' decision to limit currency appreciation, thereby restraining import demand and boosting exports.\n\nOil exporters have also contributed significantly to the recent increase in the aggregate current account balance of developing countries.  The combined current account balance of the countries of the Middle East and the former Soviet Union (which include a number of large oil exporters) rose about $150 billion between 2004 and 2006.  Again, the increase is almost entirely reflected in higher saving rates, as the oil exporters continue to save a large portion of the increased revenue resulting from higher oil prices.\n\nIn contrast to the situation in emerging markets, the aggregate current account surplus for industrial countries other than the United States declined recently, from almost $350 billion in 2004 to about $200 billion in 2006; most of the decline reflected a sharp drop in the euro-area balance.  Thus, unlike in the 1996-2004 period, industrial countries other than the United States have absorbed part of the increase in the net supply of capital coming from the emerging-market economies.  In aggregate, the recent decline in the current account balances of non-U.S. industrial economies reflects an increase in investment rates; saving rates have generally remained little changed.7  In short, in the emerging markets, realized saving and current account surpluses have increased since 2004.  In the industrial countries, over the same period, current accounts have moved further into deficit, primarily because of higher realized rates of investment.\n\nWhat about real interest rates?  Since I discussed these issues in March 2005, real interest rates have reversed some of their previous declines.  For example, in the United States, real yields on inflation-indexed government debt averaged 2.3 percent in 2006 as compared with 1.85 percent in 2004.  In the past few weeks, that yield has averaged about 2.4 percent.  Inflation-adjusted yields in other industrial countries have also started to move back up after falling in 2005.8\n\nHow does this all fit together?  My reading of recent developments is that although some of the details have changed, the fundamental elements of the global saving glut remain in place.  Most important, the emerging-market countries and oil producers remain large net suppliers of financial capital to global markets.  The mix of suppliers of funds and the factors motivating that supply have changed a bit:  China and the oil exporters account for a larger share of the developing countries' aggregate surplus, and developing Asia excluding China accounts for somewhat less.  Also, the further expansion of the region's net supply of saving in the past two years appears to reflect primarily an increase in desired saving by the emerging-market countries, whereas the previous increase in net saving also involved some decline in desired investment in East Asia after the financial crises of the 1990s.  Exchange rate policies in Asia have also influenced desired saving in that region.\n\nFurther increases in net capital flows from the developing economies, all else being equal, should have further depressed real interest rates around the world.  But as I have noted, in the past few years, real interest rates have moved up a bit.  This increase does not imply that the global saving glut has dissipated.  However, it does suggest that, at the margin, desired investment net of desired saving must have risen in the industrial countries enough to offset any increase in desired saving by emerging-market countries.  This characterization is certainly consistent with the pickup in investment rates in the industrial countries, which I noted earlier, and it is also consistent, more generally, with the recovery of domestic demand growth in Europe, Japan, and other parts of the industrial world.  In summary, economic growth over the past few years, especially in industrial countries, has apparently been sufficient to increase the net demand for saving and thus to raise global real interest rates somewhat.\n\nOnce again, however, I do not want to rely exclusively on this line of explanation for the behavior of long-term real interest rates, as other factors have no doubt been relevant.  In particular, term premiums appear recently to have risen from what may have been unsustainably low levels, in part because of the greater recent volatility in financial markets and investors' demands for increased compensation for risk-taking.\n\nAre Current Account Imbalances a Problem?\nThis analysis of the sources of global imbalances does not address the critical normative question:  Are the current account imbalances that we see today a problem?  Not everyone would agree that they are, for several reasons.\n\nFirst, these external imbalances are to a significant extent a market phenomenon and, in the case of the U.S. deficit, reflect the attractiveness of both the U.S. economy overall and the depth, liquidity, and legal safeguards associated with its capital markets.9  Of course, some foreign governments have intervened in foreign exchange markets and invested the proceeds in U.S. and other capital markets, which most likely has led to greater imbalances than would otherwise exist.  But the supply of capital from foreign governments is not as large as that from foreign private investors.  From 1998 through 2001, even as the U.S. current account deficit widened substantially, official capital flows into the United States were quite small.  During the years 2002 through 2006, net official capital inflows picked up substantially but still corresponded to less than half (47 percent) of the U.S. current account deficit over the period.  On a gross basis, during the same period, private foreign inflows were three times official capital flows.10  Moreover, even public investors are motivated to some extent by the attractions of the U.S. economy and U.S. capital markets.\n\nSecond, current account imbalances can help reduce tendencies toward recession, on the one hand, or overheating and inflation, on the other.11  During the late 1990s, for example, the developing Asian economies that had experienced financial crises and consequent collapses in domestic investment benefited from being able to run trade surpluses, which helped strengthen aggregate demand and employment.  During that same period, the trade deficits run by the United States allowed domestic demand to grow strongly without creating significant inflationary pressures.  Until a few years ago, the euro area was growing slowly and thus also benefited from running trade surpluses; more recently, as domestic demand in Europe has recovered, the trade surplus has declined.\n\nThird, although the U.S. current account deficit is certainly not sustainable at its current level, U.S. liabilities to foreigners are not, at this point, putting an exceptionally large burden on the American economy.  The net international investment position (NIIP) of the United States, although at a substantial negative 19 percent of GDP, is still smaller than the negative NIIP of several other industrial economies.  As a fraction of net household wealth, which totaled almost $56 trillion in 2006, the negative NIIP is even smaller--less than 5 percent.  Moreover, the U.S. investment income balance, which essentially represents the debt service on the NIIP, remains positive, at least for now.  Thus, even after years of current account deficits and corresponding increases in net liabilities, the United States continues to earn more on its foreign investments than it pays on its foreign liabilities.  And, as best we can tell, the share of U.S. assets in foreign portfolios does not seem excessive relative to the importance of the United States in the global economy.\n\nAll that said, the current pattern of external imbalances--the export of capital from the developing countries to the industrial economies, particularly the United States--may prove counterproductive over the longer term.  I noted some reasons for concern in my earlier speech, and they remain relevant today.\n\nFirst, the United States and other industrial economies face the prospect of aging populations and of workforces that are growing more slowly.  These trends enhance the need to save (to support future retirees) and may reduce incentives to invest (because workforces eventually will shrink).  If the United States saved more, one likely outcome would be a reduction in the U.S. current account deficit and in the rate at which the country is adding to its liabilities to the rest of the world.\n\nSecond, the large U.S. current account deficit cannot persist indefinitely because the ability of the United States to make debt service payments and the willingness of foreigners to hold U.S. assets in their portfolios are both limited.  Adjustment must eventually take place, and the process of adjustment will have both real and financial consequences.  For example, in the United States, the growth of export-oriented sectors such as manufacturing has been restrained by the shifts in relative prices and foreign demand associated with the U.S. trade deficit.  Ultimately, the necessary reduction in the trade and current account deficits will entail shifting resources out of sectors producing nontraded goods and services to those producing tradables.  The greater the needed adjustment, the more potentially disruptive and costly these shifts may be.  Similarly, external adjustment for China and other surplus countries will involve shifting resources out of the export sector and into industries geared toward meeting domestic consumption needs; that necessary shift, too, will likely be less disruptive if it occurs earlier and thus less rapidly and on a smaller scale.\n\nOn the financial side, if U.S. current account deficits were to persist at near their current levels, foreign investors would ultimately become satiated with dollar assets, and financing the deficit at a reasonable cost would become difficult.  Earlier reduction of global imbalances would reduce the potential strains associated with financing a large quantity of international liabilities and likely allow a smoother adjustment in financial markets.\n\nFinally, in the longer term, the developing world should be the recipient, not the provider, of financial capital.  Because developing countries tend to have high ratios of labor to capital and to be away from the technological frontier, the potential returns to investment in those countries are high.  Thus, capital flows toward those countries should benefit both them and the countries providing the capital.\n\nProspects for Reducing External Imbalances\nWhat are the prospects for a gradual and orderly rebalancing of spending and external accounts around the world?  The brief answer is that signs of progress have appeared but that most countries have only just begun to undertake the policy changes that will ultimately be needed.\n\nRecently, the pickup in economic growth outside the United States, together with changes in the real exchange rate and other relative prices, has assisted the process of current account adjustment.  Notably, during 2006, foreign growth helped U.S. real exports of goods and services grow 9.3 percent, and exports of capital goods rose 10.8 percent.  Some of the gain in foreign growth is cyclical, but some is due to economic reforms (in both industrial and non-industrial countries) and thus may be more persistent.  Overall, we have seen some modest indications of improvement in the U.S. external balance recently.  For example, the non-oil trade deficit has declined modestly, from 3.7 percent of U.S. GDP in 2004 to 3.5 percent of GDP in 2006.  In addition, in 2006, net exports made a positive contribution to U.S. real GDP growth, the first year that had happened since 1995.  Net exports also contributed to U.S. growth in the first half of 2007.\n\nAs is well known, however, further progress on the U.S. current account seems unlikely without significant increases in public and private saving in the United States.  The U.S. federal budget deficit has declined recently and is officially projected to improve further over the next few years.  Unfortunately, as I have noted, the United States has already reached the leading edge of major demographic changes that will result in an older population and a more slowly growing workforce.  A major effort to increase public and private saving is needed to prepare for the economic consequences of this demographic transition and to address external imbalances.\n\nAs the global perspective makes clear, the reduction of the U.S. current account deficit also requires efforts on the part of the surplus countries to reduce the excess of their desired saving over desired investment.  Over the longer term, the current account surpluses of the emerging-market countries seem likely to narrow as domestic spending catches up with income.  Economic policies in these countries can assist this process.  For example, the oil exporters have collectively saved much of the windfall arising from higher crude prices in recent years; they should spend more in the future to develop and diversify their domestic economies.  China has officially recognized the need to increase its domestic spending and scale back its reliance on exports.  Measures that could help achieve these goals include further reforms of the financial sector; increased government spending on infrastructure, environmental improvement, and the social safety net; and currency appreciation.  In East Asia excluding China, continued efforts to strengthen and deepen the banking sector and financial markets would help domestic investment recover from the lingering effects of the financial crises of the 1990s.  In each of these cases, the indicated policies would reduce global imbalances.  Moreover, as with U.S. saving efforts, these actions would convey important economic benefits to the countries undertaking them even if current account balances were not an issue.\n\nWhat implications would a gradual rebalancing have for long-term real interest rates?  The logic of the global saving glut suggests that, as the glut dissipates over the next few decades and thereby reduces the net supply of financial capital from emerging-market countries, real interest rates should rise--a tendency that seems likely to be only partly offset by increased saving in the industrial countries.  However, factors other than the saving-investment balance affect long-term interest rates, including the relative supplies of, and demands for, long-term securities and changes in the required compensation for the risk embedded in term premiums.  Moreover, distant one-year forward interest rates remain low, an indication that markets currently do not expect much change in the global balance of desired saving and investment or that they expect the effects of such a change to be offset by other developments.  Accordingly, we are again reminded of the need to maintain appropriate humility in forecasting returns and asset prices.\n\n\n\nCurrent Account Balances\n(Billions of U.S. dollars)\n\n\n\n\n\n\n\n\n\n1. Calculated as the sum of the balances of the thirteen euro-area countries. Return to table\n\nSource: For the United States, Department of Commerce, Bureau of Economic Analysis.  For some countries other than the United States, national sources; for most countries, however, International Monetary Fund (IMF), World Economic Outlook Database, April 2007 (www.imf.org/external/pubs/ft/weo/2007/01/data/index.aspx); some values for 2006 are IMF estimates.\n\nReferences\n\nBernanke, Ben S. (2005).  \"The Global Saving Glut and the U.S. Current Account Deficit,\" speech delivered for the Sandridge Lecture at the Virginia Association of Economists, Richmond, March 10, www.federalreserve.gov/boarddocs/speeches/2005/200503102/default.htm.  Similar remarks with updated data were presented for the Homer Jones Lecture, St. Louis, April 14, 2005, www.federalreserve.gov/boarddocs/speeches/2005/20050414/default.htm.\n\n------------ (2006).  \"Reflections on the Yield Curve and Monetary Policy,\" speech delivered at the Economic Club of New York, New York, March 20, www.federalreserve.gov/newsevents/speech/bernanke20060320a.htm.\n\nCaballero, Ricardo J., Emmanuel Farhi, and Pierre-Olivier Gourinchas (2006).  \"An Equilibrium Model of 'Global Imbalances' and Low Interest Rates,\" NBER Working Paper Series 11996.  Cambridge, Mass.:  National Bureau of Economic Research, January, www.nber.org/papers/w11996.pdf.\n\nMendoza, Enrique G., Vincenzo Quadrini, and Jose-Victor Rios-Rull (2007).  \"Financial Integration, Financial Deepness, and Global Imbalances,\" NBER Working Paper Series 12909.  Cambridge, Mass.:  National Bureau of Economic Research, February, www.nber.org/papers/w12909.pdf.\n\nFootnotes\n\n1. The shift was almost wholly attributable to a similar expansion of the trade deficit.  The balance on investment income actually improved over the period. Return to text\n\n2.  More precisely, investment grew from 19.0 percent to 19.3 percent of GDP, and saving declined from 16.5 percent to 13.8 percent of GDP, for a net change in investment less saving of 3.0 percent of GDP.  As implied by data noted earlier in this paragraph, the net change in the U.S. current account deficit over the same period was 3.9 percent of GDP.  In principle, the change in the excess of investment over saving and the change in the current account deficit should be the same.  The difference between the two figures is accounted for by statistical discrepancies, both within the national income and product accounts (NIPA) and between the balance of payments definitions and NIPA definitions of certain international transactions. Return to text\n\n3.  I am using the terms \"emerging-market\" and \"developing\" interchangeably. Return to text\n\n4.  As shown in the table, the surplus of industrial countries other than the United States increased from about $150 billion to nearly $350 billion over the period, and the Japanese external balance rose from $66 billion to $172 billion.  The increase in the Japanese current account balance as a share of GDP, from 1.4 percent to 3.7 percent, occurred despite a substantial fall in the GDP share of the saving rate, from 30.4 percent to 26.8 percent, as the GDP share of the investment rate fell even more dramatically, from 28.9 percent to 23.0 percent.  For the euro area as a whole, the current account balance remained at about 1 percent of GDP between 1996 and 2004, as aggregate investment and saving ratios remained largely unchanged.  Within the euro area, Germany's current account balance increased almost 5 percentage points of GDP--from -0.6 percent in 1996 to 4.3 percent in 2004--as saving moved up and investment decreased.  However, this development was offset by declines in the balances of some other euro-area countries, including France, Italy, and Spain; the decreases were mostly associated with higher investment rates.  Data on saving, investment, and current account balances for countries other than the United States are drawn primarily from the International Monetary Fund, World Economic Outlook Database, April 2007 (www.imf.org/external/pubs/ft/weo/2007/01/data/index.aspx); in some cases, data are drawn from national sources. Return to text\n\n5.  During the first part of the period, the rise in U.S. productivity and higher stock prices likely contributed to the U.S. current account deficit by increasing desired investment and reducing desired saving.  However, some of the increase in stock prices may have been the endogenous result of factors discussed later, and in any case the effects of the stock market on investment dissipated by 2004.  Finally, as noted in the text, if the driving force behind the changes in external balances was a decline in desired saving in the United States, world real interest rates would have risen rather than fallen. Return to text\n\n6.  The combined current account balance of developing Asia excluding China narrowed a bit as a share of GDP between 2004 and 2006, as the investment rate edged up while the saving rate was little changed.  Nevertheless, investment rates in this region still remain substantially below their 1996 levels. Return to text\n\n7.  The combined current account balance for the euro area moved from a surplus of $115 billion in 2004 to a deficit of about $10 billion in 2006, largely because of an increase in the aggregate investment rate.  Large declines in the balances of France, Italy, and Spain more than offset a higher surplus in the balance of Germany.  For the euro area as a whole, the movement into deficit has largely reflected an increase in the euro-area investment rate from about 20 percent of GDP in 2004 to about 21 percent of GDP in 2006.  Japan's current account surplus was almost unchanged at around $170 billion in both 2004 and 2006, as an increase in the rate of investment was matched by a higher saving rate. Return to text\n\n8.  Inflation-adjusted bonds in the United Kingdom had a yield of 2.19 percent, on average, in July 2007 as compared with a yield of 1.65 percent, on average, in July 2005.  In Canada, yields on inflation-adjusted bonds moved from 1.76 percent in July 2005 to 2.18 percent in July 2007.  Real interest rates, calculated as government bond yields minus twelve-month inflation rates, have also moved up since 2005 in Germany, Sweden, and Switzerland. Return to text\n\n9.  An interesting vein of recent research suggests that one of the reasons that developing countries seek to run current account surpluses is to finance the acquisition of high-quality assets they cannot produce in their own economies.  Refer to Caballero, Farhi, and Gourinchas (2006) and Mendoza, Quadrini, and Rios-Rull (2007).   Return to text\n\n10.  During 2002-06, gross foreign official inflows totaled $1,491 billion; net official inflows were only slightly less, as U.S. official outflows were negligible.  Private foreign inflows net of private U.S. outflows totaled $1,659 billion during the same period; gross foreign private inflows were $4,697 billion. Return to text\n\n11.  Another way to make this point is that current account balances and surpluses give countries the flexibility to spend more or less than their current output, as dictated by economic conditions and needs. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070911a.htm",
        "title": "Global Imbalances: Recent Developments and Prospects",
        "date": "9/11/2007"
    },
    {
        "content": "September 10, 2007\n\nGovernor Frederic S. Mishkin\n\nTo the Money Marketeers of New York University, New York, New York\n\nThank you for the invitation to be here this evening.1  It is always a pleasure to be back in my hometown, and it is a particular pleasure to have a chance to talk again with the Money Marketeers.  At the outset, I would like to acknowledge that tomorrow marks the sixth anniversary of the terrorist attacks on the World Trade Center, the Pentagon, and United Flight 93, which crashed in Pennsylvania.  The anniversary weighs on the minds of Americans everywhere, and it weighs especially heavily on members of the New York financial community.  That terrible day and its aftermath remind us that resilience is a defining characteristic not just of our economy and financial system but also of our country and our city.  For as long as anyone who experienced it is alive, the memory of that awful day will not fade.\n\nIn my remarks this evening, I will review the current economic situation and outlook and make some specific observations about recent developments in financial markets.  I should note that the views I will express here are my own and not necessarily those of my colleagues on the Federal Open Market Committee (FOMC).\n\nAs everyone in this room knows, financial markets have been front and center in recent discussions about the economy.  The FOMC noted on August 17 that financial market conditions deteriorated last month and that the associated tighter credit conditions and increased uncertainty have the potential to restrain economic growth going forward.  Indeed, at this point, housing demand seems likely to be crimped further by a marked reduction in the availability of mortgages, and consumer and business spending also could be damped as a consequence of the recent financial turmoil.  In light of these events, we will need to make the best possible real-time judgments about the extent to which the recent developments in financial markets are likely to affect economic activity in the period ahead.\n\nThat said, the economy ended the second quarter on a positive note.  The Commerce Department’s Bureau of Economic Analysis recently reported that the economy expanded at an annual rate of 4 percent in the second quarter, about 1/2 percentage point higher than the first estimate.  Of course, the jump in growth followed a sluggish first-quarter gain:  That choppiness in growth reflected large swings in several factors that are often volatile, including inventory investment, federal defense outlays, and net exports.  Smoothing through this volatility, the underlying pattern of activity in the first half of the year essentially represented a continuation of the moderate growth that had prevailed since the spring of last year when growth in real gross domestic product stepped down from an above-trend pace.  Many of the spending indicators for the current quarter have remained consistent with that earlier trend.  However, for the most part, these data cover a period that predates the recent onset of financial turbulence.\n\nThe step-down in the growth of real output that began in mid-2006 and proceeded through the middle of this year primarily reflected weakness in the housing market.  Declines in real residential investment subtracted nearly 1-1/4 percentage points from the growth of real GDP in the second half of last year and about 3/4 percentage point in the first half of this year.  Moreover, the demand for housing appears to be weakening further amid credit conditions for home mortgage lending that are generally tighter, but especially so in the subprime segment.  Sales of existing single-family homes in July were more than 20 percent below their peak in mid-2005, while sales of new homes in July were more than 37 percent below their peak.  Although housing starts have also moved down over the past year, the decline thus far has lagged the downturn in sales, and the backlog of unsold new homes has climbed to a very high level relative to sales.  Given this sizable backlog and the likelihood that sales will remain weak or weaken further in coming months, cutbacks in housing construction are likely to continue to be a drag on economic activity in future quarters.\n\nTurning to consumer spending, real outlays decelerated considerably in the second quarter.  To some extent, a moderation from the very rapid pace of growth around the turn of the year probably was to be expected.  In the second quarter, in particular, a jump in energy prices eroded the purchasing power of gains in household income, placing downward pressure on the growth of real spending at that point.  Even so, early indications are that household spending is holding up reasonably well thus far in the current quarter:  Real outlays for goods excluding motor vehicles posted a solid increase in July, and sales of light motor vehicles--which had slumped in June and July--rebounded last month.\n\nHaving said that, several factors suggest that consumer spending will be subdued in the period ahead.  This summer’s retrenchment in equity markets and the sharp deceleration in house prices have damped gains in household wealth this year and are likely to restrain consumer outlays.  Moreover, at least some households are likely to find it more difficult or expensive to borrow, and consumer sentiment--which turned down in August--could soften further if households become more anxious about recent financial market developments.\n\nBusiness investment in equipment and software weakened toward the end of last year and remained sluggish in the first quarter.  In part, the slowing reflected a downturn in outlays for capital goods that are used heavily by the motor vehicle and construction industries, two sectors in which activity had softened considerably.  But spending on other types of equipment also was soft in the first quarter.  In the second quarter, however, demand for these other types of equipment bounced back strongly, and the gains were extended in July--as reported in the most recent data on orders and shipments of capital goods.\n\nWe have no direct readings on capital spending in August, but the limited indicators currently in hand--such as the Institute for Supply Management’s survey of purchasing managers--have held up reasonably well and remain at levels consistent with modest growth in manufacturing production and business investment.  Of course, all this could change noticeably if many firms were to face significantly tighter credit conditions or if business sentiment were to soften appreciably.\n\nRegarding international developments, the recent deterioration in financial market conditions has had effects around the world, but the economies of our major trading partners appear set to continue their expansion.  That growth should continue to stimulate demand for U.S. exports of goods and services.\n\nTurning to the labor market, payroll employment has weakened.  As reported on Friday, nonfarm payrolls fell 4,000 last month, and private payrolls rose only 24,000.  Smoothing through the recent monthly numbers, private payrolls increased an average of about 70,000 per month over the past three months; this is down from gains near 120,000 per month in the first five months of the year and about 165,000 per month in the second half of 2006.\n\nAs for the economy’s longer-run growth rate, a key determinant is the trend in labor productivity.  As you know, from 1995 to 2000, productivity in the nonfarm business sector increased at an average annual rate of 2-1/2 percent, well above the lackluster pace of the preceding twenty-five years.  Then, remarkably, productivity accelerated further, rising at an average of about 3-1/2 percent per year for the first three years of this decade, despite the challenges of a recession, the fall of the dot-com market, a broad stock market correction, and the terrorist attacks.\n\nSince the middle of 2004, however, the growth of labor productivity has slowed, registering an average annual rate of about 1-1/4 percent.  Previously, that figure had looked higher, but this summer’s annual revision of the national income and product accounts marked down productivity growth over the 2004 to 2006 period by an average of 0.3 percentage point per year.  Part of the recent deceleration in productivity almost surely reflects a typical cyclical response to a slowing economy.  The difficult questions, of course, are whether some of the recent slowing also reflects a downshift in the underlying trend, and, if so, to what extent?  The latest evidence suggests that structural productivity might be increasing somewhat more slowly than it did during the second half of the 1990s, but the confidence band around any such estimate of trend productivity growth surely is very wide.2  These issues remain an area of active debate, the outcome of which will be essential for gauging the economy’s potential rate of growth going forward.\n\nLet me turn now to inflation and inflation dynamics.  Over the past year, the price index for total personal consumption expenditures (PCE) rose 2.1 percent, down from nearly 3-1/2 percent during the comparable period twelve months earlier.  Recently, topline inflation has been boosted by sizable increases in food prices; energy-price increases slowed considerably over the past twelve months after sizable advances in earlier years.  Excluding food and energy, core PCE prices decelerated over the past year; the July reading on the twelve-month change stood at 1.9 percent, almost 1/2 percentage point less than it was a year earlier.\n\nIn my view, inflation expectations have been a key element in the recent performance of core inflation.  By a range of measures, inflation expectations appear to have remained contained even as headline inflation moved temporarily higher.  According to the Reuters/University of Michigan Survey of Consumers, the median expectation of inflation five to ten years ahead has been essentially flat since the beginning of last year.  In addition, measures of long-run inflation compensation derived from spreads between yields on nominal and inflation-indexed Treasury securities have not pushed above the range that has prevailed in the past couple of years.\n\nAs I discussed in a speech earlier this year, I read the evidence as suggesting that households’ long-run inflation expectations are consistent with PCE price inflation in the neighborhood of 2 percent.3  To be sure, this figure is sensitive to the assumptions used to tease such estimates from the available data, so I do not want to overstate its precision.  Still, the professional forecasters surveyed by the Philadelphia Fed also project PCE inflation to be close to 2 percent over the next five to ten years.\n\nAs I look at the incoming inflation data, I would judge them to be consistent with expectations in this range; moreover, I believe that having expectations reasonably well anchored in this range has been a helpful influence on the path of actual inflation.  However, let me be clear: I do not subscribe to a deus ex machina view of the inflation process, in which inflation is driven solely by inflation expectations and is little influenced by the balance of aggregate demand and aggregate supply.  Indeed, I take the view that expectations of future resource utilization are also an important factor affecting inflation outcomes.\n\nIf households and businesses believe that the Federal Reserve will set monetary policy in a way that keeps aggregate demand in reasonable alignment with aggregate supply over time, then expectations of future resource utilization will be stable, and current resource utilization will provide less information about future inflation movements.  In that situation, which I believe describes the current environment, inflation expectations will be a key driver of inflation dynamics.\n\nThe stable inflation expectations we have seen lately derive from confidence that monetary policy will keep inflation under control.  If monetary policy allowed aggregate demand to get out of sync with supply and if the Fed was not expected to bring demand back into balance with supply, then inflation expectations would be much less likely to remain stable.  This is why I believe that the Federal Reserve must remain vigilant on inflation but give appropriate attention to keeping demand from falling below supply as well.\n\nWhat does this all mean for the inflation outlook?  At present, labor and product markets appear to be in reasonable balance.  And I would expect the pressures on inflation that we have experienced from food and energy prices to abate.  Accordingly, with inflation expectations remaining stable around their current level, I see inflation as remaining in alignment with long-run expectations at around a 2 percent pace for the PCE deflator.  In addition, I believe that the risks to the inflation outlook have become more balanced, given the greater downside risks to real growth.\n\nLet me shift gears and discuss developments in financial markets.  As you know, the recent turmoil had its beginnings in the subprime mortgage market.4  The development of the subprime market in the 1990s was an important financial innovation that enabled borrowers with higher credit risk to obtain mortgages that previously were unavailable to them.  This expansion appears likely to have been a significant factor in raising the rate of homeownership from 64 percent, the level in 1994, to about 68 percent currently.  In addition, subprime and other nonprime lending played an important role in the high volume of home sales in the mid-2000s.  Indeed, data collected under the Home Mortgage Disclosure Act indicate that about 25 percent of the loans used to purchase single-family, owner-occupied homes in 2005 were high-priced loans, including primarily subprime and some near-prime mortgages.5\n\nHowever, as has been the case in previous instances of rapid financial innovations, adequate mechanisms to control excessive risk-taking may not have been in place during the subprime market’s greatest growth.  One innovation, further development of securitized products, gave mortgage lenders greater access to the capital markets and spread risks more broadly.  However, securitization also widened the separation of the originators from the ultimate holders of the loans--that is, those who bought securities backed by loans.  In this setup, a classic principal-agent problem can arise if originators (the agents) do not have a sufficient incentive to shield the owners of the securities (the principals) from suffering higher-than-expected losses.\n\nAgainst a backdrop of continued strong investor demand for high-yielding securities, some lenders began loosening underwriting standards for subprime mortgages in late 2005.  Loans to subprime borrowers were approved with high loan-to-value ratios and incomplete income documentation.  Had house prices kept appreciating, loan-to-value ratios would have fallen and some borrowers would have been able to refinance, perhaps into a prime loan with a lower interest rate.  But, instead, as the housing market softened and interest rates rose, delinquencies in the adjustable-rate subprime market began to soar and reached nearly 15 percent in July.6  Among other types of nonprime mortgages, delinquencies on fixed-rate subprime mortgages have been fairly steady at less than 6 percent; rates on mortgages in alt-A pools have increased to nearly 3 percent, up notably from the 1 percent rate of only a year ago.\n\nThe rise in delinquencies in the subprime market has led to the collapse of some large subprime lenders and inflicted substantial losses on holders of subprime residential mortgage-backed securities (RMBS) and of some collateralized debt obligations (CDOs).  As a result, underwriting standards have been tightened, and fewer households are qualifying for subprime loans.  In addition, some borrowers apart from the subprime segment are reportedly finding it more difficult to qualify for loans or are having to pay more for them.  These developments have contributed materially to the drop in demand for housing this year.  Without a doubt, they also have caused significant hardship for many individuals and families.\n\nRecently, we have watched the deterioration in financial conditions extend beyond the subprime market.  Investors appear to have reassessed their outlook and their tolerance for risk, especially for structured financial products and for securities of highly leveraged firms.  Bond spreads--especially those for speculative-grade debt--widened substantially in June and July, and the volatility of equity prices increased as well.  In mid-August, following several events that led investors to believe that credit risks might be larger and more pervasive than previously thought, the functioning of financial markets, including short-term and interbank funding markets, became increasingly impaired.  Notably, many asset-backed commercial paper programs found rolling over their paper increasingly difficult.  To help restore orderly conditions, the Federal Reserve in recent weeks has increased the provision of reserves, cut the discount rate, and changed its usual discount-window lending practices in order to facilitate term borrowing, together with other measures.\n\nStepping back from the rush of unfolding events, we are seeing a pattern that occurs from time to time.  Financial markets and institutions perform the essential function of channeling funds to those individuals or firms having the most productive investment opportunities.  However, an increase in uncertainty and concerns about the quality of information can lead investors to pull back from financial markets and restrict productive lending--with potentially adverse implications for real activity.  That is essentially the story I laid out in a paper delivered at the Kansas City Fed’s Jackson Hole conference about ten years ago.7\n\nIn my view, such an increase in uncertainty is an important part of what we have observed recently and stems from heightened concerns about the value of financial securities related to certain types of loans, about who is holding these securities, and about how a revaluation of these securities might affect the balance sheets of various financial intermediaries.  Consequently, investors have become less willing to put funds into various financial markets, particularly into the more opaque segments of those markets.\n\nAs best we can tell thus far, the imprint of these developments on economic activity appears likely to be most pronounced in the housing sector.  However, economic activity could be affected more severely in other sectors should heightened uncertainty lead to a broader pullback in household and business spending.  That scenario cannot, in my view, be ruled out, and I believe it poses an important downside risk to economic activity.\n\nI also believe that the process of adjustment that is under way in financial markets--of investors reassessing the outlook for risk and their tolerance for that risk--will ultimately create a more solid financial footing for the real economy.  But in the meantime, the FOMC is monitoring the situation and is prepared to act as needed to mitigate the adverse effects on the economy arising from the disruptions in financial markets.\n\nThank you for your interest and attention.  I look forward to your questions and observations on recent developments.\n\nFootnotes\n\n1.   I would like to thank Daniel Sichel and Lawrence Slifman for the excellent comments and assistance on this speech. Return to text\n\n2.  Stephen D. Oliner, Daniel E. Sichel, and Kevin J. Stiroh (2007), “Explaining a Productive Decade,” Brookings Papers on Economic Activity, vol. 2007 (1), pp. 81-152.  The authors highlight the wide confidence band surrounding estimates of the growth rate of structural productivity. Return to text\n\n3.  Frederic S. Mishkin (2007), “Inflation Dynamics,” speech delivered at the Annual Macro Conference, Federal Reserve Bank of San Francisco, March 23, www.federalreserve.gov/newsevents/speech/mishkin20070323a.htm. Return to text\n\n4.  A more in-depth discussion of developments in the subprime mortgage sector can be found in Karen E. Dynan and Donald L. Kohn (2007), “The Rise in U.S. Household Indebtedness:  Causes and Consequences,” Finance and Economics Discussion Series 2007-37 (Washington:  Board of Governors of the Federal Reserve System, August), http://www.federalreserve.gov/pubs/feds/2007/200737/200737pap.pdf. Return to text\n\n5.  Calculated from table 4, p. A132, in Robert B. Avery, Kenneth P. Brevoort, and Glenn B. Canner (2006), “Higher-Priced Home Lending and the 2005 HMDA Data,” Federal Reserve Bulletin, vol. 92 (September 8), pp. A123-66, www.federalreserve.gov/pubs/bulletin/2006/hmda/bull06hmda.pdf. Return to text\n\n6.  Based on data from First American LoanPerformance. Return to text\n\n7.  Frederic S. Mishkin (1997), “The Causes and Propagation of Financial Instability:  Lessons for Policymakers (145 KB PDF),” paper presented at “Maintaining Financial Stability in a Global Economy,” a symposium sponsored by the Federal Reserve Bank of Kansas City, held in Jackson Hole, Wyo., August 28-30, http://www.kc.frb.org/publicat/sympos/1997/sym97prg.htm. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/mishkin20070910a.htm",
        "title": "Outlook and Risks for the U.S. Economy",
        "date": "9/10/2007"
    },
    {
        "content": "September 06, 2007\n\nGovernor Randall S. Kroszner\n\nAt the Federal Reserve Bank of San Francisco, Conference on the Asian Financial Crisis Revisited, San Francisco, California(via videoconference)\n\nGood morning.  I am delighted to be speaking to you today at the second of a pair of excellent conferences that the Federal Reserve Bank of San Francisco has organized on the tenth anniversary of the Asian financial crisis. Janet Yellen and her staff deserve kudos for focusing our attention on this important topic and I feel honored for the invitation to speak today.1\n\n\n\nIntroduction\nBefore moving to the main topic, I would like to reinforce remarks made last week by Chairman Bernanke on the recent turbulence in financial markets.  In the United States we have seen a fairly sharp downturn in housing markets, and in recent weeks there have been growing investor concerns about mortgage credit performance, particularly with subprime mortgages.  If current conditions persist in mortgage markets, the demand for homes could weaken further, with possible implications for the broader economy.  And financial stress has not been limited just to mortgage markets, but has spread to other markets.  In general, a shift in risk attitudes has interacted with heightened concerns about credit risks and uncertainty about how to evaluate those risks.  Fortunately, this recent period of turbulence in financial markets has occurred at a time when U.S. commercial banks are strongly capitalized, reflecting years of robust profits.\n\nAs the nation's central bank, the Federal Reserve seeks to promote general financial stability and to help to ensure that financial markets operate in an orderly manner.  Accordingly, the Federal Reserve has taken some steps in recent weeks to provide liquidity and to promote the orderly functioning of markets.  We continue to follow these developments in financial markets closely, particularly those that may have a broad impact on real economic activity.\n\nToday I plan to offer some thoughts about ways to analyze and assess the impact of banking crises on real economic activity.  Others at this conference will be addressing some of the specifics of the Asian financial crisis in 1997-98 and the status of economies and financial systems of Asian countries today, so I believe I can best contribute by setting up a conceptual and empirical framework that can be applied to various types of financial crises, including Asia in 1997-98.\n\nMuch of what I plan to discuss is based on a paper I published earlier this year with colleagues at the International Monetary Fund and World Bank, Luc Laeven and Daniela Klingbiel, titled \"Banking Crises, Financial Dependence, and Growth\".2  In particular, this research focuses on a key question for policy, namely, through what channels can financial turmoil have an impact on real economic activity?  We study the effect of different types of crises on the performance of various sectors and firms in economies with differing levels of development of their banking and financial systems.  Surprisingly, little systematic empirical work had been done detailing the mechanisms by which financial crises can generate problems in the real economy.3  I will provide a brief overview of these results and then draw some lessons that policymakers might keep in mind when considering ways to help prevent and mitigate future crises.\n\nAs a final introductory thought, I want to note that my remarks today on banking crises relate to research conducted on a range of countries, many of them emerging-market countries, and are not a commentary on current financial conditions or on the health of the U.S. banking system, which, as I noted above, is quite good.\n\nFramework for Assessing and Analyzing Banking Crises\nFinancial crises can assume various forms.  I am going to focus primarily on financial crises involving the banking sector and do so for several reasons.  First, banks play a prominent role in the credit intermediation process in most countries, providing funding to firms beyond the cash flow provided by their normal operations.  Banks also typically serve as custodian of a significant portion of household saving.  In many Asian countries, for example, banks play a key role in channeling credit to firms--particularly those firms not able to acquire funding from capital markets or other sources--and also hold substantial consumer deposits.\n\nIn addition, much of the literature on financial crises indicates that crises involving the banking sector can lead to disruptions in the real economy.  The definition of banking crisis I will use today, consistent with the definition in our recent paper, is an episode during which the capital of the banking sector has been depleted due to loan losses, resulting in a negative net worth of the banking sector.4  Therefore, the use of the term \"banking crisis\" in our research refers to major disruptions in a country's banking system, not just minor downturns or disturbances.  By focusing on banking crises in our research paper, we were able to isolate the impact of banks on the provision of credit and liquidity to firms during times of distress.  I believe that combining this type of empirical research on banking crises with practical experience from bank supervisors and market participants, such as those attending this conference, helps us all to understand and address the problems associated with banking crises--and perhaps even to help to prevent them.\n\nMuch work has been done on how financial intermediaries and financial markets facilitate investment by firms and, hence, promote economic growth.5  Financial intermediaries and financial markets are generally thought to reduce information asymmetry problems that can make raising external funds difficult and expensive for firms.  Well-functioning and well-developed financial intermediaries and markets thus should particularly benefit firms that rely most heavily on external funding to finance their growth.6  Conversely--and this is the novel aspect of our research--crises in the financial sector thus should have a disproportionately negative impact on firms that rely heavily on external sources of finance.\n\nIn particular, we investigate whether the impact of a banking crisis on sectors dependent on external sources of financing varies with the level of development of the financial system.  If the banking system is the key element allowing firms that depend heavily upon external funding sources to finance their growth, then an impairment of these intermediaries--in a system where such intermediaries are important--should have a disproportionate contractionary impact on precisely those sectors that flourished in \"normal\" times, due to their reliance on banks.  Thus, an important element of our analysis is the level of development of a country's financial system, that is, whether it is \"deep\" (more developed) or \"shallow\" (less developed).7\n\nTo address these issues, we gathered data from thirty-eight developed and developing countries that have experienced a banking crisis during the last quarter century.8  We documented clear linkages between the state of the banking system and the performance of the real economy.  More specifically, we find that in well-developed and deep financial systems, sectors highly dependent on external sources of funding tend to experience a greater contraction during a banking crisis than do externally dependent sectors in countries with shallower financial systems. In other words, sectors of the real economy that rely heavily on external finance (that is, do not fund capital expenditures through cash flow) tend to experience a substantially slower growth of value added during a banking crisis than those sectors that do not rely so heavily on external funding.  This effect is more pronounced in countries with more developed financial systems.  Our results hold for a wide group of countries and over a long time span, but as I note below, have particular relevance to emerging market countries.\n\nWhile these results are consistent with a so-called credit channel impact of banking crises on real economic activity, there are further implications of the \"credit channel\" view that we explore in more detail.  Among firms that depend heavily on outside financing, young firms with short histories and firms with a large fraction of hard-to-measure intangible assets, for example, may have particular difficulties raising funds from the market due to information problems.  Instead, such firms would tend to depend heavily on banks and other intermediaries for funding.  Consistent with this, we find a greater negative impact of banking crises on growth for industries dominated by young firms that are highly dependent on external finance and for industries with high levels of intangible assets.\n\nWhile all of these results are consistent with a \"credit channel\" view of the impact of banking crises on real economic activity, we certainly need to explore some alternative explanations before drawing final conclusions.  In particular, many factors may be correlated with the level of financial development of a country, so we want to make sure that the level of financial development is not simply standing in for something else.  The differences in financial development, for example, can arise from historical, political, cultural, and legal reasons.  There is a well-developed literature emphasizing, for instance, that the nature of a country's legal system and the manner in which laws are enforced can have an effect on the development of its financial system.9  Similarly, other country-specific factors also might have an influence on how financial institutions and markets behave.  If one of these factors, rather than financial development, is driving the results, we might have to interpret those results differently.\n\nAs a way to try to address such questions of interpretation, we controlled for the quality of the political and legal institutions in the countries included in our data analysis.  We found that our results still hold, namely that a banking crisis has a more pronounced effect on a country's economy when its financial system is more developed and when its firms rely more on external finance, even after taking into account a variety of proxies for the differences in the quality of institutions and legal structures across countries.\n\nFurthermore, we explored differences in the effect of banking crises on countries based on their overall levels of development.  In other words, we wanted to assess separately the impact of banking crises in less-developed countries versus more-developed countries since, of course, overall economic development and financial sector development tend to be correlated.  Interestingly, when we remove Organization for Economic Co-operation and Development (OECD) countries from our sample, the effects of banking crises were more pronounced, indicating that banking crises in emerging market countries likely do more harm to the overall economy than in developed countries.\n\nInterestingly, our research also provides evidence that crises affecting the banking sector can have a more serious impact on the real economy than alternative economic disruptions such as economic contractions or shocks such as currency crises.10  That is, banking crises have represented a unique shock to a country's financial system.  These results suggest that the \"credit channel\" effects on real economic activity we document are operating through the banking system.\n\nIn short, the results highlight that a healthy banking system generally contributes to strong economic growth, and banking crises can present a substantial drag on the real economy.  This underscores why we consider it important to analyze the economic impact of financial crises involving the banking sector, and to mitigate potential drivers of such crises.  Indeed, in the case of Asia a decade ago, the trouble experienced by the region's banks, given their dominant role in Asian financial systems, created disruptions that spread across Asia.  Problems in the real economies of Asian countries would likely not have been so great had the banking systems been stronger.\n\nSo far I have been discussing results that focus on the real growth impact on sectors or industries.  We were able to obtain data on the impact on individual firms but for a smaller sample than for the sectors or industries.  We use a variety of measures for individual firm performance, such as real growth in sales, real growth in earnings before interest and taxes, and real stock market returns.  The results are the same as in the sectoral level data I described above:  The performance of firms heavily dependent upon external sources of funding is disproportionately negatively affected in deep financial systems during periods of banking crisis.\n\nIn addition to providing microeconomic support for the credit channel view, individual firm data also allow us to examine important hypotheses about the impact that transparency and public disclosure by individual firms can have to mitigate the effects of banking crises on those firms.  One author, using data from the Asian crisis, has investigated the relationship between stock market performance and quality of public disclosure.11  Our recent paper also delved into this issue by using certain proxies for individual firm disclosure practices, such as having a listing in the United States (hence disclosing by U.S. standards) or by being audited by one of the major international accounting firms.  Our results suggested that greater public disclosure had a favorable impact on stock market performance, both during the crisis period as well as afterward.  Indeed, there is a large literature supporting the idea that a country can benefit by promoting transparency and disclosure.  But I know that there is still ongoing debate about the effects of globalization on financial markets and economies and I believe that further research and discussion about the benefits of openness and transparency are worthwhile.  Perhaps a discussion on this topic will ensue at this conference over the next two days.\n\nI will pose one final topic before moving on to some policy suggestions.  I believe it is important to try to understand the long-term impact of a crisis on the financial system and the real economy.  In particular, it is useful to examine whether a particularly deep crisis or a recent one may have effects on participants in that country during the next crisis.  Interestingly, our recent work provided some evidence that past crises do not necessarily create a stoppage of bank credit to firms once \"normal\" times returned, but when the next crisis occurred, its impact on growth was usually deeper in magnitude.\n\nThoughts on Preventing and Managing Banking Crises\nHaving raised some issues for consideration about banking crises--and having tried to provide some answers, where possible--I would now like to offer some thoughts that policymakers may consider as they try to prevent and manage banking crises.  I note that market participants should also consider these ideas, since it is not just policymakers who can learn lessons from past crises.\n\nFirst, one of the major lessons to be learned from past banking crises is the importance of a healthy banking system.  Maintaining a safe and sound banking system, given the key role that banks play in most financial systems, contributes to the health of a country's overall economy.  Most countries do this by some form of banking supervision, generally accepting that the added protection to the banking system in the form of supervision is worth the costs of the regulatory burden.  Effective banking supervision has helped foster a banking system in the United States that today is safe, sound, and well-capitalized.\n\nOne of the ways that bank supervisors can help promote a healthy banking system is to focus banks on the development of improved risk-management techniques.  Indeed, identifying, assessing, and promoting sound risk-management practices have become central elements of good supervisory practice.  Bankers should ensure that their risk-management practices include a focus on less likely outcomes, not just the most common ones, and that the bank is being adequately compensated for the risk it is bearing.  The use of exercises such as stress tests and scenario analyses can help bankers identify certain points of vulnerability that may arise during potential downturns.\n\nIn some countries, bank supervisors have an explicit responsibility to ensure that banks adhere to existing laws and maintain and implement appropriate consumer protection policies.  For example, in the United States the federal banking agencies play an important role in fostering not just a safe and sound banking system, but also one that is diverse and fair in meeting the needs of consumers.\n\nGood banking supervision is vital to the health of banks, particularly when the banking system has some type of government support--for example, either explicit or implicit deposit insurance.  But ensuring a safe and sound banking system that is also competitive and profitable has its challenges.  Clearly, banking supervision on its own can create some distortions or burden, so it is also very important to let market forces work as much as possible, with reliance on market participants--in their role as depositors, counterparties, creditors, and shareholders--to exercise discipline on banks.  Policymakers have to find the right balance between the more visible hand of government supervision and the invisible hand of market forces.\n\nSecond, pursuing sound macroeconomic policies is another way for policymakers to help prevent banking and financial crises.  For instance, it is beneficial to have sound and sustainable monetary and fiscal policy to provide a stable operating environment for entrepreneurs and financial institutions and markets.  Many past crises were precipitated either by an external shock affecting an already vulnerable financial system or by market participants targeting vulnerabilities in certain markets or in certain institutions or governments.  History has taught us that if a condition or policy looks unsustainable, it most likely is and market forces will eventually bring it to an end.  To paraphrase Herb Stein, if a policy cannot go on forever, it won't!\n\nThird, I referred earlier to research on the benefits of disclosure and transparency.  Our analysis contributes to the evidence that having an open and transparent financial system and economy, accompanied by reliable and accurate accounting standards, generally benefits a country and its market participants.  A core principle of economics is that markets are more competitive, and therefore more efficient, when accurate information is available to both consumers and suppliers.  Information is thus critical to the effective functioning of financial markets:  timely and accurate financial information about markets, market participants, and governments is important for all actors to be able to make informed decisions.  This is of course true during normal times, but perhaps more so during a crisis when market participants and governments are sometimes trying to determine where problems lie and how severe they might be.  Lack of information can present additional problems during a crisis, and incorrect or incomplete information provided by firms, governments, and other institutions can severely undermine their credibility, worsening the problem.\n\nConclusion \nThese conferences on the Asian crisis serve as excellent forums to analyze the events of ten years ago and share views on ways to prevent and mitigate future crises.  They also allow market participants to offer feedback on past policy steps, including which past policies helped and which ones hindered.  Policymakers and market participants must remember that preventing and mitigating financial crises requires a blend of sharp analysis, keen judgment, practical experience, and rigorous understanding of how markets work--in both normal times and times of stress.\n\nI have tried to provide a framework for analyzing the impact of banking crises on real economic activity, described some results analyzing crises from around the world during the last quarter century, and offered three policy lessons:  the importance of a healthy banking system, of sound macroeconomic policy, and of high levels of transparency and disclosure.  This is by no means an exhaustive list but I hope it can provide a useful starting point for the discussions that will take place during the rest of the day.\n\nAs a final thought, I counsel policymakers and market participants alike to remember that no two crises are the same.  Recall that the Asian crisis of 1997-98 actually manifested itself differently across Asian countries, with each country having its own set of problems and needing to find its own solutions.  In other words, there is never a single remedy to each crisis and each brings its own surprises and risks.  Clearly, we can all learn a lot from past crises--which is the value in holding conferences like this one.  But we should not assume that past remedies will fully solve the next set of problems or address all future risks.  The key is to take lessons from the past and tailor them appropriately to future situations of potential distress.\n\nReferences\n\nBeck, T., Levine, R., Loayza, N. (2000). \"Finance and the Sources of Growth,\" Journal of Financial Economics, vol. 58, pp. 261–300.\n\nBraun, M., Larrain, B., (2005). \"Finance and the Business Cycle: International, Inter-industry Evidence.\" Journal of Finance,  vol. 60 (June), pp. 1097–1128.\n\nCaprio, G., Klingbiel, D. (2002).  \"Episodes of Systemic and Borderline Financial Crises.\"  In: Klingebiel, D., Laeven, L. (Eds.), Managing the Real and Fiscal Effects of Banking Crises.  World Bank Discussion Paper No. 428, Washington, DC, pp. 31–49.\n\nKing, R.G., Levine, R. (1993).  \"Finance and Growth: Schumpeter Might Be Right.\" Quarterly Journal of Economics, vol. 153, pp. 717–738.\n\nKroszner, R.S., Strahan, P.E. (2005). \"Regulation and Deregulation of the U.S. Banking Industry: Causes, Consequences, and Implications for the Future.  In: Rose, N. (Ed.), Economics of Regulation. NBER Conference Volume, Forthcoming.\n\nKroszner, Randall and Luc Laeven and Daniela Klingbiel (2007).  \"Banking Crises, Financial Dependence, and Growth, Journal of Financial Economics,\" vol. 87 (April), pp. 187-228.\n\nLa Porta, R., Lopez-de-Silanes, F., Shleifer, A., Vishny, R.W. (1998).  \"Law and Finance,\" Journal of Political Economy, vol. 106, pp. 1113–1155.\n\nLevine, R. (1997). \"Financial Development and Economic Growth: Views and Agenda (371 KB PDF),\" Journal of Economic Literature, vol. 35, pp. 688–726.\n\nLevine, R. (2005). \"Finance and Growth.\" In: Aghion, P., Durlauf, S. (Eds.), Handbook of Economic Growth. Elsevier Science, Amsterdam, The Netherlands, Forthcoming.\n\nMilesi-Ferretti, G.M., Razin, A. (1998).  \"Current account reversals and currency crises: empirical regularities (176 KB PDF).\"  NBER Working Paper No. 6620, Cambridge, MA.\n\nMitton, T. (2002). \"A Cross-Firm Analysis of the Impact of Corporate Governance on the East Asian Financial Crisis,\" Journal of Financial Economics, vol. 64, pp. 215–241.\n\nRajan, R.G., Zingales, L. (1998). \"Financial Dependence and Growth,\" American Economic Review, vol. 88, pp. 559–596.\n\nFootnotes\n\n1.  The views I express today are, of course, my own and do not necessarily represent those of the Board of Governors or the Federal Reserve System.   Return to text\n\n2.  Kroszner, Laeven, and Klingbiel (2007). Return to text\n\n3.  Our paper provides a literature review. Return to text\n\n4.  Caprio and Klingbiel (2002). Return to text\n\n5.  See Levine (1997, 2005) and Kroszner and Strahan (2005) for overviews. Return to text\n\n6.  One measure of financial or external dependence is the fraction of capital expenditures not financing with cash flows from operations.  See Rajan and Zingales (1998).   Return to text\n\n7.  In our recent research paper, the proxy for depth of the financial system is the ratio of private credit to gross domestic product.   Return to text\n\n8.  Clearly, determining the precise timing of crises is difficult, both in terms of identifying the beginning and the end of a crisis. In our research we experimented with alternative definitions of crisis windows, expanding and contracting the length of the crisis, pre-crisis, and post-crisis periods as well as the gap between the crisis and the pre- and postcrisis periods; our results are not sensitive to the alternative definitions. Return to text\n\n9.  La Porta et al (1998). Return to text\n\n10.  For our definition of economic contractions, see Braun and Larrain (2005); for our definition of currency crises, see Milesi-Ferretti and Razin (1998). Return to text\n\n11.  Mitton (2002). Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20070906a.htm",
        "title": "Analyzing and Assessing Banking Crises",
        "date": "9/6/2007"
    },
    {
        "content": "August 31, 2007\n\nChairman Ben S. Bernanke\n\nAt the Federal Reserve Bank of Kansas City's Economic Symposium, Jackson Hole, Wyoming\n\nIn my remarks this morning, I will begin with some observations about recent market developments and their economic implications.  I will then try to place recent events in a broader historical context by discussing the evolution of housing markets and housing finance in the United States.  In particular, I will argue that, over the years, institutional changes in U.S. housing and mortgage markets have significantly influenced both the transmission of monetary policy and the economy's cyclical dynamics.  As our system of housing finance continues to evolve, understanding these linkages not only provides useful insights into the past but also holds the promise of helping us better cope with the implications of future developments.\n\nRecent Developments in Financial markets and the Economy\nI will begin my review of recent developments by discussing the housing situation.  As you know, the downturn in the housing market, which began in the summer of 2005, has been sharp.  Sales of new and existing homes have declined significantly from their mid-2005 peaks and have remained slow in recent months.  As demand has weakened, house prices have decelerated or even declined by some measures, and homebuilders have scaled back their construction of new homes.  The cutback in residential construction has directly reduced the annual rate of U.S. economic growth about 3/4 percentage point on average over the past year and a half.  Despite the slowdown in construction, the stock of unsold new homes remains quite elevated relative to sales, suggesting that further declines in homebuilding are likely.\n\nThe outlook for home sales and construction will also depend on unfolding developments in mortgage markets.  A substantial increase in lending to nonprime borrowers contributed to the bulge in residential investment in 2004 and 2005, and the tightening of credit conditions for these borrowers likely accounts for some of the continued softening in demand we have seen this year.  As I will discuss, recent market developments have resulted in additional tightening of rates and terms for nonprime borrowers as well as for potential borrowers through \"jumbo\" mortgages.  Obviously, if current conditions persist in mortgage markets, the demand for homes could weaken further, with possible implications for the broader economy.  We are following these developments closely.\n\nAs house prices have softened, and as interest rates have risen from the low levels of a couple of years ago, we have seen a marked deterioration in the performance of nonprime mortgages.  The problems have been most severe for subprime mortgages with adjustable rates:  the proportion of those loans with serious delinquencies rose to about 13-1/2 percent in June, more than double the recent low seen in mid-2005.1  The adjustable-rate subprime mortgages originated in late 2005 and in 2006 have performed the worst, in part because of slippage in underwriting standards, reflected for example in high loan-to-value ratios and incomplete documentation.  With many of these borrowers facing their first interest rate resets in coming quarters, and with softness in house prices expected to continue to impede refinancing, delinquencies among this class of mortgages are likely to rise further.  Apart from adjustable-rate subprime mortgages, however, the deterioration in performance has been less pronounced, at least to this point.  For subprime mortgages with fixed rather than variable rates, for example, serious delinquencies have been fairly stable at about 5-1/2 percent.  The rate of serious delinquencies on alt-A securitized pools rose to nearly 3 percent in June, from a low of less than 1 percent in mid-2005.  Delinquency rates on prime jumbo mortgages have also risen, though they are lower than those for prime conforming loans, and both rates are below 1 percent.\n\nInvestors' concerns about mortgage credit performance have intensified sharply in recent weeks, reflecting, among other factors, worries about the housing market and the effects of impending interest-rate resets on borrowers' ability to remain current.  Credit spreads on new securities backed by subprime mortgages, which had jumped earlier this year, rose significantly more in July.  Issuance of such securities has been negligible since then, as dealers have faced difficulties placing even the AAA-rated tranches. Issuance of securities backed by alt-A and prime jumbo mortgages also has fallen sharply, as investors have evidently become concerned that the losses associated with these types of mortgages may be higher than had been expected.\n\nWith securitization impaired, some major lenders have announced the cancellation of their adjustable-rate subprime lending programs.  A number of others that specialize in nontraditional mortgages have been forced by funding pressures to scale back or close down.  Some lenders that sponsor asset-backed commercial paper conduits as bridge financing for their mortgage originations have been unable to \"roll\" the maturing paper, forcing them to draw on back-up liquidity facilities or to exercise options to extend the maturity of their paper.  As a result of these developments, borrowers face noticeably tighter terms and standards for all but conforming mortgages.\n\nAs you know, the financial stress has not been confined to mortgage markets.  The markets for asset-backed commercial paper and for lower-rated unsecured commercial paper market also have suffered from pronounced declines in investor demand, and the associated flight to quality has contributed to surges in the demand for short-dated Treasury bills, pushing T-bill rates down sharply on some days.  Swings in stock prices have been sharp, with implied price volatilities rising to about twice the levels seen in the spring.  Credit spreads for a range of financial instruments have widened, notably for lower-rated corporate credits.  Diminished demand for loans and bonds to finance highly leveraged transactions has increased some banks' concerns that they may have to bring significant quantities of these instruments onto their balance sheets.  These banks, as well as those that have committed to serve as back-up facilities to commercial paper programs, have become more protective of their liquidity and balance-sheet capacity.\n\nAlthough this episode appears to have been triggered largely by heightened concerns about subprime mortgages, global financial losses have far exceeded even the most pessimistic projections of credit losses on those loans.  In part, these wider losses likely reflect concerns that weakness in U.S. housing will restrain overall economic growth.  But other factors are also at work.  Investor uncertainty has increased significantly, as the difficulty of evaluating the risks of structured products that can be opaque or have complex payoffs has become more evident.  Also, as in many episodes of financial stress, uncertainty about possible forced sales by leveraged participants and a higher cost of risk capital seem to have made investors hesitant to take advantage of possible buying opportunities.  More generally, investors may have become less willing to assume risk.  Some increase in the premiums that investors require to take risk is probably a healthy development on the whole, as these premiums have been exceptionally low for some time.  However, in this episode, the shift in risk attitudes has interacted with heightened concerns about credit risks and uncertainty about how to evaluate those risks to create significant market stress.  On the positive side of the ledger, we should recognize that past efforts to strengthen capital positions and the financial infrastructure place the global financial system in a relatively strong position to work through this process.\n\nIn the statement following its August 7 meeting, the Federal Open Market Committee (FOMC) recognized that the rise in financial volatility and the tightening of credit conditions for some households and businesses had increased the downside risks to growth somewhat but reiterated that inflation risks remained its predominant policy concern.  In subsequent days, however, following several events that led investors to believe that credit risks might be larger and more pervasive than previously thought, the functioning of financial markets became increasingly impaired.  Liquidity dried up and spreads widened as many market participants sought to retreat from certain types of asset exposures altogether.\n\nWell-functioning financial markets are essential for a prosperous economy.  As the nation's central bank, the Federal Reserve seeks to promote general financial stability and to help to ensure that financial markets function in an orderly manner.  In response to the developments in the financial markets in the period following the FOMC meeting, the Federal Reserve provided reserves to address unusual strains in money markets.  On August 17, the Federal Reserve Board announced a cut in the discount rate of 50 basis points and adjustments in the Reserve Banks' usual discount window practices to facilitate the provision of term financing for as long as thirty days, renewable by the borrower.  The Federal Reserve also took a number of supplemental actions, such as cutting the fee charged for lending Treasury securities.  The purpose of the discount window actions was to assure depositories of the ready availability of a backstop source of liquidity.  Even if banks find that borrowing from the discount window is not immediately necessary, the knowledge that liquidity is available should help alleviate concerns about funding that might otherwise constrain depositories from extending credit or making markets.  The Federal Reserve stands ready to take additional actions as needed to provide liquidity and promote the orderly functioning of markets.\n\nIt is not the responsibility of the Federal Reserve--nor would it be appropriate--to protect lenders and investors from the consequences of their financial decisions.  But developments in financial markets can have broad economic effects felt by many outside the markets, and the Federal Reserve must take those effects into account when determining policy.  In a statement issued simultaneously with the discount window announcement, the FOMC indicated that the deterioration in financial market conditions and the tightening of credit since its August 7 meeting had appreciably increased the downside risks to growth.  In particular, the further tightening of credit conditions, if sustained, would increase the risk that the current weakness in housing could be deeper or more prolonged than previously expected, with possible adverse effects on consumer spending and the economy more generally.\n\nThe incoming data indicate that the economy continued to expand at a moderate pace into the summer, despite the sharp correction in the housing sector.  However, in light of recent financial developments, economic data bearing on past months or quarters may be less useful than usual for our forecasts of economic activity and inflation.  Consequently, we will pay particularly close attention to the timeliest indicators, as well as information gleaned from our business and banking contacts around the country.  Inevitably, the uncertainty surrounding the outlook will be greater than normal, presenting a challenge to policymakers to manage the risks to their growth and price stability objectives.  The Committee continues to monitor the situation and will act as needed to limit the adverse effects on the broader economy that may arise from the disruptions in financial markets.\n\nBeginnings:  Mortgage Markets in the Early Twentieth Century\nLike us, our predecessors grappled with the economic and policy implications of innovations and institutional changes in housing finance.  In the remainder of my remarks, I will try to set the stage for this weekend's conference by discussing the historical evolution of the mortgage market and some of the implications of that evolution for monetary policy and the economy.\n\nThe early decades of the twentieth century are a good starting point for this review, as urbanization and the exceptionally rapid population growth of that period created a strong demand for new housing.  Between 1890 and 1930, the number of housing units in the United States grew from about 10 million to about 30 million; the pace of homebuilding was particularly brisk during the economic boom of the 1920s.\n\nRemarkably, this rapid expansion of the housing stock took place despite limited sources of mortgage financing and typical lending terms that were far less attractive than those to which we are accustomed today.  Required down payments, usually about half of the home's purchase price, excluded many households from the market.  Also, by comparison with today's standards, the duration of mortgage loans was short, usually ten years or less.  A \"balloon\" payment at the end of the loan often created problems for borrowers.2\n\nHigh interest rates on loans reflected the illiquidity and the essentially unhedgeable interest rate risk and default risk associated with mortgages.  Nationwide, the average spread between mortgage rates and high-grade corporate bond yields during the 1920s was about 200 basis points, compared with about 50 basis points on average since the mid-1980s.  The absence of a national capital market also produced significant regional disparities in borrowing costs.  Hard as it may be to conceive today, rates on mortgage loans before World War I were at times as much as 2 to 4 percentage points higher in some parts of the country than in others, and even in 1930, regional differences in rates could be more than a full percentage point.3\n\nDespite the underdevelopment of the mortgage market, homeownership rates rose steadily after the turn of the century.  As would often be the case in the future, government policy provided some inducement for homebuilding.  When the federal income tax was introduced in 1913, it included an exemption for mortgage interest payments, a provision that is a powerful stimulus to housing demand even today.  By 1930, about 46 percent of nonfarm households owned their own homes, up from about 37 percent in 1890.\n\nThe limited availability of data prior to 1929 makes it hard to quantify the role of housing in the monetary policy transmission mechanism during the early twentieth century.  Comparisons are also complicated by great differences between then and now in monetary policy frameworks and tools.  Still, then as now, periods of tight money were reflected in higher interest rates and a greater reluctance of banks to lend, which affected conditions in mortgage markets.  Moreover, students of the business cycle, such as Arthur Burns and Wesley Mitchell, have observed that residential construction was highly cyclical and contributed significantly to fluctuations in the overall economy (Burns and Mitchell, 1946).  Indeed, if we take the somewhat less reliable data for 1901 to 1929 at face value, real housing investment was about three times as volatile during that era as it has been over the past half-century.\n\nDuring the past century we have seen two great sea changes in the market for housing finance.  The first of these was the product of the New Deal.  The second arose from financial innovation and a series of crises from the 1960s to the mid-1980s in depository funding of mortgages.  I will turn first to the New Deal period.\n\nThe New Deal and the Housing Market\nThe housing sector, like the rest of the economy, was profoundly affected by the Great Depression.  When Franklin Roosevelt took office in 1933, almost 10 percent of all homes were in foreclosure (Green and Wachter, 2005), construction employment had fallen by half from its late 1920s peak, and a banking system near collapse was providing little new credit.  As in other sectors, New Deal reforms in housing and housing finance aimed to foster economic revival through government programs that either provided financing directly or strengthened the institutional and regulatory structure of private credit markets.\n\nActually, one of the first steps in this direction was taken not by Roosevelt but by his predecessor, Herbert Hoover, who oversaw the creation of the Federal Home Loan Banking System in 1932.  This measure reorganized the thrift industry (savings and loans and mutual savings banks) under federally chartered associations and established a credit reserve system modeled after the Federal Reserve.  The Roosevelt administration pushed this and other programs affecting housing finance much further.  In 1934, his administration oversaw the creation of the Federal Housing Administration (FHA).  By providing a federally backed insurance system for mortgage lenders, the FHA was designed to encourage lenders to offer mortgages on more attractive terms.  This intervention appears to have worked in that, by the 1950s, most new mortgages were for thirty years at fixed rates, and down payment requirements had fallen to about 20 percent.  In 1938, the Congress chartered the Federal National Mortgage Association, or Fannie Mae, as it came to be known.  The new institution was authorized to issue bonds and use the proceeds to purchase FHA mortgages from lenders, with the objectives of increasing the supply of mortgage credit and reducing variations in the terms and supply of credit across regions.4\n\nShaped to a considerable extent by New Deal reforms and regulations, the postwar mortgage market took on the form that would last for several decades.  The market had two main sectors.  One, the descendant of the pre-Depression market sector, consisted of savings and loan associations, mutual savings banks, and, to a lesser extent, commercial banks.  With financing from short-term deposits, these institutions made conventional fixed-rate long-term loans to homebuyers.  Notably, federal and state regulations limited geographical diversification for these lenders, restricting interstate banking and obliging thrifts to make mortgage loans in small local areas--within 50 miles of the home office until 1964, and within 100 miles after that.  In the other sector, the product of New Deal programs, private mortgage brokers and other lenders originated standardized loans backed by the FHA and the Veterans' Administration (VA).  These guaranteed loans could be held in portfolio or sold to institutional investors through a nationwide secondary market.\n\nNo discussion of the New Deal's effect on the housing market and the monetary transmission mechanism would be complete without reference to Regulation Q--which was eventually to exemplify the law of unintended consequences.  The Banking Acts of 1933 and 1935 gave the Federal Reserve the authority to impose deposit-rate ceilings on banks, an authority that was later expanded to cover thrift institutions.  The Fed used this authority in establishing its Regulation Q.  The so-called Reg Q ceilings remained in place in one form or another until the mid-1980s.5\n\nThe original rationale for deposit ceilings was to reduce \"excessive\" competition for bank deposits, which some blamed as a cause of bank failures in the early 1930s.  In retrospect, of course, this was a dubious bit of economic analysis.  In any case, the principal effects of the ceilings were not on bank competition but on the supply of credit.  With the ceilings in place, banks and thrifts experienced what came to be known as disintermediation--an outflow of funds from depositories that occurred whenever short-term money-market rates rose above the maximum that these institutions could pay.  In the absence of alternative funding sources, the loss of deposits prevented banks and thrifts from extending mortgage credit to new customers.\n\nThe Transmission Mechanism and the New Deal Reforms\nUnder the New Deal system, housing construction soared after World War II, driven by the removal of wartime building restrictions, the need to replace an aging housing stock, rapid family formation that accompanied the beginning of the baby boom, and large-scale internal migration.  The stock of housing units grew 20 percent between 1940 and 1950, with most of the new construction occurring after 1945.\n\nIn 1951, the Treasury-Federal Reserve Accord freed the Fed from the obligation to support Treasury bond prices.  Monetary policy began to focus on influencing short-term money markets as a means of affecting economic activity and inflation, foreshadowing the Federal Reserve's current use of the federal funds rate as a policy instrument.  Over the next few decades, housing assumed a leading role in the monetary transmission mechanism, largely for two reasons:  Reg Q and the advent of high inflation.\n\nThe Reg Q ceilings were seldom binding before the mid-1960s, but disintermediation induced by the ceilings occurred episodically from the mid-1960s until Reg Q began to be phased out aggressively in the early 1980s.  The impact of disintermediation on the housing market could be quite significant; for example, a moderate tightening of monetary policy in 1966 contributed to a 23 percent decline in residential construction between the first quarter of 1966 and the first quarter of 1967.  State usury laws and branching restrictions worsened the episodes of disintermediation by placing ceilings on lending rates and limiting the flow of funds between local markets.  For the period 1960 to 1982, when Reg Q assumed its greatest importance, statistical analysis shows a high correlation between single-family housing starts and the growth of small time deposits at thrifts, suggesting that disintermediation effects were powerful; in contrast, since 1983 this correlation is essentially zero.6\n\nEconomists at the time were well aware of the importance of the disintermediation phenomenon for monetary policy.  Frank de Leeuw and Edward Gramlich highlighted this particular channel in their description of an early version of the MPS macroeconometric model, a joint product of researchers at the Federal Reserve, MIT, and the University of Pennsylvania (de Leeuw and Gramlich, 1969).  The model attributed almost one-half of the direct first-year effects of monetary policy on the real economy--which were estimated to be substantial--to disintermediation and other housing-related factors, despite the fact that residential construction accounted for only 4 percent of nominal gross domestic product (GDP) at the time.\n\nAs time went on, however, monetary policy mistakes and weaknesses in the structure of the mortgage market combined to create deeper economic problems.  For reasons that have been much analyzed, in the late 1960s and the 1970s the Federal Reserve allowed inflation to rise, which led to corresponding increases in nominal interest rates.  Increases in short-term nominal rates not matched by contractually set rates on existing mortgages exposed a fundamental weakness in the system of housing finance, namely, the maturity mismatch between long-term mortgage credit and the short-term deposits that commercial banks and thrifts used to finance mortgage lending.  This mismatch led to a series of liquidity crises and, ultimately, to a rash of insolvencies among mortgage lenders.  High inflation was also ultimately reflected in high nominal long-term rates on new mortgages, which had the effect of \"front loading\" the real payments made by holders of long-term, fixed-rate mortgages.  This front-loading reduced affordability and further limited the extension of mortgage credit, thereby restraining construction activity.  Reflecting these factors, housing construction experienced a series of pronounced boom and bust cycles from the early 1960s through the mid-1980s, which contributed in turn to substantial swings in overall economic growth.\n\nThe Emergence of Capital Markets as a Source of Housing Finance\nThe manifest problems associated with relying on short-term deposits to fund long-term mortgage lending set in train major changes in financial markets and financial instruments, which collectively served to link mortgage lending more closely to the broader capital markets.  The shift from reliance on specialized portfolio lenders financed by deposits to a greater use of capital markets represented the second great sea change in mortgage finance, equaled in importance only by the events of the New Deal.\n\nGovernment actions had considerable influence in shaping this second revolution.  In 1968, Fannie Mae was split into two agencies: the Government National Mortgage Association (Ginnie Mae) and the re-chartered Fannie Mae, which became a privately owned government-sponsored enterprise (GSE), authorized to operate in the secondary market for conventional as well as guaranteed mortgage loans.  In 1970, to compete with Fannie Mae in the secondary market, another GSE was created--the Federal Home Loan Mortgage Corporation, or Freddie Mac.  Also in 1970, Ginnie Mae issued the first mortgage pass-through security, followed soon after by Freddie Mac.  In the early 1980s, Freddie Mac introduced collateralized mortgage obligations (CMOs), which separated the payments from a pooled set of mortgages into \"strips\" carrying different effective maturities and credit risks.  Since 1980, the outstanding volume of GSE mortgage-backed securities has risen from less than $200 billion to more than $4 trillion today.  Alongside these developments came the establishment of private mortgage insurers, which competed with the FHA, and private mortgage pools, which bundled loans not handled by the GSEs, including loans that did not meet GSE eligibility criteria--so-called nonconforming loans. Today, these private pools account for around $2 trillion in residential mortgage debt.\n\nThese developments did not occur in time to prevent a large fraction of the thrift industry from becoming effectively insolvent by the early 1980s in the wake of the late-1970s surge in inflation.7  In this instance, the government abandoned attempts to patch up the system and instead undertook sweeping deregulation.  Reg Q was phased out during the 1980s; state usury laws capping mortgage rates were abolished; restrictions on interstate banking were lifted by the mid-1990s; and lenders were permitted to offer adjustable-rate mortgages as well as mortgages that did not fully amortize and which therefore involved balloon payments at the end of the loan period.  Critically, the savings and loan crisis of the late 1980s ended the dominance of deposit-taking portfolio lenders in the mortgage market.  By the 1990s, increased reliance on securitization led to a greater separation between mortgage lending and mortgage investing even as the mortgage and capital markets became more closely integrated.  About 56 percent of the home mortgage market is now securitized, compared with only 10 percent in 1980 and less than 1 percent in 1970.\n\nIn some ways, the new mortgage market came to look more like a textbook financial market, with fewer institutional \"frictions\" to impede trading and pricing of event-contingent securities.  Securitization and the development of deep and liquid derivatives markets eased the spreading and trading of risk.  New types of mortgage products were created.  Recent developments notwithstanding, mortgages became more liquid instruments, for both lenders and borrowers.  Technological advances facilitated these changes; for example, computerization and innovations such as credit scores reduced the costs of making loans and led to a \"commoditization\" of mortgages.  Access to mortgage credit also widened; notably, loans to subprime borrowers accounted for about 13 percent of outstanding mortgages in 2006.\n\nI suggested that the mortgage market has become more like the frictionless financial market of the textbook, with fewer institutional or regulatory barriers to efficient operation.  In one important respect, however, that characterization is not entirely accurate.  A key function of efficient capital markets is to overcome problems of information and incentives in the extension of credit.  The traditional model of mortgage markets, based on portfolio lending, solved these problems in a straightforward way:  Because banks and thrifts kept the loans they made on their own books, they had strong incentives to underwrite carefully and to invest in gathering information about borrowers and communities.  In contrast, when most loans are securitized and originators have little financial or reputational capital at risk, the danger exists that the originators of loans will be less diligent.  In securitization markets, therefore, monitoring the originators and ensuring that they have incentives to make good loans is critical.  I have argued elsewhere that, in some cases, the failure of investors to provide adequate oversight of originators and to ensure that originators' incentives were properly aligned was a major cause of the problems that we see today in the subprime mortgage market (Bernanke, 2007).  In recent months we have seen a reassessment of the problems of maintaining adequate monitoring and incentives in the lending process, with investors insisting on tighter underwriting standards and some large lenders pulling back from the use of brokers and other agents.  We will not return to the days in which all mortgage lending was portfolio lending, but clearly the originate-to-distribute model will be modified--is already being modified--to provide stronger protection for investors and better incentives for originators to underwrite prudently.\n\nThe Monetary Transmission Mechanism Since the Mid-1980s\nThe dramatic changes in mortgage finance that I have described appear to have significantly affected the role of housing in the monetary transmission mechanism.  Importantly, the easing of some traditional institutional and regulatory frictions seems to have reduced the sensitivity of residential construction to monetary policy, so that housing is no longer so central to monetary transmission as it was.8  In particular, in the absence of Reg Q ceilings on deposit rates and with a much-reduced role for deposits as a source of housing finance, the availability of mortgage credit today is generally less dependent on conditions in short-term money markets, where the central bank operates most directly.\n\nMost estimates suggest that, because of the reduced sensitivity of housing to short-term interest rates, the response of the economy to a given change in the federal funds rate is modestly smaller and more balanced across sectors than in the past.9  These results are embodied in the Federal Reserve's large econometric model of the economy, which implies that only about 14 percent of the overall response of output to monetary policy is now attributable to movements in residential investment, in contrast to the model's estimate of 25 percent or so under what I have called the New Deal system.\n\nThe econometric findings seem consistent with the reduced synchronization of the housing cycle and the business cycle during the present decade.  In all but one recession during the period from 1960 to 1999, declines in residential investment accounted for at least 40 percent of the decline in overall real GDP, and the sole exception--the 1970 recession--was preceded by a substantial decline in housing activity before the official start of the downturn.  In contrast, residential investment boosted overall real GDP growth during the 2001 recession.  More recently, the sharp slowdown in housing has been accompanied, at least thus far, by relatively good performance in other sectors.  That said, the current episode demonstrates that pronounced housing cycles are not a thing of the past.\n\nMy discussion so far has focused primarily on the role of variations in housing finance and residential construction in monetary transmission.  But, of course, housing may have indirect effects on economic activity, most notably by influencing consumer spending.  With regard to household consumption, perhaps the most significant effect of recent developments in mortgage finance is that home equity, which was once a highly illiquid asset, has become instead quite liquid, the result of the development of home equity lines of credit and the relatively low cost of cash-out refinancing.  Economic theory suggests that the greater liquidity of home equity should allow households to better smooth consumption over time.  This smoothing in turn should reduce the dependence of their spending on current income, which, by limiting the power of conventional multiplier effects, should tend to increase macroeconomic stability and reduce the effects of a given change in the short-term interest rate.  These inferences are supported by some empirical evidence.10\n\nOn the other hand, the increased liquidity of home equity may lead consumer spending to respond more than in past years to changes in the values of their homes; some evidence does suggest that the correlation of consumption and house prices is higher in countries, like the United States, that have more sophisticated mortgage markets (Calza, Monacelli, and Stracca, 2007).  Whether the development of home equity loans and easier mortgage refinancing has increased the magnitude of the real estate wealth effect--and if so, by how much--is a much-debated question that I will leave to another occasion.\n\nConclusion\nI hope this exploration of the history of housing finance has persuaded you that institutional factors can matter quite a bit in determining the influence of monetary policy on housing and the role of housing in the business cycle.  Certainly, recent developments have added yet further evidence in support of that proposition.  The interaction of housing, housing finance, and economic activity has for years been of central importance for understanding the behavior of the economy, and it will continue to be central to our thinking as we try to anticipate economic and financial developments.\n\nIn closing, I would like to express my particular appreciation for an individual whom I count as a friend, as I know many of you do:  Edward Gramlich.  Ned was scheduled to be on the program but his illness prevented him from making the trip.  As many of you know, Ned has been a research leader in the topics we are discussing this weekend, and he has just finished a very interesting book on subprime mortgage markets.  We will miss not only Ned's insights over the course of this conference but his warmth and wit as well.  Ned and his wife Ruth will be in the thoughts of all of us.\n\nReferences\n\nBenito, A., J. Thompson, M. Waldron, and R. Wood (2006). \"House Prices and Consumer Spending (420 KB PDF),\" Bank of England Quarterly Bulletin, vol. 46 (Summer), 142-54.\n\nBennett, P., R. Peach, and S. Peristiani (2001). \"Structural Change in the Mortgage Market and the Propensity to Refinance,\" Journal of Money, Credit and Banking, vol. 33 (no. 4), pp. 955-75.\n\nBernanke, Ben S. (2007). \"The Subprime Mortgage Market,\" speech delivered at the Federal Reserve Bank of Chicago's 43rd Annual Conference on Bank Structure and Competition, Chicago, May 17, www.federalreserve.gov/boarddocs/speeches/2007.\n\nBurns, Arthur F., and Wesley C. Mitchell (1946). Measuring Business Cycles (New York: National Bureau of Economic Research).\n\nCalza, A., T. Monacelli, and L. Stracca (2007). \"Mortgage Markets, Collateral Constraints, and Monetary Policy: Do Institutional Factors Matter?\" CFS Working Paper Series No. 2007/10 (Frankfurt: Center for Financial Studies).\n\nde Leeuw, Frank, and Edward M. Gramlich (1969). \"The Channels of Monetary Policy:  A Further Report on the Federal Reserve-MIT Model,\" Journal of Finance, vol. 24 (May, Papers and Proceedings of the American Finance Association), pp. 265-90.\n\nDynan, Karen E., Douglas W. Elmendorf, and Daniel E. Sichel (2005). \"Can Financial Innovation Help to Explain the Reduced Volatility of Economic Activity?\" Journal of Monetary Economics, vol. 53 (January), pp. 123-50.\n\nEstrella, Arturo (2002). \"Securitization and the Efficacy of Monetary Policy (568 KB PDF),\" Federal Reserve Bank of New York, Economic Policy Review, vol. 9 (May), pp. 243-55.\n\nGreen, Richard K., and Susan M. Wachter (2005). \"The American Mortgage in Historical and International Context (190 KB PDF),\" Journal of Economic Perspectives, vol. 19 (no. 4), pp. 93-114.\n\nHurst, E., and F. Stafford (2004). \"Home is Where the Equity Is:  Mortgage Refinancing and Household Consumption,\" Journal of Money, Credit and Banking, vol. 36 (no. 6), pp. 985-1014.\n\nMahoney, Patrick I., and Alice P. White (1985). \"The Thrift Industry in Transition,\" Federal Reserve Bulletin, vol. 71 (March), pp. 137-56.\n\nMcCarthy, J., and R. Peach (2002). \"Monetary Policy Transmission to Residential Investment (225 KB PDF),\" Federal Reserve Bank of New York, Economic Policy Review, vol. 8 (no. 1), pp. 139-58.\n\nSnowden, Kenneth A. (1987). \"Mortgage Rates and American Capital Market Development in the Late Nineteenth Century,\" Journal of Economic History, vol. 47 (no. 3), pp. 671-91.\n\nU.S. Department of Commerce (1937). Financial Survey of Urban Housing (Washington: Government Printing Office).\n\nWeiss, Marc A. (1989). \"Marketing and Financing Home Ownership: Mortgage Lending and Public Policy in the United States, 1918-1989 (617 KB PDF),\" in Business and Economic History, series 2, vol. 18, William J. Hausman, ed., Wilmington, Del.: Business History Conference, pp. 109-18.\n\nFootnotes\n\n1.  Estimates of delinquencies are based on data from First American LoanPerformance.    Return to text\n\n2.  Weiss (1989) provides an overview of the evolution of mortgage lending over the past 100 years. Return to text\n\n3.  Snowden (1987) discusses regional variations in home mortgage rates at the end of the nineteenth century.  In addition, the U.S. Department of Commerce (1937) provides information on mortgage rates for various U.S. cities for the 1920s and early 1930s.  Return to text\n\n4.  Later, in anticipation of the end of World War II, the Congress created the Veterans' Administration Home Loan Guarantee Program, which supported mortgage lending to returning GIs on attractive terms, often including little or no down-payment requirement.  In 1948, the Congress authorized Fannie Mae to purchase these VA loans as well. Return to text\n\n5.  Regulation Q provisions that still exist restrict banks' ability to pay interest on some deposits, but these remaining provisions have little effect on the ability of depository institutions to raise funds.  Return to text\n\n6.  In detrended data, the correlation between quarterly single-family housing starts and the growth of small time deposits at thrifts during the preceding quarter was 0.53 for the 1960-1982 period; since 1983, this correlation has fallen to -0.02. Return to text\n\n7.  Mahoney and White (1985) reported that the net worth of 156 thrift institutions was less than 1 percent of assets in 1984; when reported net worth was adjusted to exclude regulatory additions that did not represent true capital, this figure swelled to 253. Return to text\n\n8.  Institutional factors can still be relevant, however, as can be seen by international comparisons.  For example, in the United Kingdom, where the predominance of adjustable-rate mortgages makes changes in short-term interest rates quite visible to borrowers and homeowners, housing has a significant role in the monetary transmission mechanism through cash-flow effects on consumption, among other channels (Benito, Thompson, Waldron and Wood, 2006).  Although adjustable-rate mortgages have become more important in the United States and now account for about 40 percent of the market, most adjustable-rate mortgages here are actually hybrids in that they bear a fixed rate for the first several years of the loan. Return to text\n\n9.  For example, McCarthy and Peach (2002) report a substantial decline in the short-run, though not long-run, interest elasticity of residential investment and real GDP after the early 1980s.  Work by Dynan, Elmendorf, and Sichel (2006) supports this conclusion as does other work at the Federal Reserve on models for forecasting residential investment.  Modeling work at the Fed also shows that the short-run sensitivity of residential investment to nominal mortgage rates fell by more than half after the end of the New Deal system, but, in line with the findings of McCarthy and Peach, remained largely static after 1982.  Estrella (2002) finds that secular changes in mortgage securitization have reduced the interest sensitivity of housing to short-term interest rates and the response of real output to an unanticipated change in monetary policy. Return to text\n\n10.  Dynan, Elmendorf and Sichel (2006) argue that financial innovation has made it easier for households to use the equity in their homes to buffer their spending against income shocks, thereby reducing the volatility of aggregate consumption.  Studies by Hurst and Stafford (2004) and Bennett, Peach and Peristiani (2001) provide indirect evidence supporting this argument. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070831a.htm",
        "title": "Housing, Housing Finance, and Monetary Policy",
        "date": "8/31/2007"
    },
    {
        "content": "August 01, 2007\n\nGovernor Randall S. Kroszner\n\nAt the Interagency Minority Depository Institutions National Conference, Miami, Florida\n\nIt is a pleasure to be here today to talk about issues and programs of mutual interest, with a focus on the Federal Reserve's work to strengthen our support of minority-owned depository institutions.  The Federal Reserve recognizes the important role that minority-owned banks play in the financial services market through the services they provide to their communities.  I have chaired the oversight committee on consumer and community affairs at the Board since March and am especially excited to be involved with a number of new and ongoing initiatives.  In particular, this morning I am delighted to announce a new program that formalizes the Federal Reserve's long-standing commitment to minority-owned depository institutions.  In addition, I will discuss several ongoing supervisory and regulatory initiatives that aim to improve consumer information and expand consumer protection.\n\nFederal Reserve System's Minority-Owned Institutions (MOI) Program\nNationally, there about 200 minority-owned depository institutions serving a broad range of communities and populations.  These banks have long played a unique and important role in our banking system by providing access to credit and financial services in markets that have historically been underserved.  The Federal Reserve, which supervises just under twenty of these institutions, is committed to promoting the success of the minority-owned depository institutions we oversee.  Supporting these institutions is fundamental to our overall supervisory responsibilities for ensuring a safe, sound, and competitive banking system that also protects consumers.  We have consistently provided ongoing assistance, through our regulatory, supervisory, and community development functions, to address the unique challenges and needs of minority-owned banks, while at the same time holding them to the high standards of supervisory performance.\n\nThe Federal Reserve has tapped the expertise at the Federal Reserve Bank of Philadelphia to spearhead development of a proactive training and technical assistance program for minority-owned depository institutions.  The new minority-owned institutions (MOI) program I will discuss today reflects our experience in addressing the needs of these institutions as well as the insights offered in a recent report issued by the Government Accountability Office.\n\nIn developing the program, Federal Reserve staff met with a number of minority-owned and de novo banking organizations across the country, as well as trade groups, bank consultants, and other federal banking agencies, to gain the insight into the challenges the organizations confront in raising capital, growing their institutions, and attracting talent.  This process provided valuable insight into the needs of minority-owned banks and contributed significantly to the design of the program.  The outreach and market research efforts also informed our understanding of the various issues that de novo and smaller institutions face in general.  As a result, some elements of the curriculum may ultimately have broader applicability.  Certainly, the components that address broad challenges, coupled with more-targeted training, will address the unique information and technical assistance needs of minority-owned banks.  To ensure broad access to the program, all aspects of the training program will be available through a variety of distribution channels:  classroom-style workshops, self-paced PC-based programs, and a web-based resource and information center.\n\nThe innovative MOI program consists of three modules, each focusing on issues that are most relevant at a particular point in a bank's life cycle:  \"Starting Up,\" \"Managing Initial Growth,\" and \"Managing a Mature Institution.\"  The program thus will be valuable for potential entrants as well as those that have been in the market for many years.  I think of the new MOI program as a type of course of study in a contemporary MBA curriculum, one that draws on insights and experiences from economics, accounting, finance, and compliance and focuses on the particular challenges of establishing and sustaining robust and vibrant minority-owned depository institutions.\n           \nThe first module, \"Starting Up,\" addresses the steps in filing an application and other issues related to a bank's chartering process, such as raising capital, assembling a successful board and management team, and conducting market analyses.  The second module, \"Managing Initial Growth,\" targets institutions that are managing growth and other transitions that occur during the first five years of their existence.  Training at this stage focuses on the institution's need to stabilize operations in a competitive environment and addresses issues essential to sustainability, such as maintaining capital and liquidity, managing credit and interest rate risk, ensuring compliance with banking laws and regulations, and developing new products.  The third module, \"Managing a Mature Institution,\" focuses on growing the institution and shareholder value.  Participants in this portion of the training will learn more about how to achieve growth targets in a safe and sound manner, measure performance of the board and management team, and expand their market.\n\nWhile these training modules are comprehensive in scope, the course designers recognize that there may be additional information and training needs.  A great deal of flexibility is being built into the curriculum, so that an expanded version of the modules can be provided to institutions that have specific concerns or issues.  In addition, each module features a section on supervisory and regulatory relations, to help build and maintain a strong dialogue between minority-owned banks and their regulators.  The program also provides for quality control and continuing assessment, through such elements as instructor coaching and curriculum evaluation, to ensure that the course remains responsive to the target audiences' needs.\n\nI am also excited about a related benefit of the new MOI program--the possibility of enhancing the Federal Reserve's role as a supervisor of these banks.  Concepts underpinning the MOI training program are being incorporated into our examiner education curricula, providing an opportunity for a deeper understanding within the supervision community of the issues unique to minority-owned depositories.  This responds directly to the interests expressed by participants in the outreach and market research phase of program development.  Many of the bankers indicated that it was important to improve understanding of their business models and strategies among the supervisory agencies, while at the same time holding their minority-owned institutions to the same supervisory standards as other depository institutions.\n\nA pilot program will be launched this fall, with a number of workshops to be held in selected Federal Reserve Districts and a roll-out of some web-based training.  Throughout the pilot period and after launch of the full program in early 2008, the Federal Reserve will continue to work with the industry and interagency partners to identify ways to increase the training's responsiveness.  This innovative MOI initiative underscores our commitment to providing essential information and supervisory support that helps institutions both improve their operating efficiency--thereby reducing costs and, ultimately, regulatory burden--and enhance their ability to serve their communities more effectively.\n\nFederal Reserve's Functions and Roles in Bank Oversight and Consumer Protection\n\nThe Federal Reserve's MOI program demonstrates a long-standing commitment to fostering a banking system that is competitive, diverse, and fair, to the benefit of both banking organizations and consumers.  Toward this end, we use all the supervisory, regulatory, and other tools available to us.  One example is the recently issued proposal on interagency Community Reinvestment Act (CRA) guidelines, which seeks to affirm that banks' investments in minority-owned banks receive favorable consideration under the investment test.  Now I'd like to discuss some of those tools and several recent Federal Reserve initiatives to address current issues of concern, with an emphasis on consumer protection.\n\nThe Federal Reserve fulfills its consumer protection mandate through four complementary processes.  The first process is examining supervised financial institutions for safety and soundness, and for compliance with consumer protection laws and regulations.  This includes taking supervisory action as appropriate to enforce the laws and resolve any consumer complaints.  The second is rulewriting--issuing regulations, either separately or jointly with other federal agencies, to implement the consumer financial services and fair lending laws.  The third is promoting consumer education through publications and through partnerships with other organizations.  And the fourth is promoting community development and fair and impartial access to credit by conducting outreach activities in lower-income communities and traditionally underserved markets.\n\nSupervisory Efforts on Subprime Mortgage Lending\nWith respect to supervision, the Federal Reserve exercises its examination and enforcement authority to monitor and address issues of concern at the banks it oversees.  In addition, we often issue guidance to provide the industry with supervisory \"best practices\" for addressing safety and soundness and consumer protection matters.  Guidance is generally a more-responsive supervisory tool than rules, and it allows institutions more flexibility in adopting new business practices.\n\nOver the years, the Federal Reserve, in concert with other supervisory agencies, has issued a series of supervisory guidance documents to address subprime and nontraditional mortgage lending issues.  The guidance has underscored the importance of a well-structured risk-management program for subprime lenders and has stressed that lending standards should include well-defined underwriting parameters, such as acceptable loan-to-value ratios and debt-to-income ratios, and minimum acceptable credit scores.  Guidance has also been used to address common predatory and abusive lending practices, including making unaffordable loans, inducing a borrower to refinance a loan repeatedly to generate fee income, and engaging in fraud or deception to conceal the true nature of the loan obligation.\n\nMost recently, supervisory guidance has emphasized the added dimension of risk when higher-risk loans are combined with other features--such as the use of simultaneous second lien loans in lieu of a down payment or the use of underwriting that involves little or no documentation of income or assets.  Guidance has also underscored the safety and soundness and consumer protection concerns prompted by other underwriting practices often seen in subprime and nontraditional mortgage lending, such as excluding taxes and insurance in the underwriting process and allowing deferred repayment of principal by offering interest-only loans.  Finally, this supervisory tool has been used to urge creditors to work with homeowners who are unable to make mortgage payments.\n\nReviews of Non-Depository Subprime Mortgage Lenders\nThis supervisory guidance, together with market forces, has contributed to adjustments in subprime lending practices, particularly by federally supervised banks.  However, we are all keenly aware that ongoing issues in the subprime market require additional action to address the very real and significant impact that rising delinquencies and foreclosures are having on individuals and communities.\n\nVarious outreach and research efforts by the Federal Reserve have deepened our understanding of the issues and revealed that many of the most-worrisome practices are found in credit extensions by nondepository lenders and brokers, many of which are outside the supervisory scope of the federal banking agencies.  The Federal Reserve Board is committed to deterring abusive lending and ensuring that all lenders are complying with consumer protection laws.  To that end, the Board has taken the lead in launching a pilot project involving consumer compliance reviews of selected nondepository lenders that have significant subprime mortgage operations.  The Board's partners in the project are the Office of Thrift Supervision, the Federal Trade Commission, and the states (through the Conference of State Bank Supervisors and the American Association of Residential Mortgage Regulators).\n\nAs part of this project, the Board will examine nonbank subsidiaries of bank holding companies for compliance with laws that the Federal Reserve enforces.  These laws include the Truth in Lending Act, the Equal Credit Opportunity Act, the Home Ownership and Equity Protection Act, the Real Estate Settlement Procedures Act, and the Home Mortgage Disclosure Act.  The other partners in the project will conduct similar reviews of nondepository subsidiaries of thrift holding companies, independent mortgage lending companies, and mortgage brokers doing business with these entities.  The partner agencies will share information about the examinations, review the lessons learned, and seek additional ways to cooperate to ensure effective and consistent supervision of these entities.  At the conclusion of the reviews, the agencies will analyze the results and determine whether the project is to be continued and, if so, what the focus of future reviews will be.\n\nRegulatory Initiatives\nIn addition to exercising supervisory authority, the Federal Reserve is using its regulatory power to review the Truth in Lending Act (TILA) regulations, with the intent of making the disclosures that help consumers understand their loan terms more effective.  To be effective, disclosures must give consumers information about credit pricing at a time when the information is relevant, in language they can easily understand.  The information must also be in a format that allows consumers to pick out and use the information that is most important to them.  Better credit disclosure facilitates better-informed credit decisions, and hence more-effective competition among creditors.  In a nutshell, effective disclosure empowers consumers and enhances competition.\n\nThe Federal Reserve Board has recently taken a new approach to increasing the effectiveness of disclosure--namely, surveying and responding to consumers through consumer testing.  Having taught at a business school for many years, I am well aware of the types of consumer testing that firms have long employed:  surveys, focus groups, and so-called \"mall intercepts,\" whereby shoppers are interviewed at random.  Systematically using such techniques to tailor the disclosure requirements set out in the regulations of the Federal Reserve and other financial regulators is, however, relatively novel.\n\nAfter extensive consumer testing, the Board issued a proposal in May that reflects consumers' thoughts on clearer and easier-to-understand credit card disclosures.  In particular, the proposed disclosures would highlight applicable rates and fees, especially penalties that might be imposed.  The proposed rules would also require card issuers to provide forty-five days' advance notice of a rate increase or any other change in account terms so that consumers will not be surprised by unexpected charges and will have time to explore alternatives.\n\nRecognizing the value of public input and our experiences in consumer testing, the Board is now engaged in a similar review of the TILA rules for mortgage loans.  This process began last summer when the Board convened four public hearings across the country to gather information on the adequacy of current mortgage disclosures, particularly disclosures for nontraditional and adjustable-rate products.  As with credit card lending, the Board is committed to conducting extensive consumer testing of proposed mortgage disclosures to identify what information consumers need, use, and retain when embarking on a search for a mortgage.  The process of designing and testing disclosures is necessarily time intensive.  However, it is a valuable investment, one that will ensure that we have a thorough review resulting in a substantive proposal that presents meaningful new disclosures.\n\nSimultaneously, the Federal Reserve is undertaking other projects that can be implemented more quickly.  For example, the Board plans to respond to concerns about misleading or incomplete marketing practices in the mortgage market.  By the end of the year, we will propose changes to TILA rules concerning mortgage loan advertisements and solicitations.  These proposed rules will also address issues relating to the timing of disclosures, to ensure not only that consumers get the information they need, but that they get it when it would be most useful to them.\n\nThe Federal Reserve recognizes that disclosures may not be sufficient to protect consumers from harmful mortgage products or terms.  Therefore, the Board plans to exercise its authority under the Home Ownership and Equity Protection Act (HOEPA) to address specific practices that are unfair or deceptive.  In June, I chaired a daylong public hearing at which we received valuable insight from both industry and consumer groups into the impact of specific mortgage practices on subprime borrowers.  Some of the practices we are looking at are pre-payment penalties, the waiver of escrow accounts for taxes and insurance, and the proliferation of stated-income or low-documentation loans.  Concerns have also been expressed about underwriting standards that do not fully take into consideration the borrower's ability to repay.  The Board is closely examining these issues and discussing possible remedies.  We expect to propose rules under HOEPA later this year.\n\nCommunity Affairs and Consumer Education Roles\nThe Federal Reserve also looks beyond its supervisory and regulatory functions to address concerns of information asymmetries and access to financial services in local markets.  A core principle of economics is that markets are more competitive, and therefore more efficient, when accurate information is available to both consumers and suppliers.\n\nThe Federal Reserve dedicates considerable resources to community development and consumer education efforts designed to help facilitate the flow of accurate information to communities and consumers, particularly those that have been traditionally underserved.  The Board, each of the twelve Federal Reserve Banks, and many of the Branch offices have a Community Affairs Office (CAO).  These offices work to promote community development and fair and impartial access to credit by focusing on low- and moderate-income consumers.  The community affairs function develops programs, partnerships, and initiatives that bring consumers into the financial and economic mainstream and that assist financial institutions in identifying viable opportunities in markets that may be underserved.  The CAOs also engage in research to help improve understanding of how financial services policies and practices affect lower-income households and neighborhoods.\n\nIn addition, the Federal Reserve has a robust consumer education program.  Through our consumer education publications, programs, and partnerships, the Federal Reserve informs consumers about financial transactions and the regulations that govern those transactions, to help them understand consumer protections and their rights when shopping for various products.  For example, the Board has published brochures to assist consumers when they are shopping for a credit card, looking for a mortgage, or leasing a vehicle.  We have also published brochures to help consumers understand their checking accounts and overdraft protection programs and to educate them on the effects of having their payments processed electronically.  Recently the Board has focused on helping consumers better understand nontraditional mortgage products and adjustable-rate mortgages (ARMs) and has prepared a consumer publication on these products.  In June, we launched an online mortgage calculator that allows consumers to easily compare their monthly payments, and the amount of equity they will build, under several kinds of fixed- and adjustable-rate mortgages.\n\nIn my time at the Federal Reserve, I have had the opportunity to see the benefits that community development and consumer education efforts offer.  As noted earlier, I am actively involved with the work of the Board's Division of Consumer and Community Affairs through my chairmanship of the oversight committee.  In addition, I am pleased to be serving on the board of NeighborWorks America® , a national nonprofit organization dedicated to promoting and preserving affordable housing for low- and moderate-income families and neighborhoods.  These roles give me valuable insight into the complexity of the financial services issues in these markets as well as an appreciation of the organizations and institutions that are committed to understanding these challenges and designing creative solutions to overcome them.\n\nConclusion\nIn closing, I would like thank you for the opportunity to underscore the Federal Reserve's commitment to promoting the vibrant, competitive, and diverse banking markets that are so vital to our economy.  We are dedicated to using our roles as supervisors, regulators, community development facilitators, and consumer educators to support the banking organizations that contribute to our robust financial services system--and the consumers who are vital to that system's success.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20070801a.htm",
        "title": "Federal Reserve Initiatives to Support Minority-Owned Institutions and Expand Consumer Protection",
        "date": "8/1/2007"
    },
    {
        "content": "July 12, 2007\n\nGovernor Randall S. Kroszner\n\nAt the New York Bankers Association Annual Washington Visit, Washington, D.C.\n\nGood morning. Thank you very much for the invitation to speak today. I hope that your visit here to Washington, D.C. provides you with useful information about regulatory and policy matters that affect your institutions. My remarks today address the latest developments on Basel II implementation in the United States.\n\nAs most of you know, the process for developing a revised international capital accord, known as Basel II, has been a long--and some might say painful--trek for both bankers and supervisors. Many countries around the world are already tailoring and implementing Basel II in their jurisdictions, while the U.S. banking agencies are in the process of finalizing their rules for implementation. The agencies have been considering the comments received on the Basel II proposals that were issued over the past year, and real progress has been made toward developing a workable rule. The substantial work to date by both the banking industry and supervisors has laid the foundation for moving the implementation process along, and I am optimistic about the current forward momentum in the United States to develop and implement a final rule for Basel II.\n\nImplementation of Basel II in the United States is necessary in order to ensure the safe and sound operation of our banking industry and the stability of our financial system. Basel II would promote continued improvements in bank risk management practices and would maintain capital levels in the U.S. banking system that are appropriate and risk-sensitive. As I will discuss in more detail, the existing Basel I capital regime has very limited risk sensitivity and is widely known to be outdated for large, complex banking organizations. If we retain Basel I for these institutions, we will be leaving in place a regulatory capital regime that could undermine the safety and soundness of our largest banking organizations by widening the gap between these banks' regulatory capital requirements and their actual risk profiles.\n\nThe Federal Reserve's role as the nation's central bank reinforces our belief in the importance of maintaining prudent and risk-sensitive capital requirements for financial institutions. Beyond its supervisory authority over individual banking organizations, the Federal Reserve is responsible for maintaining stable financial markets and ensuring a strong financial system. In this regard, the Federal Reserve has long required banking organizations to operate in a safe and sound manner, and to hold sufficient capital to protect against potential losses. Financial stability is enhanced when banks' regulatory capital measures adequately reflect risk, as well as when banks continually improve their risk-management practices. Since the Basel II regime is far superior to the current Basel I regime in aligning regulatory capital measures with risk and fostering continual improvements in risk management for our largest and most complex banking organizations, I believe it will contribute to a more resilient financial system.\n\nReasons for Basel II\nThe Federal Reserve believes very strongly that prudent and risk-sensitive regulatory capital requirements are integral to ensuring that individual banks and the financial system have an adequate cushion against losses, particularly during times of financial or economic stress. This strong belief is what motivated the Federal Reserve in the late 1980s to play a leading role in both negotiating the first international capital accord--Basel I--and supporting implementation of the accord in the United States. In light of our role in developing Basel I, let me explain why the Federal Reserve now supports moving to Basel II.\n\nFirst, although Basel I was a major step forward in capital risk sensitivity at the time, rapid and extensive evolution in the financial marketplace has substantially reduced the effectiveness of the Basel Irules for some U.S. banking organizations. The current Basel I regulatory capital rules are increasingly inadequate for large, internationally active banks that offer ever more complex and sophisticated products and services in an extremely competitive environment.\n\nThe flaws of the existing Basel I rule for large, complex U.S. banks are fairly well-known. The simple risk-bucketing approach in the existing Basel I rule, for example, creates perverse incentives for risk-taking. This approach--in which (1) the same amount of regulatory capital is assessed against all unsecured corporate loans and bonds regardless of actual risk, (2) all unsecured consumer credit card exposures are treated equivalently, and (3) almost all first-lien residential mortgage exposures are deemed equally risky--provides incentives for banking organizations to shed relatively low-risk exposures and acquire relatively high-risk exposures within each of these asset classes. The existing Basel I rule also ignores important elements of credit-risk mitigation--such as most forms of collateral, many guarantees and credit derivatives, and the maturity and seniority of an exposure--and thus blunts bank incentives to reduce or otherwise manage risk.\n\nMoreover, Basel I is particularly inadequate for dealing with capital-markets transactions, such as repurchase agreements, securities borrowing and lending, margin loans, and over-the-counter (OTC) derivatives. For example, the existing Basel I rule only imposes capital requirements on one side of a repurchase agreement, even though counterparty credit risk is present on both sides. For these reasons, a large and complex bank operating under Basel I can easily and significantly increase its credit risk, without increasing its regulatory capital.\n\nThis brings me to my second point: the advanced approaches of Basel II are designed to substantially reduce the perverse incentive effects and opportunities for regulatory capital arbitrage present in Basel I. In short, Basel II significantly increases the risk sensitivity of the capital rule. Under the advanced approaches, capital requirements for an exposure will vary on the basis of a bank's actual risk experience. If a bank increases the credit risk of its portfolio, its regulatory capital requirements will also increase, and vice versa. The enhanced risk sensitivity of Basel II will thus ensure that banks have positive incentives for lending to more creditworthy counterparties, for lending on a collateralized basis, for increasing loan seniorities, and for holding a larger capital cushion for higher-risk exposures. Basel II also includes sophisticated methods to address capital-markets transactions.\n\nThird, the Basel II regulatory capital framework has three pillars--minimum capital requirements, supervisory review of capital adequacy, and market discipline through disclosure--that build on the economic capital and other risk-management approaches of sophisticated banks and competing institutions. As a result, Basel II will be better able than the current system to adapt over time to innovations in banking and financial markets. The new framework should also establish a more coherent relationship between regulatory measures of capital adequacy and the day-to-day risk management conducted by banks.\n\nFinally, I would argue that one of the key benefits of the Basel II process is that it has prompted banks to make substantial progress in developing much more sophisticated risk-measurement and -management processes. For example, most international banks have adopted detailed rating systems for credit risk that assess both borrower and facility characteristics. That is, the banks assign one rating that reflects a borrower's overall creditworthiness, and another for each individual exposure that takes into account collateral, seniority, and other factors that affect how much a bank is likely to lose on that specific exposure if the borrower defaults. In addition, large banks are increasingly using common credit-risk measurement concepts, such as probability of default (PD), loss given default (LGD), and exposure at default (EAD). Together, these concepts help banks take a more granular approach to assessing the various drivers of credit risk, which in turn helps them to make more informed decisions about extending credit, mitigating risk, and determining capital needs. Another example of industry progress is in the measurement and management of operational risk. Under Basel II, banks are expected to weigh both quantitative and qualitative factors in order to assess potential future operational losses. As a result, Basel II has already helped the industry improve its methods for identifying and measuring risks--and for estimating the capital needed to support those risks.\n\nWe applaud these industry efforts, and we expect the Basel II framework to provide incentives for banks to continue improving their risk measurement and management on an ongoing basis. These developments not only benefit individual banks, but contribute to the resilience of the financial system as a whole. From a safety-and-soundness perspective, I believe it is critical that the industry not lose momentum in this area and that we ensure that Basel II promotes the continued improvement of risk-management processes at the largest U.S. banks.\n\nU.S. Basel II and Basel IA Proposals\nI would now like to turn to the Basel II proposal and the proposed set of revisions to the Basel I framework in the United States--the so-called Basel IA proposal--which I will also discuss very briefly.\n\nA fundamental part of the implementation process involves consideration of the comments on the Basel proposal. I have been deeply impressed with the thoughtful analysis reflected in those comments, and would like to thank all parties who took the time and effort to submit comments. Reviewing and considering comments takes time and extends the U.S. rulemaking process; nevertheless, we believe the comment process is essential. Quite simply, feedback from the industry and others leads to better rules. For something as important and far-reaching as Basel II, we understand the need to engage in a frank dialogue with the banking industry, Congress, and other relevant parties. Indeed, the Federal Reserve has been committed to an open interchange of ideas about the U.S. proposals since the start of the Basel II process. And we have found comments on our proposals to be invaluable in moving forward.\n\nA key theme voiced by the industry and many others is the need to have the Basel II process move forward expeditiously, and I heartily agree. Commenters also requested greater clarity on how the qualification for U.S. banks for Basel II would proceed and how much flexibility supervisors would apply when assessing compliance with the rules and related supervisory guidance, and I believe that such clarification is important.\n\nOne major concern raised in the comments is that the proposals differ markedly in certain respects from the Basel Committee's revised accord, first issued in June 2004 and updated in 2005, and now commonly known as the \"Mid-year Text.\" Although the U.S. proposals do diverge in a number of ways from the versions of Basel II being adopted in Europe and other industrialized countries, many of these divergences are in fact consistent with the national discretion built into the framework and used in most other countries. The U.S. proposals also included other divergences to adapt the international framework to the unique aspects of the U.S. banking system, to address issues raised through the earlier public comment process, and to ensure a safe and sound transition to Basel II. But many emphasized the need for less variance in Basel II across countries. Concerns about having to meet multiple versions of Basel II across countries are certainly reasonable ones, and I take these concerns quite seriously.\n\nI believe Basel II implementation in the United States should proceed in a manner that enhances consistency with implementation in other countries; Basel II is intended, after all, to be an international framework for internationally active banks. At the same time, the framework needs to accommodate robust U.S. supervisory practices and the unique aspects of our financial markets. I also believe that we have an obligation to retain only those divergences for which we are convinced the regulatory benefits exceed the implementation burden and costs.\n\nSome commenters also raised concerns about the complexity of the Basel II proposal. Yes, the Basel II advanced approaches are complex, but this reflects how complex our largest financial institutions have become. To be effective, risk-management practices have evolved in order to support the increasingly sophisticated services, business practices, and organizational structures of large, internationally active financial institutions. Hence, these banks already employ sophisticated risk-management practices and internal economic capital models. I fully support our proposal to review carefully these practices and models, among other factors, before granting the required supervisory approval for individual banks to use the advanced approaches of Basel II.\n\nDetermining the right level of complexity for U.S. Basel II rules remains an issue. While many bankers support the issuance of so-called principles-based regulations, some bankers have expressed a desire for more detail on certain aspects of the Basel II proposals in order to reduce uncertainty about what will be acceptable practice and what will not. My view is that these are not necessarily contradictory approaches. That is, we should take a principles-based approach that is sufficiently clear about our expectations but that is not so detailed that supervisors become de facto managers of the bank.\n\nTaking a more principles-based approach means that we must allow bankers some flexibility in meeting the requirements and permit a reasonable amount of diversity of practices across banking organizations. Such flexibility will allow banks to use and easily improve their existing risk-measurement and -management practices. More to the point, we should actively encourage such improvements. While the improvements in risk measurement and management envisioned under Basel II will require banks to bear the cost of investing in systems and human capital, we believe these institutions would have made these investments in any event, as they seek ways to effectively manage their own increasingly complex risks.\n\nAnother important issue to consider is the impact of potential distortions or unintended consequences created by the new framework. For example, if we see unreasonable declines in capital requirements at individual institutions that do not appear to be supported by either the bank's own internal capital adequacy assessments or by our supervisory view of the institution's risks and how well these risks are managed, we may seek to mitigate the impact of these declines through supervisory review and direct discussions with banks under Pillar 2--which could result in discretionary changes to capital at individual institutions.\n\nWe must also remain mindful of areas of the proposals that could unfairly tilt either the domestic or international competitive playing field if some banks have higher or lower capital requirements for certain activities or in the aggregate. One particular concern is that inconsistency in Basel II implementation across countries could put internationally active U.S. banks at a disadvantage and create advantages for U.S. investment banks and foreign banks. Achieving broad international consistency will be a challenge, but we should all remember that this problem is not really new.\n\nThe Federal Reserve and the other U.S. banking agencies have, for many years, worked with their international counterparts to limit the difficulties and burdens that have arisen as foreign banks have entered U.S. markets and as U.S. banks have established operations in other jurisdictions. We have continued this productive work with our colleagues overseas during the development and implementation of Basel II, but to most effectively tackle some of the issues that have come to our attention, we need to take the important next step of actually implementing Basel II for U.S. banking organizations. Once we do so, I believe that we can effectively manage the issues that arise, given our past experience with cross-border supervision.\n\nIn addition, some concerns have been voiced that adoption of a new capital framework for the largest and most complex U.S. banking organizations could disadvantage other U.S. banking organizations, particularly the smaller banks. In this regard, the Basel IA proposal was designed to modernize the existing Basel I framework in the United States and improve its risk sensitivity, without making it overly burdensome or complex for banks that are comparatively smaller and less complex. Moreover, Basel IA would not be required; smaller banks that wish to stay on the current Basel I framework would be allowed to do so. We are keenly aware of the need for capital requirements to make sense from the standpoint of both safety and soundness and competitiveness; we recognize that a one-size-fits-all approach is probably not feasible in this country, in light of our wide range of institutions. We remain sensitive to the principle that if we have multiple regulatory capital frameworks, they must work together to improve the safety and soundness of our entire banking system without artificially creating competitive inequalities.\n\nI want to emphasize that, amidst all of the detailed discussions and comments surrounding Basel II, the Federal Reserve continues to believe that strong capital serves the United States' interest in maintaining the safety, soundness, and resiliency of our banking system. We also know that banks maintain capital above these regulatory minimums in order to capture their full risk profiles, since minimum capital requirements do not necessarily cover all risks to which a given bank may be exposed. Banks also hold capital above regulatory minimums to support their strategic objectives. They know that customers, counterparties, creditors, and investors take into account overall bank capital adequacy when making investment or other business decisions. In addition, banks hold excess capital to be able to respond to potential business-expansion opportunities and to be able to manage the ups and downs of market- and credit-risk cycles. These market-based incentives should not change under Basel II. Indeed, I believe that greater transparency under Pillar 3 will enhance the role of market incentives in ensuring that banks hold sufficient capital.\n\nBy helping banks absorb unexpected losses, strong capital reduces the moral hazard associated with the federal safety net. A key lesson of the banking and thrift crises of the late 1980s and early 1990s is that prudent and explicit minimum regulatory capital requirements are needed to ensure that banks maintain adequate capital and to anchor an effective supervisory system. The Federal Reserve is strongly committed to the prompt-corrective-action (PCA) framework, which I have long supported,1 including the leverage ratio that will continue to bolster capital and complement risk-based measures. The PCA framework is important not only for the strong backstop it provides against declines in capital but also for the incentives it provides for banks to be considered well-capitalized, such as allowing holding companies to maintain their financial holding company status and, perhaps more importantly, meeting market expectations that banks will remain well-capitalized.\n\nTo ensure that banks maintain strong capital ratios, the U.S. banking agencies will continue to monitor the impact of Basel II during every step of its implementation. We will conduct extensive analysis of regulatory capital requirements produced by the new framework, as well as analyze the inputs behind the requirements. In addition, under the proposals, the agencies would determine whether any adjustments to the Basel II framework would be appropriate before removing the temporary floors that will be in place for a three-year transitional period. The use of these floors and other transitional safeguards during the first years that Basel II is in use will help ensure that there are no sudden drops in capital levels.\n\nThe Federal Reserve agrees with the Government Accountability Office (GAO) that (1)finalizing the U.S. Basel II rule will generate crucial information to enable the agencies to make future assessments of the strengths and weaknesses of the Basel II rule for the U.S. banking system and (2) the agencies should continue to evaluate during the transition period whether the advanced approaches of Basel II provide an appropriate regulatory capital framework for U.S. banking organizations.2 Moreover, we believe that this review should be as robust and transparent as possible, including active and meaningful dialogue among the agencies, the industry, market participants, Congress, and other interested parties. We seek to have strong risk-based capital ratios for large, complex banking organizations under Basel II that are substantially more meaningful, more representative of risk profiles, and more sensitive to changes in those risk profiles than they are today. If analysis shows that any part of this goal is not being met, we will consider ways to improve the framework.\n\nConclusion\nThe three pillars of Basel II provide a broad and coherent framework for linking regulatory capital to risk, for improving internal risk measurement and management, and for enhancing supervisory and market discipline at large, complex, and internationally active banks. Indeed, we have already seen significant progress in risk measurement and management at many banks in the United States and elsewhere as a result of the Basel II development process. It is also important to modernize the Basel I framework to improve the risk sensitivity of capital requirements at smaller and less complex banks, without artificially creating competitive inequalities.\n\nThe Federal Reserve continues to support efforts to implement the Basel II framework in the United States, and we expect more progress on implementation soon. It is critical to move forward expeditiously with Basel II implementation so that our largest and internationally active banking organizations maintain their safety and soundness and remain competitive, our supervisors bolster their assessment capabilities, and the market gains greater access to information about risk.\n\nFootnotes\n\n1. See, for example, Randall S. Kroszner and Philip E. Strahan (1996), \"Regulatory Incentives and the Thrift Crisis: Dividends, Mutual-to-Stock Conversions, and Financial Distress (765 KB PDF),\" The Journal of Finance, vol. LI (September), pp. 1285-1319. Return to text\n\n2. \"Risk-Based Capital: Bank Regulators Need to Improve Transparency and Overcome Impediments to Finalizing the Proposed Basel II Framework (1.6 MB PDF),\" Government Accountability Office, February 2007. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20070712a.htm",
        "title": "Basel II Implementation in the United States",
        "date": "7/12/2007"
    },
    {
        "content": "July 10, 2007\n\nChairman Ben S. Bernanke\n\nAt the Monetary Economics Workshop of the National Bureau of Economic Research Summer Institute, Cambridge, Massachusetts\n\nI would like to thank Christina Romer and David Romer for giving me the chance to address participants in the Summer Institute, sponsored by the National Bureau of Economic Research (NBER). As an academic, I regularly attended the Summer Institute and presented or commented on research here. I also served for a time as the director of the Monetary Economics group, the position now shared by David and Christina. The informal nature of the institute, the large number of talented people in attendance, and the opportunity to hear about the very latest work in the field--often while in early draft form--made these few weeks each summer one of the most stimulating times of the year for me. In my current position, I am keenly aware of the long history of fruitful interaction between economists inside and outside of central banks, and I am eager to see this interaction continue. This ongoing intellectual exchange, by improving our understanding of the economy and the workings of monetary policy, has had and will continue to have sizable benefits.\n\nToday I will offer a few remarks on the relationships among monetary policy, inflation, and the public's expectations of inflation, focusing--as seems appropriate for this audience--on some important open questions. I will also give a short overview of the way the Federal Reserve Board staff forecasts inflation, including some discussion of how the staff incorporates information about expected inflation into its forecasting process.\n\nAs you know, the control of inflation is central to good monetary policy. Price stability, which is one leg of the Federal Reserve's dual mandate from the Congress, is a good thing in itself, for reasons that economists understand much better today than they did a few decades ago. Inflation injects noise into the price system, makes long-term financial planning more complex, and interacts in perverse ways with imperfectly indexed tax and accounting rules. In the short-to-medium term, the maintenance of price stability helps avoid the pattern of stop-go monetary policies that were the source of much instability in output and employment in the past. More fundamentally, experience suggests that high and persistent inflation undermines public confidence in the economy and in the management of economic policy generally, with potentially adverse effects on risk-taking, investment, and other productive activities that are sensitive to the public's assessments of the prospects for future economic stability. In the long term, low inflation promotes growth, efficiency, and stability--which, all else being equal, support maximum sustainable employment, the other leg of the mandate given to the Federal Reserve by the Congress.\n\nAdmittedly, measuring the long-term relationship between growth or productivity and inflation is difficult. For example, it may be that low inflation has accompanied good economic performance in part because countries that maintain low inflation tend to pursue other sound economic policies as well. Still, I think we can agree that, at a minimum, the opposite proposition--that inflationary policies promote employment growth in the long run--has been entirely discredited and, indeed, that policies based on this proposition have led to very bad outcomes whenever they have been applied.\n\nInflation Expectations: Conceptual Frameworks\nUndoubtedly, the state of inflation expectations greatly influences actual inflation and thus the central bank's ability to achieve price stability. But what do we mean, precisely, by \"the state of inflation expectations\"? How should we measure inflation expectations, and how should we use that information for forecasting and controlling inflation? I certainly do not have complete answers to those questions, but I believe that they are of great practical importance. I hope my remarks here will stimulate some of you to work on these issues.\n\nWhat is the right conceptual framework for thinking about inflation expectations in the current context? The traditional rational-expectations model of inflation and inflation expectations has been a useful workhorse for thinking about issues of credibility and institutional design, but, to my mind, it is less helpful for thinking about economies in which (1) the structure of the economy is constantly evolving in ways that are imperfectly understood by both the public and policymakers and (2) the policymakers' objective function is not fully known by private agents. In particular, together with the assumption that the central bank's objective function is fixed and known to the public, the traditional rational-expectations approach implies that the public has firm knowledge of the long-run equilibrium inflation rate; consequently, their long-run inflation expectations do not vary over time in response to new information.\n\nBut in fact, as I will discuss in more detail later, long-run inflation expectations do vary over time. That is, they are not perfectly anchored in real economies; moreover, the extent to which they are anchored can change, depending on economic developments and (most important) the current and past conduct of monetary policy. In this context, I use the term \"anchored\" to mean relatively insensitive to incoming data. So, for example, if the public experiences a spell of inflation higher than their long-run expectation, but their long-run expectation of inflation changes little as a result, then inflation expectations are well anchored. If, on the other hand, the public reacts to a short period of higher-than-expected inflation by marking up their long-run expectation considerably, then expectations are poorly anchored.\n\nAlthough variations in the extent to which inflation expectations are anchored are not easily handled in a traditional rational-expectations framework, they seem to fit quite naturally into the burgeoning literature on learning in macroeconomics. The premise of this literature is that people do not have full information about the economy or about the objectives of the central bank, but they instead must make statistical inferences about the unknown parameters governing the evolution of the economy. In a learning context, the concept of anchored expectations is easily formalized in a variety of ways; in general, if the public is modeled as being confident in its current estimate of the long-run inflation rate, so that new information has relatively little effect on that estimate, then the essential idea of well-anchored expectations has been captured.\n\nAllowing for learning has important implications for how we think about the economy and policy. For example, some work has shown that the process of learning can affect the dynamics and even the potential stability of the economy (see, of many possible examples, Bullard and Mitra, 2002). Considerations of how the public learns about the economy affect the form of optimal monetary policy (Gaspar, Smets, and Vestin, 2006). Notably, in a world with rational expectations and in which private agents are assumed already to understand all aspects of the economic environment, talking about the effects of central bank communication would not be sensible, whereas models with learning accommodate the analysis of communication-related issues quite well (Orphanides and Williams, 2005; Bernanke, 2004). Macroeconomic models with learning also give content to the idea of an economy moving gradually from one regime to another, particularly if the central bank as well as the public is assumed to be updating its beliefs. For example, if the central bank and the public learn from experience that high inflation imposes greater costs and fewer benefits than previously thought, then the equilibrium will adjust toward one with lower inflation and lower inflation expectations. This line of explanation of how economies move between monetary regimes, which has been explored by Sargent and others, strikes me as quite plausible as a historical description (Sargent, 1999). In sum, many of the most interesting issues in contemporary monetary theory require an analytical framework that involves learning by private agents and possibly the central bank as well.\n\nImplications of Anchored Inflation Expectations\nWhy do we care about the variability of inflation expectations? As my colleague Rick Mishkin recently discussed, the extent to which inflation expectations are anchored has first-order implications for the performance of inflation and of the economy more generally (Mishkin, 2007). Mishkin illustrated this point by considering the implications of the fact that inflation expectations have become much better anchored over the past thirty years for the estimated coefficients of the conventional Phillips curve, which I define here to encompass specifications that use lagged values of inflation to proxy for expectations or other sources of inflation inertia. As he noted, many studies of the conventional Phillips curve find that the sensitivity of inflation to activity indicators is lower today than in the past (that is, the Phillips curve appears to have become flatter);1 and that the long-run effect on inflation of \"supply shocks,\" such as changes in the price of oil, also appears to be lower than in the past (Hooker, 2002). These findings are of much more than academic interest. To the extent that the Phillips curve may have flattened, inflation will now tend to be more stable than in the past in the face of variations in aggregate demand. (Of course, this can be a good thing or a bad thing, depending on whether inflation expectations are anchored in the vicinity of price stability.) Likewise, a lower sensitivity of long-run inflation to supply shocks would imply that such shocks are much less likely to generate economic instability today than they would have been several decades ago. Notably, the sharp increases in energy prices over the past few years have not led either to persistent inflation or to a recession, in contrast (for example) to the U.S. experience of the 1970s.\n\nVarious factors might account for these changes in the Phillips curve, but, as Mishkin pointed out, better-anchored inflation expectations--themselves, of course, the product of monetary policies that brought inflation down and have kept it relatively stable--certainly play some role. If people set prices and wages with reference to the rate of inflation they expect in the long run and if inflation expectations respond less than previously to variations in economic activity, then inflation itself will become relatively more insensitive to the level of activity--that is, the conventional Phillips curve will be flatter.\n\nSimilar logic explains the finding that inflation is less responsive than it used to be to changes in oil prices and other supply shocks. Certainly, increases in energy prices affect overall inflation in the short run because energy products such as gasoline are part of the consumer's basket and because energy costs loom large in the production of some goods and services. However, a one-off change in energy prices can translate into persistent inflation only if it leads to higher expected inflation and a consequent \"wage-price spiral.\" With inflation expectations well anchored, a one-time increase in energy prices should not lead to a permanent increase in inflation but only to a change in relative prices. A related implication is that, if inflation expectations are well anchored, changes in energy (and food) prices should have relatively little influence on \"core\" inflation, that is, inflation excluding the prices of food and energy.\n\nAlthough inflation expectations seem much better anchored today than they were a few decades ago, they appear to remain imperfectly anchored. A number of studies confirm that observation. For example, Gürkaynak, Sack, and Swanson (2005) found that long-run inflation expectations, as measured by the difference in yields between nominal and inflation-indexed bonds, move in response to news about the economy, rather than remaining unaffected. Levin, Natalucci, and Piger (2004) have shown that some survey measures of inflation expectations in the United States respond to recent changes in the actual rate of inflation, which would not be the case if expectations were perfectly anchored. Models of the term structure of interest rates better fit the data under the assumption that both inflation expectations and beliefs about the central bank's reaction function are evolving (Kozicki and Tinsley, 2001; Rudebusch and Wu, 2003; Cogley, 2005).\n\nAn indirect but elegant way to make the point that inflation expectations remain imperfectly anchored comes from a statistical analysis of inflation by Stock and Watson (2007). Stock and Watson model inflation as having two components, which may be interpreted as the trend and the cycle. Changes in the trend component are highly persistent whereas shocks to the cyclical component are temporary.2  The key finding of this research is that the variability of the trend component of inflation (and thus the share of the overall variability of inflation that it can explain) appears to have fallen significantly after about 1983. That is, unexpected changes in inflation are today much more likely to be transitory than they were before the early 1980s. Because it seems quite unlikely that changes in inflation could persist indefinitely unless long-run expectations of inflation also changed, I interpret the Stock-Watson finding as consistent with the view that inflation expectations have become much more anchored since the early 1980s. At the same time, that the variability of the trend component of inflation, though modest, remains positive, implies that long-run expectations of inflation are not perfectly anchored today.\n\nThe policy implications of the much-improved but still imperfect anchoring of inflation expectations are not at all straightforward. To evaluate these implications, we must understand better the historical variation in inflation expectations, the effect of this variation on actual inflation and economic activity, and the relationship between policy actions and the formation of inflation expectations. With the hope of promoting progress on these broad topics, I pose three questions to researchers, the answer to any of which would be quite useful for practical policymaking.\n\nFirst, how should the central bank best monitor the public's inflation expectations?  Theoretical treatments tend to neglect the fact that in practice many measures of inflation expectations exist, including the forecasts of professional economists, results from surveys of consumers, information extracted from financial markets such as the market for inflation-indexed debt, and limited information on firms' pricing plans. In a very interesting paper, Mankiw, Reis, and Wolfers (2003) compared the available measures, emphasizing in particular that median measures of inflation expectations often obscure substantial cross-sectional dispersion of expectations.3  On which measure or combination of measures should central bankers focus to assess inflation developments and the degree to which expectations are anchored? Do we need new measures of expectations or new surveys? Information on the price expectations of businesses--who are, after all, the price setters in the first instance--as well as information on nominal wage expectations is particularly scarce.\n\nSecond, how do changes in various measures of inflation expectations feed through to actual pricing behavior? Promising recent research has looked at price changes at very disaggregated levels for insight into the pricing decision (Bils and Klenow, 2004; Nakamura and Steinsson, 2007). But this research has not yet linked pricing decisions at the microeconomic level to inflation expectations; undertaking that next step would no doubt be difficult but also very valuable.\n\nThird, what factors affect the level of inflation expectations and the degree to which they are anchored? Answering this question essentially involves estimating the learning rule followed by the public or various components of the public, although one could consider alternative frameworks like Carroll's (2003) epidemiological model of the propagation of information among private agents. A fuller understanding of the public's learning rules would improve the central bank's capacity to assess its own credibility, to evaluate the implications of its policy decisions and communications strategy, and perhaps to forecast inflation. Realistically calibrated models with learning would also inform our thinking about policy and the economy.\n\nInflation Forecasting at the Federal Reserve\nI would like to shift gears at this point to tell you a bit about how the Federal Reserve Board staff goes about forecasting inflation. Obviously, this activity provides critical inputs into the making of monetary policy, and as I will discuss, the staff's long-term track record in forecasting inflation is quite good by any reasonable benchmark. I hope that my brief description will stimulate your interest in the complex and challenging problems of real-time macroeconomic forecasting. But, as you will see, the discussion of practical inflation forecasting will bring us back to one theme of my remarks--that our ability to forecast inflation and predict how inflation will respond to policy actions depends very much on our capacity to measure and to understand what determines the public's expectations of inflation.\n\nThe Board staff employs a variety of formal models, both structural and purely statistical, in its forecasting efforts. However, the forecasts of inflation (and of other key macroeconomic variables) that are provided to the Federal Open Market Committee are developed through an eclectic process that combines model-based projections, anecdotal and other \"extra-model\" information, and professional judgment. In short, for all the advances that have been made in modeling and statistical analysis, practical forecasting continues to involve art as well as science.\n\nThe forecasting procedures used depend importantly on the forecast horizon. For near-term inflation forecasting--say, for the current quarter and the next--the staff relies most heavily on a disaggregated, bottom-up approach that focuses on estimating and forecasting price behavior for the various categories of goods and services that make up the aggregate price index in question. For example, we know from historical experience that the prices of some types of goods and services tend to be quite volatile, including not only (as is well known) the prices of energy and some types of food but also some \"core\" prices such as airfares, apparel prices, and hotel rates. The monthly autocorrelations of price changes in these categories tend to be low or even negative. In contrast, changes in inflation rates in some services categories, such as shelter costs, tend to be more persistent. In assessing what price changes in a particular category imply for future price changes in that category, the staff uses not only various forms of time-series analysis but also specialized knowledge about how the various indexes are constructed--for example, whether certain categories are sampled every month in all localities and how seasonal adjustments are performed. In making very near-term price forecasts, the staff also uses diverse information from a variety of sources, such as surveys of prices of gasoline and other important items, news reports about price-change announcements, and anecdotal information from our business contacts. Conceptually, one might think of this effort to distinguish transitory from persistent price changes as a more nuanced way of estimating the underlying inflation trend, analogous to the trend measures provided by more mechanical indicators such as trimmed-mean or weighted-median inflation rates.\n\nAn accurate forecast of very near-term inflation is important not only for its own sake but also because it provides a better \"jumping-off point\" for the longer-term forecast. Because inflation continues to exhibit some inertia, improved near-term forecasts translate into more-accurate longer-term projections as well.\n\nFor forecasting horizons beyond a quarter or two, detailed analyses of individual price components become less useful, and thus the staff's emphasis shifts to inflation's fundamental determinants. Food and energy inflation are forecasted separately from the core, using information from futures prices and other sources. However, forecasts of core inflation must take into account the extent to which food and energy costs are passed through to other prices.\n\nTo project core inflation at longer-term horizons, the staff consults a range of econometric models. Most of the models used are based on versions of the new Keynesian Phillips curve, which links inflation to inflation expectations, the extent of economic slack, and indicators of supply shocks. Despite the common conceptual framework, the model specifications employed differ considerably in their details, including how lagged inflation enters the equation, how resource utilization is measured, and whether a survey-based measure of inflation expectations is included. In principle, formal econometric tests could determine how much weight should be put on the forecast of each model, but in practice the data do not permit sharp inferences; moreover, estimated forecasting equations may not reflect information about special factors affecting the outlook. Because of these considerations, as I have already noted, the staff's inflation forecasts inevitably reflect a substantial degree of expert judgment and the use of information not captured by the models.\n\nAnother reason for the reliance on judgment in the forecasting process is the practical requirement that the forecast for inflation be consistent with the staff's overall view of the economy, including the forecasts for key economic variables such as wages, interest rates, and consumption spending. Achieving this consistency requires a thoughtful understanding of why sectoral forecasts may be at odds and how best to reconcile those differences. Again, in principle, consistency of sectoral forecasts could be ensured by estimating the inflation equation as part of a general equilibrium system. Indeed, considerable progress has been made in recent years, at the Board and elsewhere, in developing dynamic stochastic general equilibrium (DSGE) models detailed enough for policy application. These models have become increasingly useful for policy analysis and for the simulation of alternative scenarios. They are likely to play a more significant role in the forecasting process over time as well, though, like other formal methods, they are unlikely to displace expert judgment.\n\nA potential drawback of the simple Phillips curve model for analyzing and forecasting inflation is that it does not explicitly incorporate the possible influence of labor costs on the inflation process. The Board's large macroeconomic simulation model, known as FRB/US, projects inflation through a system approach that captures the interaction of wage and price determination. Interestingly, however, the system approach does not seem to forecast price inflation as well as single-equation Phillips curve models do. This weaker performance appears to reflect, at least in part, the shortcomings of the available data on labor compensation. The two principal quarterly indicators of aggregate hourly compensation are the employment cost index (ECI) and nonfarm compensation per hour (CPH). Both are imperfect measures of the labor costs relevant to pricing decisions. For example, the ECI's fixed employment and occupation weights may not reflect changes in the labor market, and the ECI excludes stock options and similar forms of payment. CPH is volatile, perhaps in part because it measures stock options at exercise rather than when granted, and it is subject to substantial revisions. Moreover, these two hourly compensation measures often give contradictory signals. Despite these problems, labor market developments certainly influence how the staff and policymakers view the inflation process and inflation risks, illustrating yet another point in the forecasting process at which judgment must play an important role. In particular, in evaluating labor-market conditions and trends in labor costs, the staff takes note of a wide range of data, anecdotes, and other qualitative information as well as the official data on compensation.\n\nOverall, the Board staff's inflation forecasting has been remarkably good, at least compared with the available alternatives (Romer and Romer, 2000; Sims, 2002). To cite a recent study, Faust and Wright (2007) show that real-time staff forecasts of inflation reliably outperform statistical benchmarks at all horizons and that this advantage is not solely the result of the staff's expertise at estimating near-term inflation rates.\n\nTo link this discussion of forecasting to the first portion of my remarks, I turn to the treatment of inflation expectations in staff forecasts. As I noted earlier, while inflation expectations doubtless are crucial determinants of observed inflation, measuring expectations and inferring just how they affect inflation are difficult tasks. A popular shortcut is to include lagged inflation terms in the Phillips curve equation; besides being a convenient means of capturing the inertial component in inflation, the estimated coefficients on lagged inflation almost certainly reflect to some degree the formation of inflation expectations and their influence on the inflation process. However, using lagged inflation as a proxy for inflation expectations has drawbacks, notably its susceptibility to the Lucas critique.4  The staff consequently analyzes a number of survey measures of inflation expectations. One question in choosing among measures of expectations is whether to focus on measures of short-term inflation expectations (say, twelve months ahead) or of longer-term expectations (five to ten years ahead). Generally, measures of longer-term inflation expectations, such as the five-to-ten-year expected inflation measures from the Michigan/Reuters survey of households and from the Survey of Professional Forecasters, seem to be better gauges of the expectations that influence wage- and price-setting behavior.\n\nThe staff also looks at measures derived from comparing yields on nominal and inflation-indexed Treasury securities (the breakeven inflation rate). Measures of inflation compensation derived from the market for inflation-indexed securities are influenced by changes in inflation risk premiums and liquidity premiums, and analyses are constrained by the fact that these markets have been operating in the United States for only a relatively short period. Nevertheless, unlike survey measures, breakeven inflation rates are determined in a market in which investors back their views with real money. Moreover, breakeven measures of inflation expectations provide information on the expectations of a different group of agents--financial-market participants--which can be compared with the views of economists and consumers as represented by surveys.\n\nMeasurement is only one aspect of understanding inflation expectations. We also need a better understanding of how inflation expectations affect actual inflation and of the factors that determine inflation expectations. I will say a few words about the latter issue in the context of the practical problems of forecasting and policy analysis faced by the staff of the Federal Reserve Board.\n\nModel-based simulations of the inflation process are useful tools for both forecasting and policy analysis. In conducting such simulations, the analyst must specify how inflation expectations are formed--in particular, how they react to actual changes in the economy and in policy. In most simulations of the FRB/US model, the public is assumed to update its inflation projections based on the historical relationship between inflation and other key economic variables. Essentially, this approach assumes that the public updates its inflation expectations in a sensible way based on economic developments but does not assume that the public has full knowledge of the underlying model of the economy, consistent with the structure of learning models (Brayton and others, 1997).\n\nRecent staff work at the Board has analyzed the implications of expanding the set of variables allowed to influence the public's long-term inflation expectations to include, among others, the federal funds rate.5 If the public's long-term inflation expectations are influenced directly by Fed actions, as this specification suggests, a number of interesting implications follow. One is that the output costs of disinflation may be lower than those suggested by reduced-form-type Phillips curves. Intuitively, if the Fed attempts to disinflate by raising the federal funds rate, the disinflationary effect will be felt not only through the usual output gap channel but also through a direct restraint on long-term inflation expectations. This interpretation is consistent with some analyses of the Volcker disinflation; although the costs of that disinflation were high, they were perhaps less than economists would have predicted in advance, given conventional estimates of the sacrifice ratio (Erceg and Levin, 2003).\n\nTo be sure, this and similar analyses remain speculative. A good deal more must be done before such work proves a reliable basis for policy choices. Nevertheless, I hope this example illustrates for you the theme of my remarks, that a deeper understanding of the determinants and effects of the public's expectations of inflation could have significant practical payoffs.\n\nReferences\n\nBernanke, Ben (2004). \"Fedspeak,\" at the meetings of the American Economic Association, San Diego, California, January 3, 2004.\n\nBils, Mark and Peter Klenow (2004). \"Some Evidence on the Importance of Sticky Prices,\" Journal of Political Economy, vol. 112 (October), pp. 847-85.\n\nBullard, James, and Kaushik Mitra (2002). \"Learning about Monetary Policy Rules,\" Journal of Monetary Economics, vol. 49(September), pp. 1105-29.\n\nBrayton, Flint, Eileen Mauskopf, David Reifschneider, Peter Tinsley, and John Williams (1997). \"The Role of Expectations in the FRB/US Macroeconomic Model,\" Federal Reserve Bulletin, vol. 83(April), pp. 227-45.\n\nCarroll, Christopher D. (2003). \"Macroeconomic Expectations of Households and Professional Forecasters,\" Quarterly Journal of Economics, vol. 118 (February), pp. 269-98.\n\nCogley, Timothy (2005). \"Changing Beliefs and the Term Structure of Interest Rates: Cross-Equation Restrictions with Drifting Parameters,\" Review of Economic Dynamics, vol. 8 (April), pp. 420-51.\n\nCogley, Timothy, and Thomas J. Sargent (2007). \"Inflation Gap Persistence in the U.S. (5.7 MB PDF)\" University of California, Davis, working paper, January.\n\nErceg, Christopher, and Andrew Levin (2003). \"Imperfect Credibility and Inflation Persistence,\" Journal of Monetary Economics, vol. 50 (May), pp. 915-44.\n\nFaust, Jon, and Jonathan H. Wright (2007). \"Comparing Greenbook and Reduced Form Forecasts Using a Large Realtime Dataset (259 KB PDF),\" Johns Hopkins University and Board of Governors, working paper.\n\nGaspar, Vitor, Frank Smets, and David Vestin (2006). \"Adaptive Learning, Persistence, and Optimal Monetary Policy,\" Journal of the European Economic Association, vol. 4 (April-May), pp. 376-85.\n\nGordon, Robert J. (2007). \"Phillips Curve Specification and the Decline in U.S. Output and Inflation Volatility (1.1 MB PDF),\" presented at the Symposium on The Phillips Curve and the Natural Rate of Unemployment, Institut für Weltwirtschaft, Kiel, Germany, June 3-4.\n\nGürkaynak, Refet, Brian Sack, and Eric Swanson (2005). \"The Sensitivity of Long-term Interest Rates to Economic News: Evidence and Implications for Macroeconomic Models,\" American Economic Review, vol. 95 (March), pp. 425-36.\n\nHooker, Mark (2002). \"Are Oil Shocks Inflationary? Asymmetric and Nonlinear Specifications versus Changes in Regime,\" Journal of Money, Credit, and Banking, vol. 34 (May), pp. 540-61.\n\nKozicki, Sharon, and Peter Tinsley (2001). \"Shifting Endpoints in the Term Structure of Interest Rates,\" Journal of Monetary Economics, vol. 47(June), pp. 613-52.\n\nLevin, Andrew, Fabio Natalucci, and Jeremy Piger (2004). \"The Macroeconomic Effects of Inflation Targeting (406 KB PDF),\" Federal Reserve Bank of St. Louis, Review, vol. 86 (July), pp. 51-80.\n\nMankiw, N. Gregory, Ricardo Reis, and Justin Wolfers (2003). \"Disagreement about Inflation Expectations,\" NBER Macroeconomics Annual, pp. 209-248.\n\nMishkin, Frederic S. (2007). \"Inflation Dynamics,\" National Bureau of Economic Research Working Paper no. 13147, June.\n\nNakamura, Emi, and Jon Steinsson (2007). \"Five Facts About Prices: A Reevaluation of Menu Cost Models (577 KB PDF),\" Harvard University, working paper, May.\n\nNason, James (2006). \"Instability in U.S. Inflation: 1967-2005,\" Federal Reserve Bank of Atlanta, Economic Review, vol. 91 (Second Quarter), pp. 39-59.\n\nOrphanides, Athanasios, and John C. Williams (2005). \"Inflation Scares and Forecast-based Monetary Policy,\" Review of Economic Dynamics, vol. 8 (April), pp. 498-527.\n\nRoberts, John (2006). \"Monetary Policy and Inflation Dynamics,\" International Journal of Central Banking, vol. 2 (September), pp. 193-230.\n\nRomer, Christina, and David Romer (2000). \"Federal Reserve Information and the Behavior of Interest Rates,\" American Economic Review, vol. 90 (June), pp. 429-57.\n\nRudebusch, Glenn, and Tao Wu (2003). \"A Macro-Finance Model of the Term Structure, Monetary Policy, and the Economy (562 KB PDF),\" Federal Reserve Bank of San Francisco Working Paper 2003-17, September.\n\nSargent, Thomas J. (1999). The Conquest of American Inflation. Princeton, N.J.: Princeton University Press.\n\nSims, Christopher (2002). \"The Role of Models and Probabilities in the Monetary Policy Process,\" Brookings Papers on Economic Activity, vol. 2, pp. 1-40.\n\nStock, James, and Mark Watson (2007). \"Why Has U.S. Inflation Become Harder to Forecast?\" Journal of Money, Credit, and Banking, vol. 39 (February), pp. 3-34.\n\nFootnotes\n\n1. Roberts (2006) provides a recent overview. He attributes most of the \"flattening\" of the Phillips curve to changes in the conduct of monetary policy.See also Nason (2006). Gordon (2007) provides an opposing view.Return to text\n\n2. Stock and Watson assume that transitory shocks last only one quarter. Cogley and Sargent (2007) explore the Stock-Watson specification in more detail, arguing that the transitory component of inflation is best modeled as having somewhat greater persistence. Return to text\n\n3. A particularly valuable part of the paper is a case study of the evolution of expectations during the Volcker disinflation of 1979-1982. Histograms of the quarterly range of inflation expectations show only a very gradual adjustment of inflation expectations as the disinflation proceeded, with significant reductions in expectations occurring only in the third year of the disinflation. Moreover, the range of disagreement widened (and even became somewhat bimodal) as individual respondents evidently differed in their willingness to accept the Fed\"s declared commitment to reducing inflation as being a true break from the past. Capturing this behavior in a formal model would be challenging but worthwhile. Return to text\n\n4. The Lucas critique holds that reduced-form empirical relationships estimated on historical data may break down when policies change. Return to text\n\n5. In this empirical work, the public\"s long-run inflation expectations are proxied for by the long-run inflation projections taken from the Survey of Professional Forecasters (Mishkin, 2007). Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070710a.htm",
        "title": "Inflation Expectations and Inflation Forecasting",
        "date": "7/10/2007"
    },
    {
        "content": "June 23, 2007\n\nGovernor Frederic S. Mishkin\n\nAt the Econometric Society at Duke University Lecture, Durham, North Carolina\n\nGovernor Mishkin presented identical remarks at the New Perspectives on Financial Globalization Conference, International Monetary Fund, Washington, D.C., on April 26, 2007\n\nIn the United States and many other countries, students learn that the key to success is hard work.  Yet when we look at many developing countries, we see people who work extremely hard for long hours.  Their wages are low, and so they remain poor.  And as a whole, their countries remain poor.  If hard work does not make a country rich, what does?\n\nThe right institutions are essential.  Nobel laureate Douglass North defines institutions as the \"rules of the game in a society, or, more formally, humanly devised constraints that shape human intervention.\" (North, 1990, p. 3).  Among the institutions that are most crucial to economic growth are those that enable a country to allocate capital to its most productive uses.  Such institutions establish and maintain strong property rights, an effective legal system, and a sound and efficient financial system.\n\nIn recent years, the field of economic development has come to the conclusion that \"institutions rule\" and are critical to economic growth.1  An extensive literature focuses on financial development as a significant force driving economic development.2\n\nHowever, developing good institutions that foster financial development is not easy:  It takes time for institutions to evolve and adapt to local circumstances.  In addition, vested interests in poor countries often oppose the necessary reforms because they believe that such reforms will weaken their power or allow other people to cut into their profits.  How can poorer countries overcome these obstacles?  How can they change the distribution of power to forge the political will to promote institutional reform?  The answer is globalization.\n\nI should note that the opinions I will express today are my own and not necessarily those of my colleagues on the Federal Open Market Committee (FOMC).\n\nElements of Institutional Reform\nBefore examining the role of globalization in promoting financial development, let’s first look briefly at what steps must be taken to build an institutional infrastructure that will ensure a well-functioning financial system.\n\n1. Develop strong property rights.  Strong property rights are needed to encourage productive investment because it will not be undertaken if the returns on investment are likely to be taken away by the government or others.  Hernando de Soto, in his important book The Mystery of Capital, argues that the inability of the poor in developing countries to acquire property rights is a central reason that they are unable to gain access to capital and so remain mired in poverty.  For example, the use of collateral is a crucial tool that helps the financial system make loans because it reduces losses when loans go sour.  A person who would pledge land or capital for a loan must, however, legally own the collateral.  Unfortunately, as de Soto has documented, legalizing the ownership of capital is extremely expensive and time consuming for the poor in developing countries.  In one of his many astonishing examples, obtaining legal title to a dwelling on urban land in the Philippines required taking 168 bureaucratic steps through 53 public and private agencies over a period of 13 to 25 years.\n\n2. Strengthen the legal system.  A legal system that enforces contracts quickly and fairly is an essential step in supporting strong property rights and financial development.  For example, lenders write restrictive covenants into loan contracts to prevent borrowers from taking on too much risk, but such covenants have value only if they can be legally enforced.  An inefficient legal system in which loan contracts cannot be enforced will prevent productive lending from taking place.  If setting up legitimate businesses or obtaining legal title to property is too expensive, the poor will never have access to the legal system and will be cut off from lending that could help them start small businesses and escape poverty.3  Setting up a simple business in the United States generally requires only filling out a form and paying a nominal licensing fee.  In contrast, de Soto's researchers found that legally registering a small garment workshop in Peru required 289 days; at 6 hours per day, the cost was about $1,200, which was approximately thirty times the monthly minimum wage.  The lack of property rights for all but the very rich, as documented by de Soto, is a serious impediment to financial development.\n\n3. Reduce corruption.  Government is often the primary source of financial repression in developing countries.  Rapacious governments whose rulers treat their countries as personal fiefdoms are not uncommon:  We have seen these governments in Saddam Hussein's Iraq, Robert Mugabe's Zimbabwe, and Ferdinand Marcos's Philippines.  Even officials in less tyrannical governments have been known to use the power of the state to get rich.  Not surprisingly, then, many governments pay lip service to property rights but do not encourage a rule of law to protect them.\n\nEliminating corruption is essential to strengthening property rights and the legal system.  When corrupt officials demand bribes, they reduce the incentives for entrepreneurs to make investments.  The ability to buy off judges weakens the enforcement of legal contracts that enable the economic and financial system to function smoothly.4\n\n4. Improve the quality of financial information.  High-quality financial information is essential to well-functioning financial markets.  If lenders cannot figure out what is going on in a firm, they will be unable to screen out good from bad credit risks or to monitor the firm to ensure that it does not take on too much risk at the lender’s expense.  To make reliable and accurate information more accessible, accounting standards must be high enough so that prospective lenders can make sense of what is in a business’s books.  Rules that require businesses to disclose information must be enforced to enable prospective investors to make sensible decisions about whether the business deserves to get their hard-earned money.\n\n5. Improve corporate governance.  For people to be willing to buy stocks, another way to channel funds to business, rules must be established to ensure that the managers of corporations act in the stockholders’ interest.  If managers find it easy to steal from the corporation, or to use funds for their own personal use rather than for the benefit of the company, no one will want to invest in the company.  Finding the right balance of control between management and stockholders is a challenge with which even we in the United States continue to struggle.\n\n6.  Develop sound, prudential regulation and supervision of the banking system.   Banks are the main institutions that allocate credit in developing countries.  The skills necessary for bank officers to assess risks and make good lending decisions are critically important and often scarce.  Poor lending policies may cause too much capital to be channeled toward low-return projects and insufficient capital to be directed toward the high-return projects needed to propel income and growth.  Moreover, deterioration in banks' balance sheets caused by insider lending or excessive risk-taking that leads to a proliferation of bad loans can cause banks to cut back sharply on lending, with negative effects on the economy.  If the deterioration in banks’ balance sheets is severe enough, it can result in banking and currency crises that substantially disrupt the economy, phenomena that unfortunately have been all too common in developing countries over the past several decades.5   Preventing banking crises must start with prudential regulation, in which rules set by the government ensure that banks have sufficient capital and manage risks well.  To guarantee that these regulations are enforced, the government must also engage in prudential supervision, in which it monitors banks by examining them on a regular basis to ensure that they are complying with government regulations.\n\nThe role of microfinance in developing countries is receiving much attention these days.  Microfinance is a positive development; it has clearly helped substantial numbers of poor people escape poverty, and the Nobel Peace Prize awarded to Muhammad Yunus for his pioneering efforts in this area was certainly well deserved.6  However, microfinance is not a substitute for the institution building I am talking about here.\n\nGlobalizing to Advance Institutional Reform\nNow that we understand what kinds of institutions are needed to promote financial development and economic growth, let’s turn to the question of how developing countries can improve the likelihood that these institutions are developed.\n\nOne of the most powerful weapons for stimulating institutional development is globalization.  Wealth is not something that can be attained by remaining closed off to the rest of the world.  Poorer countries would do better by embracing globalization--that is, opening their financial markets and their markets for goods and services to other nations so that funds, goods, and, often, the ideas that accompany them can flow in.  Such inflows can help them achieve reforms that build productivity and wealth that will benefit all their citizens.  Of course, countries need to take care that the foundations of the fundamental institutions discussed above are in place, and they must monitor the pace of reform.\n\nOpening financial markets\nNow let’s look at how opening financial markets to foreigners promotes financial development.\n\nGlobalizing the domestic financial system by opening financial markets to foreigners encourages financial development and growth in wealth in two ways.  First, opening financial markets to foreign capital directly increases access to capital and lowers its cost for those with productive investments to make.7  We know that labor is cheap in poor countries, and so we might think that capital would be especially productive there.  Just think of how hugely profitable a factory might be in a country where wages are one-tenth of those in the United States.  Although some of that differential would likely reflect the higher productivity of American workers, capital should, nevertheless, have extremely high returns in such countries, and, in principle, we should expect substantial flows of capital from rich countries (where the returns on capital should be relatively low) to poor countries (where they should be far higher).  Such capital flows could lead to substantial benefits for poor countries in the form of larger capital stocks, higher productivity, and more rapidly growing incomes.\n\nIn fact, as we well know, at present capital flows are moving, on net, from poor countries to rich ones, that is, in a direction opposite to the one we would expect.  Many reasons have been proposed for this apparent paradox, but one of them certainly is the weakness of financial systems in poor countries, as described earlier.  This point leads us to a second benefit of financial globalization:  Opening markets to foreign financial institutions promotes reforms to the financial system that improve its functioning.  Allowing foreign financial institutions to operate in an emerging-market country brings in expertise and best practices, such as those designed to screen good from bad credit risks and to monitor borrower activities to reduce the amount of risk they take.8  Because of their familiarity with more-advanced financial systems, foreign financial firms also are likely to increase the pressure on the domestic government to institute reforms that will make the financial system work more effectively.\n\nAs domestic financial institutions start to lose business to better-run and more trustworthy foreign institutions, they will realize the need for a better legal and accounting infrastructure that will make it easier for them to make loans to new customers.  Domestic financial institutions will then be far more likely to advocate for and support the reforms that achieve this result.\n\nOf course, this is not to say that in a genuinely corrupt and anticompetitive environment financial globalization, by itself, can still engender an efficient, dynamic, and modern financial system.  Recent research has shown that when some countries opened up to international capital markets too soon in the absence of some basic supporting conditions, vulnerabilities to sudden stops of capital flows increased.  Thus, some preconditions must exist with respect to a minimum level of institutional quality, financial market development, and macroeconomic stability before financial globalization can further improve financial market and institutional development.9  That said, given these preconditions and some constituency for progress and reform, financial globalization can be a powerful force in support of such efforts.\n\nOpening trade in goods\nNext, let’s consider how opening domestic markets to foreign goods can promote the development of better institutions.\n\nAlthough not immediately obvious, opening domestic markets to foreign goods, known as \"trade liberalization,\" can be a key driver of financial development.  It can weaken the political power of entrenched business interests that might otherwise block institutional reforms, a point that is emphatically made by Rajan and Zingales (2004) in their book Saving Capitalism from the Capitalists.  Trade liberalization, which promotes a more competitive environment, will lower the revenue of entrenched firms so that they will need greater access to external sources of capital.  Thus, they will be more likely to support reforms that promote a deeper and more efficient financial system.  In fact, research indicates that a deeper financial sector is positively associated with greater trade openness (Rajan and Zingales, 2003; Svaleryd and Vlachos, 2002).\n\nFree trade also promotes financial deepening by reducing corruption.  High tariffs breed corruption because importers have incentives to pay customs officials to look the other way when the importers avoid tariffs by smuggling in goods.  Not surprisingly, countries that restrict international trade are found to be more corrupt (Ades and Di Tella, 1994).\n\nEven when developing countries are unwilling to tear down all barriers to imports of foreign goods, they can still generate incentives for institutional reform by removing obstacles that prevent domestic producers from engaging in international trade.  Facilitating production for overseas markets creates a greater need for a well-functioning financial system because, to compete effectively in the international arena, firms need better access to capital.  If they can’t get capital, they won’t be able to make the investments they need to increase productivity and price their goods competitively.   Accordingly, international trade creates a demand for reforms that will make the financial system more efficient.\n\nThe case of China\n\nWe are seeing how the globalization of trade is driving financial reform in China.  As Chinese enterprises increasingly enter international markets, they need a better financial system that can ensure that the allocation of their high domestic savings is done efficiently and is responsive to market developments.  Although it has taken time, globalization is helping to generate the demand for an improved financial system, which is driving the reform process.\n\nThe Communist leadership recognizes that the old development model must change.  The government has announced that state-owned banks are being put on the path to be privatized and has allowed foreign investment in China’s banking system ($20 billion in 2005).10  The government is also engaged in legal reform to make contracts more enforceable.  In August 2006, the National People’s Congress enacted a new bankruptcy law that gives creditors greater protection if a firm goes bankrupt, and last month it approved a law that gives individuals more legal protection for their property.11\n\nChina, of course, is an example of a country that has actively encouraged exports as a means of propelling its economic growth and development.  To some extent, China may have gone too far in its use of policy to promote export growth.  Increased reliance on market-determined prices will help ensure that the allocation of resources into the export sector does not exceed their efficient use.  The goal should be to raise productivity toward world-class standards in all sectors of the economy.  Recently China’s authorities have agreed that some rebalancing of the sources of growth away from exports and toward domestic demand is in order.  Among China’s East Asian neighbors, the importance of developing industries to meet demand for domestic uses also is receiving increasing attention.\n\nThe problem of export restrictions\n\nNevertheless, developing production for exports may still be useful for those countries at the lowest rungs of the developmental ladder, and it is surprising that many of the world’s poorest developing countries still not only do not encourage an export orientation but in fact maintain a regime of taxes, restrictions, and other policies that effectively discourage it.  This problem remains especially serious in some African economies and may help explain why their growth performance has been so disappointing.\n\nThe primary way that governments discourage exports is by imposing large taxes on them.  Because high export taxes are one method of obtaining revenue, governments may be attracted to them to solve their budget problems.  They may also use these taxes to punish their political opponents, who are often involved in a particular export industry.  The government can then distribute the resulting revenue to their supporters.\n\nThe most pernicious forms of export taxes are those that are hidden through the government’s setting a fixed official exchange rate that artificially keeps the domestic currency at a value well above what it would be worth in terms of foreign currency (say, U.S. dollars) in a free market.  The government then makes it illegal to sell dollars for the larger amount of domestic currency that could be obtained in the black market.  The difference between the official exchange rate and the free, black-market rate (often called the \"black-market premium\") imposes a tax on exporters because they are forced to sell the dollars they earn to the government or to the central bank at the official rate, and thus they receive a much lower price for their goods in terms of the domestic currency.\n\nAlthough in recent decades a great many countries have abandoned currency controls and dismantled their black markets, such controls still exist in some of the poorest economies, especially in Africa.  In some countries, the tax from the black-market premium is confiscatory.  An example from history illustrates this point.  In 1982 Ghana had a black-market premium of more than 1,000 percent, and so exporters of cocoa (primarily from a tribe different from that of the ruling government party) were getting only 6 percent of the world price. Given such a high tax rate, it came as no surprise that cocoa exports, which had accounted for 19 percent of Ghana’s gross domestic product in the 1950s, accounted for only 3 percent by 1982 (Easterly, 2001, p. 222).  During the twenty years when the black-market premium was so high, the average income of Ghanaians fell 30 percent.\n\nLike many such unwarranted controls on economic life, high black-market premiums also breed corruption, with all its negative effects, because they create strong incentives to bribe officials or to smuggle goods to avoid paying the black-market-premium tax.  (Indeed, one of the reasons that governments in poorer countries often use this method of taxation rather than an explicit tax is that it allows government officials to get rich from the bribes they receive.)\n\nOther Gains from Trade Liberalization\nAlthough we have been focusing on how globalization promotes financial development, we shouldn’t forget that trade globalization, which involves both trade liberalization and an export orientation, is a key driver of economic growth for reasons additional to those already mentioned.12\n\nThe first economics course that college students encounter always teaches the concept of comparative advantage:  By trading with another country, you can focus your production on what you are really good at so that your productivity will be high.  This higher productivity then leads to higher economic welfare.\n\nTrade liberalization, more importantly, promotes competition in domestic markets, which in turn forces domestic firms to increase productivity and make better products, both of which drive economic growth.  If a foreigner produces a better product that can be imported, domestic firms must make a better product at a lower price to keep selling their product at home.  One graphic example of how trade promotes competition occurred in India, which up until 1991 had protected its tool industry with a 100 percent tariff (tax on imports).  After the Indian government cut the tariff sharply, Taiwanese firms initially grabbed one-third of the Indian market.  Over the next decade, however, Indian firms boosted their productivity almost to the levels of Taiwanese firms, thereby winning back the domestic market.  Eventually Indian tool firms became so efficient that they were able to start selling their goods abroad and became substantial exporters.13\n\nDecreasing barriers to imports also helps promote exports.  Increased competition from imports lowers the profits firms can earn by focusing solely on the domestic market, and so they naturally concentrate more of their energy on exporting.  Moreover, trade liberalization helps developing countries gain access to foreign markets in advanced countries, as illustrated by the fact that the United States, through free-trade agreements, has been more willing to lower tariffs for countries such as Mexico and Chile if they do the same for the United States.\n\nEmpirical evidence indicates that trade liberalization has positive effects on productivity and economic growth for both importing and exporting countries:  It has even been found to be associated with more-rapid increases in life expectancy and a reduction in infant mortality.14 Yet, as is often the case in economics, empirical evidence is never completely clear cut:  Some economists question whether the evidence strongly supports a positive link between trade liberalization and growth.15  Nonetheless, the logic of the benefits of trade liberalization and the preponderance of the evidence supporting its positive effects lead most members of the economics profession, including me, to the following conclusion:  Trade liberalization is highly beneficial not only for the overall economy but also for its constituent sectors.  The resulting economic growth is a rising tide that raises all boats and is an important tool for poverty alleviation.\n\nBut even if trade liberalization is not adopted, giving domestic producers the opportunity to sell goods to rich countries’ markets can be an important engine for growth in the world’s poorest countries.  One crucial way that governments in developing countries can encourage exports is by providing the transportation infrastructure--ports, roads, and airports--that make it easier for businesses to send their goods abroad.  Because foreigners don’t have a natural predilection to buy your goods, you have to be supercompetitive--your goods have to be better and cheaper than the goods made in foreign countries.  Domestic firms have to focus even more on being highly productive, and boosting productivity will lead to rapid economic growth.\n\nJapan’s experience shows what focusing on exporting can accomplish.  In the immediate aftermath of World War II, Japan was a poor country.  Its economic infrastructure had been destroyed by the war.  To convince Americans and others to buy Japanese products, Japanese firms had to produce goods that were cheaper and better than their American-made counterparts.  As a result, the export industries in Japan became enormously productive and supercompetitive.  Productivity grew, and three decades after World War II, Japan became one of the richest countries in the world.\n\nSouth Korea, one of the great Asian success stories even with its crisis in the late 1990s, had very high barriers to trade until the 1990s, and its early development strategy did not include opening its domestic market to foreign goods.  However, through its export sector, South Korea has participated fully in global markets, and this participation has been a key to its success.  South Korea’s development strategy focused on promoting its export sector, and it is the export sector that led to high productivity and economic growth.  Indeed, all examples of successful growth stories in developing economies (China, Japan, South Korea, Singapore, Taiwan, Chile) have involved export sectors that met the test of international competition, and some of these economies have also pursued trade liberalization.\n\nIn almost all the industrializing East Asian economies, future growth will likely have to follow a more balanced path that relies less on exports and more on production for the domestic market.  Such adjustments are needed not only to secure such economies’ further development but also to alleviate the pattern of external imbalances around the global economy.  It is in the world’s poorest countries--especially in Africa and Latin America--that additional participation in global markets has the highest priority.\n\nOnly by embracing global markets can developing countries raise living standards.16   Trade liberalization has a critical role to play in economic growth by directly stimulating domestic firms to become more productive.  And along with financial globalization, it can also encourage emerging-market economies to develop the institutions that foster financial development.  Globalization should be one of the highest priorities for developing countries.\n\nThe Role of Advanced Countries\n\nCan we in the advanced countries help?  Yes, we can do so by supporting the opening of our markets to goods and services from emerging-market countries.  By encouraging these countries to increase their participation in global markets, we create exactly the right incentives for them to implement the hard measures that will enable them to grow rich.  As we have seen, exporters have strong incentives to be productive so that they can take advantage of access to our markets, and thus they will make the investments needed for growth.  They also will push for the institutional reforms to make financial markets more efficient and promote financial deepening.   By getting financial markets to work well, exporters will have access to the capital they need to increase their business.\n\nOpening our markets to emerging-market countries is an important way that those in advanced countries can help emerging-market economies become successful.  While providing aid to poor countries can, in the right circumstances, help eradicate poverty, it often will not work because it usually does not create the right incentives to promote economic growth.  A handout is almost never as effective as a hand up.\n\nSome are concerned about the consequences for us if we in the United States allow free competition in our markets for goods and services from countries where wages are low.  Keeping many countries poor and their workers unproductive may seem to be to our benefit.  But as shown in the examples of post-World War II recovery in Europe and Japan, and in the rapid growth in the 1970s and 1980s in the newly industrialized economies of Asia, higher standards of living throughout the global economy actually work to our benefit.  Prosperity in our trading partners creates growing markets for U.S. exports of high-value goods.  And over time, as workers’ productivity abroad rises, so will their wages and incomes.  It is true that the changes brought about in our economy by globalization impose significant costs on some domestic workers.  We need to develop policies to help those workers without undermining the global trading system.  The costs to us of damaging that system would far outweigh the benefits that some might gain from protectionist measures.  Promoting trade liberalization helps us not only do good but also do well.\n\nReferences\n\nAcemoglu, Daron, Simon Johnson, and James A. Robinson (2001).  \"The Colonial Origins of Comparative Development:  An Empirical Investigation,\" American Economic Review, vol. 91 (December), pp. 1369-1401.\n\nAcemoglu, Daron, Simon Johnson, and James A. Robinson (2005).  \"Institutions as the Fundamental Cause of Long-Run Growth,\" in Philippe Aghion and Steven N. Durlauf, eds., Handbook of Economic Growth, vol. 1, part 1.  Amsterdam: North Holland, pp. 385-472.\n\nAdes, Alberto, and Rafael Di Tella (1994).  \"Competition and Corruption,\" Institute of Economics and Statistics Discussion Paper Series 169.  Oxford:  University of Oxford.\n\nAlfaro, Laura, and others (2004).  \"FDI and Economic Growth: The Role of Local Financial Markets,\" Journal of International Economics, vol. 64 (October), pp. 89-112.\n\nArmendariz de Aghion, Beatriz, and Jonathan Morduch (2005).  The Economics of Microfinance.  Cambridge, Mass.:  MIT Press.\n\nBekaert, Geert, Campbell R. Harvey and Robin L. Lumsdaine (2002).  \"Dating the Integration of World Equity Markets,\" Journal of Financial Economics, vol. 65 (August), pp. 203-47.\n\nBhagwati, Jagdish N. (2004).  In Defense of Globalization.  New York:  Oxford University Press.\n\nBourguignon, Francois, Diane Coyle, Raquel Fernandez, Francesco Giavazzi, Dalia Marin, Kevin O’Rourke, Richard Portes, Paul Seabright, Anthony Venables, Thierry Verdier, and L. Alan Winters (2002).  Making Sense of Globalization:  A Guide to the Economic Issues, CEPR Policy Paper Series 8.  London:  Centre for Economic Policy Research, July.\n\nDell’Ariccia, Giovanni, and Robert Marquez (2006).  \"Lending Booms and Lending Standards,\" Journal of Finance, vol. 61 (October), pp. 2511-46.\n\nDemirguc-Kunt, Asli, and Enrica Detragiache (2005).  \"Cross-Country Empirical Studies of Systemic Bank Distress: A Survey (422 KB PDF),\" IMF Working Paper Series WP 05/96.  Washington:  International Monetary Fund, May.\n\nde Soto, Hernando (2000).  The Mystery of Capital:  Why Capitalism Triumphs in the West and Fails Everywhere Else. New York:  Basic Books.\n\nDollar, David (1992).  \"Outward-Oriented Developing Economies Really Do Grow More Rapidly: Evidence from 95 LDCs, 1976-1985,\" Economic Development and Cultural Change, vol. 40 (April), pp. 523-44.\n\nDollar, David, and Paul Collier (2001).  Globalization, Growth, and Poverty: Building an Inclusive World Economy.  New York:  Oxford University Press.\n\nEasterly, William (2001).  The Elusive Quest for Growth:  Economists’ Adventures and Misadventures in the Tropics.  Cambridge, Mass.:  MIT Press.\n\nEasterly, William, and Ross Levine (2001).  \"It’s Not Factor Accumulation: Stylized Facts and Growth Models,\" World Bank Economic Review, vol. 15 (2), pp. 177-219.\n\nEasterly, William, and Ross Levine (2003). \"Tropics, Germs, and Crops:  How Endowments Influence Economic Development,\" Journal of Monetary Economics, vol. 50 (January), pp. 3-39.\n\nEdwards, Sebastian (1998).  \"Openness, Productivity, and Growth: What Do We Really Know?\" Economic Journal, vol. 108 (March), pp. 383-98.\n\nEichengreen, Barry (2001).  \"Capital Account Liberalization: What Do Cross-Country Studies Tell Us?\" World Bank Economic Review, vol. 15 (3), pp. 341-65.\n\nFrankel, Jeffrey A., and David Romer (1999).  \"Does Trade Cause Growth?\" American Economic Review, vol. 89 (June), pp. 379-99.\n\nGlaeser, Edward L., Rafael La Porta, Florencio Lopez-de-Silanes, and Andrei Shleifer (2004).  \"Do Institutions Cause Growth?\" NBER Working Paper Series 10568.  Cambridge, Mass.: National Bureau of Economic Research, June.\n\nGoldberg, Linda (2004).  \"Financial-Sector FDI and Host Countries: New and Old Lessons,\" NBER Working Paper Series 10441.  Cambridge, Mass.:  National Bureau of Economic Research, April.\n\nHall, Robert E., and Charles I. Jones (1999).  \"Why Do Some Countries Produce So Much More Output per Worker Than Others?\"  Quarterly Journal of Economics, vol. 114 (February), pp. 83-116.\n\nHarrison, Ann (1996).  \"Openness and Growth: A Time-Series, Cross-Country Analysis for Developing Countries,\" Journal of Development Economics, vol. 48 (March), pp. 419-47.\n\nHenry, Peter Blair (2000a).  \"Stock Market Liberalization, Economic Reform, and Emerging Market Equity Prices,\" Journal of Finance, vol. 55 (2), pp. 529-64.\n\nHenry, Peter Blair  (2000b).  \"Do Stock Market Liberalizations Cause Investment Booms?\" Journal of Financial Economics 58 (1-2), pp. 301-34.\n\nJones, Benjamin F., and Benjamin A. Olken (2005).  \"The Anatomy of Start-Stop Growth,\" NBER Working Paper Series 11528.  Cambridge, Mass.:  National Bureau of Economic Research, July.\n\nKaufmann, Daniel, Aart Kray, and Pablo Zoido-Lobaton (1999).  \"Governance Matters,\" Policy Research Working Paper Series 2196.  Washington:  World Bank, October.\n\nKearl, James R., Clayne L. Pope, Gordon C. Whiting, and Larry T. Wimmer (1979).  \"A Confusion of Economists?\" American Economic Review, vol. 69 (May, Papers and Proceedings), pp. 28-37.\n\nKlein, Michael W. (2005).  \"Capital Account Liberalization: Institutional Quality and Economic Growth: Theory and Evidence,\" NBER Working Paper Series 11112.  Cambridge, Mass.:  National Bureau of Economic Research, February.\n\nKose, M. Ayhan, Eswar Prasad, Kenneth Rogoff, and Shang-Jin Wei (2006).  \"Financial Globalization: A Reappraisal (758 KB PDF),\" IMF Working Paper Series WP 06/189.  Washington:  International Monetary Fund, August.\n\nLee, Ha Yan, Luca Antonio Ricci, and Roberto Rigobon (2004).  \"Once Again, Is Openness Good for Growth?\" Journal of Development Economics, vol. 75 (December), pp. 451-72.\n\nLevine, Ross (2004).  \"Finance and Growth,\" NBER Working Paper Series 10766.  Cambridge, Mass.:  National Bureau of Economic Research, September; forthcoming in Philippe Aghion and Steven N. Durlauf, eds., Handbook of Economic Growth.  Amsterdam:  North Holland.\n\nLevine, Ross and Sara Zervos (1998).  \"Capital Control Liberalization and Stock Market Development,\" World Development 26, pp. 1169-84.\n\nMauro, Paolo (1995).  \"Corruption and Growth,\" Quarterly Journal of Economics, vol. 110 (August), pp. 681-712.\n\nNorth, Douglass C. (1990).  Institutions, Institutional Change, and Economic Performance. Cambridge:  Cambridge University Press.\n\nNorth, Douglass C., and Robert Paul Thomas (1973).  The Rise of the Western World: A New Economic History.  Cambridge:  Cambridge University Press.\n\nRajan, Raghuram, and Luigi Zingales (2003). \"The Great Reversals: the Politics of Financial Development in the 20th Century,\" Journal of Financial Economics 69 (1), pp. 5-50.\n\nRajan, Raghuram, and Luigi Zingales (2004).  Saving Capitalism from the Capitalists: Unleashing the Power of Financial Markets to Create Wealth and Spread Opportunity.  Princeton:  Princeton University Press.\n\nRodriguez, Francisco, and Dani Rodrik (2001).  \"Trade Policy and Economic Growth: A Skeptic’s Guide to the Evidence,\" in Ben S. Bernanke and Kenneth Rogoff, eds., NBER Macroeconomics Annual 2000.  Cambridge, Mass.:  MIT Press.\n\nRodrik, Dani, Arvind Subramanian, and Francesco Trebbi (2002).  \"Institutions Rule: The Primacy of Institutions over Geography and Integration in Economic Development,\" NBER Working Paper Series 9305.  Cambridge, Mass.:  National Bureau of Economic Research, November.\n\nSachs, Jeffrey D., and Andrew M. Warner (1995).  \"Economic Reform and the Process of Global Integration,\" Brookings Papers on Economic Activity, 1995:1, pp. 1-118.\n\nSchmukler, Sergio L. (2004).  \"Financial Globalization: Gain and Pain for Developing Countries (307 KB PDF),\" Federal Reserve Bank of Atlanta, Economic Review, vol. 89 (Q2), pp. 39-66.\n\nSvaleryd, Helena, and Jonas Vlachos (2002).  \"Markets for Risk and Openness to Trade:  How Are They Related?\"   Journal of International Economics, vol. 57 (August), pp. 369-95.\n\nTemple, Jonathan (1999).  \"The New Growth Evidence,\" Journal of Economic Literature, vol. 37 (March), pp. 112-56.\n\nWei, Shangjin (1997).  \"How Taxing Is Corruption on International Investors?\" NBER Working Paper Series 6030.  Cambridge, Mass.:  National Bureau of Economic Research, May.\n\nWeil, David N. (2005).  Economic Growth. Boston:  Addison-Wesley.\n\nWinters, L. Alan, Neil McCulloch, and Andrew McKay (2004).  \"Trade Liberalization and Poverty: The Evidence So Far,\" Journal of Economic Literature, vol. 42 (March), pp. 72-115.\n\nWolf, Martin (2004).  Why Globalization Works. New Haven:  Yale University Press.\n\nWorld Bank (2001).  Finance for Growth: Policy Choices in a Volatile World.  New York:  Oxford University Press.\n\nWorld Bank (2005).  Doing Business in 2005:  Removing Obstacles to Growth.  Washington:  World Bank.\n\nFootnotes\n\n1.  A large literature shows the importance of good institutions to economic growth.  See, for example, North and Thomas (1973); Hall and Jones (1999); Acemoglu, Johnson, and Robinson (2001); Easterly and Levine (2001); Rodrik, Subramanian, and Trebbi (2002); Easterly and Levine (2003); Glaeser and others (2004); and the recent survey by Acemoglu, Johnson, and Robinson (2005).  Kaufmann and others (1999) also point to the importance of various aspects of good governance. Return to text\n\n2.  An excellent nontechnical survey of the extensive empirical evidence on the link between financial development and economic growth can be found in World Bank (2001).  See also Levine (2004) and Schmukler (2004). Return to text\n\n3.  A discussion of how the costs of doing business vary across a number of countries is in World Bank (2005).  Return to text\n\n4.  Research finds that increases in corruption are associated with lower growth (for example, Mauro, 1995).  Wei (1997) also finds that corruption significantly reduces foreign direct investment, which is generally considered to be beneficial to growth. Return to text\n\n5.  A survey of the literature that links a lack of sufficient prudential regulation and supporting institutions to excessive risk-taking and the possibility of a subsequent banking crisis is in Demirguc-Kunt and Detragiache (2005).  Dell’Ariccia and Marquez (2006) also argue that under certain circumstances lending booms can make the banking system more unstable and can lead to a higher probability of a banking crisis. Return to text\n\n6.  The literature on microfinance is vast.  One thorough discussion is in Armendariz de Aghion and Morduch (2005). Return to text\n\n7.  When stock markets in emerging-market countries are opened to foreign capital, dividend yields fall, average stock prices increase, and liquidity goes up.  See Levine and Zervos (1998); Bekaert, Harvey, and Lumsdaine (2002); and Henry (2000a,b). Return to text\n\n8.  This argument is made in World Bank (2001) and Goldberg (2004). Return to text\n\n9.  An excellent discussion of the literature on financial globalization using a unified conceptual framework is in Kose and others (2006).  Studies focusing more specifically on the necessary preconditions for, and the appropriate sequencing of, financial reforms, macroeconomic policies, and institutional development, on the one hand, and capital account liberalization, on the other, include Eichengreen (2001), Alfaro and others (2004), and Klein (2005).   Return to text\n\n10.  The four largest state-owned banks, with 70 percent of China’s bank deposits, are scheduled to be privatized in the following order:  the Construction Bank, the Bank of China, the Industrial and Commercial Bank, and the Agricultural Bank. Return to text\n\n11.  The new law becomes effective on June 1, 2007, but reportedly will not apply to state-owned enterprises until 2008. Return to text\n\n12.  Indeed, almost all economists think that trade liberalization, a key element of globalization, is a good thing.  For example, in Kearl and others (1979), 97 percent of economists agreed (generally or with some provisions) with the statement that \"tariffs and import quotas reduce general economic welfare.\" A typical view advocating trade liberalization is expressed by Jagdish Bhagwati, one of the most prominent trade theorists in the world, in Bhagwati (2004).  Return to text\n\n13.  This example comes from Weil (2005, p. 322) and is described more extensively in Dollar and Collier (2001).   Return to text\n\n14.  The literature on the effects of trade liberalization on growth and poverty is immense.  See the surveys in Temple (1999); Bourguignon and others (2002); Winters, McCulloch, and McKay (2004); and Wolf (2004).  Earlier studies found that trade openness was associated with higher growth rates (Dollar, 1992; Sachs and Warner, 1995; and Edwards, 1998). However, because the direction of causation from this evidence is difficult to establish, other researchers have used instrumental variable procedures to establish causality from trade liberalization to growth (for example, Frankel and Romer, 1999).  Using a different approach to identify the direction of causation, Lee, Ricci, and Rigobon (2004) also find that trade openness has a positive effect on growth.   Return to text\n\n15.  For example, Harrison (1996) and especially Rodriguez and Rodrik (2000).  Return to text\n\n16.  The finding in Jones and Olken (2005) that growth take-offs are primarily associated with large and steady expansions in international trade provides further support for this view.   Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/mishkin20070623a.htm",
        "title": "Globalization and Financial Development",
        "date": "6/23/2007"
    },
    {
        "content": "June 15, 2007\n\nChairman Ben S. Bernanke\n\nAt the The Credit Channel of Monetary Policy in the Twenty-first Century Conference, Federal Reserve Bank of Atlanta, Atlanta, Georgia\n\nEconomic growth and prosperity are created primarily by what economists call \"real\" factors--the productivity of the workforce, the quantity and quality of the capital stock, the availability of land and natural resources, the state of technical knowledge, and the creativity and skills of entrepreneurs and managers.  But extensive practical experience as well as much formal research highlights the crucial supporting role that financial factors play in the economy.  An entrepreneur with a great new idea for building a better mousetrap typically must tap financial capital, perhaps from a bank or a venture capitalist, to transform that idea into a profitable commercial enterprise. To expand and modernize their plants and increase their staffs, most firms must turn to financial markets or to financial institutions to secure this essential input.  Families rely on the financial markets to obtain mortgages or to help finance their children's educations.  In short, healthy financial conditions help a modern economy realize its full potential.  For this reason, one of the critical priorities of developing economies is establishing a modern, well-functioning financial system.  In the United States, a deep and liquid financial system has promoted growth by effectively allocating capital and has increased economic resilience by increasing our ability to share and diversify risks both domestically and globally.\n\nJust as a healthy financial system promotes growth, adverse financial conditions may prevent an economy from reaching its potential.  A weak banking system grappling with nonperforming loans and insufficient capital or firms whose creditworthiness has eroded because of high leverage or declining asset values are examples of financial conditions that could undermine growth.  Japan faced just this kind of challenge when the financial problems of banks and corporations contributed substantially to sub-par growth during the so-called \"lost decade.\"\n\nAs the topic of this conference reminds us, financial conditions may affect shorter-term economic conditions as well as the longer-term health of the economy.  Notably, some evidence supports the view that changes in financial and credit conditions are important in the propagation of the business cycle, a mechanism that has been dubbed the \"financial accelerator.\"  Moreover, a fairly large literature has argued that changes in financial conditions may amplify the effects of monetary policy on the economy, the so-called credit channel of monetary-policy transmission.  In fact, as I will discuss, these two ideas are essentially related.  As someone who (in a former life) did research on both of these topics, I thought it might be useful for me to provide a somewhat personal overview of the financial accelerator and credit channel ideas and their common underlying logic.  Along the way I will offer a few thoughts on where future research might be most productive.\n\nMarket Frictions and the Real Effects of Financial and Credit Conditions\nEconomists have not always fully appreciated the importance of a healthy financial system for economic growth or the role of financial conditions in short-term economic dynamics.  As a matter of intellectual history, the reason is not difficult to understand.  During the first few decades after World War II, economic theorists emphasized the development of general equilibrium models of the economy with complete markets; that is, in their analyses, economists generally abstracted from market \"frictions\" such as imperfect information or transaction costs.  But without such frictions, financial markets have little reason to exist.  For example, with complete markets (and if we ignore taxes), we know that whether a corporation finances itself by debt or equity is irrelevant (the Modigliani-Miller theorem).\n\nThe blossoming of work on asymmetric information and principal-agent theory, led by Nobel laureates Joseph Stiglitz and George Akerlof and with contributions from many other researchers, gave economists the tools to think about the central role of financial markets in the real economy.  For example, the classic 1976 paper by Michael Jensen and William Meckling showed that, in a world of imperfect information and principal-agent problems, the capital structure of the firm could be used as a tool by shareholders to better align the incentives of managers with the shareholders' interests.  Thus was born a powerful and fruitful rejoinder to the Modigliani-Miller neutrality result and, more broadly, a perspective on capital structure that has had enduring influence.\n\nMy own first job as an academic was at Stanford University, where I arrived as an assistant professor in the Graduate School of Business in 1979.  At the time, Stanford was a hotbed of work on asymmetric information, incentives, and the principal-agent problem; and even though my field was macroeconomics, I was heavily influenced by that intellectual environment.  I became particularly interested in how this perspective on financial markets could help explain why financial crises--that is, extreme disruptions of the normal functioning of financial markets--seem often to have a significant impact on the real economy.  Putting the issue in the context of U.S. economic history, I laid out, in a 1983 article, two channels by which the financial problems of the 1930s may have worsened the Great Depression (Bernanke, 1983).\n\nThe first channel worked through the banking system.  As emphasized by the information-theoretic approach to finance, a central function of banks is to screen and monitor borrowers, thereby overcoming information and incentive problems.  By developing expertise in gathering relevant information, as well as by maintaining ongoing relationships with customers, banks and similar intermediaries develop \"informational capital.\"  The widespread banking panics of the 1930s caused many banks to shut their doors; facing the risk of runs by depositors, even those who remained open were forced to constrain lending to keep their balance sheets as liquid as possible.  Banks were thus prevented from making use of their informational capital in normal lending activities.  The resulting reduction in the availability of bank credit inhibited consumer spending and capital investment, worsening the contraction.\n\nThe second channel through which financial crises affected the real economy in the 1930s operated through the creditworthiness of borrowers.  In general, the availability of collateral facilitates credit extension.  The ability of a financially healthy borrower to post collateral reduces the lender's risks and aligns the borrower's incentives with those of the lender.  However, in the 1930s, declining output and falling prices (which increased real debt burdens) led to widespread financial distress among borrowers, lessening their capacity to pledge collateral or to otherwise retain significant equity interests in their proposed investments.  Borrowers' cash flows and liquidity were also impaired, which likewise increased the risks to lenders.  Overall, the decline in the financial health of potential borrowers during the Depression decade further impeded the efficient allocation of credit.  Incidentally, this information-based explanation of how the sharp deflation in prices in the 1930s may have had real effects was closely related to, and provided a formal rationale for, the idea of \"debt-deflation,\" advanced by Irving Fisher in the early 1930s (Fisher, 1933).\n\nThe External Finance Premium and the Financial Accelerator\nBoth real and monetary shocks produced the Great Depression, and in my 1983 paper I argued that banking and financial markets propagated both types of impulses, without distinguishing sharply between the two.  My subsequent research and that of many others looked separately at the role of financial conditions in amplifying both monetary and nonmonetary influences.\n\nOn the nonmonetary side, Mark Gertler and I showed how, in principle, the effects of a real shock (such as a shock to productivity) on financial conditions could lead to persistent fluctuations in the economy, even if the initiating shock had little or no intrinsic persistence (Bernanke and Gertler, 1989).  A key concept in our analysis was the external finance premium, defined as the difference between the cost to a borrower of raising funds externally and the opportunity cost of internal funds.  External finance (raising funds from lenders) is virtually always more expensive than internal finance (using internally generated cash flows), because of the costs that outside lenders bear of evaluating borrowers' prospects and monitoring their actions.  Thus, the external finance premium is generally positive.  Moreover, the theory predicts that the external finance premium that a borrower must pay should depend inversely on the strength of the borrower's financial position, measured in terms of factors such as net worth, liquidity, and current and future expected cash flows.  Fundamentally, a financially strong borrower has more \"skin in the game,\" so to speak, and consequently has greater incentives to make well-informed investment choices and to take the actions needed to ensure good financial outcomes.  Because of the good incentives that flow from the borrower's having a significant stake in the enterprise and the associated reduction in the need for intensive evaluation and monitoring by the lender, borrowers in good financial condition generally pay a lower premium for external finance.1\n\nThe inverse relationship of the external finance premium and the financial condition of borrowers creates a channel through which otherwise short-lived economic shocks may have long-lasting effects.  In the hypothetical case that Gertler and I analyzed, an increase in productivity that improves the cash flows and balance sheet positions of firms leads in turn to lower external finance premiums in subsequent periods, which extends the expansion as firms are induced to continue investing even after the initial productivity shock has dissipated.  This \"financial accelerator\" effect applies in principle to any shock that affects borrower balance sheets or cash flows.  The concept is useful in that it can help to explain the persistence and amplitude of cyclical fluctuations in a modern economy.\n\nAlthough the financial accelerator seems intuitive--certainly financial and credit conditions tend to be procyclical--nailing down this mechanism empirically has not proven entirely straightforward.  For example, empirical studies of business investment in structures, equipment, and inventories have often found that a firm's cash flow significantly determines its level of investment and that the link between cash flow and investment tends to be stronger for firms (such as relatively small firms) that have more limited access to capital markets.2  In a \"frictionless\" capital market in which borrowers do not face an external finance premium, a firm's cash position would be irrelevant to its decision to invest because efficient capital markets would supply any necessary funding for investment projects expected to yield a positive net return.  Thus, findings of a positive association between cash flow and investment tend to support the financial accelerator theory.\n\nThese findings also raise issues and questions, however.  I will mention two.  First, as a number of researchers have pointed out, the apparent empirical link between cash flow and investment may arise because cash flow proxies for difficult-to-measure factors like the prospective return on investment, which would be relevant to the investment decision even without capital-market frictions.  This identification problem is a difficult one.  However, some work that has attempted to correct for this possible misspecification has still found a role for cash flow (see, for example, Gilchrist and Himmelberg, 1999).  Second, if only the smallest firms have significant external finance premiums, as implied by some research, then the macroeconomic significance of the financial accelerator may be questioned.  One response to this point, pursued by several researchers, has been to dispute the claim that small firms do not play a significant role in business-cycle fluctuations.  For example, small firms apparently account for a significant portion of cyclical changes in employment and inventory stocks.  Another response has been to argue that even large firms with relatively good access to capital markets may face nontrivial external finance premiums.  For instance, using a sample that included large public companies, researchers at the Board have estimated external finance premiums of economically significant magnitudes, and they showed that these premiums rose sharply during the 2001 recession, as predicted by the financial accelerator theory (Levin, Natalucci, and Zakrajsek, 2004; Levin and Natalucci, 2005).\n\nFinancial accelerator effects need not be confined to firms and capital spending but may operate through household spending decisions as well.3  Household borrowers, like firms, presumably face an external finance premium, which is lower the stronger their financial position.  For households, home equity is often a significant part of net worth.  Certainly, households with low mortgage loan-to-value ratios can borrow on relatively favorable terms through home-equity lines of credit, with the equity in their home effectively serving as collateral.  If the financial accelerator hypothesis is correct, changes in home values may affect household borrowing and spending by somewhat more than suggested by the conventional wealth effect because changes in homeowners' net worth also affect their external finance premiums and thus their costs of credit.  If true, this hypothesis has various interesting implications.  For example, unlike the standard view based on the wealth effect, this approach would suggest that the distribution of housing wealth across the population matters because the effect on aggregate consumption of a given decline in house prices is greater, the greater the fraction of consumers who begin with relatively low home equity.  Another possible implication is that the structure of mortgage contracts may matter for consumption behavior.  In countries like the United Kingdom, for example, where most mortgages have adjustable rates, changes in short-term interest rates (whether induced by monetary policy or some other factor) have an almost immediate effect on household cash flows.  If household cash flows affect access to credit, then consumer spending may react relatively quickly.  In an economy where most mortgages carry fixed rates, such as the United States, that channel of effect may be more muted.  I do not think we know at this point whether, in the case of households, these effects are quantitatively significant in the aggregate.  Certainly, these issues seem worthy of further study.\n\nMonetary Policy and the Credit Channel\nThe ideas I have been discussing today have also been useful in understanding the nature of the monetary policy transmission process.  Some evidence suggests that the influence of monetary policy on real variables is greater than can be explained by the traditional \"cost-of-capital\" channel, which holds that monetary policy affects borrowing, investment, and spending decisions solely through its effect on the level of market interest rates.  This finding has led researchers to look for supplementary channels through which monetary policy may affect the economy.  One such supplementary channel, the so-called credit channel, holds that monetary policy has additional effects because interest-rate decisions affect the cost and availability of credit by more than would be implied by the associated movement in risk-free interest rates, such as Treasury rates.  The credit channel, in turn, has traditionally been broken down into two components or channels of policy influence:  the balance-sheet channel and the bank-lending channel (Bernanke and Gertler, 1995).  The balance-sheet channel of monetary policy is closely related to the idea of the financial accelerator that I have already discussed.  That theory builds from the premise that changes in interest rates engineered by the central bank affect the values of the assets and the cash flows of potential borrowers and thus their creditworthiness, which in turn affects the external finance premium that borrowers face.  For example, according to this view, a tightening of monetary policy that reduces the net worth and liquidity of borrowers would increase the effective cost of credit by more than the change in risk-free rates and thus would intensify the effect of the policy action.\n\nIn the interest of time I will confine the remainder of my remarks to the bank-lending channel.  The theory of the bank-lending channel holds that monetary policy works in part by affecting the supply of loans offered by depository institutions.  This concept is a cousin of the idea I proposed in my paper on the Great Depression, that the failures of banks during the 1930s destroyed \"information capital\" and thus reduced the effective supply of credit to borrowers.  Alan Blinder and I adapted this general idea to show how, by affecting banks' loanable funds, monetary policy could influence the supply of intermediated credit (Bernanke and Blinder, 1988).\n\nHistorically, monetary policy did appear to affect the supply of bank loans (at any given level of interest rates).  In the 1960s and 1970s, when reserve requirements were higher and more comprehensive than they are today, Federal Reserve open market operations that drained reserves from the banking system tended to force a contraction in deposits.  Regulation Q, which capped interest rates payable on deposits, prevented banks from offsetting the decline in deposits by offering higher interest rates.  Moreover, banks had limited alternatives to deposits as a funding source.  Thus, monetary tightening typically resulted in a shrinking of banks' balance sheets and a diversion of funds away from the banking system, a phenomenon known as disintermediation.  The extension of credit to bank-dependent borrowers, which included many firms as well as households, was consequently reduced, with implications for spending and economic activity.\n\nOf course, much has changed in U.S. banking and financial markets since the 1960s and 1970s.  Reserve requirements are lower and apply to a smaller share of deposits than in the past.  Regulation Q is gone.  And the capital markets have become deep, liquid, and easily accessible, either directly or indirectly, to almost all depository institutions.  Although the traditional bank-lending channel may still be operative in economies that remain relatively more bank-dependent, as recent research has found for some European countries (Ehrmann and others, 2003), in the United States today it seems unlikely to be quantitatively important.\n\nThis is not to say, however, that financial intermediation no longer matters for monetary policy and the transmission of economic shocks.  For example, although banks and other intermediaries no longer depend exclusively on insured deposits for funding, nondeposit sources of funding are likely to be relatively more expensive than deposits, reflecting the credit risks associated with uninsured lending (Stein, 1998).  Moreover, the cost and availability of nondeposit funds for any given bank will depend on the perceived creditworthiness of the institution.  Thus, the concerns of holders of uninsured bank liabilities about bank credit quality generate an external finance premium for banks that is similar to that faced by other borrowers.  The external finance premium paid by banks is presumably reflected in turn in the cost and availability of funds to bank-dependent borrowers.  Importantly, this way of casting the bank-lending channel unifies the financial accelerator and credit channel concepts, as the central mechanism of both is seen to be the external finance premium and its relationship to borrowers' balance sheets.  The only difference is that the financial accelerator focuses on the ultimate borrowers--firms and households--whereas financial intermediaries are the relevant borrowers in the theory of the credit channel.  By the way, the existence of loan sales and the originate-to-distribute model of bank lending does not fundamentally change this picture.  Loan sales and similar activities are, in essence, another form of nondeposit financing, and the effective cost of this form of funding to the bank will generally depend on its perceived financial strength and resources (which may affect recourse and reinsurance arrangements with the loan purchasers, for example).\n\nRecently, researchers have pursued a number of approaches in search of evidence of a distinct banking channel.  For example, some researchers have focused on smaller banks, which may have fewer funding alternatives to deposits and whose customer base may consist disproportionately of bank-dependent borrowers (Kashyap and Stein, 2000).  Of course, these days, even the smallest of banks has ready access to sources of funds other than retail deposits.  Thus, even for the smallest banks, the source of any bank-lending channel remains the existence of a finance premium on marginal sources of (uninsured) nondeposit funding, rather than an absolute constraint on the quantity of available funding.  Moreover, for the bank channel to affect economic activity, borrowers accustomed to relying on banks must be unable to turn to other lenders, at least not without some cost.  For some business borrowers, particularly small business borrowers that rely on banking relationships, this scenario is plausible.4  But financial innovation and deregulation imply that borrowers in the market for a mortgage or consumer credit have numerous nonbank financing alternatives, blunting any direct impact of changes in bank lending.  I will return to nonbank lending and its implications in a moment.\n\nIf relationship borrowing is the key, then--as pointed out in the paper at this conference by Black, Hancock, and Passmore--a bank with many such borrowers might defensively invest in deposit capacity, say, by increasing the number of branches.  By actively seeking to finance a high share of loans with insured deposits, such a bank could shield its borrowers from the effects of increases in the nondeposit finance premium, whether the result of monetary policy or some other factor.  Consistent with this idea, these authors find that banks that make a large share of their loans to small businesses also tend to have a high ratio of deposits to loans.\n\nThe recognition that, fundamentally, the bank-lending channel is based on changes in the quality of bank balance sheets naturally turns our attention to bank capital and its determinants (Van den Heuvel, 2002).  Raising new capital on the open market can be difficult and costly for many banks, implying that, in the short run, capital is determined by earnings and changes in asset values.  Changes in the value of capital, particularly when a bank's capital is not much higher than the level demanded by regulators or the market, potentially affect the bank's cost of funds.  In conformity with this hypothesis, various studies have found evidence that loans provided by banks that are more capital-constrained seem more sensitive to changes in market interest rates than loans provided by highly capitalized banks.5  Moreover, changes in the financial condition of banks may play a role in cyclical developments.  I have already mentioned the cases of Japan's \"lost decade.\"  Closer to home, some believe that the U.S. economy's recovery from the 1990-91 recession was delayed by \"financial headwinds,\" which arose from regional shortages of bank capital (Bernanke and Lown, 1991).\n\nOne might view the idea that banks are somehow \"special\" in their ability to gather information and to screen and monitor borrowers as rather dated.  Banks do continue to play a central role in credit markets; in particular, because of the burgeoning market for loan sales, banks originate considerably more loans than they keep on their books.  Nevertheless, nonbank lenders have become increasingly important in many credit markets, and relatively few borrowers are restricted to banks as sources of credit.  Of course, nonbank lenders do not have access to insured deposits.  However, they can fund loans by borrowing on capital markets or by selling loans to securitizers.  Does the rise of nonbank lenders make the bank-lending channel irrelevant?\n\nI am not so sure that it does.  Like banks, nonbank lenders have to raise funds in order to lend, and the cost at which they raise those funds will depend on their financial condition--their net worth, their leverage, and their liquidity, for example.  Thus, nonbank lenders also face an external finance premium that presumably can be influenced by economic developments or monetary policy.  The level of the premium they pay will in turn affect the rates that they can offer borrowers.  Thus, the ideas underlying the bank-lending channel might reasonably extend to all private providers of credit.  Further investigation of this possibility would be quite worthwhile.\n\nConclusion\nI have taken you on a whirlwind tour of several decades of research on how variations in the financial condition of borrowers, whether arising from changes in monetary policy or from other forces, can affect short-term economic dynamics.  The critical idea is that the cost of funds to borrowers depends inversely on their creditworthiness, as measured by indicators such as net worth and liquidity.  Endogenous changes in creditworthiness may increase the persistence and amplitude of business cycles (the financial accelerator) and strengthen the influence of monetary policy (the credit channel).  As I have noted today, what has been called the bank-lending channel--the idea that banks play a special role in the transmission of monetary policy--can be integrated into this same broad logical framework, if we focus on the link between the bank's financial condition and its cost of capital.  Nonbank lenders may well be subject to the same forces.\n\nLet me conclude by offering you best wishes for a stimulating and enjoyable last day of the conference.  Policymakers and scholars both will benefit from your efforts.\n\nReferences\n\nAlmeida, Heitor, Murillo Campello, and Crocker H. Liu (2006). \"The Financial Accelerator: Evidence from International Housing Markets,\" Review of Finance, vol. 10 (September), pp. 1-32.\n\nAoki, Kosuki, James Proudman, and Gertjan Vlieghe (2002). \"Houses as Collateral: Has the Link between House Prices and Consumption in the U.K. Changed?\" (188 KB PDF) Economic Policy Review, Federal Reserve Bank of New York, May, pp. 163-77\n\n-----------------------------------------------(2004). \"House Prices, Consumption, and Monetary Policy: A Financial Accelerator Approach,\" Journal of Financial Intermediation, vol. 13 (October), pp. 414-35.\n\nAvery, Robert B., and Katherine A. Samolyk (2004). \"Bank Consolidation and the Provision of Banking Services: Small Commercial Loans,\" Journal of Financial Services Research, vol. 25 (April), pp. 291-325.\n\nBernanke, Ben S. (1983). \"Non-Monetary Effects of the Financial Crisis in the Propagation of the Great Depression,\" American Economic Review, vol. 73 (June), pp. 257-76.\n\nBernanke, Ben S., and Alan S. Blinder (1988). \"Credit, Money, and Aggregate Demand,\" American Economic Review, vol. 78, Papers and Proceedings of the 100th Annual Meeting of the American Economics Association, May, pp. 435-39.\n\nBernanke, Ben S., and Mark Gertler (1989). \"Agency Costs, Net Worth, and Business Fluctuations,\" American Economic Review, vol. 79 (March), pp. 14-31.\n\n--------------------------------------(1995). \"Inside the Black Box: The Credit Channel of Monetary Policy Transmission,\" Journal of Economic Perspectives, vol. 9 (Fall), pp. 27-48.\n\nBernanke, Ben S., Mark Gertler, and Simon Gilchrist (1999). \"The Financial Accelerator in a Quantitative Business Cycle Framework,\" in Handbook of Macroeconomics, Volume 1C, Handbooks in Economics, vol. 15.  Amsterdam: Elsevier, pp. 1341-93.\n\nBernanke, Ben S., and Cara S. Lown (1991). \"The Credit Crunch,\" Brookings Papers on Economic Activity, 1991:2, pp. 205-39.\n\nBlack, Lamont, Diana Hancock, and Wayne Passmore (2007). \"Bank Core Deposits and the Mitigation of Monetary Policy,\" unpublished paper, Board of Governors of the Federal Reserve System, June.\n\nCalomiris, Charles W., Charles P. Himmelberg, and Paul Wachtel (1995). \"Commercial Paper, Corporate Finance and the Business Cycle: A Microeconomic Perspective,\" Carnegie-Rochester Series on Public Policy, vol. 42 (June), pp. 203-50.\n\nCarlstrom, CharlesT., and Timothy S. Fuerst (2001). \"Monetary Policy in a World without Perfect Capital Markets,\" Working Paper 0115, Federal Reserve Bank of Cleveland.\n\nCarpenter, Robert E., Steven M. Fazzari, and Bruce C. Petersen (1998). \"Financing Constraints and Inventory Investment: A Comparative Study with High-Frequency Panel Data,\" (265 KB PDF) Review of Economics and Statistics, vol. 80 (December), pp. 513-19.\n\nEhrmann, Michael, Leonardo Gambacorta, Jorge Martinez-Pages, Patrick Sevestre, and Andreas Worms (2003). \"Financial Systems and the Role of Banks in Monetary Policy Transmission in the Euro Area,\" in Ignazio Angeloni, Anil K Kashyap, and Benoit Mojon, eds., Monetary Policy Transmission in the Euro area: A Study by the Eurosystem Monetary Transmission Network,. Cambridge: Cambridge University Press.\n\nFisher, Irving (1933). \"The Debt-Deflation Theory of Great Depressions,\" Econometrica, vol. 1 (October), pp. 337-57.\n\nGambacorta, Leonardo (2005). \"Inside the Bank Lending Channel,\" European Economic Review, vol. 49 (October), pp. 1737-59.\n\nGilchrist, Simon, and Charles Himmelberg (1999). \"Investment, Fundamentals and Finance,\" in Ben S. Bernanke and Julio Rotemberg, eds. NBER Macroeconomics Annual 1998.  Cambridge, Mass.: MIT Press.\n\nIacoviello, Matteo (2005). \"House Prices, Borrowing Constraints, and Monetary Policy in the Business Cycle,\" American Economic Review, vol. 95 (June), pp. 739-64.\n\nJensen, Michael C., and William H. Meckling (1976). \"Theory of the Firm: Managerial Behavior, Agency Costs and Ownership Structure,\" Journal of Financial Economics, vol. 3 (October), pp. 305-60.\n\nKashyap, Anil K, and Jeremy C. Stein (2000). \"What Do a Million Observations on Banks Say about the Transmission of Monetary Policy?\" American Economic Review, vol. 90 (June), pp. 407-28.\n\nKishan, Ruby P., and Timothy Opiela (2000). \"Bank Size, Bank Capital, and the Bank Lending Channel,\" Journal of Money, Credit, and Banking, vol. 32 (February), pp. 121-41.\n\n------------------------------- (2006). \"Bank Capital and Loan Asymmetry in the Transmission of Monetary Policy,\" Journal of Banking and Finance, vol. 30 (January), pp. 259-85.\n\nKiyotaki, Nobuhiro, and John Moore (1997). \"Credit Cycles,\" Journal of Political Economy, vol. 105 (April), pp. 211-48.\n\nLevin, Andrew T., and Fabio M. Natalucci (2005). \"The Magnitude and Cyclical Behavior of Financial Market Frictions,\" 2005 Meeting Papers 443, Society for Economic Dynamics.\n\nLevin, Andrew T., Fabio M. Natalucci, and Egon Zakrajsek (2004). \"The Magnitude and Cyclical Behavior of Financial Market Frictions,\" Finance and Economics Discussion Series 2004-70, Board of Governors of the Federal Reserve System, December.\n\nStein, Jeremy C. (1998). \"An Adverse-Selection Model of Bank Asset and Liability Management with Implications for the Transmission of Monetary Policy,\" RAND Journal of Economics, vol. 29 (Autumn), pp. 466-86.\n\nVan den Heuvel, Skander (2002). \"Does Bank Capital Matter for Monetary Transmission?\" (100 KB PDF)  Economic Policy Review, Federal Reserve Bank of New York (May), pp. 1-7.\n\nFootnotes\n\n1. Over the past two decades, an extensive theoretical literature has exploited the idea that borrowers' financial positions affect their external finance premiums and thus their overall cost of credit.  See, for example, Bernanke and Gertler (1989), Kiyotaki and Moore (1997), Bernanke, Gertler, and Gilchrist (1999), Carlstrom and Fuerst (2001), Aoki, Proudman and Vlieghe (2004), and Iacoviello (2005).  Return to text\n\n2. Calomiris, Himmelberg and Wachtel,(1995), Carpenter, Fazzari and Petersen (1998), and Gilchrist and Himmelberg (1999). Return to text\n\n3. See Aoki, Proudman and Vlieghe (2002, 2004), Iacoviello (2005), and Almeida, Campello and Liu (2006). Return to text\n\n4. In recent years, community banks appear to have become increasingly important in lending to small businesses (Avery and Samolyk, 2004). Return to text\n\n5. Kishan and Opiela, (2000), Kishan and Opiela, (2006), and Gambacorta (2005). Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070615a.htm",
        "title": "The Financial Accelerator and the Credit Channel",
        "date": "6/15/2007"
    },
    {
        "content": "June 14, 2007\n\nGovernor Randall S. Kroszner\n\nAt the public hearing under the Home Ownership and Equity Protection Act (HOEPA), Federal Reserve Board, Washington, D.C.\n\nI am pleased to be here today to chair the Federal Reserve Board's public hearing under the Home Ownership and Equity Protection Act (HOEPA).  The hearing will focus specifically on how the Board might use its rulemaking authority under HOEPA to address concerns about abusive mortgage lending practices.  In the course of this hearing, we will hear from key players in the home mortgage market:  lenders, brokers, secondary market participants, consumer advocacy and community development organizations, academics and researchers, and state regulators.  Although they play different roles, all share a common goal:  encouraging responsible mortgage lending for the benefit of individual consumers and the American economy as a whole.\n\nThe Congress enacted HOEPA in 1994 in response to concerns about abusive lending in the home equity market, and the Federal Reserve Board was given broad authority to implement its provisions and to adopt regulations that the Board finds to be necessary and proper to effectuate its purposes.  In addition, the Board has the responsibility to prohibit acts or practices it finds unfair or deceptive, or otherwise designed to evade HOEPA.\n\nThe Board understands its rulemaking responsibility under HOEPA but is not alone in facing the important task of preventing unfair and deceptive practices.  Other regulators share in our responsibility to ensure responsible mortgage lending through enforcement powers.  The states have extensive regulatory authority--and responsibility--under their own anti-predatory lending statutes, various other relevant legal authorities, and especially their mortgage industry licensing acts--which give them considerable control over the activities of mortgage brokers and lenders.  Many of the states, including notably those that are represented on this afternoon's panel, have been very active in reining in bad actors in their mortgage markets.  The FTC also shares in our enforcement responsibilities under HOEPA and other federal laws.  Finally, the other federal financial regulatory agencies each have a duty to enforce federal consumer protection laws, including HOEPA, with respect to the depository institutions under their respective supervisory ambits.  In light of the sheer magnitude of the task, we are very pleased that these regulators all contribute to the goal of ensuring a healthy, competitive, and responsible mortgage market.  We are committed to working closely with other federal and state regulators to ensure that the laws that protect consumers are enforced.\n\nHOEPA also directs the Board to hold hearings such as the one we hold today, to assess the effectiveness of regulations and laws in protecting consumers.  Hearings provide us with valuable information.  In our most recent prior hearings, held last summer in four cities around the country, our goals included assessing the effectiveness of our 2001 amendments to the HOEPA rules in curbing abusive lending practices while preserving access to credit.  We also wanted to gather information on the effectiveness of the mortgage disclosures required by our Regulation Z, pursuant to the Truth in Lending Act, to inform our review of those disclosures, which is underway now.\n\nRising foreclosures in the subprime market over the past year have led the Board to consider whether and how it should use its rulemaking authority to address these concerns.  In doing so, however, we must walk a fine line.  We must determine how we can help to weed out abuses while also preserving incentives for responsible lenders.  A robust and responsible subprime mortgage market benefits consumers by allowing borrowers with blemished or limited credit histories to become homeowners, access the equity in their homes, or have the flexibility to refinance their loans as needed.\n\nIn this task we have several tools at our disposal.  These include required disclosures by lenders, rules to prohibit abusive practices, principles-based guidance with supervisory oversight, less formal efforts to work with industry participants to promote best practices, and consumer education materials.  The Federal Reserve currently is conducting a thorough review of its policies with respect to each of these tools.  Last year, together with other federal banking regulators, we issued guidance concerning so-called nontraditional mortgages.  We have also issued proposed supervisory guidance concerning underwriting standards and disclosures for subprime mortgages.  The agencies are finishing their review of the comments received and expect to issue the final version soon.  And the Federal Reserve produces a range of consumer education materials, including information to help potential borrowers understand adjustable-rate and other alternative mortgages, and we actively promote financial education by partnering with outside organizations.\n\nThe two tools that we will focus on today, however, are lender disclosures to consumers and rules that prohibit or restrict lending practices.  Disclosures provide information that is critical to the effective functioning of markets.  A core principle of economics is that markets are more competitive, and therefore more efficient, when accurate information is available to all participants.  Information helps consumers by improving their ability to compare mortgage products and to choose those that will help them meet their personal goals.\n\nWe are keenly aware, however, of the substantial volume of disclosures and other documents that mortgage lending already entails, and we are sensitive to the risk that too much information may be of practically as little value to many consumers as no information at all.  Accordingly, we intend to consider mortgage disclosures comprehensively, with an eye to improving their usefulness to consumers, while remaining mindful of the total burden for industry.  Perhaps most importantly, we will engage in extensive consumer testing of mortgage disclosures, to ensure that disclosures provide information that consumers can readily use.   Our goal is better disclosures, not necessarily more disclosures.\n\nWe also recognize that disclosures may not always be sufficient to combat abusive practices.  Because some bad lending practices may require additional measures, the Federal Reserve will seriously consider how we might use our rulemaking authority to address abusive practices without restricting consumers' access to beneficial financing options and responsible subprime credit.  In addition to improved disclosures, regulations that restrict or prohibit practices that are \"unfair or deceptive\" may also be necessary.  We have heard concerns about consumers being steered into mortgages they cannot afford and of repeated refinancings involving closing costs that strip away a borrower's home equity.  Today, we will gather information on how we might craft rules to stop such abusive practices.  We also will seek information from state officials regarding their experiences with drafting laws and rules to combat predatory lending efficiently and effectively.\n\nDuring today's hearing, we will seek information from panelists on certain specific questions.  I would like to close by briefly touching on some of those.  There are four terms or practices that have been most frequently cited as troublesome in the mortgage market, especially the subprime and home equity markets.  They are:\n\nAt least some of these practices can be beneficial to at least some consumers.  For example, an informed borrower might choose a loan with a prepayment penalty in exchange for a lower interest rate or lower closing costs.  On the other hand, prepayment penalties also can be abusive, such as when a borrower is unaware that an adjustable rate mortgage loan has a substantial prepayment penalty that will extend beyond the first adjustment of the loan's interest rate, making it costly or impossible for the borrower to refinance the loan to avoid a higher interest rate and payment.  We hope to gather information that helps us determine whether rules can prevent the abusive use of loan terms or practices while preserving their use in instances where they provide benefits to consumers.\n\nGiving adequate consideration to a borrower's ability to repay a loan obviously benefits both borrowers and lenders.  Recently, the Board and the other federal financial regulatory agencies issued guidance reinforcing our collective belief that principles of prudent underwriting require consideration of a borrower's repayment ability.  For example, the agencies have provided that lenders should qualify borrowers for nontraditional mortgage products such as interest-only loans and payment option adjustable-rate mortgage products based on a fully-indexed rate and fully amortizing payment.  Some have urged the Board to adopt this broad principle as a rule, while others have urged the Board to preserve for lenders the flexibility to exercise their judgment in determining the likelihood that a given borrower can repay a loan.  While it seems self-evident that adequate consideration of repayment ability is necessary, our experience in crafting the guidance taught us that this principle is far easier to articulate in general terms than in detailed, prescriptive rules stating which underwriting practices constitute \"adequate\" consideration.  This is especially true in the context of mortgage credit underwriting, which can depend on such a great number of pertinent, consumer-specific considerations.\n\nAgenda",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20070614a.htm",
        "title": "Encouraging Responsible Mortgage Lending: Prospective Rulemaking Initiatives",
        "date": "6/14/2007"
    },
    {
        "content": "June 05, 2007\n\nChairman Ben S. Bernanke\n\nTo the 2007 International Monetary Conference, Cape Town, South Africa (via satellite)\n\n\n\nOver the past four quarters, the U.S. real gross domestic product (GDP) has increased at an average rate of about 2 percent.  Growth during the first quarter of this year was held down by some factors--notably, significant declines in inventory accumulation, net exports, and federal defense spending--that seem likely to be at least partially reversed in the near term.  Of course, the adjustment in the housing sector is still ongoing, and the slowdown in residential construction now appears likely to remain a drag on economic growth for somewhat longer than previously expected.  Thus far, however, we have not seen major spillovers from housing onto other sectors of the economy.  On average, over coming quarters, we expect the economy to advance at a moderate pace, close to or slightly below the economy’s trend rate of expansion.\n\nAs expected, we have also seen a gradual ebbing of core inflation, although its level remains somewhat elevated.  Despite recent increases in the prices of crude oil and gasoline, energy prices overall are below last year’s peak; the rate of increase in shelter costs seems likely to slow, although the timing remains uncertain; and long-run inflation expectations, as derived from both surveys and market-based measures of inflation compensation, have remained contained.  However, although core inflation seems likely to moderate gradually over time, the risks to this forecast remain to the upside.  In particular, the continuing high rate of resource utilization suggests that the level of final demand may still be high relative to the underlying productive capacity of the economy.\n\nIn my brief remarks today I will focus on some recent developments in housing, including the emergence of some serious stresses in subprime mortgage markets.  I will also discuss some initiatives taken or planned by the Federal Reserve to respond to the problems in subprime mortgage lending.\n\nDevelopments in the Housing Market\nAs you know, the downturn in the housing market has been sharp.  From their peaks in mid-2005, sales of existing homes have declined more than 10 percent, and sales of new homes have fallen by 30 percent.  A leveling-off of sales late last year hinted at a possible stabilization of housing demand; however, once one smoothes through the monthly volatility of the data, more-recent readings indicate that demand weakened further, on net, over the first four months of this year.  House prices decelerated sharply last year, following annual gains averaging 9 percent from 2000 to 2005.  Prices have continued to be quite soft so far in 2007, although for the most part outright price declines have been concentrated in markets that showed especially large increases in earlier years.\n\nHomebuilders have responded to weak sales by curtailing construction.  Single-family housing starts have declined by a third since early 2006, sufficient to subtract about 1 percentage point from real GDP growth over the past four quarters.  Despite the drop in homebuilding, the inventory of unsold new homes has risen to more than seven months of sales, a level well above the average observed over the past decade.  Accordingly, and as reflected in the continued downward trend in permits to build single-family homes, residential construction will likely remain subdued for a time, until further progress can be made in working down the backlog of unsold new homes.\n\nRecent developments in the subprime mortgage market add somewhat to the usual uncertainty in forecasting housing demand.  Subprime mortgage borrowing nearly tripled during the housing boom years of 2004 and 2005.  But decelerating house prices, higher interest rates, and slower economic growth have contributed to an increased rate of delinquency among subprime borrowers.  This increase has occurred almost entirely among borrowers with adjustable-rate mortgages; delinquency rates for fixed-rate subprime mortgages have remained generally stable.  Some of the increased difficulties now being experienced by subprime borrowers are likely the result of an earlier loosening of underwriting standards, as evidenced by the pronounced rise in 2006 in “early payment defaults”--defaults occurring within a few months of mortgage origination.  All told, the rate of serious delinquencies for subprime mortgages with adjustable interest rates--corresponding to mortgages in the foreclosure process or with payments ninety or more days overdue--has risen to about 12 percent, roughly double the recent low seen in mid-2005.1  The rate of serious delinquencies has also risen somewhat among some types of near-prime mortgages, although the delinquency rates in those categories remain much lower than the rate in the subprime market.\n\nAs a consequence of these developments, investors are now scrutinizing nonprime loans more carefully, and lenders in turn have tightened up their underwriting.  Risk premiums on indexes of credit default swaps for subprime mortgage-backed securities (MBS) began to widen sharply late last year, especially for those on pools of mortgages originated in 2006.  Credit spreads on new subprime MBS have also risen.  Respondents to the Senior Loan Officer Opinion Survey in April indicated a substantial net tightening of standards for subprime mortgages.\n\nTighter lending standards in the subprime mortgage market--together with the possibility that the well-publicized problems in this market may dissuade potentially eligible borrowers from applying--will serve to restrain housing demand, although the magnitude of these effects is difficult to quantify.  Subprime and near-prime mortgage originations rose sharply in 2004 and 2005 and likely accounted for a large share of the increase in the number of home sales over that period.  However, originations of nonprime mortgages to purchase homes appear to have peaked in late 2005 and declined substantially since then, and by more (even in absolute terms) than prime mortgage originations.  Thus, some part of the effect on housing demand of the retrenchment in the subprime market has likely already been felt.  Moreover, indicators such as the gross issuance of new subprime and near-prime MBS suggest that the supply of nonprime mortgage credit, though reduced, has by no means evaporated.2  That said, the tightening of terms and standards now in train may well lead to some further contraction in nonprime originations in the period ahead.  We are also likely to see further increases in delinquencies and foreclosures this year and next as many subprime adjustable-rate loans face interest-rate resets.\n\nWe will follow developments in the subprime market closely.  However, fundamental factors--including solid growth in incomes and relatively low mortgage rates--should ultimately support the demand for housing, and at this point, the troubles in the subprime sector seem unlikely to seriously spill over to the broader economy or the financial system.\n\nFederal Reserve Initiatives and Possible Regulatory Actions\nWhatever their effects on the broader economy, the problems in the subprime sector are causing real distress for many homeowners.  To help mitigate the situation, the Federal Reserve and other federal supervisory agencies are encouraging the banks and thrift institutions that we supervise to work with borrowers who may be having trouble meeting their mortgage obligations, including identifying and contacting borrowers before they enter delinquency or foreclosure.  Federal Reserve Banks around the country are cooperating with community and industry groups that work with borrowers and affected communities.  We also continue to work with organizations that provide counseling about mortgage products to current and potential homeowners.  Studies suggest that counseling can be effective in helping borrowers make better financial decisions.3  In addition, we at the Federal Reserve, other regulators, and the Congress are evaluating what actions may be needed to prevent a recurrence of these problems.  In deciding, we must walk a fine line:  We have an obligation to prevent fraud and abusive lending; at the same time, we must tread carefully so as not to suppress responsible lending or eliminate refinancing opportunities for subprime borrowers.\n\nBroadly speaking, financial regulators have four types of tools to protect consumers and to promote safe and sound underwriting practices:  required disclosures by lenders, rules to prohibit abusive or deceptive practices, principles-based guidance with supervisory oversight, and less-formal efforts to work with industry participants to promote best practices.  The Federal Reserve currently is conducting a thorough review of its policies with respect to each of these instruments.\n\nEffective disclosures are the first line of defense against improper lending.  If consumers are well informed, they are in a much better position to make decisions in their own best interest.  To fulfill its responsibilities under the Truth-in-Lending Act (TILA), the Board has undertaken a full review of consumer disclosures for mortgage lending.  For example, we are considering whether to require lenders to provide certain disclosures more quickly and are evaluating existing requirements concerning the advertising of mortgage products.  As we did for our recently released proposed rulemaking on credit-card disclosures, we will be doing extensive consumer testing to evaluate the effectiveness of current and proposed disclosures required of all mortgage lenders.  Of course, the information provided by even the best-designed disclosures can be useful only when it is well understood.  Accordingly, the Federal Reserve produces a range of consumer education materials, including information to help potential borrowers understand adjustable-rate and other alternative mortgages, and we actively promote financial education by partnering with outside organizations.\n\nHowever, combating bad lending practices, including deliberate fraud or abuse, may require additional measures.  Under the Home Ownership Equity Protection Act (HOEPA), the Board has the responsibility to prohibit mortgage lending practices that it finds to be unfair and deceptive.  In 2001, the Board banned several practices for high-cost loans, such as loan flipping--a practice characterized by frequent and repeated refinancing to generate fees for lenders.  We will consider whether other specific lending practices are unfair or deceptive and should thus be prohibited under HOEPA.  Next week we are holding a public hearing to gather input about potential abuses in mortgage lending.  We will also continue to seek input from consumer and industry groups, the Federal Reserve’s Consumer Advisory Council, our fellow regulators, and others who may have useful insights about mortgage lending practices.\n\nWe have also used, and will continue to use, supervisory guidance to help mitigate problems in nonprime lending.  Last year, together with other federal banking regulators, we issued guidance concerning so-called nontraditional mortgages.  We have also issued draft supervisory guidance concerning underwriting standards and disclosures for subprime mortgages.  The agencies are now reviewing the many responses to the draft proposal.\n\nThe patchwork nature of enforcement authority in subprime lending poses a special challenge.  For example, rules issued by the Board under HOEPA apply to all lenders but are enforced--depending on the lender--by the Federal Trade Commission, state regulators, or one of the five federal regulators of depository institutions.  To achieve uniform and effective enforcement, cooperation and coordination are essential.  We are committed to working closely with other federal and state regulators to ensure that the laws that protect consumers are enforced.\n\nTogether with other regulators and the Congress, we have much to do and many issues to consider.  We undertake that effort with utmost seriousness because our collective success will have significant implications for the financial well-being, access to credit, and opportunities for homeownership of many of our fellow citizens.\n\nFootnotes\n1.  Estimates of delinquencies are based on data from First American LoanPerformance.  Return to text\n\n2.  Issuance data are from Inside Mortgage Finance.  Return to text\n\n3.  See, for example, Abdighani Hirad and Peter M. Zorn (2001), “A Little Knowledge Is a Good Thing: Empirical Evidence of the Effectiveness of Pre-Purchase Homeownership Counseling,” and Gregory Elliehausen, E. Christopher Lundquist, and Michael E. Staten (2003), \"The Impact of Credit Counseling on Subsequent Borrower Credit Usage and Payment Behavior,\" papers presented at “Seeds of Growth--Sustainable Community Development: What Works, What Doesn’t and Why?,” a Federal Reserve System Community Affairs Research Conference held in Washington, D.C., March 27-28, 2003.   Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070605a.htm",
        "title": "The Housing Market and Subprime Lending",
        "date": "6/5/2007"
    },
    {
        "content": "June 05, 2007\n\nGovernor Kevin Warsh\n\nAt the European Economics and Financial Centre, London, England\n\n\n\nThank you for the opportunity to speak today about financial intermediation and developments in the capital markets.  New financial instruments and changing models of financial intermediation are having a profound impact on global financial markets.  London, home to one of the world’s fastest growing and much admired financial centers, represents a most appropriate venue to discuss these changes.1\n\nDuring the past few decades--particularly the last few years--we have witnessed an escalating supply of new financial instruments scarcely able to match surging demand.  Through the technique of unbundling risks--dividing them by category and tranche--financial instruments proliferated to enable risks to be borne by those most willing to accept them.  And with the benefit of ample liquidity, which I described in previous remarks as broadly equal to confidence, financial products quickly found deep markets, ensuring robust trading.2\n\nIndeed, liquidity-fueled innovation has made markets substantially more “complete”--that is, more risks are more readily priced and traded without significant diminution in value than in prior periods.3  The concept of “complete markets” is, in my view, a useful, aspirational, and theoretical construct, even though perfectly complete markets, in which every agent is able to costlessly trade claims on every state of the world, represents an unachievable goal.  After all, the fundamental departures from the complete markets paradigm--transactions costs and asymmetric information--will never vanish.  Nevertheless, in these heady times, one wonders whether some market participants may treat recent developments as a testament to an all-but-assured path to complete markets.\n\nIn my remarks today, I will discuss how liquidity and financial innovation are making markets more complete--or more precisely, less incomplete--than in earlier periods.4  I will also describe how the acceleration toward complete markets conflated the roles among financial intermediaries.  Finally, I will consider the possible consequences should this trend toward complete markets be slowed or upended by some form of negative liquidity shock.\n\nFinancial Intermediation:  Remembering the “Good Old Days”\nBroadly speaking, intermediaries facilitate the transfer of capital and risk between borrowers and savers.  The case for financial intermediation rests on two premises: \n(1) borrowers hold superior information about their own financial condition and prospects than prospective providers of capital; (2) the search and screening costs to match creditors and debtors without the services of trusted third parties are prohibitive.\n\nThe history of financial intermediation, thus, is the story of individual types of institutions--FDIC-insured commercial banks, SEC-registered broker dealers, Federal Reserve-supervised holding companies, just to name a few--adapting their products and practices to meet customer demands.\n\nTraditionally, commercial banks played critical roles as principals in the financial intermediation process.  Community banks today--to a somewhat greater degree than their larger money-center brethren--most clearly retain the traditional commercial banking approach to financial intermediation.  Community banks finance relatively opaque entities, such as private companies and households.  They raise funds primarily by issuing demand deposits.  They deploy capital by underwriting loans, monitoring borrowers, and retaining some loans in portfolio as long-term investments.  Of course, when banks originate and hold loans, they internalize the costs of their own underwriting standards, and so have strong incentives to screen borrowers and enforce contractual covenants, as appropriate.5\n\nInvestment banks also served important roles in financial intermediation.  Somewhat unlike commercial banks, investment banks traditionally acted more as agents, distributing, rather than holding, most of their resulting exposures.  They focused predominantly on the business of underwriting; that is, helping firms gain access to the public securities markets.  In this role, investment banks added value to the intermediation process, in part, by de facto certification of a company’s operations, practices, and prospects.  As repeat players in the underwriting business, investment banks sought to maintain prudent underwriting standards to bolster their reputations as effective screeners of companies.  In so doing, they built significant value in their distribution networks.\n\nFinancial intermediation has also long been provided by less-regulated institutions.  For decades, relatively high net worth, sophisticated investors formed private pools of capital to invest their funds.  To allow flexibility in their investments, these pooled investment vehicles were structured to be exempt from regulation as investment companies.  Reflecting the nature of the investor base, these informal pools often sought to gain exposure to asset classes to diversify their portfolios.\n\nThese traditional roles of financial intermediaries seem, somehow, oddly quaint in 2007.  As I will next describe, financial intermediaries have been transformed by their own actions‑‑their massive investments in human and technological capital to develop new financial products‑‑and by the worldwide rise in liquidity that has added depth and resiliency to the markets for these new products.\n\nThe Brave New World:  Massive Financial Innovation and Ample Liquidity\nNo single statistic can put the recent surge of financial product and market innovation into perspective.  I will, however, cite a few illustrative indicators.\n\nThe value added by the U.S. financial sector gives a rough sense of the effect these innovations have had on the U.S. economy.  In 1960, financial services accounted for about 3-1/2 percent of U.S. gross domestic product.  By 2006, that percentage had more than doubled.\n\nSecuritized products are one important area of product and market innovation.  Of the $3.6 trillion in net borrowing in U.S. credit markets in 2006, $820 billion, or nearly one-fourth, was securitized.  As of year-end 2006, roughly 30 percent of the outstanding balances of corporate bonds and household credit resided in asset-backed securities.\n\nIn addition, use of derivative products has soared; the notional amount of interest rate swaps and options has tripled in the past four years to nearly $300 trillion.  The notional amount of credit default swaps has almost quintupled in only two years, from $6.4 trillion outstanding in 2004 to $29 trillion in 2006.\n\nProduct and market innovations, such as these, have contributed to the high degree of liquidity found today in global financial markets.  As I have discussed in prior remarks, “liquidity” in the sense of “trading liquidity” reflects the ability to transact quickly without exerting a material effect on prices.  Underlying this concept is the fact that although the many buyers and sellers have different views on the most likely outcomes, the distributions of possible outcomes for which they demand risk-based compensation can be quantified.  Liquidity exists when investors are confident and willing to take risks.  Liquidity, then, can be viewed as confidence on the part of buyers and sellers of securities.  By disaggregating a security into its constituent risk components, financial innovation can unlock this liquidity.\n\nStrong global economic performance provides another important support for the high liquidity and levels of confidence in today’s capital markets.  Many economies have achieved a marked reduction in the volatility of real output and core inflation in the past twenty years or so.  Liquidity can flourish if investors interpret strong performance to mean that future economic outcomes will be benign and that “tail” realizations are either highly improbable or, at the very least, quantifiable and, hence, can be traded upon.6\n\nThere is little doubt, then, that liquidity in most financial markets is high today and that investors seem willing to take risks, even at today’s market-prevailing prices.  In the United States, term premiums on long-term Treasury yields are very low, corporate bonds appear to be nearly “priced for perfection,” and stock prices are setting new records.  Credit markets are highly accommodative for issuers, and the volume of loans to finance highly leveraged transactions is escalating rapidly.  These prices, terms and credit conditions may reflect solid economic fundamentals--low output and inflation uncertainty, healthy corporate balance sheets, and corporate profits that exceed market expectations--and if so, they may help to ease the effects of fluctuations in liquidity should they occur.  The prices and conditions may also reflect increased appetite for risk; or, far less auspiciously, they may be indicative of investor overconfidence.\n\nThe Brave New World: More Complete Markets and Conflating Roles\nThe powerful combination of liquidity and financial innovation has made markets seemingly more complete--that is, more risks are priced and traded without undue penalty owing to their unique nature or shallowness of the relevant financial market.  Financial innovation, by definition, makes markets more complete by expanding the set of available types of securities and reducing transaction costs.  And liquidity provides some degree of assurance that funds will readily flow into new structures and new securities.\n\nThe benefits of more complete markets are three-fold.  First, they allow firms and households to hedge a variety of risks, a considerable benefit when volatility is costly.  Second, they make it more feasible for investors to fine tune the risk-return profiles of their portfolios.  The concomitant reduction in risk premiums required by investors should reduce capital costs for all economic agents.  Third, risks once held within the four walls of financial institutions are converted into tradable securities and distributed and dispersed to a broader base of institutions and interests.\n\nLiquidity and financial innovation have also led to a conflation of the roles played by financial intermediaries, themselves.  Intermediation has moved increasingly to financial markets and away from regulated bank institutions.  The core functions of financial institutions‑‑creating, distributing, and owning risk‑‑have remained the same, but the type of firm performing the various functions appears to have changed dramatically.  This conflation of roles has the potential to alter some incentives in the credit extension process.  And these roles may well change with further product and market developments, and, not least of all, due to a potential ebbing of liquidity.\n\nFirst, consider the changing role of commercial banks.  In the syndicated loan market, lead banks have increasingly distributed large shares of syndicated loans to other banks and institutional lenders, many in the form of collateralized loan obligations (CLOs).  In addition, commercial banks in some cases now sell entire loans rather than retain them on their books.  Indeed, they now securitize a variety of credit exposures, such as credit cards and mortgages, and sell them to structured entities.  While banks still maintain large loan exposures, including most recently those for “equity bridges,” the increased trend to distribute rather than hold some residual risks is unmistakable.\n\nSecond, consider the critical role played by investors purportedly with high tolerance for risk.  A CLO structure does not in and of itself convert a pool of low-rated securities into tranches of high-rated securities through some form of alchemy.  Rather, a class of investors must be willing to hold the most concentrated risks as residual claimants.  Today, that role appears to be tailor-made for hedge funds, and sometimes the trading desks at leading investment banks.  By serving as willing counterparties in a variety of contracts, these institutions, in my view, are serving as a critical linchpin in the development of more complete markets.\n\nThird, consider the evolving role of investment banks, no longer just originating and distributing primary securities, but increasingly owning assets in portfolio.  The more traditional agent role surely provided an enviable advantage in gauging investor appetite for new securities.  Similarly, their expertise in pricing public securities may have served them well in developing new financial products--so well in fact that they are increasingly creating and underwriting new securitized and risk-management products.  As part of their modern-day underwriting role, investment banks sometimes hold (at least for a time) the residual or other tranches, of these new products.  Even more recently, many one-time broker-dealers have increased their principal investing by serving as general partners of private equity funds.\n\nChanges in intermediation roles certainly raise potential challenges, though they may be alleviated somewhat by reputation and other market-based factors.  The increased specialization of some commercial banks as originators and distributors--and less as long-term owners--of some loans may have lessened their incentives to screen borrowers, and to write and monitor loan covenants.  Notably, this incentive problem may be more pronounced when they sell an entire loan, as in mortgage securitization programs, and less so when the bank retains some portion of the loan, as with syndicated loans.  Moreover, in the subprime mortgage market, for example, some believe that investment banks that pool and structure loans might have insufficient incentives in some cases to effectively screen.  After all, critics assert, a borrower walks into a commercial bank for a loan, not into the investment bank that packages and distributes the loan through a structured vehicle.\n\nGreater liquidity or confidence does little to mitigate these problems; indeed, it could exacerbate them if confidence begets complacency.  For instance, if in liquid times, investors in structured products become complacent, they may not fully evaluate or accurately model the nature of the underlying assets in structured entities.  Complacent investors also may be willing to buy new debt offerings that are light on traditional covenants if they come to believe that outcomes are assuredly benign.  Moreover, without strong covenants on existing debt, firms can raise additional funds without triggering defaults, which may decrease defaults in the short run, but perhaps increase them in the long run.  In these cases, this “gloss” of confidence could cause a misallocation of resources, if companies or consumers without compelling prospects for full repayment nonetheless readily obtain credit.\n\nWe, as policymakers, should be careful, and indeed humble, in making definitive judgments in this fast-changing area.  Investment banks and commercial banks with enduring reputations--and growing private pools seeking permanent capital--are wont to protect their credibility and financial strength, even in liquid market conditions.  To the extent that investment banks hold portions of the equity or lower-rated tranches of the securitized pools, they have strong incentives to screen effectively.  Moreover, evidence from secondary markets that suggests that investors differentiate securitized pools according to the identity of the originating banks, and not just by credit ratings, provides some comfort that traditional certification functions are at work.\n\nWhat Happens When Liquidity Falters?\nOf course, we as policymakers should be careful not to suffer from a failure of imagination in considering the causes and consequences of an unexpected negative shock to liquidity.  (Surely, financial innovators have not suffered from a lack of imagination.)  Indeed, we must ask ourselves what may happen when liquidity falters.  Well, consider the consequence if stock prices sold off globally, implied volatility jumped, and record trading volumes overwhelmed the trading capacity on the stock exchanges.  Consider a spike in a measure of implied U.S. equity-market volatility so large that it would be in the top 1/2 of one percent of one-day changes in nearly a generation.  Then, reflect on the likely divergence of opinions on the possible causes of such a rapid change in sentiment‑‑perhaps it was a freefall in stock prices in a growing emerging-market economy, or escalating concerns about lending standards in some markets, or rapidly changing animal spirits eroding investor confidence.  Well, it does not take a long memory to recall that this scenario played out for a few days in late February, a bit more than three months ago.  As you all know, share prices quickly recovered, and implied volatility reverted to near-record low levels.\n\nWhat lessons can be drawn from such an episode?  Perhaps because of more complete markets, shocks to liquidity are less likely to become self-fulfilling and less likely to impose more lasting damage.  That hypothesis seems particularly credible when the shock is based neither on rapidly changing economic fundamentals nor a genuine breakdown in market infrastructure.  In the recent episode, opportunistic capital apparently viewed large movements in asset prices as trading opportunities.\n\nOr, perhaps, striking as it was, we have not yet witnessed a scenario that subjects the latest product innovations and behavior of market participants to a sufficiently stringent stress test.  Some highly-leveraged private pools of capital may be unable to ride out bouts of very high turbulence if they are compelled to sell assets to meet margin calls or withdrawals, and by so doing, amplify the initial shocks.  The losses would be sharper and correlations would be higher for assets in which markets quickly become shallow.  In theory, to the extent that more complete markets yield deeper, more robust asset markets and better-diversified positions, the dynamics of the disturbance should be more manageable.  But, the question of actual experience versus theory remains.\n\nOf course, a reduction in liquidity rooted in economic fundamentals would likely be more protracted and beget far more serious consequences.  In a period of sustained and diminished liquidity, it is indeed plausible that financial intermediaries could revert to their traditional roles, thereby likely worsening the efficiency and completeness of markets.  In such a case, those who pine for the good old days and purported good old roles of commercial and investment banks might not enjoy the economic environment that accompanies their return.\n\nEven so, over the long-term, I remain a watchful optimist:  Reductions in liquidity are unlikely to turn back the clock on financial innovation.  The knowledge of how to bundle, distribute, and price risk cannot be erased from our collective memories.  Nor can this knowledge be walled off from the rest of the world by one country or financial center.  The culture of capitalism remains on the march.  First-mover advantages are growing, even while the half-life of financial innovation is shrinking.  The growth of competing financial centers around the globe is testament to the enduring power of financial innovation and the dynamism of financial intermediaries.  In my judgment, while the completeness of markets will remain an elusive goal, and the depth and breadth of financial markets will invariably be tested in ways that punish the ill-advised and unprepared, the secular trend toward more complete markets is unlikely to abate.  Cyclical variances, however, are far more difficult to predict with precision.\n\nPerhaps, then, we should consider the growth in financial innovation as being analogous to Moore’s Law, which has for more than a generation accurately predicted the growth in computing power of a transistor at constant cost.7  Neither Moore’s Law nor the development of new financial products is a mathematical or physical certainty based on some geometric extrapolation or law of the universe.  Rather, it is an ex post description of that which already transpired, an occurrence caused by a healthy mix of investment in human and technological capital, a culture of capitalism, and sound regulatory and legal policies.  As a result, meaningful erosion of any these core elements could impair the future evolution of financial products and jeopardize the continued development of markets.\n\nConclusion\nInnovations in financial products and practices, combined with strong liquidity, have accelerated the trend toward more complete markets.  These changes have altered the roles of traditional financial intermediaries.  In so doing, the products and practices of financial intermediation have, in my view, forever changed.  As for the financial intermediaries themselves, they will continue to evolve with changing economic and financial risks.  Indeed perhaps some may retreat to the practices of an earlier era if liquidity falters.  In this case, markets may, for a time, become less complete.   However, I believe that the advances of intellectual capital and the culture of capitalism will likely continue to increase the ability of markets to transfer risk even as liquidity fluctuates.\n\nAs policymakers, we should continually review the changing financial landscape.8  Our regulatory and supervisory responses should be as dynamic as the financial intermediaries and the products they proffer, while adhering to our long-standing objectives to promote financial stability, investor protection, and market integrity.  In general, our regulatory frameworks should not be based on a product or class of institutions.  Rather, we should strive to develop common, principles-based, risk-focused approaches that can adapt as intermediaries makes choices about whether to be originators, distributors or owners of risk (or all of the above).  Among the key elements of such approaches are an emphasis on robust stress testing, enhanced counterparty risk management, and safe and efficient market infrastructures.\n\n\n\nFootnotes\n\n1.  The opinions I will be expressing are my own and do not necessarily correspond with those of my colleagues in the Federal Reserve System.  Nellie Liang and Daniel M. Covitz, of the Federal Reserve Board’s staff, provided valuable contributions to these remarks. Return to text\n\n2.  Kevin Warsh (2007), “Market Liquidity: Definitions and Implications,” speech delivered at the Institute of International Bankers Annual Washington Conference, Washington, March 5, www.federalreserve.gov/newsevents/speech/warsh20070305a.htm.     Return to text\n\n3.  The notion of complete markets in an idealized economy was first formalized in Kenneth J. Arrow and Gerard Debreu (1954), “Existence of Equilibrium for a Competitive Economy,” Econometrica, vol. 22 (July), pp. 265-90.  Return to text\n\n4.  While some of the data I cite are U.S.-based, the trends of financial innovation and strong liquidity are also apparent in many other advanced economies.  Return to text\n\n5.  Insurance of commercial bank deposits has the potential to mitigate these incentives to screen and monitor borrowers.      Return to text\n\n6.  Other sources of liquidity include increased international capital flows as a result of excess savings in some emerging-market and oil-exporting countries, which have flowed to U.S. and other financial markets with sound legal and regulatory structures.  See Warsh (2007), “Market Liquidity:  Definitions and Implications” for discussion.   Return to text\n\n7.  Moore’s Law, attributed to Gordon Moore, co-founder of Intel, is the empirical observation that the number of transistors on an integrated circuit doubles about every eighteen to twenty-four months.   Return to text\n\n8.  See Ben S. Bernanke (2007), “Regulation and Financial Innovation,” speech delivered at the Federal Reserve Bank of Atlanta’s 2007 Financial Markets Conference, May 15, www.federalreserve.gov/newsevents/speech/bernanke20070515a.htm.  Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/warsh20070605a.htm",
        "title": "Financial Intermediation and Complete Markets",
        "date": "6/5/2007"
    },
    {
        "content": "June 01, 2007\n\nGovernor Randall S. Kroszner\n\nAt the Institute of International Finance 2007 Spring Membership Meeting, Athens, Greece\n\n\n\nThank you for inviting me to participate in this conference and offer my views on prospects for the U.S. economy.  I should note that the opinions I will express today are my own and not necessarily those of my colleagues on the Federal Open Market Committee.\n\nAfter increasing at about a 3-1/2 percent annual rate from early 2003 to early 2006, the pace of economic expansion in the United States stepped down last spring, with real gross domestic product rising since then at about a 2 percent pace.  Yesterday, the Department of Commerce announced that real GDP increased at an annual rate of 0.6 percent in the first quarter, a downward revision of its earlier estimate.  Activity was held down in the first quarter, in part, by several factors that seem likely to prove transitory--for example, a drop in exports, a significant reduction in inventory investment, and a decline in defense spending.  My view and, it seems, the view of many private forecasters is that economic growth will pick up as we move through the year, with the rate of expansion close to potential by 2008.\n\nIn recent quarters, weakness in the housing market has been the major source of the slowdown in the growth of real GDP.  Indeed, declines in real residential investment held down the growth of output nearly 1-1/4 percentage points in the second half of last year and another 1 percentage point in the first quarter of this year.  After an extended boom in housing, the demand for homes began to weaken in mid-2005.  By the middle of 2006, sales of both new and existing homes had fallen considerably from their peak levels.  Homebuilders responded to the drop in demand by sharply curtailing construction.  Even so, the inventory of unsold homes rose dramatically.  Sales of new homes weakened further in the first part of this year, and the inventory of unsold homes continued to rise.\n\nThe latest data for the housing market have been mixed.  Sales of single-family existing homes fell 2-1/2 percent in April to a level 5 percent below the rate posted in the second half of last year.  In the market for new homes, however, sales jumped in April, which brought them back in line with the average rate posted in the second half of 2006.  The inventory of unsold new homes also improved a bit in April.  Nonetheless, the supply of unsold homes is still quite high by historical norms, and further production adjustments by homebuilders seem likely, thereby suggesting that the drag on economic activity from the housing correction will continue in coming quarters.\n\nAnother area of softness has been business spending for new equipment, which slowed toward the end of last year and remained anemic in the first quarter of 2007.  Part of the slowing reflected weaknesses in the construction industry, and also in the motor vehicle industry.  While high-tech has been a bright spot, the demand in other industries generally was tepid.  Still, incoming data support prospects for an improvement in investment.  Orders and shipments of nondefense capital goods excluding aircraft rose in April for a second month.  In addition, business sentiment--as measured, for example, by the Institute for Supply Management survey of purchasing managers--has moved higher lately.  More generally, sound business financial conditions--solid balance sheets, high profits, and relatively low interest rates and credit spreads--also should support stronger investment going forward.\n\nLet me turn now to the consumer sector.  Until the early part of this year, continuing increases in employment and income helped personal consumption expenditures provide a source of strength for overall growth.  More recently, consumption growth has slowed, in part because of the effects of higher oil prices on real income.  Looking forward, consumer spending should be supported by the likelihood of further gains in employment.  Private nonfarm payroll employment increased 63,000 in April, and the latest data on initial claims for unemployment insurance as well several other labor market indicators point to additional advances in hiring.\n\nOn the international trade front, recent readings on economic activity abroad have been quite positive.  Moreover, many of the forward-looking indicators of activity suggest that prospects for further economic expansion in Europe and Japan appear good in the near term.  Despite indications of moderation in some emerging-market economies, the overall pace of activity in these countries, including China, appears to be strong.  These factors suggest that the demand for U.S. exports of goods and services will continue to be solid.\n\nI will now turn from aggregate demand to aggregate supply and inflation.  One important influence on aggregate supply and inflation is the trend growth rate of labor productivity.  From 1995 to 2000, productivity in the nonfarm business sector increased at an average annual rate of 2-1/2 percent.  It then accelerated to a rate of about 3 percent during the first half of this decade despite a recession, the fall of the dot-com market, a broad stock market correction, terrorism, and corporate governance scandals.  However, in the early part of 2006, productivity growth slowed markedly, and it has increased at less than a 1 percent rate over the past four quarters.\n\nThe recent deceleration raises the possibility that the trend has slowed significantly.  If the trend has slowed, that could have important implications for the conduct of monetary policy.  But the deceleration also could reflect, at least in part, cyclical dynamics in response to a slowing economy.  Indeed, estimates of trend productivity from a variety of outside forecasters, which generally range from 2 percent to 2-3/4 percent, point to the possibility that much of the recent weakness will prove transitory.  Of course, the middle of this range would suggest a modest slowdown from the consensus estimate from a year or so ago, which was around 2-1/2 percent.  In any event, the possibility of a slowing in trend productivity is a risk to the outlook that I will continue to monitor.\n\nAnother potential influence on inflation is the degree of pressure on resources.1  In the labor market, the unemployment rate declined over the course of 2006 from about 5 percent to 4-1/2 percent, and so far this year it has remained close to that level.  Reflecting the tightening of the labor market as well as the pressure on top-line inflation from increases in energy prices in recent years, the rate of increase in hourly compensation, although volatile, appears to be moving up.  But the markup of prices over unit labor costs is still high by longer-run standards, and the wide margin implies that firms have the capacity to absorb increases in unit labor costs, at least for a while.\n\nAs for price inflation, over the twelve months ending in April, the consumer price index excluding food and energy was up 2.3 percent, the same increase as in the preceding twelve months.  Inflation as measured by the price index for personal consumption expenditures excluding food and energy rose 2.1 percent over the twelve months ending in March (the latest available data).  The most recent consensus forecast for total CPI inflation (on a fourth-quarter to fourth-quarter basis) is 3.0 percent in 2007 and 2.4 percent in 2008.2  The anticipated moderation in inflation apparently reflects the expectation that the impetus from energy and other commodity prices will wane.  In addition, inflation pressures should be restrained by the cumulative effects of monetary policy actions and other factors restraining the growth of aggregate demand.\n\nPerhaps most important for the inflation outlook, inflation expectations appear to remain contained.  Median long-run inflation expectations from the Reuters/Michigan survey moved up to 3.1 percent in April, but this level is still in the narrow range seen over the past few years.3  Long-run inflation compensation derived from spreads between yields on nominal and inflation-indexed Treasury securities stood at 2.4 percent on May 30 (ten-year, adjusted for carry effect), in the middle of the range observed since the turn of the year.\n\nLet me close with a few comments on what I see as some of the more important risks to the outlook for the U.S. economy.  One potential upside risk is that consumer demand, which has show resilience in the face of high energy prices and the challenges in the housing market, grows at a faster pace than expected, thereby increasing pressures on resource utilization.  One risk on the downside is the possibility that new home construction will weaken substantially further or that weakness in the housing market will spill over to other sectors and dampen aggregate demand significantly.  Such an outcome could be prompted by a more pronounced weakening of home sales, a larger-than-expected drag on the housing market from the inventory of unsold homes, or more adverse developments in financial markets.\n\nAlthough developments in nonprime mortgage markets are causing hardship for many individuals and families, to date these challenges do not appear to be having a broad impact on economic activity.  As you know, delinquency rates on subprime variable-rate mortgages have sharply increased.  However, subprime variable-rate mortgages account for, at most, one in ten home-loan borrowers, and delinquency rates for most other types of residential mortgages remain low.  Thus, relative to the broader housing market, the troubles in the subprime market should remain contained.\n\nFurthermore, the effects of rising subprime delinquencies have not seriously spilled over to the broader financial markets.  Risk premiums on subprime residential-mortgage-backed securities (RMBS) and derivative products have widened considerably, as investors are requiring higher returns.  However, the issuance of subprime RMBS has slowed only moderately.  Lenders have tightened standards and terms, but much more so on subprime and nontraditional mortgages than on prime loans.4  Risk spreads on corporate bonds and premiums on credit default swaps for investment banks with some exposure to subprime markets have risen only slightly.  Moreover, we see no serious effects on banks or thrift institutions; for the most part, the troubled lenders have not been institutions with federally insured deposits.\n\nEven so, foreclosure starts are rising so an increasing number of homeowners face the prospect of losing their home.  Before continuing my assessment of the macro risks, I would like to take a moment to describe some of the current regulatory responses to the problems of elevated delinquencies and foreclosures.  First, the Federal Reserve Board and the other federal financial institution regulators have already been encouraging the banks and thrifts that they supervise to work with borrowers who may be having trouble meeting their mortgage payments and for whom workouts are in the interest of both parties.5  The federal regulators also issued draft supervisory guidance earlier this year to remind lenders of the importance of sound underwriting standards and clear disclosures, particularly in the case of subprime variable-rate mortgages.6  In addition, we are actively considering how to make mortgage disclosures more effective and empower consumers to make better-informed decisions to achieve their financial goals.7  We will, for example, review whether advertisements adequately disclose the limits of low initial rates and potential payment changes.\n\nFinally, on June 14, I will be chairing the fifth in a series of hearings on how the Board might use its rulemaking authority under the Home Ownership and Equity Protection Act, or HOEPA, to address concerns in the mortgage lending market.  The purpose of the hearing is to gather information to evaluate how the Federal Reserve can prevent predatory lending in a way that also preserves incentives for responsible lenders to continue to lend.  The Federal Reserve will do all that it can to prevent fraud and abusive mortgage lending practices.  However, any new rules should be drawn clearly to avoid creating legal or regulatory uncertainty that could have the unintended consequence of restricting consumers' access to responsible subprime credit.\n\nAnother area of possible financial risk that we are watching is leveraged lending.8  Business borrowing for mergers and acquisitions and for corporate refinancing has been quite robust over the past few years as firms have taken advantage of relatively low interest rates to reduce their cost of capital.  As underwriters have brought these deals to the market, the good earnings of corporate borrowers and several years of very low defaults have encouraged lenders and investors to fund hundreds of billions of dollars in leveraged loans.  However, with this growth we are seeing some trends in the leveraged loan market that warrant closer monitoring:  Deals continue to be structured with thin pricing, more leverage, and looser covenants than is typical for non-investment-grade borrowers.  Further, originating banks are capitalizing on the strong investor demand for these loans by underwriting to distribute them, including through securitization, while holding only nominal exposures themselves.  Although defaults and other indicators of borrower distress still remain low, and banks’ exposures are so far quite manageable, supervisors are mindful that high levels of leverage can lead to credit problems relatively quickly for both borrowers and lenders when conditions turn.  We want to ensure that any adverse events do not materially affect the banking industry and hope to encourage market participants to employ more-realistic expectations and structures in underwriting riskier corporate loans.\n\nTurning to inflation risks, the high level of resource utilization continues to have the potential to put additional upward pressure on inflation.  And, of course, higher oil prices and the possibility of further increases also pose an upside risk to inflation.  With these concerns in mind, the latest statement issued by the Federal Open Market Committee again highlighted the risk that inflation could fail to moderate as expected, and I believe that the risks to the inflation outlook are primarily to the upside.  The statement also noted that future policy adjustments will depend on the evolution of the outlook for inflation and economic growth, as implied by incoming information.  I will continue to monitor these developments, as well as those in financial markets, very closely.\n\n\n\nFootnotes\n\n1.  Kroszner, Randall S. (2007), “The Changing Dynamics of Inflation,” speech delivered at the 2007 Annual Washington Economic Policy Conference of the National Association for Business Economics, Arlington, Virginia, March 12, www.federalreserve.gov/newsevents/speech/Kroszner20070312a.htm.  Return to text\n\n2.  Blue Chip Economic Indicators (2007), May 10 (New York:  Aspen Publishers), p. 3. Return to text\n\n3.  Richard Curtin (2007), “April 2007 Survey Results,” Reuters/University of Michigan Surveys of Consumers (Ann Arbor:  University of Michigan), April. Return to text\n\n4.  Board of Governors of the Federal Reserve System (2007), “The April 2007 Senior Loan Officer Opinion Survey on Bank Lending Practices,” May, www.federalreserve.gov/boarddocs/SnLoanSurvey/200705. Return to text\n\n5.  Board of Governors of the Federal Reserve System, Federal Deposit Insurance Corporation, National Credit Union Administration, Office of the Comptroller of the Currency, and Office of Thrift Supervision (2007), \"Federal Regulators Encourage Institutions to Work with Mortgage Borrowers Who Are Unable to Make Their Payments,\" press release, April 17, www.federalreserve.gov/newsevents/press/bcreg/20070417a.htm. Return to text\n\n6.  Board of Governors of the Federal Reserve System, Federal Deposit Insurance Corporation, National Credit Union Administration, Office of the Comptroller of the Currency, and Office of Thrift Supervision (2007), \"Agencies Seek Comment on Subprime Mortgage Lending Statement,\" press release, March 2, www.federalreserve.gov//newsevents/press/bcreg/20070302a.htm. Return to text\n\n7.  Randall S. Kroszner (2007), “Creating More Effective Consumer Credit Disclosures,” speech delivered at the George Washington University School of Business, Financial Services Research Program Policy Forum, Washington, June 1, www.federalreserve.gov/newsevents/speeches/Kroszner20070523a.htm. Return to text\n\n8.  A leveraged loan is commonly defined as a syndicated loan, typically to a riskier borrower, with an interest rate of at least libor plus 125 basis points. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20070601a.htm",
        "title": "Outlook and Risks for the U.S. Economy",
        "date": "6/1/2007"
    },
    {
        "content": "May 24, 2007\n\nGovernor Frederic S. Mishkin\n\nAt the Conference on Price Measurement for Monetary Policy, Federal Reserve Bank of Dallas, Dallas, Texas\n\nThis conference focuses on measurement issues, and in my remarks I want to focus on one of the most important measurement issues that we at the Federal Reserve and other central banks face:  How do we determine whether the economy is operating above or below its maximum sustainable level?  That is, how do we estimate the path of potential output?1\n\n\n\nThe Federal Reserve operates under a dual mandate to achieve both price stability and maximum sustainable employment.  In that context, it is natural to think of potential output as the level of output that is consistent with the maximum sustainable level of employment:  That is, it is the level of output at which demand and supply in the aggregate economy are balanced so that, all else being equal, inflation tends to gravitate to its long-run expected value.\n\nThe combination of the dual mandate and this definition suggests two reasons that estimating the path of potential output is so central to the conduct of monetary policy.  First, to evaluate whether our policies will help achieve the maximum sustainable employment objective of the dual mandate, we need know the level of future output that would be consistent with that objective.  Second, the level of output relative to potential output, which is referred to as the output gap, plays an important role in the inflation process.  When the actual level of output is above potential output--so that the output gap is positive--labor and product markets are excessively tight; then, if things such as expected inflation and temporary supply factors are held constant, inflation will tend to rise.  Conversely, when the output gap is negative and labor and product markets are slack, inflation will tend to fall.  Estimates of the future path of potential output are therefore needed to assess whether the projected path of output that is implied by current monetary policy will lead inflation to move in a direction that is consistent with price stability.\n\nBecause estimates of potential output are an important part of central bankers' toolkits, the Federal Reserve and other central banks devote considerable resources to getting the best measures of potential output possible.  In this talk, I want to explore something that Bismarck warned us we shouldn't want to examine:  \"what goes into the sausage\"--or in this case, what goes into central bankers' thinking about how to estimate potential output.\n\nBroadly speaking, there are three basic approaches to estimating potential output: (1) aggregate approaches; (2) production function, or growth-accounting, approaches; and (3) the newest kid on the block, dynamic stochastic general equilibrium (DSGE) approaches.  Let's look at each of these in turn, with the major focus on the production function approach, one to which we at the Federal Reserve currently pay a lot of attention.\n\n(Please note that my comments here reflect my own views and not necessarily those of the Board of Governors or the Federal Reserve System.)\n\nAggregate Approaches\nAggregate approaches to estimating potential output can be thought of as top down approaches because they look at relationships involving aggregate variables and use them to derive measures of potential output. For example, one way of estimating potential output is to assume that if a change in employment or output is sustainable, then it is likely to be permanent.  This assumption suggests using univariate statistical methods to identify the permanent component of changes in output, which could then be viewed as a reasonable measure of potential output.  Examples of such an approach include the work of Beveridge and Nelson (1981) and Clark (1987).  Although univariate approaches to measuring potential output have the advantage of simplicity and can provide a feel for what potential output might be, they suffer from several disadvantages.  First, they require a variety of statistical assumptions about which economic theory provides little guidance--for example, about the correlation between permanent and transitory components or whether the permanent component should be modeled as a random walk.  Perfectly sensible alternative assumptions can lead to very different estimates of potential output.  Second, these purely statistical approaches do not tell us whether this measure of the permanent component of output movements provides information about the most important aspect of potential output from a central banker's perspective--namely, its association with a stable rate of inflation.\n\nIn measuring potential output, we therefore need to bring in some economics.  One potentially valuable economic relationship we can use is the \"natural rate\" version of the Phillips curve, which followed from the seminal research of Nobel prize winners Milton Friedman (1968) and Edmund (Ned) Phelps (1967).  Friedman and Phelps demonstrated that there should not be a long-run tradeoff between inflation and unemployment and that the economy will gravitate to some natural rate of unemployment in the long run no matter what the rate of inflation is.  In other words, the long-run Phillips curve is vertical, and attempts to lower unemployment below the natural rate will only result in higher inflation.\n\nAccording to the natural-rate hypothesis, there is a natural rate of unemployment--also more commonly referred to as the NAIRU (non-accelerating inflation rate of unemployment)--at which inflation tends to gravitate to its long-run expected value.2   A natural rate of output--that is, potential output--corresponds to the NAIRU.  The difference between actual and potential output, the output gap, tells us whether inflation will tend to move up or down, holding things like inflation expectations, energy prices, import prices, and so forth constant.  The natural-rate hypothesis thus suggests that potential output can be estimated from a multivariate approach in which potential output is an unobserved component in the relationship between inflation and the output gap.  Kuttner (1994) provides a good example of this approach.3\n\nAn alternative approach involves deriving the NAIRU directly from estimates of Phillips curves and then using Okun's law--which relates the output gap to the unemployment gap (the actual unemployment rate minus the NAIRU)--to estimate potential output.4   These multivariate approaches are reasonably simple and make intuitive sense, but they also have serious drawbacks.  First, they require that the specification of the Phillips curve is correct.  For example, the model needs to correctly characterize the relationship between the unemployment rate gap and inflation dynamics while taking into account how inflation expectations are formed, and it should not leave out any other variables that have an impact on inflation.5   Indeed, many economists criticize the Phillips curve, with some even declaring it dead.6\n\nSecond, using Okun's law to derive potential output requires an appropriate specification for the dynamics of the relationship between output and unemployment gaps.  However, cyclical fluctuations in productivity and labor supply can complicate this relationship.  Moreover, Okun's law can be thrown well off course during periods of unanticipated structural change in the economy, such as the early 1970s, when U.S. productivity growth slowed.  As a result of these influences, Okun's law has not always been a reliable guide to the relationship between the unemployment gap and the output gap and thus has not always been the most useful guide for estimating potential output.7  Finally, even if the Phillips curve and the Okun's law relationships are specified correctly--a big if--the statistical uncertainty about the estimates of the NAIRU, and therefore also about potential output, is very large--certainly larger than a policymaker would like.  For example, the estimates of Staiger, Stock, and Watson (1997a and b) of the 95 percent confidence interval for the NAIRU were as much as 3 percentage points wide.    Thus estimates of the NAIRU, in isolation, provide policymakers with little real-time insight for assessing the effect of labor markets on inflation pressures.\n\nProduction Function (Growth-accounting) Approaches\nBecause of the shortcomings of the aggregate approaches described above, some researchers estimate potential output using a production function approach that generates an estimate of potential from the underlying factors of production.  This approach is sometimes referred to as \"growth accounting\" because, after the log of a production function is differentiated, output growth can be expressed as a weighted average of the growth of factor inputs--that is, capital services and labor input (hours worked and labor quality)--and a residual--multifactor (also called total factor) productivity growth.  The NAIRU concept still plays an important role in this approach because it helps to determine the level of potential output.\n\nA major advantage of the growth-accounting approach is that it focuses on the various factors that drive growth in potential output, rather than simply on the historical behavior of output growth or on the historical relationship between output and labor inputs as in Okun's law.  The disaggregated nature of the growth-accounting approach means that more data can be used to estimate potential output.  These additional data are likely to be particularly valuable when the economy is undergoing major structural changes--for example, the productivity slowdown starting around the early 1970s, the surge and subsequent slowdown in population growth from the baby-boom generation's entry and (now) exit from the labor force, the remarkable upsurge in labor participation of females in the 1970s and 1980s, and the pickup in productivity growth starting in the second half of the 1990s.  Given these advantages, it is not surprising that the growth-accounting framework is widely used to estimate the path of potential output, both by central banks such as the Federal Reserve and by academic researchers.\n\nIn its simplest formulation, the growth-accounting framework characterizes output growth as the sum of the growth rate of raw labor hours and the growth rate of output per hour, that is, labor productivity.8   In turn, the growth rate of labor hours is described as the sum of population growth (civilian non-institutional population aged sixteen and older), the rate of change of the labor force participation rate, the rate of change in the employment rate, and the rate of change in the number of hours in the average workweek.  Labor productivity is decomposed into the contributions of capital deepening (the marginal product of capital--typically estimated as capital's share of income--times the growth rate of capital services per hour), changes in labor quality, and the growth rate of multifactor productivity.\n\nTo obtain estimates of potential output growth, researchers can use economic analysis to estimate the individual components above that go into the growth-accounting framework.  Much research has been conducted along these lines in the Federal Reserve System and elsewhere.  For example, Aaronson and others (2006) examine what variables influence individual decisions to participate in the labor market (birth cohort, age, sex, number and age of children, and so forth).  They then use this information together with the relative size of different cohorts to show that the aging of the baby-boom cohort can explain much of the decline in labor force participation since 2000 and why the participation rate is likely to continue to decline further in the future.  Given slower projected population growth and what appears to be a downward trend in the number of hours in the average workweek, these results suggest that potential output growth will be slower than it otherwise would have been.  Of course, such projections are subject to uncertainty, and economists hold a range of views about the prospects for future labor force growth.  One important unknown is whether increases in longevity and better health will boost the labor force participation rates of older individuals more than currently embedded in these projections.\n\nSimilarly, economists at the Fed have been at the forefront of research on why labor productivity growth ratcheted upward in the second half of the 1990s and continued at a surprisingly robust pace over the first half of this decade.  Oliner, Sichel, and Stiroh (2007), for example, find that the IT (information technology) sector has been a key element in the higher productivity growth that we have been experiencing since the mid-1990s.9   But they also find that the sources of growth in the second half of the 1990s were quite different than in the period after 2000.  In the 1995-2000 period, labor productivity growth was driven by substantial gains in multifactor productivity in the tech sector, which, in turn, led to sharp reductions in the prices of high-tech equipment and stimulated investment in this type of capital in other sectors of the economy.  Since 2000, high productivity growth has apparently been driven importantly by industry restructuring in response to pressures on profits (the firms that saw the sharpest drops in profits were those that had the largest gains in labor productivity) and by a reallocation of material and labor inputs across industries.  Their estimate for the current trend labor productivity growth is centered around 2-1/4 percent, but they find large uncertainty around this estimate, with a 95 percent confidence interval that ranges from 1-1/4 percent to 3-1/4 percent.  Roberts (2001) estimates even larger uncertainty around structural labor productivity growth.\n\nDespite the advantages of the growth-accounting framework, it still presents some difficulties.  For one, there is, as I just highlighted, a large degree of uncertainty surrounding the estimates of the components that go into the growth-accounting formulas.  In addition, the growth-accounting framework requires a substantial amount of data, some of which are not especially reliable.  For example, it is especially difficult to measure the growth rate of capital services, and the Bureau of Labor Statistics releases its initial estimates with a lag of at least one year.  In addition, the bureau's two measures of employment, one derived from a survey of firms and the other from a survey of households, often provide very different pictures of what is happening in the labor market.  Also data for capital services, labor composition, and multifactor productivity are not readily available for all sectors of the U.S. economy.\n\nBecause it is so difficult to reliably estimate potential output using either the aggregate or the growth-accounting approach, it should come as no surprise that we at the Federal Reserve use a lot of judgment in constructing our estimates of potential output.  In particular, we see judgment as playing three important roles in our procedures.  First, it enables us to take account of the effects of structural changes in the economy that cannot be modeled directly.  Second, it allows us to deal with model misspecifications that cannot be corrected.  Third, we can use judgment to correct for measurement errors or inconsistencies in economic data.  For example, we judgmentally adjust model-based estimates of the NAIRU to account for movements in the unemployment rate unrelated to changes in labor market slack.  Of these, the most important has been the shift in the demographic composition of the labor force, driven largely by the entrance and subsequent maturation of the baby-boom generation.  But economists have pointed to a number of other factors that would influence the NAIRU as well.\n\nAnother example relates to the way in which we estimate the trend growth rate of multifactor productivity.  In particular, estimates of trend multi-factor productivity growth tend to be sensitive to the choice of modeling strategy, a problem that became particularly apparent during the \"jobless recovery\" of the early 1990s, when the normal relationship between output growth and employment growth appeared to break down.  Statistical filtering models like those described by Roberts (2001) tended in real time to attribute much of the weak employment growth to an acceleration in trend productivity.  Later on, however, after employment recovered, it became clear that the unusual behavior of employment during that period was explained better as a temporary reluctance by firms to hire than as a step-up in the rate of trend productivity growth.  In a similar vein, it may at times make sense to down weight more-recent estimates of the data used in filtering exercises to account for the possibility of future revisions to the data.\n\nFinally, it can often be useful to look at Okun's law as a check on the estimates of potential output derived from the growth-accounting approach.  Although, as I noted above, one should not expect Okun's law to hold from quarter to quarter, the relationship is relatively robust over longer periods, and a persistent deviation in the unemployment rate from that predicted by Okun's law might call into question the estimated trends in one or more of the components in potential output.\n\nDynamic Stochastic General Equilibrium Approaches\nThe real business cycle literature, which started with the work of Nobel Prize winners Finn Kydland and Edward Prescott (1982), features optimizing agents and emphasizes the role of technology shocks in explaining both economic growth and business cycles.  Dynamic stochastic general equilibrium (DSGE) models contain many features of the earlier real business cycle literature, but, because they allow for rigidities and imperfections in markets, they are often referred to as New Keynesian models.  The New Keynesian DSGE models provide more-realistic, yet still theoretically elegant, representations of the economy, and their development has been an exciting area of research in macroeconomics in recent years.\n\nNew Keynesian DSGE models provide a somewhat different, but complementary, perspective on the definition of potential output than the one I outlined at the beginning of this speech.  In particular, we might think of potential output as the level of output that an economy could attain if the inefficiencies resulting from nominal wage and price rigidities were removed--that is, if wages and prices were fully flexible.10    The definition of potential output as a flexible price equilibrium has much in common with the more conventional definition I discussed earlier because over time prices (and wages) do gravitate toward their equilibrium levels.  As a result, the DSGE definition accords with the idea that potential output is the level of output at which inflation tends neither to rise nor fall.\n\nThat said, the DSGE view of potential output also has important differences with the earlier approaches to estimating potential output.  Although research on using DSGE models to estimate potential output is in its infancy and so should be read cautiously, papers such as Neiss and Nelson (2005) and Edge, Kiley, and Laforte (2007) are finding that the properties of potential output and output gap fluctuations can be quite different from conventional measures.  For example, in many DSGE models, potential output can undergo swings over the business cycle, a result that should not be surprising considering that the early real business cycle models viewed the business cycle as being primarily an efficient response to shocks to the economy.  In addition, fiscal policy shocks, changes in households' preferences with regard to saving and consumption, changes in preferences about leisure that affect labor supply, and terms-of-trade shocks can all cause potential output to fluctuate.  In contrast, growth-accounting approaches to estimating potential output generally assume that such shocks have no important effects on potential output at business-cycle frequencies.  As a consequence, their estimates typically have smaller fluctuations than measures of potential output derived from DSGE models, and thus the output gaps in the current generation of DSGE models tend to be less variable than conventional measures and can be quite different for particular periods.\n\nAlthough the research on DSGE models is promising, measures of potential output and the output gap from these models are controversial.  The DSGE measures of potential output are far more model dependent than more-conventional measures because they depend on the estimated parameters of the model and on the model's estimates of the structural shocks hitting the economy. As a result, DSGE models with different characterizations of the economy's underlying structure can produce substantially different estimates of potential output.  This is apparent, for example, in the large differences between the potential output measures in the DSGE models of Neiss and Nelson (2005) from those in Edge, Kiley, and Laforte (2007).  Moreover, DSGE models often require strong assumptions to identify the shocks to potential output from model equation residuals.  The finding that these models imply smaller and less persistent output gaps than traditional models may simply reflect the fact that inefficiencies other than price rigidities, such as real wage rigidities, are important for output fluctuations.11   As a result, some policymakers have been quite critical of the implication of DSGE models that a substantial fraction of business-cycle fluctuations are efficient and so do not require a response from monetary policymakers.12\n\nImplications for Policy\nNow that we have looked inside the sausage of estimating potential output, I hope you have not lost your appetite for thinking about what these measurement issues mean for monetary policy.  As I indicated earlier, considerable uncertainty surrounds the measures of potential output derived from any of the approaches I have discussed.  In addition, there is also what economists call Knightian uncertainty (named after the famous University of Chicago economist Frank Knight)--the fact that we are not even sure of the appropriate modeling approach to measure potential output.  Adding even more to the uncertainty of potential output measures are (1) that the observable data do not always correspond to the data we would like to have to produce measures of potential output and (2) that initial estimates of observable data can subsequently be revised substantially, resulting in a very different picture of what is happening to potential output and the output gap.  Orphanides (2001) points out that output gaps were grossly mismeasured in the 1970s, in part because the initially published data did not reflect the true state of the economy.13\n\nWhere does the high uncertainty about actual and potential output leave us at central banks?  Does it mean that we should abandon our focus on potential output and output gaps in making decisions on monetary policy?  I think not.  For better or worse, we cannot escape the need for information on output gaps so that we can forecast the future path of inflation and evaluate the current setting of our monetary policy instruments.  However, we also need to recognize that because measures of potential output and output gaps are so uncertain, we must always be aware that they might be providing misleading signals as to the future course of inflation and the appropriateness of the stance of policy.  In assessing whether there is slack in the economy, we at central banks look not only at our estimates of output gaps but also at a wide range of indicators drawn from the labor, product, and financial markets to provide us with a perspective on the balance of supply and demand in the economy.  Most important, the substantial uncertainty in our measures of potential output implies that we need to be cautious about taking on board the implications of our current estimates of the output gap.  For example, if inflation is moving in a different direction than the output gap would suggest, then we should take seriously the possibility that our output gap measure is not providing us with reliable information.\n\nThe bottom line is that we must never take our eye off of the inflation ball.  Good policymaking requires that we acknowledge what we are unsure about, and this requirement applies particularly to measures of potential output.\n\nReferences\n\nAaronson, Stephanie, Bruce Fallick, Andrew Figura, Jonathan Pingle, and William Wascher (2006). \"The Recent Declines in the Labor Force Participation Rate and Its Implications for Potential Labor Supply,\" Brookings Papers on Economic Activity, 1: 2006, pp. 69-154.\n\n\n\nAltig, David, Terry Fitzgerald, and Peter Rupert (1997). \"Okun's Law Revisited: Should We Worry about Low Unemployment?\" Economic Commentary, Federal Reserve Bank of Cleveland (May).\n\nApel, Mikael, and Per Jansson (1999). \"A Theory-Consistent System Approach for Estimating Potential Output and the NAIRU,\" Economics Letters, vol. 64, pp. 271-75.\n\nAtkeson, Andrew, and Lee Ohanian (2001). \"Are Phillips Curves Useful for Forecasting Inflation?\" Federal Reserve Bank of Minneapolis Quarterly Review, vol. 25, no. 1, pp. 2-11.\n\nBean, Charles (2005). \"Comment on Bob Hall's 'Separating the Business Cycle from Other Economic Fluctuations',\" speech delivered at \"The Greenspan Era: Lessons for the Future,\" a symposium sponsored by the Federal Reserve Bank of Kansas City, held in Jackson Hole, Wyo., August 25-27.  http://www.kansascityfed.org/PUBLICAT/SYMPOS/2005/PDF/Bean2005.pdf (86 KB PDF)\n\nBeveridge, Stephen, and Charles R. Nelson (1981). \"A New Approach to Decomposition of Economic Time Series into Permanent and Transitory Components with Particular Attention to Measurement of the 'Business Cycle',\" Journal of Monetary Economics, vol. 7 (March), pp. 151-74.\n\nBlanchard, Olivier, and Jordi Gali (2007). \"Real Wage Rigidities and the New Keynesian Model,\" Journal of Money, Credit, and Banking, vol. 39 (February), pp. 35-65.\n\nClark, Peter K. (1987). \"The Cyclical Component of U.S. Economic Activity,\" Quarterly Journal of Economics, vol. 102 (November), pp. 797-814.\n\nCochrane, John H. (1994). \"Permanent and Transitory Components of GNP and Stock Prices,\" Quarterly Journal of Economics, vol. 109 (February), pp. 241-65.\n\nCorrado, Carol, and Lawrence Slifman (1999). \"Decomposition of Productivity and Unit Costs,\" American Economic Review, vol. 89 (May), pp. 328-32.\n\nDupasquier, Chantal, Alain Guay, and Pierre St-Amant (1999). \"A Survey of Alternative Methodologies for Estimating Potential Output and the Output Gap,\" Journal of Macroeconomics, vol. 21 (Summer), pp. 577-95.\n\nEdge, Rochelle M., Michael T. Kiley, and Jean-Pierre Laforte (2007). \"Natural Rate Measures in an Estimated DSGE Model of the U.S. Economy,\" Finance and Economics Discussion Series 2007-8 (Washington: Board of Governors of the Federal Reserve System, March).\n\nEstrella, Arturo, and Frederic S. Mishkin (1999). \"Rethinking the Role of NAIRU in Monetary Policy: Implications of Model Formulation and Uncertainty,\" in John B. Taylor, ed., Monetary Policy Rules. Chicago:  University of Chicago Press, pp. 405-30.\n\nFallick, Bruce, Charles A. Fleischman, and Jonathan Pingle (2006). \"How the Graying of the Baby Boom Affects the U.S. Labor Market,\" in The Economic Outlook for 2007:  Papers Presented at the Fifty-Third Annual Conference on the Economic Outlook. Ann Arbor: University of Michigan, pp. 102-18.\n\nFriedman, Milton (1968). \"The Role of Monetary Policy,\" American Economic Review, vol. 58 (March), pp. 1-17.\n\nGoodfriend, Marvin, and Robert G. King (1997). \"The New Neoclassical Synthesis and the Role of Monetary Policy,\" in Ben S. Bernanke and Julio J. Rotemberg, eds., NBER Macroeconomics Annual, 1997.  Cambridge, Mass.: MIT Press, pp. 231-83.\n\nGordon, Robert J.  (1982). \"Inflation, Flexible Exchange Rates, and the Natural Rate of Unemployment,\" in Martin Neil Baily, ed., Workers, Jobs, and Inflation. Washington, D.C.: Brookings Institution, pp. 89-158.\n\nGroshen, Erica L., and Simon Potter (2003). \"Has Structural Change Contributed to a Jobless Recovery?\" Current Issues in Economics and Finance, Federal Reserve Bank of New York, vol. 9 (August).\n\nJorgenson, Dale W., Mun Sing Ho, and Kevin Stiroh (2004). \"Will the Productivity Resurgence Continue?\" Current Issues in Economics and Finance, Federal Reserve Bank of New York, vol. 10 (December).\n\nKydland, Finn E., and Edward C. Prescott (1982). \"Time to Build and Aggregate Fluctuations,\" Econometrica, vol. 50 (November), pp. 1354-70.\n\nKuttner, Kenneth N. (1994). \"Estimating Potential Output as a Latent Variable,\" Journal of Business and Economic Statistics, vol. 12 (July), pp. 361-68.\n\nModigliani, Franco, and Lucas Papademos (1978). \"Optimal Demand Policies against Stagflation,\" Weltwirtschafliches Archiv, vol. 114, no. 4, pp. 736-82.\n\nNeiss, Katharine S., and Edward Nelson (2005). \"Inflation Dynamics, Marginal Cost, and the Output Gap:  Evidence from Three Countries,\" Journal of Money, Credit, and Banking, vol. 37 (December), pp. 1019-45.\n\nOkun, Arthur (1962). \"Potential GNP: Its Measurement and Significance,\" Proceedings of the Business and Economics Section of the American Statistical Association, pp. 98-104.\n\nOliner, Stephen D., and Daniel E. Sichel (2000). \"The Resurgence of Growth in the late 1990s:  Is Information Technology the Story?\" Journal of Economic Perspectives, vol. 14 (Fall), pp. 3-22.\n\nOliner, Stephen D., and Daniel E. Sichel (2002). \"Information Technology and Productivity:  Where Are We Now and Where Are We Going?\" Federal Reserve Bank of Atlanta Economic Review, vol. 87 (3rd Quarter), pp. 15-44.\n\nOliner, Stephen D., Daniel E. Sichel, and Kevin Stiroh (2007). \"Explaining a Productive Decade\", Brookings Papers on Economic Activity, forthcoming.\n\nOrphanides, Athanasios (2001). \"Monetary Policy Rules Based on Real-Time Data,\" American Economic Review, vol. 91 (September), pp. 964-85.\n\nOrphanides, Athanasios, Richard D. Porter, David Reifschneider, Robert Tetlow, and Frederico Finan (2000). \"Errors in the Measurement of the Output Gap and the Design of Monetary Policy,\" Journal of Economics and Business, vol. 52, (January-April), pp. 117-41.\n\nOrphanides, Athanasios, and Simon van Norden (2002). \"The Unreliability of Output Gap Estimates in Real Time,\" Review of Economics and Statistics, vol. 84 (November), pp. 569-83.\n\nPerry, George L. (1970). \"Changing Labor Markets and Inflation,\" Brookings Papers on Economic Activity, 3: 1970, pp.411-48.\n\nPhelps, Edmund S. (1967). \"Phillips Curves, Expectations of Inflation, and Optimal Inflation over Time,\" Economica, vol. 34 (August), pp. 254-81.\n\nRoberts, John M. (2001). \"Estimates of the Productivity Trend Using Time-Varying Parameter Techniques,\" Finance and Economics Discussion Series 2001-8 (Washington, D.C.: Board of Governors of the Federal Reserve Board, February).\n\nRudebusch, Glenn D. (2000). \"How Fast Can the New Economy Grow?\" FRSBF Economic Letter 2000-05, Federal Reserve Bank of San Francisco, February 25.\n\nStaiger, Douglas, James H. Stock, and Mark W. Watson (1997a). \"How Precise Are Estimates of the Natural Rate of Unemployment?\" in Christina Romer and David Romer, eds., Reducing Inflation: Motivation and Strategy. Chicago: University of Chicago Press, pp. 195-242.\n\nStaiger, Douglas, James H. Stock, and Mark W. Watson (1997b). \"The NAIRU, Unemployment, and Monetary Policy,\" Journal of Economic Perspectives, vol. 11 (Winter), pp. 33-49.\n\nWoodford, Michael (2003). Interest and Prices: Foundations of a Theory of Monetary Policy. Princeton: Princeton University Press.\n\nFootnotes\n\n1.  I want to thank Andrew Figura, Charles Fleischman, John Roberts, and William Wascher for their helpful comments and assistance on this speech. Return to text\n\n2.  The term \"NAIRU\" comes from a paper by Nobel prize winner Franco Modigliani and Lucas Papademos, now the vice president of the European Central Bank (Modigliani and Papademos, 1978).  Although NAIRU and the natural rate of unemployment are frequently used interchangeably, there is a subtle distinction between the two.  The natural rate of unemployment is the rate at which inflation would tend to gravitate to its long-run expected value, while NAIRU, as originally defined, is the unemployment rate at which inflation will have no tendency to move up or down.  Depending on what shocks are hitting the economy, the NAIRU could deviate from the natural rate of unemployment (for example, see Estrella and Mishkin, 1999).  Because the NAIRU terminology is more common than the natural rate of unemployment terminology, I am using the NAIRU terminology even though I think that the natural rate of unemployment terminology is more accurate. Return to text\n\n3.  Other examples of the multivariate approach include Apel and Jansson (1999), Cochrane (1994), and Dupasquier, Guay and St-Amant (1999). Return to text\n\n4.  Okun's law was originally specified as the relationship between real GDP growth and changes in the unemployment rate (see Okun, 1962). Return to text\n\n5.  See Gordon (1982 and many subsequent papers) for a description of the \"triangle\" model of inflation.  The three sides of the triangle are inflation inertia (captured by the lags of inflation), excess demand (measured by the unemployment rate gap or GDP gap), and supply shocks--such as the relative prices of imports, food, and energy.  Following George Perry's (1970) early work, it is common to adjust the NAIRU for changes in the composition of the labor force.  Fallick, Fleischman, and Pingle (2006) estimate that shifts in the demographic composition of the labor force can explain a decline in the unemployment rate of nearly 1 percentage point between 1977 and 2006. Return to text\n\n6.  See, for example, Atkeson and Ohanian (2001). Return to text\n\n7.  See, for example, Altig, Fitzgerald, and Rupert (1997), Rudebusch (2000), and Groshen and Potter (2003).   Return to text\n\n8.  Because official series for productivity for all sectors of the economy are not available, the growth-accounting framework most often focuses on the private nonfarm business (NFB) sector, which in the United States accounts for more than three-quarters of total output.  Productivity for the overall economy is then derived from estimates for the nonfarm business sector and then cruder estimates for the other sectors (government, farm, housing services, and households and institutions) generally based on the univariate and multivariate statistical approaches described earlier. Return to text\n\n9.  See also Oliner and Sichel (2000, 2002), Corrado and Slifman (1999), and Jorgenson, Ho, and Stiroh (2004), among others.  Return to text\n\n10.  See, for example, Goodfriend and King (1997) or Woodford (2003). Return to text\n\n11.  For example, Blanchard and Gali (2007) have proposed a model with real wage rigidities that, if incorporated into a DSGE model, would likely show output gap estimates that are more similar to traditional gaps. Return to text\n\n12.  See, for example, Bean (2005). Return to text\n\n13.  See also Orphanides and others, (2000) and Orphanides and van Norden (2002). Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/mishkin20070524a.htm",
        "title": "Estimating Potential Output",
        "date": "5/24/2007"
    },
    {
        "content": "May 23, 2007\n\nGovernor Randall S. Kroszner\n\nAt the George Washington University School of Business, Financial Services Research Program Policy Forum, Washington, D.C.\n\nInformation is critical to the effective functioning of markets.  A core principle of economics is that markets are more competitive, and therefore more efficient, when accurate information is available to both consumers and suppliers.  When information on alternatives is readily available, product offerings will have to meet customers’ demands and offering prices will have to reflect those of market competitors.1  In addition, information helps individual consumers by improving their ability to compare products and to choose those that will help them meet their personal goals.\n\nThese arguments are not just theoretical.  There is systematic evidence that in practice, changes in disclosure affect both consumer and supplier behavior in a number of consumer product markets.2  In 1968, Congress passed the Truth in Lending Act (TILA), which significantly changed a number of consumer credit disclosure practices.  For example, it required that creditors disclose an “annual percentage rate.”  These disclosures are generally believed to have improved competition and helped individual consumers.3\n\nToday, we face the challenge of ensuring that disclosures for consumer credit  remain effective.  To be effective, disclosures must give consumers information about credit pricing at a time when it is relevant, and in language consumers can easily understand.  The information must also be in a format that allows consumers to pick out and use the information that is most important to them.  Effective disclosures give consumers information they notice, understand, and can use.  Better credit disclosure permits better-informed credit decisions and, hence, more effective competition among credit card issuers.  In a nutshell, effective disclosure empowers consumers and enhances competition.\n\nThe Federal Reserve Board has recently undertaken an innovative approach to improve the effectiveness of disclosure--namely, surveying and responding to consumers through consumer testing.  Having taught at a business school for many years, I am well aware of the types of consumer testing that firms have long employed:  surveys, focus groups, and so-called “mall intercepts” in which shoppers are interviewed at random.  Systematically using such techniques to improve the effectiveness of disclosure requirements set out in the regulations of the Federal Reserve and other financial regulators, is, however, relatively novel.\n\nConsumer testing can help the Federal Reserve address the considerable challenge of making disclosures more effective.  Consumers increasingly face more-diverse and more-complex financial products, including nontraditional mortgages and credit cards with multiple features.  Given this complexity, we have to be mindful of the dangers of information overload.  We must seek to carry out the responsibilities Congress has given us to design disclosures that are not only accurate, but also clear and simple enough that they are meaningful and useful to  consumers.\n\nIn other words, pages and pages of fine print may provide comprehensive descriptions that lawyers might love but that consumers find confusing, or, worse, useless.  We need to translate legalese into something consumers can use.  This requires the Board to make judgments about which credit terms are most important to highlight and which could be eliminated.  However, we plan to make these judgments with the benefit of feedback from actual consumers gathered through surveys and testing.  We recently completed several rounds of consumer testing for credit card disclosures, and that testing was critical to our effort to redesign and, I believe, dramatically improve those disclosures.\n\nJust this morning, the Board took a major step toward making credit disclosure more effective, by voting to seek public comment on proposed revisions to the Board’s Regulation Z, which implements the Truth In Lending Act--revisions that will improve credit card disclosures.  I will share with you some of the results of our consumer testing, how they influenced our newly issued proposal concerning credit card disclosures, and how consumer testing will influence future proposals for improvements of mortgage loan disclosures.\n\nThe Board’s Recent Actions Related to Mortgage Loan Disclosures\nFirst, however, I want to note that the Board has already taken some steps to improve disclosures concerning mortgages, including subprime mortgages, which is the topic of this forum.  Last summer, the Board held a series of four public hearings on home-equity lending, where we gathered views on the impact of federal and state predatory lending laws and on the adequacy of mortgage disclosures, particularly those concerning nontraditional mortgage products.  Following those hearings, the Board revised the consumer handbook that creditors are required to provide with applications for adjustable-rate mortgages, or ARMs.  As you well know, a substantial majority of recent subprime mortgage originations have been ARMs.  The revised handbook gives consumers a better explanation of the features and risks of nontraditional ARMs, especially “payment shock” and the risk of increasing loan balances, also known as “negative amortization.”\n\nThe Board will be holding a fifth hearing on June 14, here in Washington, D.C., and I will chair that meeting.  We will focus on how the Board might use its rulemaking authority under the Home Ownership and Equity Protection Act, or HOEPA, to address concerns about abusive mortgage lending practices.\n\nThe purpose of the hearing is to gather information to evaluate how the Federal Reserve can prevent predatory lending in a way that also preserves incentives for responsible lenders.  A robust and responsible subprime mortgage market benefits consumers by allowing borrowers with non-prime or limited credit histories to become homeowners, access the equity in their homes, or have the flexibility to refinance their loans as needed.  Following the hearing, we will consider whether there are lending practices that should be prohibited under HOEPA.  The Federal Reserve will do all that it can to prevent fraud and abusive mortgage lending practices.  However, any new rules should be drawn clearly to avoid creating legal or regulatory uncertainty that could have the unintended consequence of restricting consumers' access to responsible subprime credit.\n\nAs I mentioned earlier, effective disclosure can make markets more competitive and weed out abuse.  Therefore, we will also consider how mortgage disclosures can be improved to ensure a robust subprime market and responsible subprime lending.  In fact, the Board is conducting a comprehensive review of all Truth in Lending Act disclosures.  As the Board reviews mortgage disclosures, it will consider a variety of recent suggestions.  For example, given the large volume of documents presented to consumers at mortgage loan closings, the Board will consider whether creditors should be required to provide disclosures for home-refinance loans and home-equity loans within three days of application instead of at consummation, as currently required for home-purchase loans.  The Board will also review the requirements concerning advertisements, to ensure that when lenders promote low initial rates and low monthly payments they also adequately disclose the limits of those low rates and payments and the potential payment shock.  Our review of mortgage disclosures is now under way.\n\nI would now like to turn to the initial phase of the Board’s review of credit disclosures, which has focused on credit cards and other revolving credit accounts.  The substantial investment we have made in developing and testing revised credit card disclosures has given us insights that will surely contribute to our ability to make mortgage disclosures more effective.\n\nThe Challenges of Disclosing Credit Card Costs\nIn our effort to create more effective credit card disclosures, we face several challenges.  First, disclosing the cost of revolving credit to facilitate consumer shopping is inherently challenging because key variables are not known in advance, such as the amount of credit that the consumer will use and the timing and amount of the consumer’s payments.  Creditors’ solicitations disclose only a nominal APR based on the periodic interest rate, and they list any fees separately.  Related variables, such as the grace period and balance calculation method, also have to be explained.  On periodic billing statements, an “effective APR” can be disclosed after the credit is extended, but even then assumptions must be made about the consumer’s payments.  For example, the effective APR assumes that transaction fees are amortized over one billing cycle, regardless of how long the credit is actually outstanding.\n\nThe increasing complexity and diversity of credit card features and pricing present a second challenge.  Because creditors use risk-based pricing, their solicitations might disclose a range of possible rates for which the consumer could qualify.  Creditors discount their initial interest rates, sometimes deeply.  While competing on the initial rate, creditors compensate with “back-end” pricing.4  Temporary introductory rates are followed by a higher rate, and penalty rates and fees have increased and are now easier to trigger.  Consumers might not focus on costs that are contingent on future events, and it can be challenging to explain clearly how these contingencies can increase consumers’ costs.\n\nA third major challenge is crafting disclosure rules that are, on the one hand, clear and specific enough to facilitate compliance and promote consistency among creditors but, on the other hand, flexible enough to accommodate market developments as products and pricing continue to change.  We have a responsibility to avoid undue burdens that would hinder innovation and raise costs without producing sufficient offsetting benefits in the credit card market.\n\nFaced with these developments, the Board’s challenge is to determine what information should be highlighted for consumers.  How much information is enough, and how much is too much?  How can we encourage plainer language, recognizing that there may be a trade-off between simplicity and accuracy?  What formats work best in presenting the information?  When is the best time to present information so it will be most relevant and useful to consumers?  How can we craft disclosure requirements that are flexible enough to accommodate innovation and change and to enhance competition?\n\nUsing Consumer Testing to Improve Credit Card Disclosures: Highlights of the Proposal and Lessons Learned\nAlthough the Board has used consumer focus groups in the past, this is the first time that we have conducted extensive, in-depth interviews with individual consumers to study the effectiveness of disclosures.  The first step was using both focus groups and one-on-one interviews to evaluate how well consumers understand and use the current credit card disclosures.  With a consultant’s help, we then redesigned the disclosures and conducted more one-on-one interviews using the revised formats.  We went through this process several times, each time adjusting our designs to incorporate the lessons learned.  We plan to follow the same testing process with mortgage disclosures.  In addition, there were other sources of information, such as public comment letters and informal input from industry and consumer groups, discussions by the Board’s Consumer Advisory Council, a study by the Government Accountability Office, and data from consumer surveys.\n\nAs I noted earlier, our task is to take a complex credit product and design disclosures that can explain the terms not only accurately but also clearly enough to be meaningful and useful to consumers--all while enhancing, not stifling, competition and innovation.  I will briefly describe the highlights of the proposal issued by the Board this morning and use them to illustrate some general lessons we have learned about making disclosure more effective for consumers.\n\nFirst, we learned firsthand what information consumers find useful when making credit decisions and what information they ignore.  Second, we learned what information consumers comprehend and what information they do not.  Third, we saw the impact that different formats and presentation can have on consumers’ ability to notice and use the information.\n\nThe first significant lesson from our consumer testing was that there are some disclosures that consumers just do not find useful.  Most participants acknowledged that they do not read the cardholder agreements that contain their account-opening disclosures because they are often written in small print and dense prose.  At the same time, they wanted some sort of disclosure they could use, before opening an account, to confirm that the terms are what they expect.  Participants also said that often they do not carefully read their change-in-terms notices because they are frustrated by the format and technical language.\n\nWith this information in hand, we explored how we could improve the disclosures to change consumer behavior.  Under the proposed rules, when an account is opened, the consumer would receive a summary of the credit agreement in the form of a table, which they can compare with the original solicitation.\n\nCreditors would also have to provide a disclosure table with their change-in-terms notices, to summarize the changes.  In our tests, such a table was very effective in making the change-in-terms notice more usable, and consumers were more likely to correctly identify the changes to the terms of their accounts.  The Board’s proposal also would require creditors to give consumers more advance notice before increasing the interest rate or making changes to other key terms.  Creditors would have to send a notice to consumers 45 days in advance of a rate increase, compared with the current 15 days.  The intent is to give consumers more time to make other credit arrangements, if necessary, before the higher rate becomes effective.  The Board is expressly seeking comment on whether 45 days is the appropriate amount of time.\n\nThe second lesson learned from our consumer testing was which disclosures consumers comprehend and which they do not.  Before deciding that the core information was not useful, we explored other ways to explain complex information and to improve consumers’ understanding.  We explored whether consumers would benefit from using different terminology or whether particular terms need to be accompanied by an explanation.  This is a particular challenge in the case of the term “effective APR.”  We experimented with terminology and explanatory language as well as formats.  The proposed changes for disclosing the effective APR have shown promise, but this is an area where we will be conducting additional consumer testing.\n\nThe test participants also often did not correctly understand what creditors meant by the term “fixed rate.”  They equated this with the fixed rate on their mortgage.  Thus, under the proposed rules we would allow creditors to describe their rates as “fixed” only if there is a specified period during which the rate cannot be increased for any reason.  If no period is specified, the term “fixed” could be used only if the rate cannot increase while the credit plan is open.\n\nConsumers in our tests also did not understand the term “default APR.”  We would therefore require creditors to refer to the “penalty APR” rather than the “default APR,” which improved consumer understanding in our tests.  Consumers also did not consistently understand the term “grace period,” which is significant considering the cost implications of not paying on time.  Based on language we tested with consumers, we are proposing to add an explanation of the term.\n\nThe third lesson of our consumer testing was the impact that different formats and presentation can have on consumers’ ability to notice and use the information.  Our testing confirmed that one of the most effective ways to present credit card disclosures is in the table provided with solicitations, commonly known as the “Schumer box.”  Although this table is effective, we were nevertheless able to use the one-on-one interviews to improve both the content and format.  For example, when a creditor uses risk-based pricing, solicitations may show a range of rates, and many test participants had trouble understanding how a card issuer would select an initial APR.  We are therefore proposing to add a simple explanation that consumers’ initial rate is based on their creditworthiness.  We also reformatted the Schumer box into two major sections, to separate information about rates from the information about fees.  This seemed to enhance consumers’ understanding and improve the usability of the disclosures.\n\nIn addition, disclosures inside the Schumer box were more readily noticed than when the same information was placed just beneath it.  We propose to move information that was more important to consumers--such as the circumstances that trigger a penalty APR--inside the Schumer box.  The creditor’s balance-calculation method, however, was removed and located beneath the Schumer box, because our testing showed that consumers do not use the information to shop.\n\nA related finding concerning format is that regrouping information in the Schumer box improved consumers’ ability to use the disclosures.  Specifically, we found that grouping certain information together on periodic statements made it easier to understand.  For example, consumers more easily determined the number and amount of fees when they were itemized, grouped together, and totaled.  So the revised rules would require creditors to locate the fees and interest charges in one place on the statement.  Fees would have to be grouped together with a total dollar amount instead of listed chronologically with purchase transactions, as they are today.  Interest charges would also be grouped together, with a total dollar amount.\n\nFinally, most consumers said that they throw away any inserts included with their periodic statement, including any inserts that describe changes to their account terms.  To alert consumers to the significance of the insert, creditors that insert a change-in-terms notice with the periodic statement would have to summarize the changes on the front of the periodic statement rather than on the insert.\n\nSimilar Challenges in Improving Mortgage Disclosures\nAs I mentioned earlier, the Board also plans to conduct consumer testing as part of its review of mortgage disclosures.  Like credit cards, mortgage products have become more diverse and more complex.  In some cases, creditors are using pricing strategies similar to those used for credit cards, for example, offering customers discounted introductory rates that will be replaced in a short time by a much higher rate, often a variable rate.  Of course there is an inherent difficulty in adjustable-rate mortgage disclosures because future interest rate changes are not known.  Consumer testing is needed to determine whether consumers would find disclosure of the maximum rate and a worst-case payment example useful, given that these might not occur for several years or more and that the consumer’s own financial circumstances may change.\n\nThe wider marketing of payment-option mortgages presents another challenge.  Consumers have the choice of making low minimum monthly payments that increase the overall cost of the credit and may ultimately lead to longer amortization periods.  Just as with credit cards, however, disclosing a consumer’s repayment obligation and the cost of the credit is more complex when there are unknowns--such as the rate, the amount of the consumer’s monthly payment, and the possibility of negative amortization.  When the Board reviews mortgage disclosures, it will consider these developments and conduct consumer testing to determine how the features and risks of today’s mortgage products can be communicated effectively.\n\nThe Board’s Upcoming Hearing on Mortgage Lending Practices\nAs I mentioned earlier, in June the Board will be conducting another public hearing on mortgage lending, which will focus on how the Board might use its rulemaking authority to prevent abusive lending.  Some of these concerns may call for more-effective disclosures.\n\nLast year, the federal agencies that supervise depository institutions issued guidance on nontraditional mortgage products, and they recently proposed similar guidance for subprime mortgages.  Both guidance statements discuss underwriting practices as well as the need for lenders to give consumers more complete and balanced information while they are still shopping for a mortgage, before they apply for the loan.\n\nPrepayment penalties as well the use of “stated-income” or “low-doc” loans will also be discussed at the upcoming hearing.  We will hear arguments about such practices and consider whether disclosures can be made more effective to adequately inform consumers about how penalty clauses operate and the implications of not documenting their income.\n\nConclusion\nIn fulfilling its responsibility to protect consumers, the Federal Reserve will do all that it can to prevent fraudulent and abusive mortgage lending practices.  Because information is critical to more competitive, and thus more efficient, markets, effective disclosure also has the capacity to weed out abuses.  Consumers who do not have accurate information and an understanding of what that information means will have difficulty choosing among competing products and making decisions that are in their best interest.  This is true in both credit card and mortgage markets.  Accordingly, we will consider how mortgage disclosures can be more effective and empower consumers to make better-informed decisions and achieve their financial goals.  Better-informed consumers will strengthen market competition.  The Federal Reserve also will consider how we might use our rulemaking authority to address predatory practices without restricting consumers' access to responsible subprime credit.\n\nFootnotes\n\n1.  See, for example, George J. Stigler (1961), “The Economics of Information,” Journal of Political Economy, vol. 69 (June), pp. 213-25.  Return to text\n\n2.   Alan D. Mathios (2000), “The Impact of Mandatory Disclosure Laws on Product Choices: An Analysis of the Salad Dressing Market,” Journal of Law and Economics, vol. 43 (October), pp. 651-675; Ginger Zhe Jin and Phillip Leslie (2003), “The Effect of Information on Product Quality: Evidence from Restaurant Hygiene Grade Cards,” The Quarterly Journal of Economics, vol. 118 (May), pp. 409-451. Return to text\n\n3.  Board of Governors of the Federal Reserve System (1987), Annual Percentage Rate Demonstration Project (Washington: Board of Governors of the Federal Reserve System). See also Consumer Credit in the United States: The Report of the National Commission on Consumer Finance (GPO 1972).  Return to text\n\n4.  Furletti, Mark (2003), \"Credit Card Pricing Developments and Their Disclosure (739 KB PDF),\" discussion paper, Payment Cards Center, Federal Reserve Bank of Philadelphia.  Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20070523a.htm",
        "title": "Creating More Effective Consumer Disclosures",
        "date": "5/23/2007"
    },
    {
        "content": "May 22, 2007\n\nChairman Ben S. Bernanke\n\nAt the Princeton Prize in Race Relations Awards Program, Washington, D.C.\n\nAs a former member of the Princeton community, I am very pleased to see the university recognizing these young people--the two prize winners, John Gentile and Brianna Casey Lyons, as well as the eight certificate recipients--for demonstrating exemplary leadership in the area of race relations. Slavery and segregation cast long dark shadows on our nation's history and our society, but there have been flickers of light in the form of people of good will and courage who fought against those evils. Today, I can think of no higher calling than promoting harmony, understanding, and respect among people of different racial and ethnic backgrounds. The two prize winners, though young, have already contributed significantly. As you have heard, John energetically advocated improved race relations in his own school and helped to bring diversity concerns before a wider group of high schools in the District of Columbia metro area. Casey led a group of exurban teenagers in starting a 4-H Club at a more-urban, and racially mixed, elementary school.\n\nIf you are a baseball fan, as I am, you know that we recently observed the sixtieth anniversary of an important event in the history of race relations--Jackie Robinson's breaking of the color barrier in major league baseball. Robinson was a great baseball player, but--critically, for the mission he set out to accomplish--he was also a great leader, a person of courage and character. As a second lieutenant in the Army in 1944, he refused to obey an order to move to the back of a military bus in Texas. Lieutenant Robinson was court-martialed but then acquitted by a military jury, and he received an honorable discharge. It was Jackie Robinson's character as much as his daring style of play that commanded the respect of players and fans and paved the way for other black athletes to enter the major leagues. No one who watched Robinson perform under often-hostile conditions could long deny that he was the equal of any white player, not only as an athlete but as a human being. Other flickers of light appear in this story as well, including the decision of Dodgers General Manager Branch Rickey to give Robinson a chance and the public support provided Robinson by a few of his white teammates.\n\nIn a way, Jackie Robinson was lucky, because he was rewarded for his skills and courage. He was named Rookie of the Year, played on six World Series teams, and was once named the National League's Most Valuable Player. Someone less fortunate in this respect was Josh Gibson, considered the greatest power hitter of the Negro Leagues. He played right here in Washington for the Homestead Grays in the 1930s and 1940s. Some people say he was the equal of Babe Ruth as a hitter. But he never got the chance to play in the major leagues. He died at the age of thirty-five, three months before Jackie Robinson first trotted onto Ebbets Field with the Brooklyn Dodgers in 1947. Both men played the game superbly, but whereas Jackie Robinson was honored and recognized in his lifetime for his achievements, both as a baseball player and a civil rights leader, recognition of Josh Gibson came only after his death. Gibson, along with other greats of the Negro Leagues, was finally admitted to the National Baseball Hall of Fame in 1972, a quarter century after he died and a decade after Robinson was admitted.\n\nIt is tragic that Gibson did not live to see the integration of major league baseball or to enjoy the honors that were due him. However, even though society's recognition of Gibson's achievements came too late for him to enjoy it, honoring him was still worthwhile. The belated recognition of Gibson illustrates a most important reason to honor achievement: We do it not so much for the person being honored but rather for ourselves. Please do not misunderstand me. I hope today is a joyous and proud day for today's prize winners and certificate recipients and their families. But I strongly suspect that when they set out on the path that earned them this recognition, they were not motivated at all by--and probably were not even aware of--the prospect of an award such as the Princeton Prize. They did what they did from inner motivation. So, if the prospect of recognition had little or nothing to do with their achievement, why go through the exercise? The reason they are being honored, and the reason we remember Jackie Robinson and Josh Gibson and countless other achievers in countless other endeavors, is because doing so provides inspiration for all of us. And, usually, the aspect of an achievement that is most worth recognizing is not the achievement itself but the spirit of energy, determination, and courage that made it possible. So, let me say to today's honorees: Thank you. Thank you not only for serving as a role model for your peers in high school but also for being exemplars for us all.\n\nNow, because we are in the midst not only of baseball season but also of graduation season, I would like to touch briefly on the theme of education. The saddest aspect of Josh Gibson's story is that he had talent but was denied an opportunity. Then as now, the principal path to opportunity is through education. As an economist, I am persuaded that a strong educational system--one that promotes lifetime learning and skill development--is a critical factor in our nation's prosperity. The economic importance of education will only increase as technology advances and as the global economy becomes increasingly integrated and complex.\n\nBut education is important for non-economic reasons as well. By providing us with a broader view of the world, education helps each of us become the most complete person we can be. Many--I hope all--of the young people here today will continue their education, and I hope it leads them to work that brings financial success. But I also hope it cultivates their creativity and appreciation for other cultures and leads them to work they find personally satisfying and meaningful. I know it will help them continue to demonstrate the kind of leadership that they have already shown. Perhaps, as they acquire a deeper knowledge of places and times other than their own and a fuller understanding of people from backgrounds other than their own, it will also lead them to contribute positively at the national or international levels, as they already have done in their schools and local communities.\n\nBut this evening, I don't think we should dwell entirely on the future. I hope each of the honorees will take pride in what he or she has already achieved and will celebrate that achievement with family and friends. Congratulations to all of you.",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070522a.htm",
        "title": "Recognizing Leadership",
        "date": "5/22/2007"
    },
    {
        "content": "May 17, 2007\n\nChairman Ben S. Bernanke\n\nAt the Federal Reserve Bank of Chicago’s 43rd Annual Conference on Bank Structure and Competition, Chicago, Illinois\n\nThe recent sharp increases in subprime mortgage loan delinquencies and in the number of homes entering foreclosure raise important economic, social, and regulatory issues.  Today I will address a series of questions related to these developments.  Why have delinquencies and initiations of foreclosure proceedings risen so sharply?  How have subprime mortgage markets adjusted?  How have Federal Reserve and other policymakers responded, and what additional actions might be considered?  How might the problems in the market for subprime mortgages affect housing markets and the economy more broadly?\n\nThe Development of the Subprime Mortgage Market\nLet me begin with some background.  Subprime mortgages are loans made to borrowers who are perceived to have high credit risk, often because they lack a strong credit history or have other characteristics that are associated with high probabilities of default.  Having emerged more than two decades ago, subprime mortgage lending began to expand in earnest in the mid-1990s, the expansion spurred in large part by innovations that reduced the costs for lenders of assessing and pricing risks.  In particular, technological advances facilitated credit scoring by making it easier for lenders to collect and disseminate information on the creditworthiness of prospective borrowers.  In addition, lenders developed new techniques for using this information to determine underwriting standards, set interest rates, and manage their risks.\n\nThe ongoing growth and development of the secondary mortgage market has reinforced the effect of these innovations.  Whereas once most lenders held mortgages on their books until the loans were repaid, regulatory changes and other developments have permitted lenders to more easily sell mortgages to financial intermediaries, who in turn pool mortgages and sell the cash flows as structured securities.  These securities typically offer various risk profiles and durations to meet the investment strategies of a wide range of investors.  The growth of the secondary market has thus given mortgage lenders greater access to the capital markets, lowered transaction costs, and spread risk more broadly, thereby increasing the supply of mortgage credit to all types of households.\n\nThese factors laid the groundwork for an expansion of higher-risk mortgage lending over the past fifteen years or so.  Growth in the market has not proceeded at a uniform pace, but on net it has been dramatic.  About 7-1/2 million first-lien subprime mortgages are now outstanding, accounting for about 14 percent of all first-lien mortgages.1  So-called near-prime loans--loans to borrowers who typically have higher credit scores than subprime borrowers but whose applications may have other higher-risk aspects--account for an additional 8 to 10 percent of mortgages.2\n\nThe expansion of subprime mortgage lending has made homeownership possible for households that in the past might not have qualified for a mortgage and has thereby contributed to the rise in the homeownership rate since the mid-1990s.  In 2006, 69 percent of households owned their homes; in 1995, 65 percent did.  The increase in homeownership has been broadly based, but minority households and households in lower-income census tracts have recorded some of the largest gains in percentage terms.  Not only the new homeowners but also their communities have benefited from these trends.  Studies point to various ways in which homeownership helps strengthen neighborhoods.  For example, homeowners are more likely than renters to maintain their properties and to participate in civic organizations.  Homeownership has also helped many families build wealth, and accumulated home equity may serve as a financial reserve that can be tapped as needed at a lower cost than most other forms of credit.\n\nBroader access to mortgage credit is not without its downside, however.  Not surprisingly, in light of their weaker credit histories and financial conditions, subprime borrowers face higher costs of borrowing than prime borrowers do and are more likely to default than prime borrowers are.  For borrowers, the consequences of defaulting can be severe--possibly including foreclosure, the loss of accumulated home equity, and reduced access to credit.  Their neighbors may suffer as well, as geographically concentrated foreclosures tend to reduce property values in the surrounding area.\n\nThe Recent Problems in the Subprime Mortgage Sector\nWith this background in mind, I turn now to the recent problems in the subprime mortgage sector.  In general, mortgage credit quality has been very solid in recent years.  However, that statement is no longer true of subprime mortgages with adjustable interest rates, which currently account for about two-thirds of subprime first-lien mortgages or about 9 percent of all first-lien mortgages outstanding.  For these mortgages, the rate of serious delinquencies--corresponding to mortgages in foreclosure or with payments ninety days or more overdue--rose sharply during 2006 and recently stood at about 11 percent, about double the recent low seen in mid-2005.3   The rate of serious delinquencies has also risen somewhat among some types of near-prime mortgages, although the rate in that category remains much lower than the rate in the subprime market.  The rise in delinquencies has begun to show through to foreclosures.  In the fourth quarter of 2006, about 310,000 foreclosure proceedings were initiated, whereas for the preceding two years the quarterly average was roughly 230,000.4  Subprime mortgages accounted for more than half of the foreclosures started in the fourth quarter.\n\nThe sharp rise in serious delinquencies among subprime adjustable-rate mortgages (ARMs) has multiple causes.  \"Seasoned\" mortgages--mortgages that borrowers have paid on for several years--tend to have higher delinquency rates.  That fact, together with the moderation in economic growth, would have been expected to produce some deterioration in credit quality from the exceptionally strong levels seen a few years ago.  But other factors, too, have been at work.  After rising at an annual rate of nearly 9 percent from 2000 through 2005, house prices have decelerated, even falling in some markets.  At the same time, interest rates on both fixed- and adjustable-rate mortgage loans moved upward, reaching multi-year highs in mid-2006.  Some subprime borrowers with ARMs, who may have counted on refinancing before their payments rose, may not have had enough home equity to qualify for a new loan given the sluggishness in house prices.  In addition, some owners with little equity may have walked away from their properties, especially owner-investors who do not occupy the home and thus have little attachment to it beyond purely financial considerations.  Regional economic problems have played a role as well; for example, some of the states with the highest delinquency and foreclosure rates are among those most hard-hit by job cuts in the auto industry.\n\nThe practices of some mortgage originators have also contributed to the problems in the subprime sector.  As the underlying pace of mortgage originations began to slow, but with investor demand for securities with high yields still strong, some lenders evidently loosened underwriting standards.  So-called risk-layering--combining weak borrower credit histories with other risk factors, such as incomplete income documentation or very high cumulative loan-to-value ratios--became more common.  These looser standards were likely an important source of the pronounced rise in \"early payment defaults\"--defaults occurring within a few months of origination--among subprime ARMs, especially those originated in 2006.\n\nAlthough the development of the secondary market has had great benefits for mortgage-market participants, as I noted earlier, in this episode the practice of selling mortgages to investors may have contributed to the weakening of underwriting standards.  Depending on the terms of the sale, when an originator sells a loan and its servicing rights, the risks (including, of course, any risks associated with poor underwriting) are largely passed on to the investors rather than being borne primarily by the company that originated the loan.  In addition, incentive structures that tied originator revenue to the number of loans closed made increasing loan volume, rather than ensuring quality, the objective of some lenders.  Investors normally have the right to put early-payment-default loans back to the originator, and one might expect such provisions to exert some discipline on the underwriting process.  However, in the most recent episode, some originators had little capital at stake and did not meet their buy-back obligations after the sharp rise in delinquencies.5  Intense competition for subprime mortgage business--in part the result of the excess capacity in the lending industry left over from the refinancing boom earlier in the decade--may also have led to a weakening of standards.  In sum, some misalignment of incentives, together with a highly competitive lending environment and, perhaps, the fact that industry experience with subprime mortgage lending is relatively short, likely compromised the quality of underwriting.\n\nThe accuracy of much of the information on which the underwriting was based is also open to question.  Mortgage applications with little documentation were vulnerable to misrepresentation or overestimation of repayment capacity by both lenders and borrowers, perhaps with the expectation that rising house prices would come to the rescue of otherwise unsound loans.  Some borrowers may have been misled about the feasibility of paying back their mortgages, and others may simply have not understood the sometimes complex terms of the contracts they signed.\n\nAs the problems in the subprime mortgage market have become manifest, we have seen some signs of self-correction in the market.  Investors are scrutinizing subprime loans more carefully and, in turn, lenders have tightened underwriting standards.  Credit spreads on new subprime securitizations have risen, and the volume of mortgage-backed securities issued indicates that subprime originations have slowed.  But although the supply of credit to this market has been reduced--and probably appropriately so--credit has by no means evaporated.  For example, even as purchases of securitized subprime mortgages for collateralized debt obligations--an important source of demand--have declined, increased purchases by investment banks, hedge funds, and other private pools of capital are beginning to fill the void.  Some subprime originators have gone out of business as their lenders have cancelled credit lines, but others have been purchased by large financial institutions and remain in operation.  Importantly, we see no serious broader spillover to banks or thrift institutions from the problems in the subprime market; the troubled lenders, for the most part, have not been institutions with federally insured deposits.\n\nWhat about borrowers already in distress?  The Board and other federal supervisory agencies have taken actions to encourage the banks and thrift institutions we supervise to work with borrowers who may be having trouble meeting their mortgage obligations.  Often, loan workouts are in the interest of both parties.  With effective loan restructuring, borrowers facing temporary economic setbacks may be able to work through their problems while staying in their homes, and lenders may be able to avoid the costs of foreclosure and the losses usually associated with selling a repossessed home.\n\nServicers of loans aim to minimize losses, and they appear to be actively working with thousands of individual borrowers to modify their mortgages.  To some extent, the dispersed ownership of mortgages may combine with legal and accounting rules to make successful workouts more difficult to achieve.  For example, the \"pooling and servicing agreement\" associated with a given securitized mortgage pool may restrict the share of accounts that can be modified.  Accounting rules that, in some cases, require substantially modified pools to be brought back on the originator’s balance sheet may dissuade lenders from undertaking workouts.  And extensive modifications that reallocate expected cash flows across different securities associated with the pool could trigger a review of those securities by the ratings agencies.  At the same time, if workouts are economically viable, then an incentive exists for third parties to purchase distressed pools at a discount and to undertake the workout process.  We see these purchases taking place in the marketplace, a development that should help to increase the number of successful workouts.\n\nAlso, local community organizations that work to promote homeownership and prevent foreclosures have stepped up their efforts.  For example, NeighborWorks America advises borrowers about restructuring their mortgages.  A survey conducted by this group found that many homeowners do not understand that lenders also want to avoid foreclosure.  Thus, the simple step of encouraging borrowers in trouble to contact their lenders can be very productive.  The Federal Reserve and the other supervisory agencies have encouraged financial institutions to identify and contact borrowers who, with counseling and financial assistance, may be able to avoid entering delinquency or foreclosure.  Indeed, some lenders are being proactive in this regard--for example, by contacting borrowers to discuss possible options well before a scheduled interest-rate reset.\n\nPossible Regulatory Responses\nLooking forward, the Federal Reserve, other regulators, and the Congress must evaluate what we have learned from the recent episode and decide what additional regulation or oversight may be needed to prevent a recurrence.  In deciding what actions to take, regulators must walk a fine line; we must do what we can to prevent abuses or bad practices, but at the same time we do not want to curtail responsible subprime lending or close off refinancing options that would be beneficial to borrowers.\n\nBroadly speaking, financial regulators have four types of tools to protect consumers and to promote safe and sound underwriting practices.  First, they can require disclosures by lenders that help consumers make informed choices.  Second, they can prohibit clearly abusive practices through appropriate rules.  Third, they can offer principles-based guidance combined with supervisory oversight.  Finally, regulators can take less formal steps, such as working with industry participants to establish and encourage best practices or supporting counseling and financial education for potential borrowers.\n\nIn the area of disclosure, the Federal Reserve is responsible for writing the regulation that implements the Truth in Lending Act (TILA), known as Regulation Z.  The purpose of Regulation Z is to ensure that lenders provide borrowers or potential borrowers with clear, accurate, and timely information about the terms and conditions of loans.  The Federal Reserve is also authorized to write rules; notably, the Home Ownership Equity Protection Act (HOEPA) gives the Board the power to prohibit acts and practices in mortgage lending deemed \"unfair\" or \"deceptive.\"6  Both the disclosures required by TILA and the rules developed under HOEPA (which is part of TILA) apply to all lenders, not just banks.  In cooperation with the other federal banking regulators, the Board can also draft supervisory guidance and back it up with regular examinations.  Supervisory guidance applies only to banks and thrift institutions, although state regulators of nonbank lenders can and sometimes do adopt guidance written by the federal regulators.\n\nIn my judgment, effective disclosures should be the first line of defense against improper lending.  If consumers are well informed, they are in a much better position to make decisions in their own best interest.  However, combating bad lending practices, including deliberate fraud or abuse, may require additional measures.  Rules are useful if they can be drawn sharply, with bright lines, and address practices that are never, or almost never, legitimate.  Sometimes, however, specific lending practices that may be viewed as inappropriate in some circumstances are appropriate in others, and the conditions under which those practices are appropriate cannot be sharply delineated in advance.  In such cases, supervisory guidance that establishes principles or guidelines is, when applicable, probably the better approach.  Guidance can be modified as needed to apply to different situations, and thus can be a more flexible tool than rules for accomplishing regulators’ goals.\n\nAs I noted, markets are adjusting to the problems in the subprime market, but the regulatory agencies must consider what additional steps might be needed.  The Federal Reserve is currently undertaking a thorough review of all its options under the law.  Under its TILA authority, the Board last summer began a top-to-bottom evaluation of mortgage-related disclosures with a series of four open hearings around the country, in which we heard public concerns about various mortgage-related issues, including predatory lending and the effectiveness of the currently required disclosures.  Using consumer testing, we will be working to improve the disclosures associated with mortgage lending and to fight deceptive marketing practices.  This effort will draw heavily on our nearly-completed review of disclosures relating to open-end credit, including credit cards, for which we made extensive use of consumer testing to determine which disclosure formats are most effective and informative.7\n\nOf course, the information provided by even the best-designed disclosures can be useful only when it is well understood.  Accordingly, the Federal Reserve produces and regularly updates a range of materials, including a booklet that lenders are required to provide to potential ARM borrowers, to help consumers understand ARMs and other alternative mortgages; and we will continue to promote financial education through a variety of partnerships with outside organizations.  Federal Reserve Banks around the country will also continue their cooperation with educational and community organizations that provide counseling about mortgage products and the responsibilities of homeownership.\n\nWe are also actively reviewing the possible use of our rule-making authority to prohibit certain specific practices.  In 2001, the Board acted under its HOEPA authority to ban several practices for high-cost loans that were determined to be unfair or deceptive, such as loan flipping--frequent and repeated refinancing to generate fees for lenders.  The Board will consider whether other lending practices meet the legal definition of unfair and deceptive and thus should be prohibited under HOEPA.  Any new rules that we issue should be sharply drawn, however.  As lenders are subject not only to regulatory enforcement action but possibly also to private lawsuits for redress of HOEPA violations, insufficiently clear rules could create legal and regulatory uncertainty and have the unintended effect of substantially reducing legitimate subprime lending.  Next month, we will conduct a public hearing to consider how we might further use our HOEPA authority to curb abuses while preserving access to credit.  We have invited people representing all sides of the debate to present their views.\n\nWe have also used, and will continue to use, supervisory guidance to help mitigate problems in the subprime sector.  Earlier this year, the Board and other federal bank and thrift regulators issued draft supervisory guidance to address concerns about underwriting and disclosure practices, particularly of subprime ARMs.  Many industry and consumer groups have responded to our proposal, and we are now reviewing the comments.  Regulators in 1999 issued guidance on subprime lending and in 2001 expanded that guidance.  Last year, we issued guidance concerning so-called nontraditional mortgages, such as interest-only mortgages and option ARMs.  For both subprime and nontraditional mortgages, our guidance has reminded lenders of the importance of maintaining sound underwriting standards and of providing consumers with clear, balanced, and timely disclosures about the risks and benefits of these mortgages.\n\nThe patchwork nature of enforcement authority in subprime lending--in particular, the fact that the authority to make rules and the responsibility to enforce those rules are often held by different agencies--poses additional challenges.  For example, rules issued by the Board under TILA or HOEPA apply to all mortgage lenders but are enforced--depending on the lender--by one of five federal regulators of depository institutions, the Federal Trade Commission (FTC), or state regulators.  To ensure consistent and effective enforcement, close cooperation and coordination among the regulators are essential.  The Board remains committed to working closely with other regulators to achieve uniform and effective enforcement.  We can continue to improve the sharing of information and the coordination of some activities, such as examiner training, through the Federal Financial Institution Examination Council, which the Conference of State Banking Supervisors (CSBS) recently joined, as well as through other channels, such as the CSBS’s State/Federal Working Group.  We will also draw on the expertise of other regulators as we consider changes in required disclosures and rules.\n\nMacroeconomic Implications\nThe problems in the subprime mortgage market have occurred in the context of a slowdown in overall economic growth.  Real gross domestic product has expanded a little more than 2 percent over the past year, compared with an average annual growth rate of 3-3/4 percent over the preceding three years.  The cooling of the housing market is an important source of this slowdown.  Sales of both new and existing homes have dropped sharply from their peak in the summer of 2005, the inventory of unsold homes has risen substantially, and single-family housing starts have fallen by roughly one-third since the beginning of 2006.  Although a leveling-off of sales late last year suggested some stabilization of housing demand, the latest readings indicate a further stepdown in the first quarter.  Sales of new homes moved down to an appreciably lower level in February and March, and sales of existing homes have also come down on net since the beginning of this year.\n\nHow will developments in the subprime market affect the evolution of the housing market?  We know from data gathered under the Home Mortgage Disclosure Act that a significant share of new loans used to purchase homes in 2005 (the most recent year for which these data are available) were nonprime (subprime or near-prime).  In addition, the share of securitized mortgages that are subprime climbed in 2005 and in the first half of 2006.  The rise in subprime mortgage lending likely boosted home sales somewhat, and curbs on this lending are expected to be a source of some restraint on home purchases and residential investment in coming quarters.  Moreover, we are likely to see further increases in delinquencies and foreclosures this year and next as many adjustable-rate loans face interest-rate resets.  All that said, given the fundamental factors in place that should support the demand for housing, we believe the effect of the troubles in the subprime sector on the broader housing market will likely be limited, and we do not expect significant spillovers from the subprime market to the rest of the economy or to the financial system.  The vast majority of mortgages, including even subprime mortgages, continue to perform well.  Past gains in house prices have left most homeowners with significant amounts of home equity, and growth in jobs and incomes should help keep the financial obligations of most households manageable.\n\nConclusion\nCredit market innovations have expanded opportunities for many households.  Markets can overshoot, but, ultimately, market forces also work to rein in excesses.  For some, the self-correcting pullback may seem too late and too severe.  But I believe that, in the long run, markets are better than regulators at allocating credit.\n\nWe at the Federal Reserve will do all that we can to prevent fraud and abusive lending and to ensure that lenders employ sound underwriting practices and make effective disclosures to consumers.  At the same time, we must be careful not to inadvertently suppress responsible lending or eliminate refinancing opportunities for subprime borrowers.  Together with other regulators and the Congress, our success in balancing these objectives will have significant implications for the financial well-being, access to credit, and opportunities for homeownership of many of our fellow citizens.\n\nFootnotes\n\n1.  This estimate is based on data from the Mortgage Bankers Association, adjusted to reflect the limited coverage of the association’s sample. Return to text\n\n2.   Near-prime loans include those securitized in \"alt-A\" pools and similar loans that are held on lenders’ books. Return to text\n\n3.  Estimates of delinquencies are based on data from First American LoanPerformance.  The rate of serious delinquencies for variable-rate subprime mortgages also reached about 11 percent in late 2001 and early 2002. Return to text\n\n4.  Foreclosure starts are based on data from the Mortgage Bankers Association, adjusted to reflect the limited coverage of their sample. Return to text\n\n5.  Many mortgage brokers are subject to minimum licensing standards and bonding or net worth criteria, but these standards and criteria vary across states. Return to text\n\n6.  For home refinance loans, the Board can prohibit practices that it finds to be associated with abusive practices or not in the best interest of the borrower. Return to text\n\n7.  The results of the review of disclosures for open-end credit and the associated notice of proposed rule-making will be discussed at an open meeting of the Board of Governors on May 23, 2007. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070517a.htm",
        "title": "The Subprime Mortgage Market",
        "date": "5/17/2007"
    },
    {
        "content": "May 16, 2007\n\nGovernor Randall S. Kroszner\n\nAt the Center for Financial Stability (CEF), Buenos Aires, Argentina\n\nI am delighted to be back in Argentina. Today I want to talk about some important macroeconomic and financial developments around the world. Inflation rates and interest rates are at historically low levels, and yield curves are relatively flat nearly everywhere. In addition, until recently, many emerging-market countries simply did not have yield curves because there was effectively no market for debt issued in the domestic currency beyond a very short term. I believe that the taming of inflation and the improved credibility of monetary policy have been crucial to the deepening of domestic capital markets, which is typically associated with higher economic growth. I will explore some possible explanations for these developments, focusing on the prospects for, and risks to, the long-term inflation outlook, particularly in emerging-market economies. Before I continue, please note that the opinions I express here today are my own and not necessarily shared by my colleagues on the Federal Open Market Committee.1\n\nIn brief, I argue that globalization, deregulation, and financial innovation, in part spurred by the experiences of high inflation in the 1980s, have fostered currency competition that has led to improved central bank performance and, hence, the recent reduction of worldwide inflation. Writing in the 1970s, Friedrich Hayek advocated greater competition among currencies, arguing that it would produce a race to the top rather than a race to the bottom.2 In practice, regardless of what one might think of Hayek's policy proposals, we have seen increased competition among currencies issued by central banks owing to technological change in a globalized and competitive marketplace.\n\nThe increased competition among currencies has changed the ability and the incentives of governments and central banks to pursue high-inflation policies. Such changes have allowed improvements in central bank governance and credibility, thereby leading to better inflation outcomes, especially in many emerging-market economies. In addition, greater central bank credibility has allowed the development of long-term bond markets in many countries and flattened yield curves around the globe as concerns about future inflation risks have declined. Deeper bond markets with a wider range of available maturities encourage long-term planning and investment and thus convey lasting gains, particularly in emerging markets. The important issue is whether the reduction of worldwide inflation will persist or be a temporary phenomenon.\n\nThe Worldwide Decline in Inflation\n\nFrom the 1950s until the late 1960s, inflation rates were relatively contained, and episodes of high inflation were rare. Then, especially following the early-1970s collapse of the Bretton Woods fixed-exchange-rate system, inflation became a worldwide phenomenon. Even in Germany, which had the most stable prices of any country in the world, inflation eroded purchasing power by more than half between 1972 and the launch of the euro, in 1999. In terms of cumulative inflation, $480 would be required today in the United States to purchase $100 worth of goods and services at 1972 prices. For most countries reporting data to the International Monetary Fund (IMF), cumulative inflation has been even higher than that experienced in the United States. Here in Argentina, the price level is now about 177 billion times higher than it was in 1972. But Brazil takes the prize for greatest inflation of the post-World War II era: The price level in Brazil is approximately 500 billion times higher today than it was in 1972.3\n\nSince the early 1990s, however, worldwide inflation has significantly declined. Indeed, the April 2006 issue of the IMF's World Economic Outlook shows that, on average, inflation rates in the advanced economies as well as in the developing countries in recent years have been at their lowest levels since at least the early 1970s. The April 2007 issue forecasts inflation in both regions to remain near these recent lows through 2008.\n\nBut is this lower inflation regime likely to persist? One way to approach this issue is to investigate what market participants think by examining measures of expected inflation. Overall, these surveys suggest that market participants do expect relatively low inflation to continue in most major industrial and emerging-market countries. In the industrial countries and a few Asian emerging-market economies, for example, long-term inflation expectations dropped below 5 percent a decade or more ago and are currently around 2 percent to 3 percent.4 In Brazil and Mexico, long-term inflation expectations have declined from a range of 7 percent to 10 percent a decade ago to a fairly steady 3 percent to 4 percent in recent years. In Argentina, long-term inflation expectations are around 7 percent.\n\nIn addition, the risks of high inflation appear to have decreased as well. In particular, the volatility of inflation has declined notably, especially in many emerging- market countries.5 On occasion in the early to middle 1990s, the standard deviation of inflation exceeded 30 percent in Mexico and 100 percent in Argentina and Brazil. In the current decade, however, the standard deviation of inflation has been relatively stable in Brazil, at around 5 percent, and it has been declining in Mexico, where it is now around 2 percent. In Argentina, inflation volatility is currently around 15 percent.\n\nFactors Behind the Taming of Worldwide Inflation\n\nMy brief review of worldwide inflation performance suggests that in general, over the past decade, inflation has become substantially lower and less volatile, and expectations of future inflation have also become substantially lower. In understanding the key factors behind this change, we can also shed further light on the question of whether this low-inflation regime will persist.\n\nIn a nutshell, I believe that the factors of globalization, deregulation, and financial innovation, arising partly in response to episodes of high inflation, have effectively eroded the central bank monopoly on the provision of monetary services and have enhanced global competition among currencies. These changes have, in turn, altered the incentives for central banks to behave badly and for finance ministries to use central banks as \"piggy banks\" to finance their fiscal policies. The resulting constraint on monetary policy, combined with increased public understanding of the costs of inflation, have led to institutional changes in central bank governance that bolster their credibility for maintaining price stability in the future. Thus, improved central bank performance and credibility are the consequences of this combination of factors.\n\nTo develop this explanation in more detail, I will start by describing how globalization, deregulation, and innovation can alter the ability and incentives of a government to pursue a high-inflation policy.6 These factors are closely related and mutually reinforcing in many respects. Many countries have turned increasingly to private markets and trade to deliver growth and progress. The resulting deregulation and greater openness has boosted innovation and has helped increase global competition, or globalization, by shrinking the barriers of time and distance. Accordingly, trade and financial linkages between countries have soared to record levels in recent years.\n\nHow does this affect inflation? When governments resort to printing money to finance their spending, inflation rises and nominal assets lose their value. This loss of value is also known as the inflation tax. Globalization, deregulation, and innovation make it easier for citizens to move their wealth out of nominal assets in the domestic currency should their government resort to an inflation tax. When the tax base shrinks in response to inflation, governments have a reduced incentive to resort to the inflation tax.\n\nThe specific channels by which globalization, deregulation, and financial innovation affect competition among currencies are many. Increased circulation of banknotes in dollars or other hard currencies enable citizens to conduct transactions and store liquid wealth without holding inflationary currency. The fraction of U.S. currency estimated to be held in foreign countries rose dramatically over the 1980s and 1990s, from less than 20 percent to about 60 percent, and it has remained near this high level even as inflation rates have come down globally.\n\nSubstantial financial innovations--including advances in electronic payment and trading systems as well as more widespread credit card networks and increased use of mutual funds--have enabled consumers, investors, and banks to shift wealth cheaply and quickly away from currencies and assets subject to inflation and related risks.\n\nGiven the stronger competition among currencies, a government that pressures a central bank to pursue an inflationary policy gets much less benefit from increased inflation because people can more rapidly and conveniently switch out of the domestic currency. Indeed, the website of the Central Bank of Brazil explicitly acknowledges the role of inflation in driving financial innovations that enabled firms and households to economize on cash balances in that country. It states that, \"Prior to the mid-1990s [when inflation was stabilized], changes in the payment system in Brazil were motivated by the need to cope with high inflation rates. During that time, the system achieved significant technological progress, especially aimed at enhancing the speed of processing financial transactions.\" 7\n\nIn addition to encouraging financial innovation, the painful experience of high inflation helped to educate the public and economists about the costs of inflation.8 Although the specific experiences differed across countries, public opinion almost everywhere eventually reinforced the trend against inflationary policies. Economists and central bankers also devoted great attention to understanding the causes and consequences of inflation, providing the intellectual underpinning to policies oriented toward price stability.\n\nThe fundamental forces I have described today--globalization, deregulation, financial innovation, and public understanding about the costs of inflation--provided the impetus for fighting inflation and opened the path for policies that enhance central bank credibility. As the benefits of stable prices accrue and as financial markets deepen and become more sophisticated, the benefits of sound economic policies will help create support for institutional reforms that make returning to inflation harder--albeit not impossible--for future governments.\n\nImplications of Low Inflation for Bond Markets\n\nWhat are the implications of low inflation and low inflation volatility for interest rates and yield curves? I believe that market confidence in continued low inflation has helped drive down the slope of the yield curve around the world by reducing the premium demanded for holding long-term nominal assets. The taming of inflation also has extended bond maturities and yield curves further into the future than ever before, most notably in many emerging-market countries.\n\nThe current low level of long-term yields in the United States and other advanced economies is widely acknowledged as somewhat of a puzzle, or, as some have called it, a conundrum.9 Of course, flat and even inverted yield curves in advanced economies are nothing new--we know that the short end of the yield curve is dominated by monetary policy and cyclical factors.\n\nTo abstract from the potential effect of cyclical factors on the yield curve, consider the pattern of forward rates many years into the future, at which point the effects of current cyclical shocks would be expected to have dissipated. The yield on a ten-year bond, for instance, can be thought of as an average of a series of consecutive forward rates. If you could borrow and lend at the same rate as the U.S. Treasury, then you could lock in a three-month loan ten years from now by borrowing for ten years and three months and simultaneously lending the same principal for ten years. The difference between the interest you pay and the interest you earn on this transaction determines the implied forward rate ten years from today. The forward rate reflects not only the market expectation of the future short-term interest rate but also a \"term premium\" to compensate for the risk of a commitment to extend credit so far in the future, including the risk of future inflation.\n\nAt any point in time, then, we can calculate the short-term forward rate ten years ahead using the yield curve of U.S. Treasury coupon securities. This \"far forward\" rate has hovered around 4-1/2 percent to 5 percent over the past two years, a level about 2 percentage points below its average since 1990. Far-forward rates in other advanced economies have also declined over the past decade and are currently at or near historic lows. 10\n\nTo some extent, low forward rates may reflect a persistent decline in expected future real rates of interest or in the real term premium. Chairman Bernanke has suggested that an excess of ex ante global saving relative to global investment has held down real interest rates around the world.11 Some of the factors behind this excess of saving over investment include the surge in revenues of oil and commodity exporters, a retreat in Asian investment demand from the boom that preceded the late 1990s, and a reduction in fiscal deficits in some Latin American countries. But these low bond yields also have a nominal aspect. The declines in inflation rates, in the volatility of inflation, and in long-term inflation expectations all point to a reduction in the compensation required by investors for the effects of future inflation on the returns to holding long-term bonds.\n\nThis development is particularly remarkable in many emerging-market countries, in which longer-dated fixed-coupon bonds issued in domestic currencies had ceased to exist during the inflationary 1970s and 1980s. The recent lengthening of maturities of domestic-currency debt has, in many cases, not only extended the yield curve but--and this is one of the key results of the taming of inflation--effectively created a domestic-currency yield curve that previously did not exist.\n\nSince 2000, ten-year nominal fixed-coupon bonds in the domestic currency have been introduced in Brazil, Chile, Colombia, Indonesia, Mexico, and Russia. Korea and Thailand introduced ten-year fixed-coupon bonds in their respective currencies in the 1990s. Last year the government of Mexico issued a thirty-year fixed-coupon bond in pesos for the first time. The proportion of domestic-currency debt in Mexico maturing within one year was nearly 90 percent in 2002 and is now less than 75 percent.12 The Korean government continues to increase the proportion of its domestic-currency debt in longer maturities, with the one-year-and-under segment falling from roughly one-half in 1999 to less than one-fifth by the end of last year. Moreover, maturity extension is not limited to emerging-market countries. France and the United Kingdom, for example, issued fifty-year bonds in 2005.\n\nBesides the fact that yield curves have been extended in many countries, they have also been relatively low and flat worldwide, at least in part because of the decline in inflation. The flattening or slight inversion of yield curves in the major industrial economies, such as the United States, Japan, the euro area, the United Kingdom, and Canada is well known. For example, ten-year yields in the euro area are less than 25 basis points higher than three-month yields, and the yield curve is currently downward sloping in the United Kingdom.\n\nAt the same time that maturities have been extended, bond yields in the domestic currencies of emerging-market countries have also declined. It is perhaps not surprising that, given the high rates of saving and generally high level of development in their economies, the governments of Hong Kong and Korea can borrow at levels close to those in the advanced economies. More notable, however, is that the Mexican government can borrow in pesos at a thirty-year maturity at roughly 8 percent. Although Mexico is perhaps the most striking example, it is not alone. Other middle-income emerging-market countries with single-digit yields on fixed-rate ten-year bonds in the domestic currency include Chile, Colombia, Malaysia, Russia, South Africa, and Thailand, to name but a few. The computation of forward rates for most of these countries is difficult because of the relative sparseness of the maturity distribution, but for those countries in which five-year forward rates can be computed, they have been declining and have reached very low levels in the past year or so.\n\nOverall, the combination of lower and less volatile inflation worldwide has reduced inflation expectations and perceived inflation risk and resulted in a lower premium in long rates for inflation uncertainty. I believe that these factors have been key contributors to the lower long-term yields and the flattening of yield curves, particularly in emerging markets. The establishment of markets for long-term nominal government and corporate debt in countries in which they did not exist a decade ago is powerful evidence of the faith that investors place in a future environment of price stability.\n\nBroader Economic Benefits\n\nThe economic benefits of price stability are too numerous and well known for me to cover here in detail. Long-run price stability certainly is essential for achieving maximum employment. However, I would like to underscore some often-overlooked benefits related to the development of markets for long-term bonds.\n\nPrice stability boosts growth by deepening financial markets. Given stable prices, savers and investors have more confidence about the ultimate value of their investments and are more willing to enter long-term financial contracts. A number of studies have concluded that the development of banking and financial markets is a key driver of economic growth. Thus, greater central bank credibility, which permits more development of local financial markets, can have an economic benefit beyond the financial sector. 13\n\nThe increasing issuance of local-currency bonds is leading to an improved distribution of risk. When emerging-market countries borrowed mostly in dollars or other foreign currency, they bore the exchange rate risk while lenders bore default risk. This was a key reason why currency crises of the past few decades were so costly: Sharp depreciations of the exchange rate boosted the domestic-currency value of foreign debt and wreaked havoc with government finances and corporate balance sheets. With reduced reliance on foreign-currency borrowing, emerging-market economies should be able to weather future storms with less disruption than in the past.\n\nThe development of long-term domestic-currency bond markets is one of the factors that help lower the costs of long-term planning and enhance the ability to undertake long-term investments In particular, investment decisions are less likely to be constrained by having only short-term financing available for longer-term projects, thereby allowing improved decisionmaking for governments, firms, and individuals. These improvements enhance the prospects of economic development.\n\nHigher and more-stable growth combined with a better ability to undertake long-term plans can also improve the fiscal outlook for a country. A better fiscal outlook, in turn, increases confidence, facilitates financial market development, and thus further boosts growth. And, in a virtuous cycle, it reinforces prospects for continued price stability. More-prudent fiscal policies, including lower deficits, longer debt maturities, and reduced foreign-currency debt can reduce the likelihood and potential severity of financial crises. These policies make the financial positions of emerging-market governments less vulnerable to movements in interest rates and exchange rates. A reduction in the perceived risk that a government may not be able to service its debts makes changes in investor sentiment and financial contagion less likely, thereby reducing financial market volatility.\n\nMaintaining Progress\n\nI have argued that globalization, deregulation, and financial innovation, in part spurred by recent experiences of high inflation, have fostered currency competition that has led to improved central bank performance and, hence, the reduction of inflation worldwide. The resulting enhancement of central bank governance and credibility has allowed the development of long-term bond markets in many countries and the flattening of yield curves around the globe.\n\nI want to conclude by returning to the question of whether these phenomena are likely to persist. Globalization and innovation are genies that may prove difficult to put back in their bottles. Nonetheless, although I am an optimist, I would be remiss if I did not point out some risks. In particular, deregulation and global competition may be subject to change. The difficulty of reaching agreement in the Doha Round of trade negotiations highlights the risk of renewed protectionism and backtracking on deregulation. Trade barriers and regulations are anathema to globalization and competition. Barriers to the free flow of goods, services, capital, and technology would also diminish the force of innovation that has been so beneficial in the struggle against inflation. Transaction taxes and administrative barriers may hinder the development and liquidity of bond markets, and much progress that is still required in many emerging-market countries on these fronts.\n\nHigh inflation can destroy an economy and result in enormous hardship for everyone involved, as Argentina painfully experienced not long ago. The benefits achieved through more-stable prices are substantial. Fortunately, economic forces have led to better central bank behavior around the world during the past decade. If citizens and politicians lose sight of these benefits, and the forces that have led to enhanced currency competition are thwarted, these gains could prove fleeting. I believe that we must continue to work hard to lock in the gains achieved so far. The lessons of the high- inflation episodes are too important to forget.\n\nFootnotes\n\n1. Joseph E. Gagnon, of the Board's Division of International Finance, contributed to this speech, portions of which draw on Randall S. Kroszner (2006), \"The Conquest of Worldwide Inflation: Currency Competition and Its Implications for Interest Rates and the Yield Curve,\" speech delivered at the Cato Institute Monetary Policy Conference, Washington, November 16, www.federalreserve.gov/newsevents/speech/kroszner20061116a.htm.  Return to text\n\n2. Friederich A. von Hayek (1976), Denationalisation of Money: The Argument Refined: An Analysis of the Theory and Practice of Concurrent Currencies (London: Institute of Economic Affairs). Return to text\n\n3. Data for Germany based on 1972-88 changes in consumer prices, and data for the United States, Argentina, and Brazil based on 1972-2006 changes; IMF (various years), International Financial Statistics (Washington: IMF).  Return to text\n\n4. Long-term forecasts refer to forecasts six to ten years ahead from the April and October surveys of Consensus Economics, www.consensuseconomics.com.    Return to text\n\n5. Volatility is defined as the twenty-quarter rolling standard deviation of annualized inflation. Return to text\n\n6. This section draws on aspects of Randall S. Kroszner (2003), \"Currency Competition in the Digital Age,\" in David E. Altig and Bruce D. Smith, eds., Evolution and Procedures in Central Banking (New York: Cambridge University Press), pp. 275-99; and Randall S. Kroszner (2006), \"Why Are Yield Curves So Flat and Long Rates So Low Globally?\" speech delivered at the Institute of International Bankers, New York, June 16, www.federalreserve.gov/newsevents/speech/kroszner20060615a.htm. Kenneth Rogoff has proposed another effect of globalization on inflation. According to Rogoff, greater competition leads not only to lower but also to more-flexible prices. When prices are more flexible, a central bank's ability to temporarily influence output is diminished, and its influence on inflation is enhanced. Thus, more-competitive markets naturally help central banks achieve price stabilization. Kenneth S. Rogoff (2003), \"Globalization and Global Disinflation,\" in Monetary Policy and Uncertainty: Adapting to a Changing Economy, a symposium sponsored by the Federal Reserve Bank of Kansas City, Jackson Hole, Wyo. (Kansas City: the Reserve Bank), pp. 77-112.    Return to text\n\n7.  www.bcb.gov.br/?PAYSYSREFORM  Return to text\n\n8.  This hypothesis was raised in the discussion of Rogoff, \"Globalization and Global Disinflation,\" pp. 119-30. Evidence that voters in Latin America in recent years have punished politicians for bad inflation outcomes is in Eduardo Lora and Mauricio Oliveira (2005), \"The Electoral Consequences of the Washington Consensus,\" Economia, vol. 5 (Spring), pp. 1-61. Return to text\n\n9.  Alan Greenspan (2005), statement before the Committee on Banking, Housing, and Urban Affairs, U.S. Senate, February 16, www.federalreserve.gov/boarddocs/hh/2005/february/testimony.htm. Return to text\n\n10. Although far-forward rates in yen are up about 1 percentage point from the historical lows associated with the Japanese deflation scares of 1998 and 2003, they are obviously still low in historical context.  Return to text\n\n11. Ben S. Bernanke (2005), \"The Global Saving Glut and the U.S. Current Account Deficit,\" Sandridge Lecture at the Virginia Association of Economists, March 10, www.federalreserve.gov/boarddocs/speeches/2005/200503102.  Return to text\n\n12. I have included floating-rate debt in the one-year maturity category.  Return to text\n\n13. Refer to Ross Levine (2005), \"Finance and Growth: Theory and Evidence,\" in Philippe Aghion and Steven Durlauf, eds., Handbook of Economic Growth (New York: Elsevier); and Randall S. Kroszner and Philip E. Strahan (2006), \"Regulation and Deregulation of the U.S. Banking Industry: Causes, Consequences, and Implications for the Future,\" unpublished paper.  Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20070516a.htm",
        "title": "Globalization and Capital Markets: Implications for Inflation and the Yield Curve",
        "date": "5/16/2007"
    },
    {
        "content": "May 16, 2007\n\nVice Chairman Donald L. Kohn\n\nAt the Federal Reserve Bank of Atlanta's 2007 Financial Markets Conference, Sea Island, Georgia\n\nAs Chairman Bernanke noted yesterday in his keynote address, the development and growth of the credit derivatives markets has implications for a range of public policy objectives. I will focus on the implications for our objectives for financial stability and the containment of systemic risk.1\n\nTo formulate appropriate policy responses, we first must identify how credit derivatives might be affecting systemic risk. In considering this question, we need to recognize that credit derivatives are an extension of decades-long trends. The Chicago futures exchanges introduced financial derivatives in the 1970s. Since the early 1980s, over-the-counter (OTC) derivatives have become increasingly important. Over the same period, the securitization of mortgages and other assets has been transforming regulated depository institutions from holders of interest rate and credit risk to originators and distributors of such risk. Perhaps the most significant aspect of credit derivatives has been the marriage of derivatives and securitization technologies in synthetic collateralized debt obligations (CDOs).\n\nThere are good reasons to think that these developments have made the financial system more resilient to shocks originating in the real economy and have made the economy less vulnerable to shocks that start in the financial system. Borrowers have a greater variety of credit sources and are less vulnerable to the disruption of any one credit channel; risk is dispersed more broadly to people who are most willing to hold and manage it. One can see the effects of these changes in the reduced incidence of financial crises in recent years. From the 1970s through the early 1990s, we seemed to be in almost continuous crisis mode. These crises were centered on depository institutions, and because borrowers were so dependent on depository institutions for credit availability, problems at depository institutions meant problems for the financial system and for the economy more generally.\n\nTo be sure, much, perhaps most, of the improvement has been due to the better performance of the overall economy--low and stable inflation and damped business cycles since the early 1980s. In addition, financial stability has been greatly enhanced by improvements in bank capital regulation that were implemented in the late 1980s and after the crisis in the early 1990s. But greater diversification of risk has also contributed significantly; we could see this in the 2001-03 period, when a sharp decline in equity prices, a spike in spreads on corporate debt, and unanticipated business failures did not threaten depository institutions or the broader financial system.\n\nBut we certainly should not read this experience as meaning that we are free of systemic risk--the risk of financial-sector problems spilling into the real sector or aggravating already difficult economic circumstances. Indeed, I see several reasons for carefully considering the potential for such problems to emerge.\n\nNew players and new instruments have become important since 2002, when the last adverse credit cycle peaked. New and already existing market participants are maneuvering for greater shares in a rapidly evolving market structure. Although leverage has declined in the nonfinancial businesses whose credit is being priced and traded, it may well have increased in the structure of intermediary finance. In any event, the growth of tranched CDOs and other structured credit products with substantial embedded leverage has made it more difficult to assess the degree of leverage of individual institutions or of the financial system as a whole.\n\nIn addition, the extraordinarily rapid growth of credit derivatives markets in the past few years has occurred against the backdrop of relatively benign macroeconomic performance--good global growth, low inflation, historically high corporate profits and low business failures, and reasonably predictable monetary policies. Partly as a consequence, the prices of financial assets seem to embody relatively low expected volatilities and relatively little reward for taking credit risk or for extending the duration of investor portfolios.\n\nWith more risk traded in markets and more participants managing that risk through portfolio adjustments made in markets, the importance of market liquidity has increased and the potential knock-on effects from an erosion of liquidity have multiplied. We could face situations in which asset price movements are exacerbated by the actions of market participants, including dynamic hedging strategies or forced liquidations of assets to meet margin calls, and those asset price movements could feed back onto the economy.\n\nMoreover, the layers of intermediation between borrower and lender that increasingly characterize the financial system may have created new channels for the transmission of shocks within the financial markets and into the economy. We have seen some indications of this in the recent experience in the subprime mortgage market. Packagers of securities whose performance is tied to subprime mortgages have suffered unanticipated losses, and as some originators have gone out of business and secondary markets have been disrupted, institutions along the chain have found themselves with unexpected exposures from warehousing or financing the holding of loans before securitization. However, thus far the dramatic widening of spreads on the riskier tranches of subprime securitizations has not produced spillovers large enough to threaten stability.\n\nFinally, although risks are better dispersed, the credit-risk transfer markets are dependent on a small set of key intermediaries. In the extreme, price variations and other adverse developments could call into question the viability of these intermediaries, threatening a larger cumulative real effect.\n\nIn short, systemic risk in the financial markets most likely has declined on balance, but it still exists, and central banks and other regulators need to adapt to the evolving nature of markets and the changing character of the systemic risk.\n\nBefore discussing what we are doing and what we should be doing, I want to take a few minutes to call attention to some limits and constraints on our actions. We need to accept that accidents will happen--that asset prices will fluctuate, often over wide ranges, and those fluctuations will be driven in part by trading strategies, by the cycles of greed and fear that have always been with us, and by the ebb and flow of competition for market share. The fluctuations will result in redistributions of wealth and, on occasion, will confront us with financial crises. But we cannot and should not try to prevent this process through a monetary policy that puts special emphasis on stabilizing asset prices or through regulatory policies that limit access to markets by qualified participants or that attempt to restrain competition materially. Monetary policy that proactively leans against asset price movements runs a considerable risk of yielding macroeconomic results that fall short of maximum sustainable growth and price stability. Regulatory policies that try to prevent failures of core participants or others under all conceivable circumstances will tend to stifle innovation and reduce our economy's potential for long-run growth.\n\nSystemic events in market-based financial systems are perhaps more likely to involve price fluctuations and abrupt changes in market liquidity than are systemic events in depository-based financial systems. But that is not really bad news because such events can more readily be countered by macroeconomic policy instruments than could old-fashioned crises of depository intermediation. Supplying additional liquidity and reducing borrowing costs can greatly ameliorate the effects of market events on the economy, and those types of macroeconomic interventions will carry less potential for increasing moral hazard than would the discount window lending that was a prominent feature of crisis management when depositories funded more credit.\n\nMarket-intermediated finance also requires us to live with less control and less knowledge than we had when banks were dominant. Greater uncertainty about where risks are lodged is the flip side of better dispersion of those risks, especially to less regulated sectors, and of more resilience of the whole system. Gathering additional information about the risk profiles of currently less regulated institutions is unlikely to yield insights that can be acted upon and may create a false sense of comfort among market participants, which could make the system substantially more risky. We need to have confidence in the invisible hand. But confidence does not mean blind faith, a thought that brings me to what we can productively do to reduce systemic risks within the boundaries that I just described.\n\nAlthough credit derivatives and other derivatives have facilitated the dispersion of risk, the mechanisms for carrying out this function are highly dependent on a small number of institutions that are critical to the functioning of the markets. These institutions are the principal dealers in the OTC derivatives markets and often also among the leading clearing firms for exchange-traded derivatives. They also originate securitized assets, and provide financing to other originators, and often provide financing to the buyers of those assets, including buyers of the riskiest tranches. If their ability to perform these functions were impaired when a shock hit the economy or the financial markets, the ability of the buyers and sellers to manage their positions, provide trading liquidity, and facilitate stabilizing financial flows would in turn be impaired. Consequently, our efforts to manage the potential for systemic risk focus on these core institutions, nearly all of which already are subject to prudential supervision.\n\nIn light of the possibility of unusual asset price movements, which I discussed earlier, we need to encourage these institutions to practice robust enterprise-wide risk management. A critical aspect of such risk-management efforts is the use of stress tests across the various aspects of their businesses that allow them to ascertain and appropriately limit their market and counterparty exposures in a scenario in which credit spreads widen rapidly and asset market liquidity decreases markedly. Good risk management can limit potential losses under many circumstances, but these institutions will still be vulnerable to major unexpected events, and their viability will depend on their capital cushions. As a consequence, we also need to strengthen the connection of capital to risk by moving forward with the implementation of Basel II.\n\nWe should also work toward ensuring that clearing and settlement arrangements on which core institutions and other participants depend are safe and efficient. Weaknesses in such systems can be a source of systemic contagion. Conversely, when such arrangements are robust, the potential for contagion is significantly diminished. The benefits of such supervisory initiatives can extend beyond the core regulated institutions themselves because improvements in their counterparty-risk-management practices will strengthen market discipline on their unregulated counterparties.\n\nAlthough the critical roles that these core institutions play in global markets require us to focus on them, we need to address the possibility that this approach could increase moral hazard by reinforcing perceptions that these institutions are too big to fail. This risk can be mitigated by the steps that I have mentioned earlier, including the promotion of enhanced risk-management practices and the implementation of capital requirements that are more risk sensitive, steps that greatly reduce the chances that government intervention will be needed.\n\nIn all of this work, coordination and cooperation among regulators, domestically and internationally, are critical because the same firms are the core firms in each of the principal global financial centers. Effective oversight of these firms generally can be conducted only by the home country supervisor, which alone has the authority over a firm's consolidated global operations. When supervisory coordination and cooperation exist, much good work can be accomplished relatively quickly. An excellent example is the work that the Federal Reserve Bank of New York and other prudential supervisors have undertaken over the past year and a half to encourage and support market participants' progress in addressing what were serious weaknesses in the infrastructure of the credit derivatives markets. Those supervisors are now systematically reviewing the core firms' management of counterparty exposures to hedge funds and other highly leveraged counterparties.\n\nBoth market participants and public authorities should understand that, despite our best efforts, crises are inevitable, and so we need to work on crisis management as well. Here, too, cooperation among authorities here and abroad is critical. We must understand the market structures and vulnerabilities and the objectives and constraints under which authorities with different jurisdictions would be working in those circumstances.\n\nInevitably, uncertainty on the part of market participants and public authorities will be heightened in the event of market turmoil, and that uncertainty can feed on itself. Both authorities and participants need to think through how they will handle such crises. For the authorities, that process includes considering how to resolve any failures of large institutions in ways that impose costs on shareholders and uninsured liability holders while preserving orderly markets. Such a resolution will be necessary to limit the moral hazard of any interventions that we are forced to undertake. Market participants need to consider how they would settle contracts and work with troubled borrowers in a distress situation. More planning will reduce the rise in uncertainty in a crisis and the likelihood that fear will lead to precipitous actions that are in no one's best interest.\n\nIn sum, there are good reasons to think that financial innovation over the past few decades, including the emergence and growth of the credit derivatives markets, has made the financial system and the economy more resilient. But it would be foolish to think that these innovations have eliminated systemic risk. Both market participants and public authorities need to think carefully about how systemic risks might crystallize in the new world that credit derivatives have helped create. They need to identify the implications for risk management and ensure that risk-management practices are adapting to target new sources of risk. And they need to think about how they would respond to crises that sooner or later will emerge in that new world.\n\nFootnotes\n\n1.The views expressed are my own and do not necessarily reflect those of the other members of the Board. Return to text",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20070516a.htm",
        "title": "Financial stability and policy issues",
        "date": "5/16/2007"
    },
    {
        "content": "May 15, 2007\n\nChairman Ben S. Bernanke\n\nTo the Federal Reserve Bank of Atlanta's 2007 Financial Markets Conference, Sea Island, Georgia(via satellite)\n\nGood morning. I'm pleased to be able to join you for this year's financial markets conference, albeit from afar. Last year the focus was on hedge funds, and the main theme of this year's gathering is credit derivatives. This pairing makes eminent sense, in that the increasing prominence of hedge funds and the growth of the market for credit derivatives are both aspects of the remarkable wave of financial innovation that we have seen in recent years. Both of these developments have also been the subject of public policy debates, including calls for increased regulation. In my remarks today I will address, from the 30,000-foot level, the challenges that financial innovation poses for public policy and the nature of the appropriate regulatory response. I will argue that central banks and other regulators should resist the temptation to devise ad hoc rules for each new type of financial instrument or institution. Rather, we should strive to develop common, principles-based policy responses that can be applied consistently across the financial sector to meet clearly defined objectives.\n\nIn addressing the challenges and the risks that financial innovation may create, we should also always keep in view the enormous economic benefits that flow from a healthy and innovative financial sector. The increasing sophistication and depth of financial markets promote economic growth by allocating capital where it can be most productive. And the dispersion of risk more broadly across the financial system has, thus far, increased the resilience of the system and the economy to shocks. When proposing or implementing regulation, we must seek to preserve the benefits of financial innovation even as we address the risks that may accompany that innovation.\n\nClear thinking is therefore essential. In developing a regulatory framework, we need to be explicit both about what the public policy objectives of regulation are and about how, if at all, fresh developments threaten to undermine those objectives. We should also take into account the role that the market itself can play in controlling risks to public objectives; as I noted last month, market discipline can be an important element in a well-functioning regulatory scheme. And as I have already observed this morning, any regulatory changes should fulfill the test of consistency, across both institutions and instruments.\n\nEnsuring a Consistent Approach\nIn thinking about how, or whether, to regulate innovative financial institutions (such as hedge funds) or instruments (such as credit derivatives), we should be wary of drawing artificial distinctions. Are the characteristics of hedge funds or credit derivatives that arouse concern peculiar to these institutions and instruments, or are they associated with others as well? If the characteristics in question are in fact a feature of the broader financial landscape, then a narrowly focused approach to regulation will be undermined by the incentives such an approach creates for regulatory arbitrage.\n\nFor example, while the complexity of new financial instruments and trading strategies is potentially a concern for policy, as I will discuss, not all credit derivatives are complex and--to state the obvious--not all complex financial instruments are linked to credit risk. Single-name credit default swaps and credit default swap indexes are relatively simple instruments, whereas derivatives based on other asset classes--such as exotic interest-rate and foreign-exchange options--can, by contrast, be quite complex. Moreover, derivatives in general are not necessarily more complex than some types of structured securities. In short, if complexity per se is the concern, we cannot address that concern by focusing on a single class of financial instruments. Similarly, hedge funds are hardly a homogeneous group of institutions, nor can their trading strategies be unambiguously distinguished from those of large global banks or of some traditional asset managers. A consistent regulatory strategy needs to be tailored to the essential characteristics of institutions or instruments that pose risks for policy objectives, not to arbitrary categories.\n\nAt last year's conference, I discussed a policy proposal focused narrowly on hedge funds--namely, the development of a database of hedge fund positions and portfolios. As I noted last year, given the complexity of trading strategies and the rapidity with which positions change, creating a database that would be sufficiently timely and detailed to be of practical use to hedge funds' creditors and investors or to regulators would be extremely difficult. Collecting such information also risks moral hazard, if some traders conclude that, in gathering the data, the regulators have somehow reduced financial risk. The principle of consistency on which I am focusing today raises an additional objection to this proposal, which is that it would make little sense to collect data on hedge funds' positions without gathering the same information for other groups of market participants that use similar strategies and take similar risks.\n\nAn analogous issue arises in the debate over transparency in the credit derivatives market. Some argue that policymakers should act to make trading in the credit derivatives market more transparent, on the grounds that the market and policymakers should know just who is holding the credit risk associated with a particular issuer. But if transparency about risk-bearing is important, then consistency seems to imply that full transparency should be required of credit markets broadly, not just of credit derivatives. And why stop with credit markets? Do we know exactly who is bearing the risk in equity markets or foreign exchange markets, for example?\n\nRather than addressing specific institutions or instruments in isolation, regulators should begin by identifying their objectives and then address the implications of the broad range of financial innovations for those objectives. By returning to the basics, we can increase the coherence, consistency, and effectiveness of the regulatory framework.\n\nPublic Policy Objectives\nAs public policymakers, we have three principal objectives in the financial sphere, objectives that have remained essentially unchanged over many decades even as the pace of financial innovation has accelerated. These objectives are financial stability, investor protection, and market integrity. These goals are widely shared by policymakers around the world and thus provide a basis for international cooperation.\n\nFrom a central banker's point of view, the objective of ensuring financial stability remains critical. Indeed, the Federal Reserve was founded in large part because of concerns about periodic bouts of instability that damaged both the financial system and the broader economy. Policymakers cannot prevent financial shocks, but we can try to mitigate their effects by ensuring that the system remains fundamentally sound. In particular, as I will discuss, we can use our supervisory authority to ensure that the large institutions that form the core of the financial system--which happen to be the leading dealers in the credit derivatives markets and the principal counterparties and creditors of hedge funds--manage the risks that they face in a safe and sound manner.\n\nInvestor protection is another vital public objective. A loss of confidence in the financial system by investors, too, could undermine the system's stability and functioning. Of course, we cannot--and should not--prevent all investor losses. To avoid moral hazard and let market discipline work, investors must be allowed to bear the consequences of the decisions they make and the risks they accept. But investors are entitled to the information they need to make decisions appropriate to their personal circumstances.\n\nClosely linked to the imperative of investor protection is the third public policy objective: preserving the integrity of the market. The stability and the efficiency of the market depend on a common understanding of and adherence to the rules of the game. Thus, policymakers must attach a high priority to preventing insider trading, market manipulation, and other activities that rig the game and undermine public confidence.\n\nChallenges to Public Policy Objectives\nThe rapid pace of financial innovation creates challenges for policymakers with respect to each of these policy objectives. In particular, financial stability depends on adequate risk measurement and risk management by market participants. Failures of risk management by large institutions, or by a sufficient number of smaller ones, would threaten not only the solvency of the institutions themselves but also the health of the whole system.\n\nOf course, in some respects financial innovation makes risk management easier. Risk can now be sliced and diced, moved off the balance sheet, and hedged by derivative instruments. Indeed, the need for better risk sharing and risk management has been a primary driving force behind the recent wave of innovation. But in some respects, new instruments and trading strategies make risk measurement and management more difficult. Notably, risk-management challenges are associated with the complexity of contemporary instruments and trading strategies; the potential for market illiquidity to magnify the riskiness of those instruments and strategies; and the greater leverage that their use can entail.\n\nComplexity--especially when combined with illiquidity--amplifies the difficulty of measuring risk, both market risk and counterparty credit risk. For example, some complex instruments can be valued only with the aid of sophisticated modeling techniques. The problems of valuation and of risk measurement faced by investors in tranches of bespoke collateralized debt obligations (CDOs) are a good example. Similar problems are faced by the core financial intermediaries that often act as counterparties to hedge funds in complex synthetic CDO transactions or that finance hedge funds' investments in bespoke CDO tranches. Complex trading strategies and positions, too, can create problems. For example, counterparty risks may be underestimated because of failures to aggregate exposures to risks across instruments and counterparties. What is essentially the same risk can appear in different forms; for example, investments in a CDO tranche, a bond, and a credit default swap may all entail credit risk to a given obligor.\n\nIlliquidity, or the potential for illiquidity under some conditions, is also a problem for managers of market risk and counterparty credit risk. Substantial market risk may be associated with holdings of illiquid instruments; again, tranches of bespoke CDOs illustrate this well. A pattern of crowded trades may lead to market illiquidity--sometimes in surprising locations--when risk aversion heightens. In particular, counterparty exposures can be significantly increased if the closeout of positions of one or more hedge funds by their dealer counterparties leads to, or exacerbates, market illiquidity.\n\nMarket liquidity depends not only on the presence of willing buyers and sellers but also on the underlying infrastructure, including market-making capacity and the system for clearing and settling financial transactions. Twenty years ago this fall, the 1987 stock market crash was significantly worsened by the inability of trade-processing systems to keep up with order flows, including orders resulting from program trading. Of course, automated trading is far more pervasive today, and overall trading volumes have expanded greatly. As trading volumes grow, market infrastructures must adapt. Until 2005, reliance on paper-based procedures for confirming trades in the rapidly growing credit derivatives markets sometimes resulted in large backlogs of unconfirmed trades, which increased the risks to market participants. With leadership from their prudential regulators, dealers in those markets have adopted electronic confirmation platforms and greatly reduced the backlogs. Currently, regulators and market participants are beginning to address large backlogs of confirmations in the equity derivatives markets.\n\nThe leverage that can be embedded in new financial instruments and trading strategies compounds the difficulties of risk management. Embedded leverage can be difficult to measure; at the same time, like conventional leverage, it may increase investor vulnerability to market shocks. Some credit derivatives do make it easier for investors to take leveraged exposures to credit risk. For example, the first-loss tranche of the investment-grade CDX credit default swap index is exposed to the first 3 percent of losses on the index portfolio. Holding a $3 million position in this tranche exposes an investor to losses on an underlying portfolio of $100 million. A dealer taking the other side of the trade obviously needs to enhance its counterparty risk-management practices to take this greater leverage into account.\n\nComplexity, illiquidity, and embedded leverage also create challenges for policymakers with respect to the objectives of protecting investors and maintaining market integrity. If hedge funds and the large banks that are hedge funds' counterparties and creditors have difficulty assessing the risks associated with complex financial instruments, many investors will find gaining a sufficient understanding of the risks even more burdensome. Investors may also not appreciate the extent to which they may have multiple exposures to the same source of risk--for example, arising from effective exposures to the same hedge fund through funds of funds or from investments in different funds with similar trading strategies. Current restrictions on hedge fund investors, which limit direct investors to institutions or wealthy individuals, reflect the recognition of the difficulties that a retail investor would face in adequately assessing these types of risk. But as instruments and trading strategies become more complex and intertwined, even the most sophisticated investors will be challenged to make reliable judgments about their risk exposures. Likewise, complex and difficult-to-value financial instruments could be exploited as vehicles for profiting from insider trading or market manipulation, although, as history shows, simpler instruments can be used in this way as well. Policymakers must be confident of their ability to detect such market abuses when they occur.\n\nA Principles-Based, Risk-Focused Approach\nHow best to respond to these daunting challenges? As I noted, there are powerful arguments against ad hoc instrument-specific or institution-specific regulation. The better alternative is a consistent, principles-based, and risk-focused approach that takes account of the benefits as well as the risks that accompany financial innovation.\n\nSome commentators have sought to draw a sharp distinction between the approach to financial regulation in the United States and that in the United Kingdom. These observers have characterized the British approach as being principles-based and as using a \"light touch\"--the implication being that these two features somehow go together. In a speech in February of this year, Sir Callum McCarthy, the head of the United Kingdom's Financial Services Authority (FSA), took issue with this interpretation.1 Sir Callum confirmed that the FSA's approach is built on a framework of principles, although he noted that the FSA also has an 8,500-page rulebook to accompany the eleven principles it has laid out. But the FSA head rejected the view that their approach is \"light touch.\" Rather, he said, it is risk-based, which means that regulatory resources and attention are devoted to firms, markets, or instruments in proportion to the perceived risks to the FSA's regulatory objectives.\n\nIn fact, as in the United Kingdom, the principles-based, risk-focused approach to regulation has had considerable influence on this side of the Atlantic as well. For example, as you may know, the President's Working Group on Financial Markets (PWG) recently issued a statement of principles--ten in this case--relating to the regulation of private pools of capital, including hedge funds. Our aim in presenting these principles was to spell out how a combination of market discipline and government oversight could be most effective in addressing the challenges to public policy objectives that I have described. The principles make clear that regulators and supervisors should adopt the risk-focused approach described by Sir Callum. In particular, they emphasize that risks to financial stability are best addressed by focusing our attention on the large institutions at the core of the financial system.\n\nSome care is needed in applying a risk-focused approach to regulation, however. In particular, when the government singles out particular institutions or markets as being especially critical to the stability of the system, moral hazard concerns may well follow. A perception that some institutions are \"too big to fail\" may create incentives for excessive risk-taking on the part of those institutions or their creditors. For that reason, part of an effective risk-focused approach is the promotion of market discipline as the first line of defense whenever possible. Market discipline is enhanced whenever regulators take positive steps to ensure that investors and managers bear the consequences of their financial decisions.\n\nReliance on market discipline should not be confused with a policy of laissez-faire or benign neglect. To the contrary, as the PWG's principles spell out, market discipline often needs to be buttressed by government oversight. Notably, supervisors must diligently ensure that regulated firms--especially those core financial firms that act as creditors, counterparties, and clearing firms for highly leveraged entities, including hedge funds--adopt and implement best practices for monitoring and managing risks. These best practices could include those identified through cooperative private-sector initiatives, such as those of the Counterparty Risk Policy Management Group II. Importantly, best practices must address the challenges I mentioned earlier, including those relating to the complexity of instruments and strategies (which can make exposures difficult to measure), the illiquidity or potential illiquidity of positions held by the firm or its counterparties, and the risks of embedded as well as explicit leverage.\n\nIn implementing risk-focused and principles-based policies, we must also face the reality that finance does not stop at the water's edge. Financial globalization and financial innovation are closely tied, with each trend promoting the other. As a consequence, global regulatory coordination and collaboration are more vital than ever. We already work closely with our counterparts in the major industrial countries as well as in international forums such as the Basel Committee on Banking Supervision and the International Organization of Securities Commissions (IOSCO). To the extent possible, we should work toward common principles and approaches as well as improved information sharing. International cooperation is also essential for establishing and maintaining effective oversight of the payment and settlement systems that constitute the infrastructure of global financial markets. Organizations such as the Committee on Payments and Settlements Systems (CPSS) and IOSCO have developed shared international principles to ensure the safety and efficiency of payment systems.\n\nInvestor protection can also be addressed in a risk-focused, principles-based manner. Most important, disclosures and protections should be tailored to the level of sophistication of the investor. Mutual funds, for example, must provide disclosures sufficient to help retail investors make informed choices. When instruments and strategies are so complex that an unsophisticated investor could not be expected to effectively evaluate and manage the associated risks, U.S. regulators have chosen to limit the exposure of those investors. For example, most retail investors are effectively precluded from engaging in over-the-counter credit derivative transactions or from investing directly in hedge funds unless they meet various criteria regarding income and net worth.\n\nRetail investors may have indirect exposures to complex instruments and strategies--for example, through pension funds. The appropriate principle for investor protection in this case is that the investors' agents--pension fund managers, for example--must apply sound risk-management practices and take risks consistent with the stated objectives of the ultimate investors. Regulators have a role to play in imposing fiduciary duties and standards on the investors' agents. For example, the Employee Retirement Income Security Act (ERISA) sets standards for private pension fund managers, including the requirements that, as fiduciaries, they act prudently and solely in the interest of the pension fund participants. Supervision of these fiduciaries must ensure that these standards are consistently met and that fiduciaries themselves fully understand the nature of their risk exposure.\n\nMarket integrity is the third public policy objective that I noted earlier. Consistent with a principles-based approach, U.S. securities laws against insider trading and market manipulation apply broadly to all financial institutions, including hedge funds, and to trading in a wide range of financial instruments, including securities-based over-the-counter derivatives transactions. Just as institutions and other investors need to adopt best practices to measure and manage risk, they should also have robust internal controls to ensure that the laws are not violated. For example, some market participants have expressed a concern that a bank may use nonpublic information in the credit derivatives market that it has obtained through its lending activities. To protect against such abuses, private-sector groups have proposed practices and principles for handling material nonpublic information--for example, by creating barriers between the staff members with access to such information and others. Risk-focused regulators and supervisors in turn should encourage effective implementation of these best practices, particularly in situations in which the potential for misuse, either intentional or unintentional, is high.\n\nConclusion\nFinancial innovation has great benefits for our economy. The goal of regulation should be to preserve those benefits while achieving important public policy objectives, including financial stability, investor protection, and market integrity. Although financial innovation promotes those objectives in some ways, for example by allowing better sharing of risks, certain aspects of financial innovation--including the complexity of financial instruments and trading strategies, the illiquidity or potential illiquidity of certain instruments, and explicit or embedded leverage--may pose significant risks. These risks should not be taken lightly.\n\nDevising an appropriate regulatory response to financial innovation is challenging. I have argued today that we should strive to implement a regulatory regime that is principles-based, risk-focused, and consistently applied. Enhancing market discipline can complement and strengthen such an approach. As in the United Kingdom, a principles-based approach is not inconsistent with the use of rules, which can provide needed clarity or a safe haven from legal and regulatory risks. However, rules should implement principles rather than develop in an ad hoc manner. Admittedly, a fully consistent regulatory framework that focuses on the most significant threats to public policy objectives is an ideal that may never be fully realized, either here or abroad. However, determined efforts to work toward such a regime could provide substantial economic and social benefits.\n\nFootnotes\n\n1. See Callum McCarthy (2007), \"Financial Regulation: Myth and Reality,\" speech delivered at the British American Business London Insight Series and Financial Services Forum, February 13. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070515a.htm",
        "title": "Regulation and Financial Innovation",
        "date": "5/15/2007"
    },
    {
        "content": "May 15, 2007\n\nGovernor Randall S. Kroszner\n\nAt the Central de la Republica Argentina (BCRA) Seminar, Central Bank of Argentina, Buenos Aires, Argentina\n\nI am delighted to be here in Buenos Aires at a sister central bank, the Banco Central de la República Argentina.1  The topic of my discussion today--international capital flows--is sure to be of interest to any student of Argentina's economic history.  As in all countries, the evolution of the Argentine economy has been conditioned fundamentally by the nation's own framework of laws, policies, and social practices.  Nevertheless, the progression of economic developments in Argentina has always been intertwined with developments in global capital markets, and that connection will likely persist as Argentina continues to open up to the world economy.\n\nI will start by identifying three interrelated aspects of international capital flows in today's global economy.  First, the scale of gross capital flows throughout the world is expanding, reflecting financial innovation, lowered barriers to capital movements, and a decline in what economists refer to as \"home bias.\"  Second, increased capital mobility is making it possible to finance ever larger current account deficits, and, indeed, these deficits have grown in recent years relative to the size of the global economy.  Finally, in the aggregate, we see that capital has been flowing, on net, from emerging-market economies to industrial countries in recent years, the reverse of the pattern in previous decades.2\n\nI would like to focus on the last of these features--the flow of capital between developing and industrial countries.  We will see that the aggregate data obscure important features of the flow of capital between the two global regions.  After discussing various explanations for the net flow of capital to industrial countries, I will address the steps that emerging-market economies can take to enhance their prospects for capital formation and financial investment.  I will also touch on changes in emerging-market financing and how this has altered the distribution of risks in the global financial system.\n\nPlease note that the opinions I'll be expressing today are my own and not necessarily those of my colleagues on the Federal Open Market Committee.\n\nThe Scale and Composition of Capital Flows\nLet's begin by getting some sense of the size, source, and composition of the net capital flows that are moving from developing to industrial countries.  One reasonable measure of the size of these flows is the combined current account balance of the developing economies.  According to estimates by the International Monetary Fund (IMF), the developing economies as a group had a current account surplus of $640 billion last year (IMF, 2007a).3  Because the financial counterpart to this surplus is a deficit on the financial accounts, it represents the net capital outflow to the industrial economies.  $640 billion is a big number and stands in sharp contrast to the situation preceding the Asia crisis.  For example, in 1996 the combined current account balance of the developing economies was a deficit of $80 billion, representing a capital inflow of that amount from the industrial world.\n\nThe sources of the $640 billion in net capital flow out of the developing economies are remarkably concentrated.  Of those developing economies running current account surpluses, a mere seventeen of them--China, four other Asian economies, Russia, and eleven members of the Organization of Petroleum Exporting Countries--accounted for a combined surplus of $710 billion.  And about half of that was generated by the major oil-exporting countries in the Middle East and by Russia, whose surpluses ballooned in the past several years as oil prices soared.  Thus, the other 131 developing economies in our data had a combined current account deficit, or net capital inflow, of $70 billion.  This is not to say that things have not changed for these 131 countries in the past decade:  In 1996, their combined current account deficit was twice as large as it was last year.\n\nThat's the big picture in terms of the size and country source of last year's net capital flows.  Let's dig a little deeper and take a look at the gross capital inflows and outflows that constitute these net figures.  Unfortunately, the information on gross capital flows is not as timely as that for the current account, and the data sets are not as consistent as we might like.  Nonetheless, the estimates we have are informative and, again, a bit surprising.  For example, for 2005, the latest year for which we have reasonably complete data, the developing economies as a group reported a gross capital inflow of $720 billion, more than double the inflow recorded for 1996 (IMF, 2007b).\n\nSo how did we end up with a switch to net outflows?  In 2005, gross capital outflows from the developing economies totaled almost $1.2 trillion--more than a tripling of the 1996 figure.  Thus, we see that the question we have to answer is not, \"Why has capital stopped flowing from the industrial countries to the developing economies?\"  In gross terms it has not stopped; in fact, the flow has accelerated!  The question instead is, \"Why are the developing economies investing so much in the industrial countries?\"\n\nIn a moment I will turn to several explanations for the recent pattern of capital flows.  Before doing that, however, we should take one additional insight from the gross figures:  Of the gross capital outflow from the developing economies in 2005, fully half reflects the further accumulation of foreign exchange reserves by the official sector.  To be sure, a large part of this reserve accumulation was by the seventeen economies singled out earlier as large capital exporters (IMF, 2007c).  But the other 131 economies also recorded substantial reserve accumulations.  Once we put aside these official reserve flows, it turns out that, in 2005, private capital flowed, on net, from the industrial economies to the developing economies.\n\nWhy Is Capital Flowing Uphill?  Why Does it Matter?\nRecognizing that the aggregate data lump together the very different circumstances of different economies, it is still true that, on the whole, net capital is flowing from the developing to the industrial world.  Economists sometimes refer to this phenomenon as capital flowing \"uphill\" because it appears to contradict economic logic (See Prasad, Rajan, and Subramanian, 2006).  There are two elements of that logic.  First, in developing economies, labor is generally much more available than capital; accordingly, capital should in principle be more productive in these economies and should thus flow there from the relatively labor-scarce industrial countries.  Second, the relatively rapid income growth expected by developing economies as they catch up to industrial countries should provide them with incentives to borrow against their expected higher future incomes.  These considerations lead economists to puzzle over both the net outflow of capital from the developing economies and its implications for the global economy.\n\nOne approach to explaining the uphill flow of capital focuses on divergent patterns of growth and investment.  According to one such view, the rise in U.S. productivity growth since the mid-1990s boosted perceived rates of return on U.S. assets and thus attracted capital; expectations of higher rates of return and higher incomes likely boosted U.S. investment and consumption spending as well (Erceg, Guerrieri, and Gust, 2006; Ferguson, 2005).  A complementary explanation, labeled the \"global saving glut\" by Federal Reserve Chairman Bernanke several years ago, argues that during the past decade, declines in investment spending outside the United States--in part because of emerging-market crises--led to a surplus of saving over investment abroad that was channeled toward the U.S. economy (Bernanke, 2005; Gruber and Kamin, forthcoming).\n\nThe global saving glut argument suggests that developing economies have benefited from the recent pattern of global capital flows, gaining both demand for their products and a safe return on their assets at a time when an investment slump threatened to depress domestic activity and rates of return.  The implications of the U.S. productivity story are a little harder to read, but developing economies, especially in Asia, likely have benefited from the expansion of investment and production opportunities created by the revolution in information technology.\n\nNeither the global saving glut nor the U.S. advantage in productivity growth, however, can be expected to persist indefinitely.  How long the differences will persist depends upon a number of factors.  In rapidly developing countries that have had high savings rates such as China, for example, consumption demand is likely to increase as they grow wealthier, thereby reducing their savings rates.  Concerning productivity differences, it may require more than simply the diffusion and adoption of information technology around the globe.  As I will come back to later, flexible labor, product, and financial markets appear to be crucial to reaping the full benefits of the information technology revolution, so national policy choices will play an important role in maintaining or closing productivity differences.  In any event, this set of explanations for the uphill flow of capital suggests that it would be likely and desirable for net capital flows to reverse direction and head back toward the developing economies at some point.\n\nA second--but not mutually exclusive--set of explanations for this phenomenon focuses on what might be more-persistent, structural differences between developing and industrial economies.  The financial systems in many developing nations are relatively weak and are not effective at directing saving toward appropriate investment projects.  That failing leads to inadequate investment, particularly if business activity is further impeded by inadequate property rights and faulty regulations.  As a result, excess saving flows to countries with better financial systems (Prasad, Rajan, and Subramanian, 2006; Ju and Wei, 2006).\n\nA closely related point is that, compared with developing economies, industrial countries are believed to produce financial assets that are safer, less volatile, and more liquid, advantages that also draw capital out of developing economies (Caballero, Farhi, and Gourinchas, 2006; Mendoza, Quadrini, and Rios-Rull, 2007).  These considerations suggest that the current pattern of international capital flows represents a win-win scenario:  Developing economies gain access to better financial services, and industrial economies enjoy the larger quantities of imports they can purchase with this financing.  These explanations also suggest that the uphill flow of capital will be reversed to the extent that financial systems of emerging-market economies develop and improve.  Because such a process is likely to take time, this set of explanations suggests that the uphill flow of capital is likely to persist for a while.\n\nA third set of explanations traces the emergence of current account deficits in the industrial countries, especially the United States, to increases in both public and private consumption which show up as declines in national savings rates.4  Some observers draw a less sanguine message from this approach than the first two mentioned above, namely, that current increases in consumption in the industrial economies could potentially cost them greater indebtedness and lower future consumption, and developing economies might then have less investment than otherwise.  The high level of gross flows into emerging markets and the \"downhill\" flow of private capital to some extent temper this interpretation.\n\nThe three sets of explanations I have just discussed are only some of the many stories that have been offered to explain the uphill flow of capital, and none of them have gained wide acceptance as the best.  Elements of each of them could account for some aspects of the current state of global capital flows, and none are complete explanations for this complex phenomenon.  For example, concerning structural differences, as I discussed earlier, the aggregate current account balance of the developing economies turned to surplus only in the late 1990s; before then, it was in deficit, even though the financial sectors in those economies were, if anything, even less mature than they are today.\n\nThat said, the financial-sector stories as well as the productivity stories I've reviewed probably are at least part of the explanation for the global pattern of capital flows.  Moreover, these stories help to identify policies that developing economies can undertake to boost domestic capital formation and enhance the attractiveness of financial assets, both for foreign and domestic investors.  Such policies lay the foundation for future economic growth as well as promise to alter the current pattern of capital flows and external balances.\n\nMaking Emerging-Market Economies a More Attractive Place to Invest\nBusinesses in many emerging-market economies face a multitude of hurdles.  Red tape, rigid regulations, and weak legal systems impede the formation of businesses, their ongoing operation, and their confidence in having their contracts enforced without long and costly litigation.  Accordingly, governments in these countries could greatly improve the environment for domestic capital formation by simplifying business regulations, strengthening property rights, including the rights of creditors, and improving contract enforcement (Kroszner 2003 and 2006b; Mishkin, 2006).  These kinds of institutional improvements would also help ensure that capital inflows are channeled in a growth-enhancing manner (Arteta, Eichengreen, and Wyplosz, 2003).\n\nAs noted above, I believe it will take more than simply investment in information technology (IT) for a country to enjoy the large productivity enhancements that can be brought about by the IT revolution (Kroszner 2003 and 2006b). The IT revolution has not simply allowed a worker to turn the crank faster on an improved machine (the traditional way we think of technological innovation) but opened the possibility of fundamentally altering the way production (or provision of a service) takes place.  In this way, IT can contribute larger productivity improvements when an economy has flexible labor, product, and financial markets.\n\nTo provide the greatest incentive for investment in the context of the IT revolution, business must operate in an environment that will permit them to transform themselves in ways that allow technology-intensive investment to have the highest possible effect on productivity growth.  Similarly, labor markets must be flexible enough to allow for the prompt re-allocation of resources in response to changes in demand.  The economy also must be competitive enough to allow useful innovations at some firms to be transmitted throughout the industry by market pressure.  This underscores the importance of the overall regulatory and property rights environment to fostering investment necessary for productivity growth in emerging economies.\n\nMore generally, a related set of policies should focus on strengthening the financial system so that it better channels domestic saving into appropriate domestic investment projects.  Adopting international best practices in financial supervision and regulation would help promote prudent risk management and effective intermediation.  Intermediation could also be enhanced by developing capital markets outside the banking system.  The creation of secondary markets for mortgage debt and, as I will describe in more detail shortly, the development of local bond markets more generally has promoted deeper and more liquid financial systems in some emerging markets.  Installing modern trading and electronic payment systems, which facilitate quick and easy allocation of capital, would also help.\n\nA simple, intuitive principle governs all efforts to develop markets that channel funds efficiently from a large number investors (both domestic and foreign).  The principle is that all investors must believe they all have the same access to information about investments and that they all will be treated in the same way.  To this end, policies to increase transparency, disclosure, and restrictions on insider advantages are critical (Gelos and Wei, 2005; Ahearne and others, 2004).\n\nFinally, prudent macroeconomic policies--fiscal consolidation and monetary policies aimed at price stability--are crucial to any effort to boost investment and attract financing (Kroszner 2003 and 2006b).\n\nEmerging-Market Financing and the Distribution of Risks\nEmerging-market economies have, to varying degrees, already made progress in improving their financial environment over the past decade.  Inflation has been substantially reduced, and fiscal balances have been brought under control.  Many countries have reformed their financial systems and modernized their business regulations.  These policies have helped attract private capital inflows, even though total capital flows still move from developing to industrial economies on net.\n\nFinancing mechanisms in the emerging-market economies have also evolved.  Most notably, external borrowing increasingly is in the form of bonds denominated in  domestic currency, often issued at fixed interest rates, and dated for long maturities, in contrast to the foreign-currency instruments that dominated external borrowing in earlier years.  Government bonds of this new type were first issued by Korea and Thailand in the 1990s; Brazil, Chile, Colombia, Indonesia, Mexico, and Russia soon followed suit (Kroszner, 2006a).\n\nThese new instruments have helped establish long-dated benchmark yield curves and thus encourage corporate bond issuance and mortgage lending in domestic currency and at longer maturities.  The new instruments are also changing the distribution of risk.  When entities in the emerging markets borrowed in foreign currencies, they bore the exchange rate risk while lenders bore default risk.  This arrangement made the financial crises of the 1990s very costly:  Sharp currency depreciations caused the domestic-currency value of foreign-currency debt to balloon.\n\nWith domestic-currency financing, lenders now bear most of the exchange rate risk.  And with fixed-rate bonds, the interest rate risk, too, is being shifted to the lenders.  We would expect that emerging-market borrowers would have to pay higher yields to compensate lenders for this additional risk.  But yields on the new instruments have generally been moving down.  For example, Mexico, even with its history of macroeconomic instability, can borrow in pesos at a thirty-year maturity at roughly 8 percent.\n\nThese low yields are part of a more general decline in compensation for risk in emerging markets, a trend also evidenced by low risk spreads on dollar-denominated bonds and rising stock prices.  In part, this trend reflects the low volatility in international financial markets in recent years.  However, it also reflects improvements in the economic policies and debt positions of emerging-market economies.  In the past, an environment of low risk spreads contributed to overborrowing, booms, and then busts.  So far, that cycle has not developed, and it is crucial that sound and prudent policies continue to guard against it's doing so.  Credible macroeconomic and financial policies in the emerging-market economies provide double benefits:  They help keep borrowing costs low, and, in the event of a future retreat from risk by global investors, they will help the affected economies weather any ensuing financial turbulence.\n\nConclusion\nIn conclusion, I believe that the current net flow of capital toward the industrial world is not in the long-term interest of the developing economies.  To raise incomes and reduce poverty, the developing economies must boost their productivity, and that, in turn, will require complementing their large and growing labor forces with increasing quantities of capital.  I would add that the current pattern of net capital flows is more the deviation than the norm.  The developing economies in the aggregate swung into current account surplus only in the past decade.  Moreover, at present, only a subset of developing economies account for those surpluses, and even those are enjoying substantial gross capital inflows.\n\nOf course, there are better ways and worse ways to achieve a reversal in net capital flows.  In the past, current account deficits in the emerging-market economies were frequently the outcome of large budget deficits and inappropriate exchange rate regimes, and I am certainly not calling for a return to such policies.  Rather, a swing in net capital flows back toward the developing economies would best be achieved through policies they need--regardless of their effect on external balances--to promote capital formation and economic growth.  Such policies would improve the environment for business investment, strengthen domestic financial systems, and encourage the development of more-attractive financial instruments.  In particular, an increase in transparency and disclosure is important so that all investors believe they will be treated equally and have an equal chance to enjoy the returns from their investments.  These steps, against the background of prudent monetary and fiscal policies, will help both to encourage capital inflows and to ensure that they are used to best advantage.  By the same token, industrial countries can undertake policies to enhance their prospects for solid, sustainable growth that would also have the effect of altering the current pattern of international capital flows.  In the United States, it is crucial to address the implications of demographic changes for the longer-term path of entitlement spending.  Other industrial economies also face this challenge.\n\nTo a certain extent, financial and economic reforms are already under way in many emerging-market economies, and private capital flows to these countries have expanded sharply over the past decade.  Moreover, financial flows increasingly are in the form of fixed-rate domestic-currency bonds, which makes emerging-market borrowers less vulnerable to precipitous movements in exchange rates and other asset prices.  Financial crises may materialize and disrupt economic activity, as they have in the past.  However, provided that emerging-market economies continue to pursue structural reforms and stabilizing macroeconomic policies, their prospects for solid economic growth and resilience in the face of crisis will improve.\n\nReferences\n\nAhearne, Alan G., William L. Griever, and Francis E. Warnock (2004). \"Information Costs and Home Bias: An Analysis of U.S. Holdings of Foreign Equities,\" Journal of International Economics, vol. 62 (March), pp. 313-36.\n\nArteta, Carlos, Barry Eichengreen, and Charles Wyplosz (2003).  \"When Does Capital Account Liberalization Help More than It Hurts?\" in Elhanan Helpman and Efraim Sadka, eds., Economic Policy in the International Economy:  Essays in Honor of Assaf Razin.  New York:  Cambridge University Press.\n\nBernanke, Ben (2005).  \"The Global Saving Glut and the U.S. Current Account Deficit,\" the Homer Jones Lecture, St. Louis, Missouri, April 14, www.federalreserve.gov/boarddocs/speeches/2005/20050414/default.htm\n\nCaballero, Ricardo J., Emmanuel Farhi, and Pierre-Olivier Gourinchas (2006).  \"An Equilibrium Model of ‘Global Imbalances' and Low Interest Rates,\" NBER Working Paper Series 11996.  Cambridge, Mass.:  National Bureau of Economic Research, http://www.nber.org/papers/w11996\n\nChinn, Menzie D. (2005).  Getting Serious About the Twin Deficits.  Council Special Report 10.  New York:  Council on Foreign Relations.\n\nCline, William R. (2005).  The United States as a Debtor Nation:  Risks and Policy Reform.  Washington:  Institute for International Economics.\n\nErceg, Christopher J., Luca Guerrieri, and Christopher Gust (2006).  \"SIGMA:  A New Open Economy Model for Policy Analysis,\" International Journal of Central Banking, vol. 2 (March).\n\nFerguson, Roger W., Jr. (2005).  \"U.S. Current Account Deficit:  Causes and Consequences,\" speech delivered to the Economics Club of the University of North Carolina at Chapel Hill, April 20, www.federalreserve.gov/boarddocs/speeches/2005/20050420/default.htm\n\nGelos, R. Gaston, and Shang-Jin Wei (2005).  \"Transparency and International Portfolio Holdings,\" Journal of Finance, vol. 60 (December), pp. 2987-3020.\n\nGruber, Joseph W., and Steven B. Kamin (forthcoming).  \"Explaining the Global Pattern of Current Account Imbalances,\" Journal of International Money and Finance.\n\nInternational Monetary Fund (2007a).  World Economic Outlook.  Washington:  IMF, April, http://www.imf.org/external/pubs/ft/weo/2007/01/data/index.aspx\n\n__________ (2007b). Global Financial Stability Report.  Washington:  IMF, April, http://www.imf.org/external/pubs/ft/gfsr/2007/01/pdf/statappx.pdf (794 KB PDF)\n\n__________ (2007c). International Financial Statistics.  Database maintained by the Board of Governors of the Federal Reserve System.  Washington:  IMF.\n\nJu, Jiandong, and Shang-Jin Wei (2006). \"A Solution to Two Paradoxes of International Capital Flows,\" IMF Working Paper WP/06/178.  Washington:  International Monetary Fund.\n\nKroszner, Randall S. (2003). \"Promoting Global Economic Growth: The Productivity Challenge (64 KB PDF),\" working paper. Chicago: University of Chicago, July.\n\n__________ (2006a).  \"Why Are Yield Curves So Flat and Long Rates So Low Globally?\"  speech delivered at the Institute of International Bankers, New York, New York, June 16, www.federalreserve.gov/newsevents/speech/kroszner20060615a.htm\n\n__________ (2006b).  \"What Drives Productivity Growth? Implications for the Economy and Prospects for the Future,\" speech delivered to the Forecasters Club of New York, September 27, www.federalreserve.gov/newsevents/speech/kroszner20060927a.htm\n\nMendoza, Enrique G., Vincenzo Quadrini, and Jose-Victor Rios-Rull (2007).  \"Financial Integration, Financial Deepness and Global Imbalances,\" NBER Working Paper 12909.  Cambridge, Mass.:  National Bureau of Economic Research, http://www.nber.org/papers/w12909\n\nMishkin, Frederic S. (2006).  The Next Great Globalization:  How Disadvantaged Nations Can Harness Their Financial Systems to Get Rich.  Princeton:  Princeton University Press.\n\nPrasad, Eswar, Raghuram Rajan, and Arvind Subramanian (2006).  \"Foreign Capital and Economic Growth,\" paper presented at an IMF workshop, November, https://www.imf.org/External/NP/seminars/eng/2006/growth/as.pdf (564 KB PDF)\n\nFootnotes\n\n1.  Steve Kamin, Charles P. Thomas, and Carlos Arteta, of the Board's Division of International Finance, contributed to this speech. Return to text\n\n2.  I will use the terms \"developing economies\" and \"emerging-market economies\" interchangeably in this speech, although some observers draw distinctions between them.   Return to text\n\n3.  In this section, the developing economies consist of those listed under \"Other Emerging Market\" and under \"Developing\" in the IMF's World Economic Outlook plus Hong Kong SAR, Israel, Korea, Singapore, and Taiwan.  The data for these countries are drawn from the database published with IMF (2007a). Return to text\n\n4.  Elaborations on the role of fiscal policy in the U.S. current account deficit are in Cline (2005) and Chinn (2005). Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20070515a.htm",
        "title": "International Capital Flows and the Emerging-Market Economies",
        "date": "5/15/2007"
    },
    {
        "content": "May 10, 2007\n\nGovernor Randall S. Kroszner\n\nAt the 2007 Payments Conference Competitive Forces Shaping the Payments Environment: What's Next?, Federal Reserve Bank of Chicago, Chicago, Illinois\n\nI am pleased to have the opportunity to speak at this conference on competitive forces shaping the payments environment.1  Spurred by a period of rapid innovation, we are at a crucial juncture in the evolution of the U.S. payments system.  The long-predicted decline of the paper check is materializing as consumers and businesses continue to embrace alternative ways of making and clearing payments.  A 2004 Federal Reserve study of retail payments revealed that electronic payments have now surpassed the use of checks as the preferred means of making noncash payments.2  The use of debit cards has grown strongly, and the number of checks being converted into ACH payments has grown to nearly 3 billion annually.  Electronic check clearing has begun to grow exponentially in response to changes in check law.\n\nAs a consequence of the accelerating pace of change, we participants in the payments system cannot help but challenge our fundamental assumptions about payment operations, which were based on long-standing processes for clearing paper.  And adjustments are being made.  The Federal Reserve Banks, for example, have cut the number of offices at which they process checks in half since 2003, and more consolidations are planned for later this year.  Check transportation networks, particularly air transportation networks, are also beginning to shrink.\n\nOur conference organizers are to be applauded for bringing together industry participants, regulators, and academics at this crucial juncture to consider \"what’s next\" for the payments system.  After briefly describing the evolution of the payments system, I will discuss how recent regulatory changes and technological innovation have interacted to affect the payments system--that is, how regulatory changes have affected the use of technology in the payments system and how technological innovations have affected how the payments system is regulated.  I will then discuss some key characteristics of successful payments innovations and highlight some challenges and tradeoffs, such as the tradeoff between convenience and security, that need to be considered as the payments system continues to evolve.  Finally, I will discuss the future of electronic check clearing and the important and complementary roles the Federal Reserve and the private sector will need to play in the continued evolution of the payments system.\n\nEvolution in the Structure of the Payments System\nThe different forces now shaping the evolution of the payments system have been gathering for decades.  From a legal and regulatory perspective, a series of fundamental changes in U.S. banking laws, especially the laws governing interstate banking, have facilitated a long-term consolidation of banking across geographic boundaries in this country.3  Banking consolidation, in turn, has fostered the ongoing consolidation of payments infrastructure both within banks and among the clearinghouses, networks, and other financial utilities that serve the banking industry.  From a technological perspective, decades of investment by the financial industry, businesses, and more recently households have broadened and deepened the infrastructure for supplying and accessing electronic payments.  Payment cards and other access devices have proliferated, and the demand for electronic payments has grown apace.  As a result, the use of electronic payments has doubled over the first half of this decade.\n\nFrom a historical perspective, it is clear that the Federal Reserve has an important role to play in the evolution of the payments system.  Indeed, the Federal Reserve was founded, in part, to address significant problems in the payments system, such as nonpar check collection and financial stability issues that existed in the nineteenth and early twentieth centuries.\n\nRegulation and Technology\nTurning now to regulation and technology, technological change can create the need for regulatory change.  And regulatory change can stimulate new applications of technology that foster greater efficiency and growth.  At times, there is a complex interplay between changing technology and regulation.  In the payments arena, regulators and rulemakers need to be aware of how technology is changing the industry and, when appropriate, remove artificial barriers to innovation.\n\nLet me highlight several important examples of recent adjustments to rules and regulations that have changed the way payments are initiated and processed.  Beginning in the late 1990s, the National Automated Clearing House Association (NACHA) changed its rules to permit the so-called conversion of checks to electronic fund transfers via the ACH system; subsequently, the Board modified its staff commentary to clarify the application of Regulation E to these transactions.4  Today, businesses can use the information on eligible checks they receive over the counter and at lockbox locations to create ACH debit transactions.  Partly as a result of the private sector’s expanding use of check conversion, the total volume of checks collected by Federal Reserve Banks has in recent years fallen faster than the estimated decline in the number of checks being written.\n\nIn 2003, Congress passed the Check Clearing for the 21st Century Act (Check 21).5  Broadly speaking, Check 21 was designed to foster market-based improvements in the check collection and return system, mainly by authorizing--but not requiring--banks to truncate or stop the flow of original paper checks and to create legally equivalent substitute checks for presentment to the paying bank.6\n\nIn 2004, the Board published regulatory changes to implement Check 21.  The legal and regulatory changes associated with Check 21 are encouraging the banking industry to use new technologies to create and process digital images of checks and to print substitute checks only when necessary to meet legal presentment or customer requirements.  In a recent report to Congress on Check 21, the Board noted that the adoption of new clearing techniques was slow at first but has accelerated significantly in the past year.7  For example, between March 2006 and January 2007, the banking industry’s use of substitute checks increased three-fold and its use of electronic check images increased five-fold.  This pattern of adoption is consistent with early conjecture from the banking industry that the scale and scope of required changes to individual bank technology systems and operations would take time to implement.  In addition, Check 21 has started to alleviate the check system’s dependence on large-scale air transportation to move checks around the country, a vulnerability that was highlighted when planes were grounded following the events of September 11.\n\nTo address a barrier to the use of electronic payments, the Federal Reserve has recently proposed modifications to Regulation E that would eliminate the requirement for paper receipts on small-dollar payments.8  The Federal Reserve must, however, also be mindful of how changing payment practices can affect risk in the payments system and take regulatory action, when appropriate.  For example, the Federal Reserve recently modified Regulation CC to reallocate the liability for unauthorized remotely created checks among depository institutions, shifting liability to institutions that are better positioned to understand and mitigate those risks.9\n\nClearly, both regulators and the private sector have a role to play in the continuing evolution of the payments system.  Regulators must continue to evaluate and adjust their rules in light of new realities in the marketplace.  At the same time, the private sector must put forward sound and prudent innovations that address the needs of the marketplace, including the need to control risk.\n\nCharacteristics of Successful Innovation\nOur conference organizers have challenged us to think about \"what’s next.\"  One tool to help understand the future is to analyze why certain innovations have been successful in the past.  Infrastructure changes have been one source of successful innovation.  Much of the innovation in the early days of electronic payments involved building new electronic processing systems for ACH and card systems, and then gradually expanding electronic access to these core systems.  The goal of these infrastructure changes was typically to speed up payments and reduce the societal costs of dependence on paper.\n\nFollowing the implementation of Check 21, the banking industry and the Reserve Banks are again making major infrastructure changes.  The expansion of access to this infrastructure and the development of tools for using it may well follow the path of earlier innovation and foster additional change over the next few years.  Experience in the late 1990s, however, gives us reason for caution.  At that time, there were attempts to introduce whole new payments systems for card and Internet payments.  A few of these attempts succeeded, but most failed.  Thus, the introduction of new technology alone is not sufficient for successful innovation; innovations must also meet important market demands.\n\nA key characteristic of successful innovation is that it generally reduces cost or increases the convenience of making one or more types of payments.  Examples include the introduction of ATM transactions many years ago, as well as new methods for making payments over the Internet.  Both of these examples highlight the importance payment system end-users place on the convenience of making their payments, specifically the time and location of their transactions.  In the case of ATMs, banking customers shifted their payment activity from inside a bank branch to a more convenient time and location. ATMs also provided banks with a lower-cost way to serve their customers.  In the case of Internet and telephone payments, the use of credit cards, new person-to-person systems, and new ACH payment types allowed payments to be made more rapidly at any time of the day, even when the parties to the transaction were thousands of miles apart.\n\nThe information linked to a payment has also emerged as an important component of end-users’ demand for payment innovations.  This point challenges us to think of payments as more than simple instructions to debit and credit bank accounts when we attempt to predict the needs of consumers and businesses for innovative payment products.\n\nIn business-to-business remittance payments, for example, firms need to link traditional payment information to information about the bills being paid in order to post and reconcile their accounts.  Over the years, businesses have made significant investments to automate their internal accounting processes for check payments.  As a result, checks are still very heavily used for corporate remittances.  After years of industry discussion, it is now clear that linking billing information to electronic payment instructions is a key to the greater automation of business payments.10  Payments system operators, banks, and businesses must continue to cooperate on standards and encourage the necessary software changes to enable the broad-based automation of business payments.  This is no simple task, as there are competing standards both domestically and internationally.  Nonetheless, business users of the payments system continue to call for change, and we all need to press forward on this issue.\n\nChallenges to Innovation\nAlthough improving convenience has been an important force behind successful innovation, consumers and businesses have increasingly demanded greater security in their electronic payments, particularly as more information is linked to these payments.  In general, there is often a basic tradeoff between security and convenience:  the easier a system is to use and access, the less secure it tends to be.  Striking the appropriate balance between convenience and security is a key challenge.  How can we manage fraud and operational risks, along with credit and other risks, without unduly interfering with the ease of using a particular system?  As I noted earlier, technological change can help stimulate successful innovations and make it more convenient for businesses and consumers to make and receive payments and link information to those payments.  But technological change can also generate new methods of perpetrating fraud and disrupting payments systems--and thus require us to develop new responses to these challenges and risks.\n\nCheck fraud has long been an important issue for the banking system and has been increasing recently, as described in the Board’s Check 21 report.  In the electronic sphere, card systems have also had to deal with fraud since their inception.  Naturally, cost-benefit calculations guide the pace of innovation to address fraud.  These costs typically include technology and labor costs, as well as the time and other requirements needed to authenticate and authorize transactions.  Benefits typically include the monetary losses, customer problems, and legal costs that are avoided.  Important but more difficult-to-quantify benefits include the avoidance of reputational damage to payments system participants, as well as sustained public confidence in the payments system.  In the context of this cost-benefit framework, we can anticipate that when the costs of prevention fall or fraud begins to increase, banks, businesses, and consumers, will react by implementing or demanding changes in policies, business processes, and technology to address risk.  This dynamic balancing process is part of the ongoing development of the payments system.\n\nIn the past, the banking industry has taken steps to address its own security concerns, as well as those of its customers.  The experience with card systems and the ACH are instructive.  In the 1970s, major credit card companies began to deploy electronic authorization systems for transactions.  This step was strongly influenced by the desire of banks and merchants to control fraud.  The more recent development of intelligent detection systems represents yet a further step in combating fraud.  Other new technologies are beginning to be deployed, such as cards that have computer chips, to further reduce fraud and to introduce new services to the market.\n\nIn the ACH system, the original risk design relied heavily on trusted, long-term relationships among known counterparties.  Key participants were supervised depository institutions that originated and received the electronic payments.  The primary types of commercial payments were electronic payroll deposits and recurring payments for utility bills.  Fraud concerns did not justify establishing centralized controls at the system level.  The risk model, however, has been changing as parties have begun to use the ACH to make one-time payments over the telephone and Internet, payments that are not necessarily based on long-term, trusted relationships.  As the ACH network has grown and become more complex, layers of nonbank service providers may now be involved in originating certain types of payments, which further attenuates the trust relationships on which the network was founded.\n\nTo address these emerging risks, ACH operators and NACHA have been working cooperatively with the industry to control new occurrences and types of fraud and other closely related risks.  ACH operators are now providing their customers with innovative risk-management services, and NACHA has initiated a risk-monitoring program to better identify problems.  NACHA is also developing programs to better monitor and control the ACH originations of depository institution customers.  While the ultimate success of these efforts cannot yet be assessed, they represent important steps in the right direction.  As the banking industry develops new payments solutions, it must be proactive in balancing convenience and risks, as well as cost, to ensure continued customer confidence in the payments system.\n\nThe Future of Electronic Check Clearing\nPerhaps the greatest current opportunity in the retail payments system is for the banking industry to move rapidly to the electronic clearing of checks.  As envisioned by Congress, Check 21 has created a strategic vision for moving the U.S. check clearing system into the new century.  The historic infrastructure for paper check clearing is consolidating rapidly.  Banks are changing their internal systems so they can accept deposits and collect checks electronically and are increasingly accepting their check presentments electronically.  Bank technology vendors are playing an important role in facilitating change.  And the Reserve Banks are working to implement the vision of Check 21, both by enhancing their electronic check infrastructure and by working closely with other payments system operators and the industry.\n\nIn this environment of changing rules, infrastructure, and, yes, costs, it is imperative for banks and their customers to look closely at their own payment policies, operations, and technologies.  Banks will need to develop broad and thoughtful payment strategies to navigate the changing environment and work constructively within the industry.  Inevitably, banks will also need to change their operations.\n\nThe Role of the Federal Reserve\nThe Federal Reserve will continue to play an important role in fostering a smoothly functioning payments system that is safe, efficient, and accessible.  We also need to be flexible in carrying out our traditional payments system functions--provider of payment services, regulator, and catalyst for change--in the rapidly changing payments landscape.\n\nSince its inception, the Federal Reserve’s broad role in the payments system has been to improve efficiency and foster financial stability.  When the Federal Reserve was formed almost one hundred years ago, the U.S. banking system was highly fragmented and checks were not cleared at par.  To address this situation, the Federal Reserve began providing a national check-clearing service to banks that joined the Federal Reserve System. This service and other Federal Reserve policies helped guide the nation toward a more unified payments system.\n\nIn its role as service provider, the Federal Reserve will continue to promote the efficiency and stability of the nation’s payments system.  The Reserve Banks are now pricing their check services to encourage greater use of electronic check clearing.  The Reserve Banks are also leaders in providing Check 21 services that encourage depository institutions to shift to a greater use of electronics in check processing and are working collaboratively with the industry on electronic check standards and other technical issues.  Most importantly, the Reserve Banks will continue to compete as payment services providers on a fair and equitable basis by pricing their services to recover their costs, including imputed profits and taxes, over the long run, as required by the Monetary Control Act of 1980.\n\nIn its role as a regulator, the Federal Reserve will need to be alert to the application of regulations in changing circumstances.  As I have already noted, the Federal Reserve has, in recent years, modified its regulations to facilitate the greater use of electronics in the payments system and to address emerging risks.  When considering potential regulatory changes, the Federal Reserve must also ensure that any proposed changes are consistent with the changing technological environment and adequately protect consumers.\n\nFinally, in its role as a catalyst for change, the Federal Reserve continues to work with the private sector to identify and, when appropriate, address barriers to payments system innovation.  This past March, I participated in a roundtable in Minneapolis on retail payments fraud that was sponsored by the Payments System Policy Advisory Committee.  The roundtable promoted dialogue on key fraud issues with a wide range of participants in the retail payments systems.  It is our objective to continue to sponsor different types of forums over time as an important part of our public outreach activities.  We believe that through its payments system roles, the Federal Reserve is well positioned to encourage both future payments system evolution and the appropriate balancing of efficiency and risk as new products and services are developed.\n\nConclusion\nWe are living in a period of rapid innovation and transition in the payments system.  Powerful forces have been converging to reshape the retail payments system.  Yet the United States continues to enjoy safe, efficient, and reliable systems for making payments.  As innovations occur, market forces will ultimately sort out which of these will best serve the needs of consumers and businesses.  Both the private and public sectors have contributed to the evolution of the national payment system of the United States; clearly, both will have an important role to play in the future.  As we ask each other \"what’s next\" during this time of transition, I believe that dialogue among payments system participants and users is critically important.  Information and different points of view will help us all identify and address issues of innovation, risk, and efficiency in a balanced and thoughtful manner.  This conference is a welcome and constructive element in this important dialogue.\n\nFootnotes\n\n1. Helena L. Tenenholtz, Jeffrey S. H. Yeganeh, Jack K.Walton II, and Jeffrey C. Marquardt of the Board’s Division of Reserve Bank Operations and Payments Systems contributed to this speech.  Return to text\n\n2. See Gerdes, Geoffrey R., Jack K. Walton II, May X. Liu, and Darrel W. Parke, \"Trends in the Use of Payment Instruments in the United States,\" Federal Reserve Bulletin, Spring 2005, pp. 180-201. (http://www.federalreserve.gov/pubs/bulletin/2005/spring05_payment.pdf) Return to text\n\n3. See, for example, Kroszner, Randall S., \"The Effect of Removing Geographic Restrictions on Banking in the United States: Lessons for Europe.\"  Conference on the Future of Financial Regulation, London School of Economics, April 6, 2006.  See also Kroszner, Randall S., \"What Drives Deregulation?  Economics and Politics of the Relaxation of Bank Branching Restrictions,\" Quarterly Journal of Economics, November 1999, pp. 1437-67, with Philip Strahan. Return to text\n\n4. The term \"conversion\" generally refers to \"check-to-ACH conversion,\" in which data from a check is used to create an electronic funds transfer that is cleared and settled over the ACH network.  Such transactions include accounts receivable (ARC), point-of-purchase (POP), and back-office conversion (BOC) transactions.   Return to text\n\n5. Check 21 became effective in October 2004. Return to text\n\n6. Before Check 21, banks were required to present the original paper check to the paying bank unless the paying bank had agreed to accept presentment of the check electronically.   Return to text\n\n7.  See Board of Governors of the Federal Reserve System, Report to the Congress on the Check Clearing for the 21st Century Act of 2003, April 2007. http://www.federalreserve.gov/boarddocs/rptcongress/check21/check21.pdf Return to text\n\n8.  See 71 FR 69500 (72 KB PDF) (December 1, 2006). Return to text\n\n9.  See 70 FR 71218 (84 KB PDF) (November 28, 2005). Return to text\n\n10. The Clearing House and FRB Financial Services, \"Business-to-Business Wire Transfer Payments: Customer Preferences and Opportunities for Financial Institutions,\"  (October 2006). http://www.frbservices.org/Wholesale/pdf/wire_transfer_research.pdf\n\n\"A Summary of the Roundtable Discussion on the Role of Wire Transfers in Making Low-value Payments,\" Federal Reserve Bank of New York (May 2006). http://www.federalreserve.gov/paymentsystems/lowvaluepay/default.htm  Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20070510a.htm",
        "title": "The Future of Payments: Challenges and Opportunities",
        "date": "5/10/2007"
    },
    {
        "content": "May 01, 2007\n\nChairman Ben S. Bernanke\n\nAt the Montana Economic Development Summit 2007Butte, Montana\n\nTrade is as old as humanity, or nearly so. Archaeological sites demonstrate that ancient peoples traded objects such as rare stones and shells across fairly long distances even in prehistoric times (Guisepi, 2000). Over the centuries, with stops and starts, the volume of trade has expanded exponentially, driven in large part by advances in transportation and communication technologies. Steamships replaced sailing ships; railroads succeeded canal barges; the telegraph supplanted the Pony Express. Today, in a world of container ships, jumbo jets, and the Internet, goods and many services are delivered faster and more cheaply (in inflation-adjusted terms) than ever before.1\n\nToday I will discuss the crucial economic benefits we receive from the ongoing expansion of international trade. I will also address the adverse effects of trade and some possible ways to mitigate them. I will argue that one possible response to the dislocations that may result from trade--a retreat into protectionism and isolationism--would be self-defeating and, in the long run, probably not even feasible. Instead, our continued prosperity depends on our embracing the many opportunities provided by trade, even as we provide a helping hand to individuals and communities that may have suffered adverse consequences.\n\nThe Benefits of Trade\nAt the most basic level, trade is beneficial because it allows people to specialize in the goods and services they produce best and most efficiently. For example, we could conceivably all grow our own food and provide our own medical care. But because farming and medicine require special knowledge and skills, a far more efficient arrangement is for the farmer to specialize in growing food and for the doctor to specialize in treating patients. Through the specialization made possible by trade, the farmer can benefit from the doctor's medical knowledge and the doctor can enjoy lunch. The opportunity to trade allows everyone to play to his or her own strengths while benefiting from the productive skills of the whole community. Indeed, economists have demonstrated that trade between two people can be beneficial even if one of them is more skilled than the other at every task, so long as the more-skilled person specializes in those tasks at which he or she is relatively more productive.\n\nWhat applies to individuals applies to nations as well. Two centuries ago the economist David Ricardo famously observed that, if England specialized in making cloth while Portugal specialized in producing wine, international trade would allow both countries to enjoy more of both goods than would be possible if each country produced only for domestic consumption and did not trade. As in the case of individuals, this conclusion applies even if one country can produce both cloth and wine more cheaply than the other, so long as each country specializes in the activity at which it is relatively more productive. A telling confirmation of Ricardo's insight is that, when nations go to war, their first order of business is often to try to block the other's access to trade. In the American Civil War, the North won in large part because its blockade of Southern ports prevented the Confederacy from exporting its cotton. In the twentieth century, the fact that Great Britain and its allies were able to disrupt German trade more successfully than Germany could impede the flow of goods into and out of Great Britain bore importantly on the ultimate outcomes of both world wars.\n\nPatterns of trade are determined by variations in a number of factors, including climate, the location of natural resources, and the skills and knowledge of the population. I suppose that one could grow roses commercially here in Montana for Valentine's Day, but it would likely require climate-controlled greenhouses complete with artificial lighting--very expensive. A much less costly solution is for Montanans to grow and sell wheat, then use the proceeds to buy roses from localities where the weather is balmy in February.\n\nThis is all standard textbook material, and it may well leave you unconvinced of the importance of international trade. After all, the United States is a big country, and we can certainly achieve many of the benefits of specialization by trading within our own borders. How important is it for the health of our economy to trade actively with other countries? As best we can measure, it is critically important. According to one recent study that used four approaches to measuring the gains from trade, the increase in trade since World War II has boosted U.S. annual incomes on the order of $10,000 per household (Bradford, Grieco, and Hufbauer, 2006).2 The same study found that removing all remaining barriers to trade would raise U.S. incomes anywhere from $4,000 to $12,000 per household. Other research has found similar results. Our willingness to trade freely with the world is indeed an essential source of our prosperity--and I think it is safe to say that the importance of trade for us will continue to grow.\n\nIn practice, the benefits of trade flow from a number of sources. By giving domestic firms access to new markets, trade promotes efficient specialization, permits economies of scale, and increases the potential returns to innovation.3 U.S. firms increasingly seek to expand production and profits through new export opportunities; indeed, U.S. exports grew about 9 percent in real (that is, inflation-adjusted) terms last year. Export-oriented U.S. manufacturing industries include producers of aircraft, construction equipment, plastics, and chemicals. The United States also excels in the manufacture and export of sophisticated capital goods and scientific equipment. Outside of manufacturing, a number of U.S. high-tech companies, including software developers and online service providers, are world leaders in their fields. American films and music attract large worldwide audiences. Montana's exports include wheat, metal ores, and high-tech materials that are critical to the production of semiconductors.\n\nFirms that emphasize exports are among America's most dynamic and productive companies. Relative to firms that produce strictly for the domestic market, exporters tend to be more technologically sophisticated and to create better jobs. Among U.S. manufacturers, for example, exporters pay higher wages and add jobs more rapidly than non-exporters (Bernard and Jensen, 1999). A significant portion of U.S. international trade is conducted by multinational firms; studies show that these firms generally pay higher wages than purely domestic firms, both in the United States and in developing countries (Doms and Jensen, 1998; Bhagwati, 2004, p. 172). U.S. firms with a global reach tend to be better diversified and are better able to respond to new market opportunities wherever they may arise.\n\nExports are important, but so are imports. Without trade, some goods would be extremely expensive or not available at all, such as the Valentine's Day roses of my earlier example or out-of-season fruits and vegetables. Trade also makes goods available in more brands and varieties; examples include automobiles, consumer electronics, garments and footwear, wines, and cheeses. One of the great attractions of globalization is that it brings to consumers the best of many cultures. And of course, global trade allows many types of goods, especially consumer goods, to be purchased at lower prices. Lower prices help all consumers but may be especially helpful to those with tight budgets. Indeed, a number of the large, import-intensive retail chains in the United States are focused on low- and moderate-income consumers, who benefit from being able to buy a wide variety of lower-priced goods.\n\nAnother substantial benefit of trade is the effect it tends to have on the productivity of domestic firms and on the quality of their output.4 By creating a global market, trade enhances competition, which weeds out the most inefficient firms and induces others to improve their products and to produce more efficiently. The U.S. manufacturing sector, which is perhaps the sector most exposed to international competition, has achieved truly remarkable increases in its productivity in the past decade or so. In addition, international supply chains, made possible by advances in communication and transportation, reduce costs and increase the competitiveness of U.S. firms. Trade also promotes the transfer of technologies, as when multinational firms or transplanted firms bring advanced production methods to new markets.\n\nTrade and finance are closely linked and mutually supporting, and in recent decades international financial flows have grown even more quickly than trade volumes. The globalization of finance plays to the strengths of U.S. financial institutions and financial markets. The United States has a large surplus in trade in financial services, and U.S. firms are leaders in providing banking, investment, and insurance services to the world. Financial openness allows U.S. investors to find new opportunities abroad and makes it possible for foreigners to invest in the United States. The ability to invest globally also permits greater diversification and sharing of risk.\n\nTrade benefits advanced countries like the United States, but open trade is, if anything, even more important for developing nations. Trade and globalization are lifting hundreds of millions of people out of poverty, especially in Asia, but also in parts of Africa and Latin America (Bhagwati, 2004). As a source of economic growth and development in poor countries, trade is proving far more effective than traditional development aid (Easterly, 2006). The transition economies of central and eastern Europe have also benefited greatly from trade, especially trade with the rest of the European Union. A recent study by the World Bank compared two groups of developing countries, dubbed the \"globalizers\" and the \"nonglobalizers.\" Collectively, the globalizers have doubled the ratio of trade to their gross domestic product (GDP) over the past twenty years, in part because of sharp cuts in tariffs on imports; the nonglobalizers, collectively, have seen a decline in their trade-to-GDP ratio over the same period (Dollar and Kraay, 2004). Among the globalizers, economic growth accelerated from 2.9 percent per year in the 1970s, to 3.5 percent in the 1980s, to 5 percent in the 1990s. In contrast, the nonglobalizers have seen their growth decline from 3.3 percent per year in the 1970s to 0.8 percent in the 1980s and 1.4 percent in the 1990s. The study also found that, among the globalizers, absolute poverty declined significantly and the degree of income inequality changed little.5\n\nIf trade is so beneficial, why do we sometimes see political resistance to freer, more open trade? Notably, negotiations in the so-called Doha Round of trade talks now under way have proceeded very slowly, notwithstanding a consensus among economists that all countries involved would enjoy substantial benefits from further trade liberalization. One important reason is that, although trade increases overall prosperity, the benefits for some people may not exceed the costs, at least not in the short run. Clearly, the expansion of trade helps exporting firms and their workers. As consumers, nearly all of us benefit from trade by gaining access to a broader range of goods and services. But some of us, such as workers in industries facing new competition from imports, are made at least temporarily worse off when trade expands. Because the benefits of trade are widely diffused and often indirect, those who lose from trade are often easier to identify than those who gain, a visibility that may influence public perceptions and the political process. That said, the job losses and worker displacement sometimes associated with expanded trade are a legitimate economic and social issue. In the remainder of my remarks, I will focus on the impact of trade on U.S. jobs--both positive and negative--and discuss some possible policy responses.\n\nTrade and Jobs\nDoes opening U.S. markets to foreign producers destroy jobs at home? The expansion of trade or changes in trading patterns can indeed destroy specific jobs. For example, foreign competition has been an important factor behind declining employment in the U.S. textile industry, including in my home state of South Carolina. Job loss--from any cause--can create hardship for individuals, their families, and their communities. I will return shortly to the question of how we should respond to the problem of worker displacement.\n\nFor now, however, I will point out that trade also creates jobs--for example, by expanding the potential market overseas for goods and services produced in the United States, as I have already discussed. Trade creates jobs indirectly as well, in support of export activities or as the result of increased economic activity associated with trade. For example, gains in disposable income created by lower consumer prices and higher earnings in export industries raise the demand for domestically produced goods and services. Domestic production and employment are also supported by expanded access to raw materials and intermediate goods. The U.S. jobs created by trade also tend to offer higher pay and demand greater skill than the jobs that are destroyed--although a downside is that, in the short run, the greater return to skills created by trade may tend to increase the wage differential between higher-skilled and lower-skilled workers and thus contribute to income inequality (Bernanke, 2007).\n\nThe effects of trade on employment must also be put in the context of the remarkable dynamism of the U.S. labor market. The amount of \"churn\" in the labor market--the number of jobs created and destroyed--is enormous and reflects the continuous entry, exit, and resizing of firms in our ever-changing economy. Excluding job layoffs and losses reversed within the year, over the past decade an average of nearly 16 million private-sector jobs have been eliminated each year in the United States, an annual loss equal to nearly 15 percent of the current level of nonfarm private employment.6 The vast majority of these job losses occur for a principal reason other than international trade (Kletzer, 2001; Bernanke, 2004). Moreover, during the past ten years, the 16 million annual job losses have been more than offset by the creation of about 17 million jobs per year--some of which, of course, are attributable to the direct and indirect effects of trade. Truly, the U.S. labor market exhibits a phenomenal capacity for creative destruction.\n\nIf trade both destroys and creates jobs, what is its overall effect on employment? The answer is, essentially none. In the long run, the workings of a competitive labor market ensure that the number of jobs created will be commensurate with the size of the labor force and with the mix of skills that workers bring. Thus, in the long run, factors such as population growth, labor force participation rates, education and training, and labor market institutions determine the level and composition of aggregate employment. To see the irrelevance of trade to total employment, we need only observe that, between 1965 and 2006, the share of imports in the U.S. economy nearly quadrupled, from 4.4 percent of GDP to 16.8 percent. Yet, reflecting growth in the labor force, employment more than doubled during that time, and the unemployment rate was at about 4-1/2 percent at both the beginning and end of the period. Furthermore, average real compensation per hour in the United States has nearly doubled since 1965.\n\nAlthough many readily accept that balanced trade does not reduce aggregate employment, some might argue that the United States' current large trade deficit must mean that the number of U.S. jobs has been reduced on net. However, the existence of a trade deficit or surplus, by itself, does not have any evident effect on the level of employment. For example, across countries, trade deficits and unemployment rates show little correlation. Among our six Group of Seven partners (the world's leading industrial countries), three have trade surpluses (Canada, Germany, and Japan). However, based on the figures for February of this year, the unemployment rates in Canada (5.3 percent) and in Germany (9.0 percent) are significantly higher than the 4.5 percent rate in the United States; and Japan's unemployment rate, at 4.0 percent, is only a bit lower.7 Factors such as the degree of flexibility in the labor market, not trade, are the primary source of these cross-country variations in unemployment.\n\nWhat About Outsourcing Abroad?\nThe debate about the effects of trade on employment has been intensified by the phenomenon of outsourcing abroad, or \"offshoring.\" Offshoring has been driven by several factors, including improvements in international communication, the computerization and digitization of some business services, and the existence of educated, often English-speaking workers abroad who will perform the same services for less pay. A portion, though not all, of these wage differentials reflects differences in skills and productivity; for example, outsourced programming work is usually simpler and more routine than programming done in the United States.\n\nThe increase in outsourcing abroad has led to dire predictions about a wholesale \"export\" of U.S. jobs in coming years. Although globalization and trade will continue to be forces for economic change, concerns about a massive loss of jobs due to offshoring do not seem justified. Companies have found outsourcing abroad profitable primarily for jobs that can be routinized and sharply defined. Certainly, advancing technology will continue to increase the feasibility of providing services from remote locations. For the foreseeable future, however, most high-value work will require creative interaction among employees, interaction which is facilitated by physical proximity and personal contact. Moreover, in many fields, closeness to customers and knowledge of local conditions are also of great importance. These observations suggest that, for some considerable time, outsourcing abroad will be uneconomical for many types of jobs, particularly high-value jobs.8\n\nMoreover, a balanced discussion of outsourcing abroad should reflect that, just as U.S. firms use the services of foreigners, foreign firms make considerable use of the services of U.S. residents. Many do not realize that, in contrast to its trade deficit in goods, the United States runs a significant trade surplus in services--particularly in business, professional, and technical services. This country provides many high-value services to users abroad, including financial, legal, engineering, architectural, and software development services, whereas many of the services imported by U.S. companies are less sophisticated and hence of lower value.9 A recent study of twenty-one occupations that are most likely to be affected by outsourcing found that net job losses were concentrated almost exclusively in the lower-wage occupations and that strong employment gains have occurred in the occupations that pay the highest wages.10 Further expansion of trade in services will help, not hurt, the U.S. economy and the labor market.\n\nJust as discussions of the outsourcing of business services tend to ignore the services U.S. firms sell to other countries, so do discussions of the movement of jobs offshore ignore the fact that foreign firms also move jobs to the United States. Between 1996 and 2004 (the most recent data available), the employment of U.S. residents by majority-owned nonbank affiliates of foreign companies operating within the United States increased by about 1 million jobs. In 2004, U.S. affiliates of foreign companies accounted for more than $500 billion in value added (about half in manufacturing) and about $180 billion in exports. Globalization and offshoring work both ways.\n\nResponding to Job Displacement\nAlthough trade has many positive effects in the labor market, nothing I have said this morning is intended to minimize the real costs imposed on workers and communities when new competition from abroad leads to job losses and displacement. What can be done to help workers who lose their jobs as a consequence of expanded trade?\n\nRestricting trade by imposing tariffs, quotas, or other barriers is exactly the wrong thing to do. Such solutions might temporarily slow job loss in affected industries, but the benefits would be outweighed, typically many times over, by the costs, which would include higher prices for consumers and increased costs (and thus reduced competitiveness) for U.S. firms. Indeed, studies of the effects of protectionist policies almost invariably find that the costs to the rest of society far exceed the benefits to the protected industry. In the long run, economic isolationism and retreat from international competition would inexorably lead to lower productivity for U.S. firms and lower living standards for U.S. consumers (Bernanke, 2004).\n\nThe better approach to mitigating the disruptive effects of trade is to adopt policies and programs aimed at easing the transition of displaced workers into new jobs and increasing the adaptability and skills of the labor force more generally. Many suggestions for such policies have been made. Currently, the government's principal program for helping workers displaced by trade is the Trade Adjustment Assistance program, which is up for renewal before the Congress this year. As now structured, the program offers up to two and a half years of job training, allowances for job search and relocation, income support for eligible workers, and health insurance assistance for some. Elements of other proposals being discussed (Kletzer and Rosen, 2006; Kling, 2006; Mann 2003, 2004) include job-training tax credits and wage insurance, which would help offset pay cuts that often occur when displaced workers change jobs. Another approach is to focus on establishing policies that reduce the cost to workers of changing jobs, for example, by increasing the portability of pensions or health insurance between employers. As new technologies expand the range of occupations that may be subject to international competition, measures to assist affected workers become all the more important. It would not be appropriate for me to endorse specific programs; that is the prerogative of the Congress. However, I can safely predict that these and other policy proposals to address concerns about worker displacement will be the subject of active debate in coming years.\n\nMore generally, investing in education and training would help young people entering the labor force as well as those already in mid-career to better manage the ever-changing demands of the workforce (Bernanke, 2007). A substantial body of research demonstrates that investments in education and training pay high rates of return to individuals and to society as a whole (Acemogulu and Angrist, 2001; Becker, 1964; Card, 1999; Topel, 2004). Importantly, workforce skills can be improved not only through K‑12 education, college, and graduate work but also through a variety of expeditious, market-based channels such as on-the-job training, coursework at community colleges and vocational schools, extension courses, and online training. An eclectic, market-responsive approach to increasing workforce skills is the most likely to be successful.\n\nWhatever the specific approaches chosen, helping workers who have lost jobs--whether because of trade or other causes--to find new productive work is good for the economy as well as for the affected workers and their families. Moreover, if workers and their families are less fearful of change, political pressure in favor of trade barriers or other measures that would reduce the flexibility and dynamism of the U.S. economy would be reduced (Kull, 2004).\n\nConclusion\nTo sum up, international trade in goods, services, and assets, like other forms of market-based exchange, allows us to transform what we have into what we need or want under increasingly beneficial terms. Trade allows us to enjoy both a more productive economy and higher living standards.\n\nOf course, current trading arrangements are far from perfect. Some features of the world trading regime, such as excessive restrictions on trade in services and the uneven protection of intellectual property rights, are both unfair and economically counterproductive. Working through the World Trade Organization or in other venues, we should continue to advocate the elimination of trade distortions and barriers in our trading partners even as we increase the openness of our own economy. We should also work to ensure that both we and our trading partners live up to existing agreements under the World Trade Organization. When trading partners do not meet their obligations, we should vigorously press our case. Ultimately, a freer and more open trading system is in everyone's best interest.\n\nAlthough expansion of trade makes the U.S. economy stronger, as I have noted today, the broad benefits of trade and the associated economic change may come at a cost to some individuals, firms, and communities. We need to continue to find ways to minimize the pain of dislocation without standing in the way of economic growth and change. Indeed, the willingness to embrace difficult challenges is a defining characteristic of the American people. With our strong institutions, deep capital markets, flexible labor markets, technological leadership, and penchant for entrepreneurship and innovation, no country is better placed than the United States to benefit from increased participation in the global economy. If we resist protectionism and isolationism while working to increase the skills and adaptability of our labor force, the forces of globalization and trade will continue to make our economy stronger and our citizens more prosperous.\n\nReferences\n\nAcemoglu, Daron, and Joshua Angrist (2001). \"How Large Are Human Capital Externalities? Evidence from Compulsory Schooling Laws,\" in Ben S. Bernanke and Kenneth Rogoff, eds., NBER Macroeconomics Annual. Cambridge, Mass.: MIT Press, pp. 9-59.\n\nBecker, Gary S. (1964). Human Capital: A Theoretical and Empirical Analysis with Special Reference to Education. New York: National Bureau of Economic Research.\n\nBernanke, Ben S. (2004). \"Trade and Jobs,\" speech delivered at the Distinguished Speaker Series, Fuqua School of Business, Duke University, March 30,  www.federalreserve.gov/boarddocs/speeches/2004/20040330/default.htm\n\n______ (2006). \"Global Economic Integration: What's New and What's Not?\" speech delivered at the thirtieth annual economic symposium sponsored by the Federal Reserve Bank of Kansas City, Jackson Hole, Wyo., August 25, www.federalreserve.gov/newsevents/speech/bernanke20060825a.htm\n\n______ (2007). \"The Level and Distribution of Economic Well-Being,\" speech delivered at the Greater Omaha Chamber of Commerce, February 6, www.federalreserve.gov/newsevents/speech/bernanke20070206a.htm\n\nBernard, Andrew B., and J. Bradford Jensen (1999). \"Exceptional Exporter Performance: Cause, Effect, or Both?\" Journal of International Economics, vol. 47 (February), pp. 1-25.\n\nBernard, Andrew B., J. Bradford Jensen, and Peter K. Schott (2006). \"Trade Costs, Firms, and Productivity,\" Journal of Monetary Economics, vol. 53 (July), pp. 917-37.\n\nBhagwati, Jagdish (2004). In Defense of Globalization. New York: Oxford University Press.\n\nBradford, Scott C., Paul L. E. Grieco, and Gary Clyde Hufbauer (2006). \"The Payoff to America from Globalisation,\" The World Economy, vol. 29 (July), pp. 893-916.\n\nCard, David (1999). \"The Causal Effect of Education on Earnings,\" in Orley Ashenfelter and David Card, eds., Handbook of Labor Economics, vol. 3A. New York: Elsevier, pp. 1801-63.\n\nCox, Michael, and Richard Alm (2007). \"The Best of All Worlds: Globalizing the Knowledge Economy,\" in Federal Reserve Bank of Dallas, 2006 Annual Report, pp. 3-28.\n\nDavis, Steven, John Haltiwanger, and Scott Schuh (1996). Job Creation and Destruction. Cambridge, Mass.: MIT Press.\n\nDollar, David, and Aart Kraay (2004). \"Trade, Growth, and Poverty,\" The Economic Journal, vol. 114 (February), pp. F22-F49.\n\nDoms, Mark E., and J. Bradford Jensen (1998). \"Comparing Wages, Skills, and Productivity between Domestically and Foreign-Owned Manufacturing Establishments in the United States,\" in Robert E. Baldwin, Robert E. Lipsey, and J. David Richardson, eds., Geography and Ownership as Bases for Economic Accounting. Chicago: University of Chicago Press.\n\nEasterly, William (2006). The White Man's Burden: Why the West's Efforts to Aid the Rest Have Done So Much Ill and So Little Good. New York: Penguin Press.\n\nGuisepi, Robert A. (2000). \"The Stone Age: The General Picture,\" International World History Project, http://history-world.org/stone_age2.htm (accessed April 13, 2007)\n\nHummels, David (2006). \"Transportation Costs and Trade over Time,\" in David Hummels, Anthony Venables, Harry Broadman, and John S. Wilson, rapporteurs, Transport and International Trade: Round Table 130. Organisation for Economic Co-operation and Development and European Conference of Ministers of Transport. Paris: OECD.\n\nInstitute of International Education (2006). \"New Enrollment of Foreign Students in the U.S. Climbs in 2005/06,\" press release, November 13, http://opendoors.iienetwork.org/?p=89251.\n\nKletzer, Lori G. (2001). Job Loss from Imports: Measuring the Costs. Washington: Institute for International Economics.\n\nKletzer, Lori G., and Howard Rosen (2006). \"Reforming Unemployment Insurance for the Twenty-First Century Workforce,\" Hamilton Project Discussion Paper. Washington: Brookings Institution, September.\n\nKling, Jeffrey R. (2006). \"Fundamental Restructuring of Unemployment Insurance: Wage-Loss Insurance and Temporary Earnings Replacement Accounts,\" Hamilton Project Discussion Paper. Washington: Brookings Institution, September.\n\nKull, Steven (2004). Americans on Globalization, Trade, and Farm Subsidies , The American Public on International Issues, PIPA/Knowledge Networks Poll, Program on International Policy Attitudes and Knowledge Networks, January 22, www.pipa.org/OnlineReports/Globalization/GlobalTradeFarm_Jan04/GlobalTradeFarm_Jan04_rpt.pdf (900 KB PDF).\n\nMann, Catherine L. (2003). \"Globalization of IT Services and White Collar Jobs: The Next Wave of Productivity Growth,\" International Economics Policy Briefs PB03-11. Washington: Institute for International Economics, December, www.iie.com/publications/pb/pb03-11.pdf (389 KB PDF).\n\n______ (2004). \"Global Sourcing and High-Tech Jobs: Productivity Gains and Policy Challenges,\" presentation on \"White Collar Outsourcing\" at the Institute for International Economics, March 11,www.iie.com/publications/papers/mann0304.pdf (138 KB PDF).\n\n______ (2006). Accelerating the Globalization of America: The Role for Information Technology, with Jacob Funk Kirkegaard. Washington: Institute for International Economics.\n\nTopel, Robert (2004). \"The Private and Social Values of Education,\" in Education and Economic Development, proceedings of a conference held at the Federal Reserve Bank of Cleveland, November 18-19, pp. 47-57, www.clevelandfed.org/research/conferences/2004/November/cbook.pdf (891 KB PDF).\n\nFootnotes\n\n1. Hummels (2006). Bernanke (2006) provides a brief history of globalization. Return to text\n\n2. The estimates ranged from $7,000 to $13,000. Return to text\n\n3. Cox and Alm (2007) discuss the benefits of trade in the modern global economy. Return to text\n\n4. Bernard and Jensen (1999) find that exporting firms are more productive than non-exporters. Bernard, Jensen, and Schott (2006) document the tendency of trade to reduce production at low-productivity plants and to increase output at high-productivity plants in the United States, a shift that raises average productivity. Return to text\n\n5. Refer also to Bhagwati (2004). Return to text\n\n6. According to the Bureau of Labor Statistics (BLS), over the past ten years, gross job losses in the United States have averaged about 7.8 million per quarter. Multiplying 7.8 million by 4 suggests that about 31 million U.S. jobs come to an end each year. This figure includes temporary layoffs, seasonal closings, and other short-term job losses; some research suggests that longer-term job losses amount to about half of the total (Davis, Haltiwanger, and Schuh, 1996). Dividing 31 million gross job losses by 2 yields about 16 million long-term job losses each year. Return to text\n\n7. February 2007 is the latest month for which these rate comparisons are available. The data are from the Bureau of Labor Statistics, which has adjusted them to approximate the U.S. definition of unemployment. Return to text\n\n8. The economic importance of physical proximity is the underlying reason that people and businesses are willing to pay high rents and other costs to live in or near major cities, where they can be near large numbers of other people and businesses that have related expertise and interests. Return to text\n\n9. Another type of service in which the United States has a strong export position is higher education. In 2005-06, U.S. institutions of higher learning trained nearly 600,000 foreign students, of whom about half were studying for graduate and professional degrees. Many foreign students who study in the United States spend at least some time here subsequently, adding their skills to those of the domestic workforce (Institute of International Education, 2006). Return to text\n\n10. Mann (2006, pp. 140-41) analyzes changes from 1999 to 2004. Updating the analysis with 2005 data from the Bureau of Labor Statistics does not change these results. Some of the low-wage occupations, such as data entry and word processing, may have lost jobs to automation rather than outsourcing. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070501a.htm",
        "title": "Embracing the Challenge of Free Trade: Competing and Prospering in a Global Economy",
        "date": "5/1/2007"
    },
    {
        "content": "April 26, 2007\n\nGovernor Frederic S. Mishkin\n\nTo the New Perspectives on Financial Globalization ConferenceWashington, D.C.\n\nGovernor Mishkin presented identical remarks at the Econometric Society at Duke University Lecture, Durham, North Carolina, on June 23, 2007\n\nIn the United States and many other countries, students learn that the key to success is hard work.  Yet when we look at many developing countries, we see people who work extremely hard for long hours.  Their wages are low, and so they remain poor.  And as a whole, their countries remain poor.  If hard work does not make a country rich, what does?\n\nThe right institutions are essential.  Nobel laureate Douglass North defines institutions as the \"rules of the game in a society, or, more formally, humanly devised constraints that shape human intervention.\" (North, 1990, p. 3).  Among the institutions that are most crucial to economic growth are those that enable a country to allocate capital to its most productive uses.  Such institutions establish and maintain strong property rights, an effective legal system, and a sound and efficient financial system.\n\nIn recent years, the field of economic development has come to the conclusion that \"institutions rule\" and are critical to economic growth.1  An extensive literature focuses on financial development as a significant force driving economic development.2\n\nHowever, developing good institutions that foster financial development is not easy:  It takes time for institutions to evolve and adapt to local circumstances.  In addition, vested interests in poor countries often oppose the necessary reforms because they believe that such reforms will weaken their power or allow other people to cut into their profits.  How can poorer countries overcome these obstacles?  How can they change the distribution of power to forge the political will to promote institutional reform?  The answer is globalization.\n\nI should note that the opinions I will express today are my own and not necessarily those of my colleagues on the Federal Open Market Committee (FOMC).\n\nElements of Institutional Reform\nBefore examining the role of globalization in promoting financial development, let’s first look briefly at what steps must be taken to build an institutional infrastructure that will ensure a well-functioning financial system.\n\n1. Develop strong property rights.  Strong property rights are needed to encourage productive investment because it will not be undertaken if the returns on investment are likely to be taken away by the government or others.  Hernando de Soto, in his important book The Mystery of Capital, argues that the inability of the poor in developing countries to acquire property rights is a central reason that they are unable to gain access to capital and so remain mired in poverty.  For example, the use of collateral is a crucial tool that helps the financial system make loans because it reduces losses when loans go sour.  A person who would pledge land or capital for a loan must, however, legally own the collateral.  Unfortunately, as de Soto has documented, legalizing the ownership of capital is extremely expensive and time consuming for the poor in developing countries.  In one of his many astonishing examples, obtaining legal title to a dwelling on urban land in the Philippines required taking 168 bureaucratic steps through 53 public and private agencies over a period of 13 to 25 years.\n\n2. Strengthen the legal system.  A legal system that enforces contracts quickly and fairly is an essential step in supporting strong property rights and financial development.  For example, lenders write restrictive covenants into loan contracts to prevent borrowers from taking on too much risk, but such covenants have value only if they can be legally enforced.  An inefficient legal system in which loan contracts cannot be enforced will prevent productive lending from taking place.  If setting up legitimate businesses or obtaining legal title to property is too expensive, the poor will never have access to the legal system and will be cut off from lending that could help them start small businesses and escape poverty.3  Setting up a simple business in the United States generally requires only filling out a form and paying a nominal licensing fee.  In contrast, de Soto's researchers found that legally registering a small garment workshop in Peru required 289 days; at 6 hours per day, the cost was about $1,200, which was approximately thirty times the monthly minimum wage.  The lack of property rights for all but the very rich, as documented by de Soto, is a serious impediment to financial development.\n\n3. Reduce corruption.  Government is often the primary source of financial repression in developing countries.  Rapacious governments whose rulers treat their countries as personal fiefdoms are not uncommon:  We have seen these governments in Saddam Hussein's Iraq, Robert Mugabe's Zimbabwe, and Ferdinand Marcos's Philippines.  Even officials in less tyrannical governments have been known to use the power of the state to get rich.  Not surprisingly, then, many governments pay lip service to property rights but do not encourage a rule of law to protect them.\n\nEliminating corruption is essential to strengthening property rights and the legal system.  When corrupt officials demand bribes, they reduce the incentives for entrepreneurs to make investments.  The ability to buy off judges weakens the enforcement of legal contracts that enable the economic and financial system to function smoothly.4\n\n4. Improve the quality of financial information.  High-quality financial information is essential to well-functioning financial markets.  If lenders cannot figure out what is going on in a firm, they will be unable to screen out good from bad credit risks or to monitor the firm to ensure that it does not take on too much risk at the lender’s expense.  To make reliable and accurate information more accessible, accounting standards must be high enough so that prospective lenders can make sense of what is in a business’s books.  Rules that require businesses to disclose information must be enforced to enable prospective investors to make sensible decisions about whether the business deserves to get their hard-earned money.\n\n5. Improve corporate governance.  For people to be willing to buy stocks, another way to channel funds to business, rules must be established to ensure that the managers of corporations act in the stockholders’ interest.  If managers find it easy to steal from the corporation, or to use funds for their own personal use rather than for the benefit of the company, no one will want to invest in the company.  Finding the right balance of control between management and stockholders is a challenge with which even we in the United States continue to struggle.\n\n6.  Develop sound, prudential regulation and supervision of the banking system.   Banks are the main institutions that allocate credit in developing countries.  The skills necessary for bank officers to assess risks and make good lending decisions are critically important and often scarce.  Poor lending policies may cause too much capital to be channeled toward low-return projects and insufficient capital to be directed toward the high-return projects needed to propel income and growth.  Moreover, deterioration in banks' balance sheets caused by insider lending or excessive risk-taking that leads to a proliferation of bad loans can cause banks to cut back sharply on lending, with negative effects on the economy.  If the deterioration in banks’ balance sheets is severe enough, it can result in banking and currency crises that substantially disrupt the economy, phenomena that unfortunately have been all too common in developing countries over the past several decades.5   Preventing banking crises must start with prudential regulation, in which rules set by the government ensure that banks have sufficient capital and manage risks well.  To guarantee that these regulations are enforced, the government must also engage in prudential supervision, in which it monitors banks by examining them on a regular basis to ensure that they are complying with government regulations.\n\nThe role of microfinance in developing countries is receiving much attention these days.  Microfinance is a positive development; it has clearly helped substantial numbers of poor people escape poverty, and the Nobel Peace Prize awarded to Muhammad Yunus for his pioneering efforts in this area was certainly well deserved.6  However, microfinance is not a substitute for the institution building I am talking about here.\n\nGlobalizing to Advance Institutional Reform\nNow that we understand what kinds of institutions are needed to promote financial development and economic growth, let’s turn to the question of how developing countries can improve the likelihood that these institutions are developed.\n\nOne of the most powerful weapons for stimulating institutional development is globalization.  Wealth is not something that can be attained by remaining closed off to the rest of the world.  Poorer countries would do better by embracing globalization--that is, opening their financial markets and their markets for goods and services to other nations so that funds, goods, and, often, the ideas that accompany them can flow in.  Such inflows can help them achieve reforms that build productivity and wealth that will benefit all their citizens.  Of course, countries need to take care that the foundations of the fundamental institutions discussed above are in place, and they must monitor the pace of reform.\n\nOpening financial markets\nNow let’s look at how opening financial markets to foreigners promotes financial development.\n\nGlobalizing the domestic financial system by opening financial markets to foreigners encourages financial development and growth in wealth in two ways.  First, opening financial markets to foreign capital directly increases access to capital and lowers its cost for those with productive investments to make.7  We know that labor is cheap in poor countries, and so we might think that capital would be especially productive there.  Just think of how hugely profitable a factory might be in a country where wages are one-tenth of those in the United States.  Although some of that differential would likely reflect the higher productivity of American workers, capital should, nevertheless, have extremely high returns in such countries, and, in principle, we should expect substantial flows of capital from rich countries (where the returns on capital should be relatively low) to poor countries (where they should be far higher).  Such capital flows could lead to substantial benefits for poor countries in the form of larger capital stocks, higher productivity, and more rapidly growing incomes.\n\nIn fact, as we well know, at present capital flows are moving, on net, from poor countries to rich ones, that is, in a direction opposite to the one we would expect.  Many reasons have been proposed for this apparent paradox, but one of them certainly is the weakness of financial systems in poor countries, as described earlier.  This point leads us to a second benefit of financial globalization:  Opening markets to foreign financial institutions promotes reforms to the financial system that improve its functioning.  Allowing foreign financial institutions to operate in an emerging-market country brings in expertise and best practices, such as those designed to screen good from bad credit risks and to monitor borrower activities to reduce the amount of risk they take.8  Because of their familiarity with more-advanced financial systems, foreign financial firms also are likely to increase the pressure on the domestic government to institute reforms that will make the financial system work more effectively.\n\nAs domestic financial institutions start to lose business to better-run and more trustworthy foreign institutions, they will realize the need for a better legal and accounting infrastructure that will make it easier for them to make loans to new customers.  Domestic financial institutions will then be far more likely to advocate for and support the reforms that achieve this result.\n\nOf course, this is not to say that in a genuinely corrupt and anticompetitive environment financial globalization, by itself, can still engender an efficient, dynamic, and modern financial system.  Recent research has shown that when some countries opened up to international capital markets too soon in the absence of some basic supporting conditions, vulnerabilities to sudden stops of capital flows increased.  Thus, some preconditions must exist with respect to a minimum level of institutional quality, financial market development, and macroeconomic stability before financial globalization can further improve financial market and institutional development.9  That said, given these preconditions and some constituency for progress and reform, financial globalization can be a powerful force in support of such efforts.\n\nOpening trade in goods\nNext, let’s consider how opening domestic markets to foreign goods can promote the development of better institutions.\n\nAlthough not immediately obvious, opening domestic markets to foreign goods, known as \"trade liberalization,\" can be a key driver of financial development.  It can weaken the political power of entrenched business interests that might otherwise block institutional reforms, a point that is emphatically made by Rajan and Zingales (2004) in their book Saving Capitalism from the Capitalists.  Trade liberalization, which promotes a more competitive environment, will lower the revenue of entrenched firms so that they will need greater access to external sources of capital.  Thus, they will be more likely to support reforms that promote a deeper and more efficient financial system.  In fact, research indicates that a deeper financial sector is positively associated with greater trade openness (Rajan and Zingales, 2003; Svaleryd and Vlachos, 2002).\n\nFree trade also promotes financial deepening by reducing corruption.  High tariffs breed corruption because importers have incentives to pay customs officials to look the other way when the importers avoid tariffs by smuggling in goods.  Not surprisingly, countries that restrict international trade are found to be more corrupt (Ades and Di Tella, 1994).\n\nEven when developing countries are unwilling to tear down all barriers to imports of foreign goods, they can still generate incentives for institutional reform by removing obstacles that prevent domestic producers from engaging in international trade.  Facilitating production for overseas markets creates a greater need for a well-functioning financial system because, to compete effectively in the international arena, firms need better access to capital.  If they can’t get capital, they won’t be able to make the investments they need to increase productivity and price their goods competitively.   Accordingly, international trade creates a demand for reforms that will make the financial system more efficient.\n\nThe case of China\n\nWe are seeing how the globalization of trade is driving financial reform in China.  As Chinese enterprises increasingly enter international markets, they need a better financial system that can ensure that the allocation of their high domestic savings is done efficiently and is responsive to market developments.  Although it has taken time, globalization is helping to generate the demand for an improved financial system, which is driving the reform process.\n\nThe Communist leadership recognizes that the old development model must change.  The government has announced that state-owned banks are being put on the path to be privatized and has allowed foreign investment in China’s banking system ($20 billion in 2005).10  The government is also engaged in legal reform to make contracts more enforceable.  In August 2006, the National People’s Congress enacted a new bankruptcy law that gives creditors greater protection if a firm goes bankrupt, and last month it approved a law that gives individuals more legal protection for their property.11\n\nChina, of course, is an example of a country that has actively encouraged exports as a means of propelling its economic growth and development.  To some extent, China may have gone too far in its use of policy to promote export growth.  Increased reliance on market-determined prices will help ensure that the allocation of resources into the export sector does not exceed their efficient use.  The goal should be to raise productivity toward world-class standards in all sectors of the economy.  Recently China’s authorities have agreed that some rebalancing of the sources of growth away from exports and toward domestic demand is in order.  Among China’s East Asian neighbors, the importance of developing industries to meet demand for domestic uses also is receiving increasing attention.\n\nThe problem of export restrictions\n\nNevertheless, developing production for exports may still be useful for those countries at the lowest rungs of the developmental ladder, and it is surprising that many of the world’s poorest developing countries still not only do not encourage an export orientation but in fact maintain a regime of taxes, restrictions, and other policies that effectively discourage it.  This problem remains especially serious in some African economies and may help explain why their growth performance has been so disappointing.\n\nThe primary way that governments discourage exports is by imposing large taxes on them.  Because high export taxes are one method of obtaining revenue, governments may be attracted to them to solve their budget problems.  They may also use these taxes to punish their political opponents, who are often involved in a particular export industry.  The government can then distribute the resulting revenue to their supporters.\n\nThe most pernicious forms of export taxes are those that are hidden through the government’s setting a fixed official exchange rate that artificially keeps the domestic currency at a value well above what it would be worth in terms of foreign currency (say, U.S. dollars) in a free market.  The government then makes it illegal to sell dollars for the larger amount of domestic currency that could be obtained in the black market.  The difference between the official exchange rate and the free, black-market rate (often called the \"black-market premium\") imposes a tax on exporters because they are forced to sell the dollars they earn to the government or to the central bank at the official rate, and thus they receive a much lower price for their goods in terms of the domestic currency.\n\nAlthough in recent decades a great many countries have abandoned currency controls and dismantled their black markets, such controls still exist in some of the poorest economies, especially in Africa.  In some countries, the tax from the black-market premium is confiscatory.  An example from history illustrates this point.  In 1982 Ghana had a black-market premium of more than 1,000 percent, and so exporters of cocoa (primarily from a tribe different from that of the ruling government party) were getting only 6 percent of the world price. Given such a high tax rate, it came as no surprise that cocoa exports, which had accounted for 19 percent of Ghana’s gross domestic product in the 1950s, accounted for only 3 percent by 1982 (Easterly, 2001, p. 222).  During the twenty years when the black-market premium was so high, the average income of Ghanaians fell 30 percent.\n\nLike many such unwarranted controls on economic life, high black-market premiums also breed corruption, with all its negative effects, because they create strong incentives to bribe officials or to smuggle goods to avoid paying the black-market-premium tax.  (Indeed, one of the reasons that governments in poorer countries often use this method of taxation rather than an explicit tax is that it allows government officials to get rich from the bribes they receive.)\n\nOther Gains from Trade Liberalization\nAlthough we have been focusing on how globalization promotes financial development, we shouldn’t forget that trade globalization, which involves both trade liberalization and an export orientation, is a key driver of economic growth for reasons additional to those already mentioned.12\n\nThe first economics course that college students encounter always teaches the concept of comparative advantage:  By trading with another country, you can focus your production on what you are really good at so that your productivity will be high.  This higher productivity then leads to higher economic welfare.\n\nTrade liberalization, more importantly, promotes competition in domestic markets, which in turn forces domestic firms to increase productivity and make better products, both of which drive economic growth.  If a foreigner produces a better product that can be imported, domestic firms must make a better product at a lower price to keep selling their product at home.  One graphic example of how trade promotes competition occurred in India, which up until 1991 had protected its tool industry with a 100 percent tariff (tax on imports).  After the Indian government cut the tariff sharply, Taiwanese firms initially grabbed one-third of the Indian market.  Over the next decade, however, Indian firms boosted their productivity almost to the levels of Taiwanese firms, thereby winning back the domestic market.  Eventually Indian tool firms became so efficient that they were able to start selling their goods abroad and became substantial exporters.13\n\nDecreasing barriers to imports also helps promote exports.  Increased competition from imports lowers the profits firms can earn by focusing solely on the domestic market, and so they naturally concentrate more of their energy on exporting.  Moreover, trade liberalization helps developing countries gain access to foreign markets in advanced countries, as illustrated by the fact that the United States, through free-trade agreements, has been more willing to lower tariffs for countries such as Mexico and Chile if they do the same for the United States.\n\nEmpirical evidence indicates that trade liberalization has positive effects on productivity and economic growth for both importing and exporting countries:  It has even been found to be associated with more-rapid increases in life expectancy and a reduction in infant mortality.14 Yet, as is often the case in economics, empirical evidence is never completely clear cut:  Some economists question whether the evidence strongly supports a positive link between trade liberalization and growth.15  Nonetheless, the logic of the benefits of trade liberalization and the preponderance of the evidence supporting its positive effects lead most members of the economics profession, including me, to the following conclusion:  Trade liberalization is highly beneficial not only for the overall economy but also for its constituent sectors.  The resulting economic growth is a rising tide that raises all boats and is an important tool for poverty alleviation.\n\nBut even if trade liberalization is not adopted, giving domestic producers the opportunity to sell goods to rich countries’ markets can be an important engine for growth in the world’s poorest countries.  One crucial way that governments in developing countries can encourage exports is by providing the transportation infrastructure--ports, roads, and airports--that make it easier for businesses to send their goods abroad.  Because foreigners don’t have a natural predilection to buy your goods, you have to be supercompetitive--your goods have to be better and cheaper than the goods made in foreign countries.  Domestic firms have to focus even more on being highly productive, and boosting productivity will lead to rapid economic growth.\n\nJapan’s experience shows what focusing on exporting can accomplish.  In the immediate aftermath of World War II, Japan was a poor country.  Its economic infrastructure had been destroyed by the war.  To convince Americans and others to buy Japanese products, Japanese firms had to produce goods that were cheaper and better than their American-made counterparts.  As a result, the export industries in Japan became enormously productive and supercompetitive.  Productivity grew, and three decades after World War II, Japan became one of the richest countries in the world.\n\nSouth Korea, one of the great Asian success stories even with its crisis in the late 1990s, had very high barriers to trade until the 1990s, and its early development strategy did not include opening its domestic market to foreign goods.  However, through its export sector, South Korea has participated fully in global markets, and this participation has been a key to its success.  South Korea’s development strategy focused on promoting its export sector, and it is the export sector that led to high productivity and economic growth.  Indeed, all examples of successful growth stories in developing economies (China, Japan, South Korea, Singapore, Taiwan, Chile) have involved export sectors that met the test of international competition, and some of these economies have also pursued trade liberalization.\n\nIn almost all the industrializing East Asian economies, future growth will likely have to follow a more balanced path that relies less on exports and more on production for the domestic market.  Such adjustments are needed not only to secure such economies’ further development but also to alleviate the pattern of external imbalances around the global economy.  It is in the world’s poorest countries--especially in Africa and Latin America--that additional participation in global markets has the highest priority.\n\nOnly by embracing global markets can developing countries raise living standards.16   Trade liberalization has a critical role to play in economic growth by directly stimulating domestic firms to become more productive.  And along with financial globalization, it can also encourage emerging-market economies to develop the institutions that foster financial development.  Globalization should be one of the highest priorities for developing countries.\n\nThe Role of Advanced Countries\n\nCan we in the advanced countries help?  Yes, we can do so by supporting the opening of our markets to goods and services from emerging-market countries.  By encouraging these countries to increase their participation in global markets, we create exactly the right incentives for them to implement the hard measures that will enable them to grow rich.  As we have seen, exporters have strong incentives to be productive so that they can take advantage of access to our markets, and thus they will make the investments needed for growth.  They also will push for the institutional reforms to make financial markets more efficient and promote financial deepening.   By getting financial markets to work well, exporters will have access to the capital they need to increase their business.\n\nOpening our markets to emerging-market countries is an important way that those in advanced countries can help emerging-market economies become successful.  While providing aid to poor countries can, in the right circumstances, help eradicate poverty, it often will not work because it usually does not create the right incentives to promote economic growth.  A handout is almost never as effective as a hand up.\n\nSome are concerned about the consequences for us if we in the United States allow free competition in our markets for goods and services from countries where wages are low.  Keeping many countries poor and their workers unproductive may seem to be to our benefit.  But as shown in the examples of post-World War II recovery in Europe and Japan, and in the rapid growth in the 1970s and 1980s in the newly industrialized economies of Asia, higher standards of living throughout the global economy actually work to our benefit.  Prosperity in our trading partners creates growing markets for U.S. exports of high-value goods.  And over time, as workers’ productivity abroad rises, so will their wages and incomes.  It is true that the changes brought about in our economy by globalization impose significant costs on some domestic workers.  We need to develop policies to help those workers without undermining the global trading system.  The costs to us of damaging that system would far outweigh the benefits that some might gain from protectionist measures.  Promoting trade liberalization helps us not only do good but also do well.\n\nReferences\n\nAcemoglu, Daron, Simon Johnson, and James A. Robinson (2001).  \"The Colonial Origins of Comparative Development:  An Empirical Investigation,\" American Economic Review, vol. 91 (December), pp. 1369-1401.\n\nAcemoglu, Daron, Simon Johnson, and James A. Robinson (2005).  \"Institutions as the Fundamental Cause of Long-Run Growth,\" in Philippe Aghion and Steven N. Durlauf, eds., Handbook of Economic Growth, vol. 1, part 1.  Amsterdam: North Holland, pp. 385-472.\n\nAdes, Alberto, and Rafael Di Tella (1994).  \"Competition and Corruption,\" Institute of Economics and Statistics Discussion Paper Series 169.  Oxford:  University of Oxford.\n\nAlfaro, Laura, and others (2004).  \"FDI and Economic Growth: The Role of Local Financial Markets,\" Journal of International Economics, vol. 64 (October), pp. 89-112.\n\nArmendariz de Aghion, Beatriz, and Jonathan Morduch (2005).  The Economics of Microfinance.  Cambridge, Mass.:  MIT Press.\n\nBekaert, Geert, Campbell R. Harvey and Robin L. Lumsdaine (2002).  \"Dating the Integration of World Equity Markets,\" Journal of Financial Economics, vol. 65 (August), pp. 203-47.\n\nBhagwati, Jagdish N. (2004).  In Defense of Globalization.  New York:  Oxford University Press.\n\nBourguignon, Francois, Diane Coyle, Raquel Fernandez, Francesco Giavazzi, Dalia Marin, Kevin O’Rourke, Richard Portes, Paul Seabright, Anthony Venables, Thierry Verdier, and L. Alan Winters (2002).  Making Sense of Globalization:  A Guide to the Economic Issues, CEPR Policy Paper Series 8.  London:  Centre for Economic Policy Research, July.\n\nDell’Ariccia, Giovanni, and Robert Marquez (2006).  \"Lending Booms and Lending Standards,\" Journal of Finance, vol. 61 (October), pp. 2511-46.\n\nDemirguc-Kunt, Asli, and Enrica Detragiache (2005).  \"Cross-Country Empirical Studies of Systemic Bank Distress: A Survey (422 KB PDF),\" IMF Working Paper Series WP 05/96.  Washington:  International Monetary Fund, May.\n\nde Soto, Hernando (2000).  The Mystery of Capital:  Why Capitalism Triumphs in the West and Fails Everywhere Else. New York:  Basic Books.\n\nDollar, David (1992).  \"Outward-Oriented Developing Economies Really Do Grow More Rapidly: Evidence from 95 LDCs, 1976-1985,\" Economic Development and Cultural Change, vol. 40 (April), pp. 523-44.\n\nDollar, David, and Paul Collier (2001).  Globalization, Growth, and Poverty: Building an Inclusive World Economy.  New York:  Oxford University Press.\n\nEasterly, William (2001).  The Elusive Quest for Growth:  Economists’ Adventures and Misadventures in the Tropics.  Cambridge, Mass.:  MIT Press.\n\nEasterly, William, and Ross Levine (2001).  \"It’s Not Factor Accumulation: Stylized Facts and Growth Models,\" World Bank Economic Review, vol. 15 (2), pp. 177-219.\n\nEasterly, William, and Ross Levine (2003). \"Tropics, Germs, and Crops:  How Endowments Influence Economic Development,\" Journal of Monetary Economics, vol. 50 (January), pp. 3-39.\n\nEdwards, Sebastian (1998).  \"Openness, Productivity, and Growth: What Do We Really Know?\" Economic Journal, vol. 108 (March), pp. 383-98.\n\nEichengreen, Barry (2001).  \"Capital Account Liberalization: What Do Cross-Country Studies Tell Us?\" World Bank Economic Review, vol. 15 (3), pp. 341-65.\n\nFrankel, Jeffrey A., and David Romer (1999).  \"Does Trade Cause Growth?\" American Economic Review, vol. 89 (June), pp. 379-99.\n\nGlaeser, Edward L., Rafael La Porta, Florencio Lopez-de-Silanes, and Andrei Shleifer (2004).  \"Do Institutions Cause Growth?\" NBER Working Paper Series 10568.  Cambridge, Mass.: National Bureau of Economic Research, June.\n\nGoldberg, Linda (2004).  \"Financial-Sector FDI and Host Countries: New and Old Lessons,\" NBER Working Paper Series 10441.  Cambridge, Mass.:  National Bureau of Economic Research, April.\n\nHall, Robert E., and Charles I. Jones (1999).  \"Why Do Some Countries Produce So Much More Output per Worker Than Others?\"  Quarterly Journal of Economics, vol. 114 (February), pp. 83-116.\n\nHarrison, Ann (1996).  \"Openness and Growth: A Time-Series, Cross-Country Analysis for Developing Countries,\" Journal of Development Economics, vol. 48 (March), pp. 419-47.\n\nHenry, Peter Blair (2000a).  \"Stock Market Liberalization, Economic Reform, and Emerging Market Equity Prices,\" Journal of Finance, vol. 55 (2), pp. 529-64.\n\nHenry, Peter Blair  (2000b).  \"Do Stock Market Liberalizations Cause Investment Booms?\" Journal of Financial Economics 58 (1-2), pp. 301-34.\n\nJones, Benjamin F., and Benjamin A. Olken (2005).  \"The Anatomy of Start-Stop Growth,\" NBER Working Paper Series 11528.  Cambridge, Mass.:  National Bureau of Economic Research, July.\n\nKaufmann, Daniel, Aart Kray, and Pablo Zoido-Lobaton (1999).  \"Governance Matters,\" Policy Research Working Paper Series 2196.  Washington:  World Bank, October.\n\nKearl, James R., Clayne L. Pope, Gordon C. Whiting, and Larry T. Wimmer (1979).  \"A Confusion of Economists?\" American Economic Review, vol. 69 (May, Papers and Proceedings), pp. 28-37.\n\nKlein, Michael W. (2005).  \"Capital Account Liberalization: Institutional Quality and Economic Growth: Theory and Evidence,\" NBER Working Paper Series 11112.  Cambridge, Mass.:  National Bureau of Economic Research, February.\n\nKose, M. Ayhan, Eswar Prasad, Kenneth Rogoff, and Shang-Jin Wei (2006).  \"Financial Globalization: A Reappraisal (758 KB PDF),\" IMF Working Paper Series WP 06/189.  Washington:  International Monetary Fund, August.\n\nLee, Ha Yan, Luca Antonio Ricci, and Roberto Rigobon (2004).  \"Once Again, Is Openness Good for Growth?\" Journal of Development Economics, vol. 75 (December), pp. 451-72.\n\nLevine, Ross (2004).  \"Finance and Growth,\" NBER Working Paper Series 10766.  Cambridge, Mass.:  National Bureau of Economic Research, September; forthcoming in Philippe Aghion and Steven N. Durlauf, eds., Handbook of Economic Growth.  Amsterdam:  North Holland.\n\nLevine, Ross and Sara Zervos (1998).  \"Capital Control Liberalization and Stock Market Development,\" World Development 26, pp. 1169-84.\n\nMauro, Paolo (1995).  \"Corruption and Growth,\" Quarterly Journal of Economics, vol. 110 (August), pp. 681-712.\n\nNorth, Douglass C. (1990).  Institutions, Institutional Change, and Economic Performance. Cambridge:  Cambridge University Press.\n\nNorth, Douglass C., and Robert Paul Thomas (1973).  The Rise of the Western World: A New Economic History.  Cambridge:  Cambridge University Press.\n\nRajan, Raghuram, and Luigi Zingales (2003). \"The Great Reversals: the Politics of Financial Development in the 20th Century,\" Journal of Financial Economics 69 (1), pp. 5-50.\n\nRajan, Raghuram, and Luigi Zingales (2004).  Saving Capitalism from the Capitalists: Unleashing the Power of Financial Markets to Create Wealth and Spread Opportunity.  Princeton:  Princeton University Press.\n\nRodriguez, Francisco, and Dani Rodrik (2001).  \"Trade Policy and Economic Growth: A Skeptic’s Guide to the Evidence,\" in Ben S. Bernanke and Kenneth Rogoff, eds., NBER Macroeconomics Annual 2000.  Cambridge, Mass.:  MIT Press.\n\nRodrik, Dani, Arvind Subramanian, and Francesco Trebbi (2002).  \"Institutions Rule: The Primacy of Institutions over Geography and Integration in Economic Development,\" NBER Working Paper Series 9305.  Cambridge, Mass.:  National Bureau of Economic Research, November.\n\nSachs, Jeffrey D., and Andrew M. Warner (1995).  \"Economic Reform and the Process of Global Integration,\" Brookings Papers on Economic Activity, 1995:1, pp. 1-118.\n\nSchmukler, Sergio L. (2004).  \"Financial Globalization: Gain and Pain for Developing Countries (307 KB PDF),\" Federal Reserve Bank of Atlanta, Economic Review, vol. 89 (Q2), pp. 39-66.\n\nSvaleryd, Helena, and Jonas Vlachos (2002).  \"Markets for Risk and Openness to Trade:  How Are They Related?\"   Journal of International Economics, vol. 57 (August), pp. 369-95.\n\nTemple, Jonathan (1999).  \"The New Growth Evidence,\" Journal of Economic Literature, vol. 37 (March), pp. 112-56.\n\nWei, Shangjin (1997).  \"How Taxing Is Corruption on International Investors?\" NBER Working Paper Series 6030.  Cambridge, Mass.:  National Bureau of Economic Research, May.\n\nWeil, David N. (2005).  Economic Growth. Boston:  Addison-Wesley.\n\nWinters, L. Alan, Neil McCulloch, and Andrew McKay (2004).  \"Trade Liberalization and Poverty: The Evidence So Far,\" Journal of Economic Literature, vol. 42 (March), pp. 72-115.\n\nWolf, Martin (2004).  Why Globalization Works. New Haven:  Yale University Press.\n\nWorld Bank (2001).  Finance for Growth: Policy Choices in a Volatile World.  New York:  Oxford University Press.\n\nWorld Bank (2005).  Doing Business in 2005:  Removing Obstacles to Growth.  Washington:  World Bank.\n\nFootnotes\n\n1.  A large literature shows the importance of good institutions to economic growth.  See, for example, North and Thomas (1973); Hall and Jones (1999); Acemoglu, Johnson, and Robinson (2001); Easterly and Levine (2001); Rodrik, Subramanian, and Trebbi (2002); Easterly and Levine (2003); Glaeser and others (2004); and the recent survey by Acemoglu, Johnson, and Robinson (2005).  Kaufmann and others (1999) also point to the importance of various aspects of good governance. Return to text\n\n2.  An excellent nontechnical survey of the extensive empirical evidence on the link between financial development and economic growth can be found in World Bank (2001).  See also Levine (2004) and Schmukler (2004). Return to text\n\n3.  A discussion of how the costs of doing business vary across a number of countries is in World Bank (2005).  Return to text\n\n4.  Research finds that increases in corruption are associated with lower growth (for example, Mauro, 1995).  Wei (1997) also finds that corruption significantly reduces foreign direct investment, which is generally considered to be beneficial to growth. Return to text\n\n5.  A survey of the literature that links a lack of sufficient prudential regulation and supporting institutions to excessive risk-taking and the possibility of a subsequent banking crisis is in Demirguc-Kunt and Detragiache (2005).  Dell’Ariccia and Marquez (2006) also argue that under certain circumstances lending booms can make the banking system more unstable and can lead to a higher probability of a banking crisis. Return to text\n\n6.  The literature on microfinance is vast.  One thorough discussion is in Armendariz de Aghion and Morduch (2005). Return to text\n\n7.  When stock markets in emerging-market countries are opened to foreign capital, dividend yields fall, average stock prices increase, and liquidity goes up.  See Levine and Zervos (1998); Bekaert, Harvey, and Lumsdaine (2002); and Henry (2000a,b). Return to text\n\n8.  This argument is made in World Bank (2001) and Goldberg (2004). Return to text\n\n9.  An excellent discussion of the literature on financial globalization using a unified conceptual framework is in Kose and others (2006).  Studies focusing more specifically on the necessary preconditions for, and the appropriate sequencing of, financial reforms, macroeconomic policies, and institutional development, on the one hand, and capital account liberalization, on the other, include Eichengreen (2001), Alfaro and others (2004), and Klein (2005).   Return to text\n\n10.  The four largest state-owned banks, with 70 percent of China’s bank deposits, are scheduled to be privatized in the following order:  the Construction Bank, the Bank of China, the Industrial and Commercial Bank, and the Agricultural Bank. Return to text\n\n11.  The new law becomes effective on June 1, 2007, but reportedly will not apply to state-owned enterprises until 2008. Return to text\n\n12.  Indeed, almost all economists think that trade liberalization, a key element of globalization, is a good thing.  For example, in Kearl and others (1979), 97 percent of economists agreed (generally or with some provisions) with the statement that \"tariffs and import quotas reduce general economic welfare.\" A typical view advocating trade liberalization is expressed by Jagdish Bhagwati, one of the most prominent trade theorists in the world, in Bhagwati (2004).  Return to text\n\n13.  This example comes from Weil (2005, p. 322) and is described more extensively in Dollar and Collier (2001).   Return to text\n\n14.  The literature on the effects of trade liberalization on growth and poverty is immense.  See the surveys in Temple (1999); Bourguignon and others (2002); Winters, McCulloch, and McKay (2004); and Wolf (2004).  Earlier studies found that trade openness was associated with higher growth rates (Dollar, 1992; Sachs and Warner, 1995; and Edwards, 1998). However, because the direction of causation from this evidence is difficult to establish, other researchers have used instrumental variable procedures to establish causality from trade liberalization to growth (for example, Frankel and Romer, 1999).  Using a different approach to identify the direction of causation, Lee, Ricci, and Rigobon (2004) also find that trade openness has a positive effect on growth.   Return to text\n\n15.  For example, Harrison (1996) and especially Rodriguez and Rodrik (2000).  Return to text\n\n16.  The finding in Jones and Olken (2005) that growth take-offs are primarily associated with large and steady expansions in international trade provides further support for this view.   Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/mishkin20070426a.htm",
        "title": "Globalization and Financial Development",
        "date": "4/26/2007"
    },
    {
        "content": "April 25, 2007\n\nChairman Ben S. Bernanke\n\nAt the Greater Washington Jump$tart Coalition financial education event, Washington, D.C.\n\nGood morning. I am pleased to be able to be here today, at the Greater Washington Jump$tart Coalition's inaugural event, to celebrate Financial Literacy Month 2007 and to say a few words about the importance of financial education. I join other Jump$tart partners as well as professionals from the fields of banking, finance, government, and law who will be speaking to nearly 2,000 public high school students about financial literacy.\n\nIt is a special privilege to be with the students here today, the juniors and seniors of Washington's own Woodrow Wilson High School. You are ambassadors of your school and indeed of Washington, our nation's capital. I know that this is a school committed to academic excellence. As you think about the time, not so far away now, when you will leave Wilson High, I hope that you will resolve to keep, throughout your life, the habits of learning and of thinking for yourself that you have gained in your years here.\n\nAs you think about your future, don't forget the importance of financial literacy. Although financial matters are probably not at the front of your minds today, the day will come when you will be responsible for managing your own or your family's budget or when you will find that you need to save to get the things you want--a college education, a new car, or even your own home. To achieve these personal goals and to build financial security, you will need to understand the fundamentals of budgeting, banking, saving, and investment. It is also essential that you know how to use--properly and responsibly--the many types of credit that will be at your disposal, such as credit cards. Later this morning, my colleagues from the Federal Reserve will be talking with you about how to manage your credit. Because credit has become such an integral part of our economy, and because there are so many sources and forms of credit available, much more financial sophistication is required today than when I attended high school. As a parent of two young adults myself, I believe that helping young people become financially literate is critical for their future economic well-being and should be a high priority for educators.\n\nIn addition to celebrating Financial Literacy Month 2007, we are here today to celebrate the inaugural event of the Greater Washington, D.C., chapter of the Jump$tart Coalition for Personal Finance. The Coalition is a national leader among organizations that work to improve the personal financial literacy of students from kindergarten to the university level. In particular, through its biennial survey of high school seniors, Jump$tart has brought increased attention to the issue of financial literacy among youth in the United States. The results of the surveys, which have shown that students' financial knowledge is often much less extensive than we would like, remind us of the need for continued efforts to help all students--as well as adults--obtain the skills they need to navigate today's increasingly complex financial markets. This new chapter of Jump$tart has joined forces with over fifty individuals, businesses, and not-for-profit, educational, and government organizations, including the Federal Reserve Board, to increase knowledge about personal finance among schoolchildren and young adults in the Greater Washington, D.C., region. The Federal Reserve is strongly committed to Jump$tart's mission and to working closely with the local Washington, D.C., chapter--as it does with the other Jump$tart chapters around the country through its Reserve Banks and Branches--to help achieve this mission.\n\nFinally, I would like to take this opportunity to express my appreciation of the teachers and staff of Wilson Senior High and the administrators of the Washington, D.C., public school district here today. It is your hard work and dedication to these students that will help guide them toward successful futures. There is no higher nor more worthy objective than preparing our young people for fulfilling and productive lives. To the students of Wilson Senior High, I wish you the best of luck in the future, and I especially want to congratulate the seniors who will be graduating at the end of this term. Lastly, a special thanks to the Greater Washington, D.C., chapter of the Jump$tart Coalition and their partners for their continued support and commitment to furthering the financial education of our youth. Thank you.",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070425a.htm",
        "title": "Welcoming remarks",
        "date": "4/25/2007"
    },
    {
        "content": "April 20, 2007\n\nGovernor Frederic S. Mishkin\n\nAt The Levy Economics Institute of Bard College, Annandale-on-Hudson, New York\n\nThank you for inviting me to participate in this conference and offer my views on prospects for the U.S. economy. I should note that the opinions I will express today are my own and not necessarily those of my colleagues on the Federal Open Market Committee (FOMC).\n\nWe are now almost 5-1/2 years into the current economic expansion. A slow start in 2002 was followed by three years of strong gains in real (that is, inflation adjusted) economic activity and a substantial decline in unemployment. Initially, monetary policy was very accommodative as the FOMC focused on providing support for the recovery and avoiding an unwelcome disinflation. From early 2003 to early 2006, real gross domestic product (GDP) rose at an annual rate of 3-1/2 percent--well above consensus estimates of its underlying sustainable rate--and the unemployment rate declined to 4-3/4 percent.\n\nSince the spring of 2006, however, the expansion of the U.S. economy appears to have been undergoing a transition to a more moderate and sustainable pace. Although such a transition will no doubt be marked by some bumps in the road, it represents a desired macroeconomic rebalancing that over the longer run can help ensure sustained non-inflationary growth. One of the fundamental factors underlying the deceleration in real activity is the lagged effect of the FOMC's removal of monetary policy accommodation between June 2004 and June 2006. Another source of the rebalancing is the substantial correction in housing markets that has been under way since last spring as the unrealistic expectations about home price appreciation that fueled the extended boom in homebuilding have been unwinding.\n\nLooking ahead, the most likely outcome for the coming quarters is, in my judgment, a continued moderate rate of economic expansion accompanied by some easing of pressures on resources. With inflation expectations contained, I would expect such an economic environment to foster a gradual slowing over time in the rate of core consumer price inflation. However, the actual path for economic activity and inflation could, at times, be uneven; and as is the case for all forecasts, it involves a number of risks and uncertainties on both the downside and the upside.\n\nTurning first to the prospects for economic activity, two particular areas have emerged recently that have heightened uncertainty about the near-term outlook. The first area is housing: Where do we stand in the housing adjustment, and what effect will recent developments in subprime lending have on that adjustment? The second area is business investment: How should we interpret the incoming data showing that business spending on equipment and software has been weak this year?\n\nRegarding the housing adjustment, new single-family homes were started at an average annual rate of a bit under 1.2 million units in the first three months of this year--a pace roughly one-third below the unsustainable peak in new construction reached in mid-2005. At the beginning of the year, the ongoing cutbacks in starts of new homes, together with a lowered but fairly steady pace of home sales, were beginning to reduce the elevated backlog of new homes for sale. However, a further weakening in sales of new homes in January and February reversed some of the progress in reducing those inventories. As a result, cutbacks in new residential construction may well persist for a while.\n\nMore recently, developments in the subprime mortgage market have raised some additional concern about near-term prospects for the housing sector. The sharp rise in delinquencies on variable-interest-rate loans to subprime borrowers and the exit of a number of subprime lenders from the market have led to tighter terms and standards on such loans. While these problems have caused undeniable hardship for many families and communities, spillovers to other segments of the mortgage market or to financial markets in general appear to have been minimal. Variable-interest-rate loans to subprime borrowers account for a bit less than 10 percent of all mortgages outstanding, and at this point the expected losses are relatively small. Moreover, because most subprime mortgages are securitized, the risks associated with these loans are spread widely. Banks and thrift institutions that hold mortgages are well-capitalized, and exposures of individual banks to possible subprime losses do not appear to be large. On the whole, some borrowers may find credit more difficult to obtain, but most borrowers are not likely to face a serious credit constraint.\n\nIndeed, I should note some positive news for the housing sector. Sales of existing homes strengthened a bit during January and February, and the Mortgage Bankers Association index of applications for home purchase suggests that demand has been fairly steady through early April. Also, mortgage rates are still at historically low levels, and mortgages to prime borrowers and fixed-rate mortgages to all classes of borrowers continue to show low rates of delinquency.\n\nThe second major area of concern in the near-term outlook, and one that perhaps could pose noticeable downside risks, is business investment. Real outlays for new equipment and software weakened in the final quarter of 2006, and the recent data on orders and shipments of nondefense capital goods suggest that the softness in demand has extended into early this year. Part of the weakness can be clearly traced to a decline in demand for investment goods that are used heavily in residential construction. In addition, demand for goods used by the motor vehicle industry also has softened of late. But, demand for other types of non-high-tech business equipment also appears to have slowed recently, raising more fundamental questions about business views on the current and prospective environment for capital spending.\n\nTo be sure, the pace of output has moderated, which typically would lead to some deceleration in capital spending. But the magnitude of the recent pullback seems to be greater than would be expected. Adding to the puzzle has been a weakening in demand for non-high-tech equipment even as financial conditions for investment have remained generally favorable. In particular, business balance sheets are strong, and although profits have slowed, profit margins remain elevated. Interest rates and credit spreads are relatively low, and firms appear to have ample ability to raise funds at a reasonable cost.\n\nThe unwillingness of businesses to invest might be due to concerns about the prospects for long-term productivity growth and the expected rate of return on capital investment. Moreover, businesses may be anticipating a more pronounced deceleration in sales than would be consistent with the moderate expansion that I am expecting. Respondents to the Blue Chip Economic Indicators survey suggest that the recent reluctance to invest reflects greater uncertainty about the outlook for sales and earnings. If so, the continuation of a moderate economic expansion is likely over time to restore confidence and lead to a firming in business investment.\n\nNot all of the recent news on business spending has been to the downside, however. Demand for high-tech equipment appears to have picked up early this year after leveling off in the final quarter of 2006. Demand for computers, which was likely boosted by the introduction of the Windows Vista operating system, seems to be advancing at a healthy pace. Technological innovations--such as circuitry that boosts computer performance and lowers energy consumption--appear to be generating demand to upgrade equipment in data centers. In addition, major U.S. cable companies are forecasting a step-up in capital spending, and telecommunications carriers are planning a further expansion of fiber-optic networks.\n\nAlthough questions related to the prospects for housing and business spending appear to have widened the range of uncertainty about the near-term outlook, developments in other areas appear to support a continued moderate rate of economic expansion. Monthly gains in employment, while down some from last year's pace, remain solid; the average monthly increase in nonfarm payrolls over the first three months of this year was about 150,000, compared with about 190,000 in 2006. To date, job cutbacks have been centered in industries related to residential construction and manufacturing, with no indication--either from the monthly labor market reports or the weekly unemployment insurance data--of widening weakness.\n\nIn addition, the incoming information on consumer spending has been consistent with a moderate pace of demand. The steady labor market has been generating income; and despite the ups and downs in energy prices, real disposable income has been trending up at about a 2-3/4 percent rate since early 2006. In addition, household credit quality remains generally favorable. As is the case for prime mortgages, delinquency rates on consumer loans are low. And despite the deceleration of house prices, the ratio of household net worth to disposable income is still elevated.\n\nEconomic activity also should be supported by fiscal policy, which is likely to remain mildly stimulative this year. At the federal level, that stimulus is likely to continue to come from defense spending rather than from other outlays, which are expected to change little in real terms. Although defense outlays tend to be volatile from quarter to quarter, the available and expected appropriations should keep real defense spending on a moderate uptrend. At the state and local government level, the economic expansion in recent years has broadly restored fiscal health. Many of these governments have been spending at a moderate rate while also building their rainy-day funds, and this year they appear poised for further hiring and more construction spending.\n\nOn the international trade front, recent readings on economic activity abroad have been positive, which suggests that the demand for U.S. exports of goods and services will continue to be solid. Prospects for further economic expansion in Europe and Japan appear good in the near term. And despite indications of moderation in some countries, the overall pace of economic activity in emerging economies, including China, appears to be strong.\n\nTurning now to the inflation outlook, in February the twelve-month change in core personal consumption expenditure (PCE) prices was 2.4 percent, and in March the year-over-year change in the core consumer price index (CPI) was 2.5 percent. Each of those readings was higher than the corresponding result for early 2006. Increases in market rent and in owners' equivalent rent account for much of that acceleration. Prices of consumer goods in both the PCE and CPI measures have been relatively flat for the past two years, while prices of services other than energy and shelter have been rising at about a 3 percent rate. My forecast of a gradual slowing in inflation reflects my view that making further progress in lowering inflation is desirable. Although I expect that core inflation will drift down, I recognize that achieving further reductions in inflation may take time. In the near term, the recent rebound in prices for gasoline and other petroleum-based goods is likely to put upward pressure on the costs of many non-energy goods and services. Moreover, the evolution of shelter costs, which have boosted core inflation over the past year, is difficult to predict. Here, my expectation is that as the supply of rental units increases and the market for owner-occupied housing stabilizes, the rise in rent will slow.\n\nOther concerns about prospects for inflation are related to developments on the supply side of the economy. Since last fall, the jobless rate has been hovering around 4 1/2 percent, a relatively low level by historical standards, and it fell to 4.4 percent this past month. With the labor market that tight, I am not surprised that our business contacts have been reporting shortages of workers in some occupations--both skilled and unskilled, depending on the region--and some associated wage pressures. To date, various measures of worker compensation are giving mixed signals on whether wages are accelerating. The narrowest measure, the average hourly earnings of production or nonsupervisory workers, shows a noticeable pickup in wage inflation from 3 percent in 2005 to around 4 percent more recently; another measure, hourly compensation in the nonfarm business sector rose in 2006 at close to 5 percent, also faster than a year earlier. But the employment cost index has been rising at a moderate 3 percent rate for the past two years.\n\nWhether a pickup in nominal labor compensation will lead to upward pressure on inflation will depend on several factors. Importantly, an acceleration in compensation might be offset by higher labor productivity; indeed, during most of this expansion, strong gains in labor productivity have checked the rise in unit labor costs. And, with profit margins wide, businesses might accommodate increases in labor compensation without passing them on to consumers in the form of higher prices. In these circumstances, gains in nominal compensation for workers would translate into gains in real compensation.\n\nSo how is labor productivity doing? During the first three years of the current expansion, output per hour in the nonfarm business sector rose 3 percent per year; in the past two years, it has decelerated to 2 percent. I suspect that this slowdown does not represent a fundamental weakening in the longer-run trend, but is rather a normal cyclical transition from an above-trend rate of increase to a more sustainable rate. Of course, I also recognize a potential downside risk to the outlook for productivity, especially given the weakness in business investment that I noted earlier. Nonetheless, averaging through the entire expansion to date, the underlying rate of productivity advance still seems to be close to the 2-1/2 percent rate that has prevailed since the mid-1990s.\n\nMore fundamentally, I believe that long-run inflation expectations remain a key determinant of the path of inflation. But what are the current expectations for long-term inflation? Unfortunately, that is not an easy question to answer. The results from the Survey of Professional Forecasters, readings on household opinion such as the Reuters/Michigan survey, and the spread between standard Treasury securities and Treasury inflation-protected securities--taken together--suggest that long-term inflation expectations are currently around 2 percent, although this guess is far from certain.\n\nGiven my estimate of the current level of long-run inflation expectations as well as the likelihood of some easing of resource pressures in labor and product markets, I expect that core inflation will slow to around 2 percent over the next couple of years. Although I believe that inflation expectations will play a primary role in determining the course of inflation, I want to emphasize that neither economists nor policymakers understand the expectations-formation process very well. However, one aspect of expectations formation that we have come to regard as crucial is the credibility of monetary policy. Consistent with its dual mandate to foster maximum sustainable employment and price stability, the Fed must therefore continue to respond aggressively to shocks that have potentially persistent adverse effects on both inflation and real activity. And we need to monitor long-run inflation expectations closely to avoid losing credibility with the markets.\n\nIn closing, I want to emphasize that the Federal Reserve will continue to play its part in ensuring the longer-run health of the economy by implementing policies designed to achieve its dual mandate. As you know, since last June the FOMC has left its target for the federal funds rate at 5-1/4 percent. I recognize that uncertainties surrounding the economic outlook have increased recently, and I remain concerned that the persistence of inflation at the recent elevated rate could have adverse consequences for economic performance. However, I continue to believe that the current stance of monetary policy is likely to foster sustainable economic expansion and a gradual ebbing in core inflation. As always, future policy adjustments will depend on the evolution of the outlook for both inflation and economic growth as implied by incoming information.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/mishkin20070420a.htm",
        "title": "The U.S. Economic Outlook",
        "date": "4/20/2007"
    },
    {
        "content": "April 11, 2007\n\nChairman Ben S. Bernanke\n\nAt the New York University Law School, New York, New York\n\nMarket forces determine most outcomes in our economy, a fact that helps to explain much of our nation’s success in creating wealth. Markets aggregate diffuse information more effectively and set prices more efficiently than any central planner possibly could. The result is powerful competitive incentives for businesses to produce, at the least cost, the goods and services that our citizens value most. Writing in the eighteenth century, Adam Smith conceived of the free-market system as an “invisible hand” that harnesses the pursuit of private interest to promote the public good. Smith’s conception remains relevant today, notwithstanding the enormous increase in economic complexity since the Industrial Revolution.\n\nAlthough the market system is the principal source of America’s economic dynamism, economic theory and practice both suggest that targeted government regulation and intervention can sometimes benefit the economy. In the particular case of financial markets, for example, government regulation helps to promote general financial stability and to protect investors and consumers against fraud. Of course, the benefits of regulation come with direct and indirect costs. Direct costs include those arising from compliance with a thicket of complicated rules--costs that can add significantly to a firms’ costs of production, leading to higher prices for consumers. Indirect costs include reductions in innovation or competition that can result from overly restrictive regulations. Therefore, before government decides to regulate in a particular situation, it must weigh the social costs and benefits of the contemplated intervention.\n\nIn recent decades, public policy has been increasingly influenced by the insight that the market itself can often be used to achieve regulatory objectives. For example, in the area of environmental protection, the trading of emissions permits has been widely embraced as a cost-effective means of controlling pollution. That market-based approach is regulation by the invisible hand, as opposed to the very visible hand of direct government regulation and enforcement. The invisible-hand approach to regulation aims to align the incentives of market participants with the objectives of the regulator, thereby harnessing the same powerful forces that allow markets to work so efficiently. In the financial arena, as I will discuss, this approach often takes the form of creating incentives for market participants to monitor and control the risk-taking behavior of financial firms--that is, to exert market discipline--thereby reducing the need for direct oversight by the government.\n\nToday I would like to explore the market-based approach to financial regulation by considering its application to two important--but very different--types of financial institutions: commercial banks and hedge funds. For both types of institutions, market-based regulation has proven an effective supplement to (or substitute for) conventional command-and-control approaches.\n\nCommercial Banks\nFor much of U.S. history, bank shareholders and creditors--not government regulators--were responsible for overseeing banks’ lending and investment decisions. For example, in the years between the demise of the Second Bank of the United States, in 1836, and the National Banking Act of 1863, private commercial banks issued non-interest-bearing notes that served as the principal medium of exchange.\n\nAlthough the notes issued by each bank were supposedly redeemable for gold (or other safe assets, such as government bonds), they did not always trade at face value in every location. Instead, notes issued by different banks traded at variable discounts that depended on (among other things) the perceived financial condition of the issuing bank and the distance a note-holder would have to travel to redeem the notes (Gorton, 1996). Banks did not like to see their notes trade for much less than face value, however, because holders of deeply discounted notes had an incentive to demand the face amount in gold from the issuing bank. If too many note-holders redeemed their notes, the bank might have to pay out all its reserves of gold and close down. Concern about potential note redemptions discouraged banks from taking excessive financial risks that might cause their notes to trade at a deeper discount and, consequently, trigger increased redemptions. Market forces thereby exerted discipline on banks’ activities and the types of assets they held.\n\nSimilarly, later in the nineteenth century, when demand deposits became the primary form of bank liability, bankers recognized that any loss of confidence by depositors might provoke a run--panicky withdrawals--that could force the bank to close its doors. Presumably the potential for deposit runs led banks to take fewer risks than they might have otherwise--another instance of market discipline (Calomiris and Gorton, 1991).\n\nAlthough the actions of note-holders and depositors constrained banks’ risk-taking, they had some undesirable side effects. Throughout the nineteenth century and in the early decades of the twentieth, bank runs often helped to precipitate general financial panics that shut down large parts of the financial system and constricted economic activity. Bank runs, by extinguishing deposits, also tended to sharply reduce the nation’s money supply. And they sometimes wreaked considerable hardship on the depositors themselves. Certainly, from a modern perspective, putting the burden of monitoring banks on small depositors seems inappropriate. Most retail depositors don’t have the time and resources to gather information on bank assets and investments, and typically they cannot absorb losses when a bank fails.\n\nIn response to this series of financial panics, the Congress in 1913 founded the Federal Reserve to provide the nation with a safer, more flexible, and more stable monetary and financial system. Specifically, the Fed was established \"to furnish an elastic currency, to afford means of rediscounting commercial paper, [and] to establish a more effective supervision of banking in the United States.\"1 Such powers can be used to provide liquidity when an otherwise solvent bank experiences unexpected and widespread withdrawals during a financial panic. Today, the Federal Reserve's discount window and its oversight of the payment system are core components of the so-called federal \"safety net\" provided to banks.\n\nHowever, the Federal Reserve failed to avert the banking panics of the Great Depression of the 1930s, the longest and most severe series of banking panics in U.S. history. Thousands of banks failed, many after suffering runs on their deposits. As a consequence, the Congress added another element to the safety net when it established government-backed deposit insurance. Deposit insurance both protects depositors (subject to limits on the size and type of account) and reduces the liquidity risks faced by banks. It has achieved its objective in that runs on insured institutions have virtually disappeared.\n\nHowever, despite its evident benefits, deposit insurance created a new type of problem. Unlike the note-holders and depositors of earlier years, insured bank depositors, who know that their funds are protected even if their bank fails, have little or no incentive to evaluate the risk-taking activities of their bank. In the absence of other measures, this safety net reduces the restraint that creditors would otherwise place on unsound lending because the insurance reduces the amount of money they have at stake. The tendency of banks to take on excessive risks when they face little economic penalty for doing so is an example of what economists call a moral hazard problem.\n\nTo help counter the moral hazard created by deposit insurance, and to try to ensure that banks operate safely and soundly, the United States has developed an extensive system of banking supervision and regulation. In effect, regulators assumed the role of monitor, the role that insured depositors no longer have any incentive to play. Somewhat ironically, however, this system of government supervision may itself exacerbate moral hazard if the banks’ uninsured creditors assume that the government’s oversight obviates any need for them to monitor banks. Market discipline may erode further if market participants believe that, to avoid the risk of a financial crisis, the government will step in to prevent the failure of any very large institution--the “too big to fail” problem. With little or no help from market forces, the burden of ensuring banks’ soundness falls entirely on the supervisory agencies. But as we saw in the savings and loans crisis of the 1980s, regulatory oversight may also fail if the regulators are not sufficiently vigilant or lack the political will or financial resources to promptly close insolvent institutions.\n\nThe lesson of history appears to be that neither market discipline nor regulatory oversight alone is completely adequate for keeping the banking system safe and sound. However, regulators have increasingly come to appreciate the value of a hybrid system that supplements direct regulation with a substantial amount of market discipline. Fortunately, regulators have a variety of ways to restore and strengthen market discipline for banks, notwithstanding the existence of the federal safety net.\n\nMinimum capital requirements are one method. A bank’s capital provides a buffer that can absorb losses before they fall on the deposit insurance fund (and, ultimately, the taxpayers). In addition, however, capital requirements enhance market discipline; they do so by ensuring that the bank will have shareholders (and possibly other types of creditors as well) who have a significant amount of money at risk. These shareholders have a substantial financial incentive to monitor the bank’s activities and to insist on changes if they are dissatisfied. Bank managers are also quite aware that a low share price could invite a hostile takeover, an outcome they usually want to avoid.\n\nThe incentives for a bank to control risk are even stronger if regulations link the amount of regulatory capital to the risks taken by banks. (Tying capital requirements to risk is a primary objective of the Basel II capital accord, currently in the process of review and implementation.) Risk-based capital regulations, if properly designed, require banks that take more risks to hold more capital. Because equity capital is the most expensive form of funding, this linkage gives banks an incentive to better measure and control risks.\n\nCredible receivership provisions for insolvent banks are another method of enhancing market discipline. Effective market discipline requires that uninsured investors believe they could lose some, or all, of their stake. This belief is especially important in the case of very large banks, which investors may otherwise perceive to be too big to fail. Receivership rules that make clear that investors will take losses when a bank becomes insolvent should increase the perceived risk of loss and thus also increase market discipline.\n\nIn the United States, the banking authorities have ensured that, in virtually all cases, shareholders bear losses when a bank fails. Historically, however, bondholders and uninsured depositors have at times doubted that regulators would impose significant losses on them in the event of a bank’s failure. To address this issue, the Congress has reduced regulators’ discretion when dealing with troubled banks. For example, the requirement for prompt corrective action prohibits regulatory forbearance when a bank’s capital falls to a predetermined level; and the least-cost-resolution requirement compels regulators to resolve a troubled bank at the lowest cost to the deposit insurance fund.2\n\nImproving and broadening the requirements for disclosing information are yet another method of strengthening the invisible hand. For example, the part of the Basel II capital initiative called Pillar 3 requires banks to release additional information to the public about their risk-taking. Pillar 3 should thus help investors, creditors, and other counterparties better assess banks’ risk profiles. Transparency about risk has become all the more important because modern financial assets increasingly entail greatly different degrees of economic leverage per dollar invested.\n\nWhen pursuing regulatory objectives through the application of market discipline, regulators must consider the nature of the incentives faced by different types of stakeholders in varying circumstances as well as the ways each type of stakeholder affects the bank’s risk-taking. For example, as I mentioned, bank capital usually gives equity investors an incentive to monitor and control risk. But when a bank falls into financial distress, its capital may be all but depleted, and its equity investors may thus have little left to lose. Consequently, equity holders may “gamble for resurrection” by encouraging rather than discouraging excessive risk-taking. Thus, as was evident in the savings and loan crisis of the 1980s, market discipline by equity holders may break down when it is most needed.\n\nThis example suggests that the mix of instruments a bank uses to raise funds can matter. For instance, holders of uninsured debt (such as large certificates of deposits or corporate debt issues) care mostly about the risk of bankruptcy. They don’t benefit if the bank’s stock price rises when undue risk-taking pays off. Consequently, they focus on what bank managers are doing to avoid default. The incentive to monitor risk-taking is particularly keen for holders of subordinated debt, as they are last in line in the event of failure. Because debt holders are sensitive to changes in the probability of financial distress, risk-taking by a bank raises its cost of funding in credit markets, and that connection creates an incentive for banks to control risks. Moreover, the price of a bank’s debt provides useful information about the bank’s riskiness. With that information, the bank’s counterparties and supervisors can take steps of their own to ensure that the bank is operating safely.3\n\nHedge Funds\nHedge funds provide a second illustration of how the invisible hand can be used to support regulatory objectives. Their rapid growth is one of the most important developments in U.S. financial markets in the past decade or so. Hedge funds vary widely in their investment strategies and in the types of risks they take. Overall, however, most economists agree that the rise of hedge funds has been a positive development for investors and for financial markets. They have stimulated an extraordinary amount of financial innovation in recent years; and, using many of these new financial tools, they have greatly enhanced the liquidity, efficiency, and risk-sharing capabilities of our financial system.\n\nRegulatory oversight of hedge funds is relatively light. Because hedge funds deal with highly sophisticated counterparties and investors, and because they have no claims on the federal safety net, the light regulatory touch seems largely justified. However, the growing market share of hedge funds has raised concerns about possible systemic risk. The complexity and rapid change inherent in the strategies of many funds make them relatively opaque to outsiders, and so the concern arises that the collapse of a hedge fund might come with little warning. In addition, many hedge funds are either highly leveraged or hold positions in derivatives or other assets that make their net asset positions very sensitive to changes in asset prices (the functional equivalent of high leverage). Highly leveraged investors are intrinsically more vulnerable to market shocks, of course, but leverage also increases the risks to the broader financial system. The failure of a highly leveraged fund holding large, concentrated positions could involve the forced liquidation of those positions, possibly at fire-sale prices, thereby imposing heavy losses on counterparties. In the worst scenarios, these counterparty losses could lead to further defaults or threaten systemically important institutions. In addition, market participants that were not creditors or counterparties of the defaulting firm might be harmed indirectly through changes in asset prices, liquidity strains, and increased market uncertainty.\n\nAs I have noted, the market discipline provided by creditors and investors is potentially a powerful mechanism for controlling leverage and other aspects of risk-taking. But market discipline can fail, as is illustrated by one notable case--the hedge fund Long-Term Capital Management (LTCM), which was at the center of an episode of severe financial stress in 1998. Perhaps because of the stellar reputations of LTCM’s principals, banks and broker-dealers provided credit on generous terms, even though LTCM took exceptional risks. LTCM’s investors and counterparties simply did not ask the tough questions necessary to understand the risks they were taking. Together with the admittedly extraordinary market conditions of August 1998, these risk-management lapses were a major source of the LTCM crisis.\n\nIn response to the LTCM episode, the Congress might have imposed a much more intrusive regulatory regime on private pools of capital. However, doing so would have been costly and technically difficult, would have increased moral hazard by relieving investors and counterparties of the responsibility for monitoring the funds, and likely would have reduced the social benefits of hedge funds by hampering the ability of their managers to respond quickly and flexibly to changing market conditions. Instead, the regulatory approach taken in the United States has followed recommendations set forth in 1998 by the President’s Working Group on Financial Markets and recently reaffirmed in a set of principles by the same group.4\n\nThe market-discipline approach to regulating hedge funds imposes responsibilities on four sets of actors: hedge fund investors, creditors and other counterparties, the regulatory agencies, and the hedge funds themselves.\n\nIn discussing the regulation of commercial banks, I noted that most small retail investors are ill-equipped to provide effective market discipline because monitoring complex financial activities demands considerable time, effort, and sophistication. In the case of hedge funds, securities laws effectively allow only institutions and high-wealth individuals to invest in them. These investors generally have the resources and sophistication, as well as the incentive, to monitor the activities of the hedge funds. Large investors are not only well equipped to assess the management, strategies, performance, risk-management practices, and fee structures of individual hedge funds but they also have the clout to demand the information they need to make their evaluations. Although regulations limit the direct access of retail investors to hedge funds, small investors may obtain indirect exposure, through pension funds for example. However, managers of pension funds and similar institutions generally have a fiduciary duty to their investors to research and understand their investments and to ensure that their overall risk profile is appropriate for their clientele. In practice, most pension funds have only a small exposure to hedge funds.\n\nCounterparties are another important source of market discipline. The principal counterparties of most hedge funds are large commercial and investment banks, which provide the funds with credit and a range of other services. As creditors, counterparties have a clear economic incentive to monitor and perhaps impose limits on hedge funds’ risk-taking, as well as an incentive to protect themselves from large losses should one or more of their hedge-fund customers fail. Counterparties seek to protect themselves against large losses through risk management and risk mitigation. Risk management includes the use of stress tests to estimate potential exposure under adverse market conditions; risk-mitigation techniques include collateral agreements under which hedge funds must daily mark to market and fully collateralize their current exposures.\n\nThe incentives of hedge fund counterparties line up well with regulators’ objectives, which include not only constraining excess risk-taking by hedge funds but also preventing losses that would threaten the stability of other major financial market participants. However, for various reasons, including competitive pressures and the existence of the safety net for some counterparties, private counterparties may not fully account for risks to general financial stability. Thus, supervisors seek to ensure that hedge-fund counterparties--primarily very large commercial and investment banks--protect themselves and, in so doing, protect the broader financial system. Supervisors also monitor markets and key institutions, coordinate with their domestic and foreign counterparts, and work with the private sector to strengthen market infrastructures. For example, the Federal Reserve Bank of New York has been leading joint public-private efforts to improve the clearing and settlement of credit derivatives. Coordination of this type can improve market functioning and reduce risks to financial stability without harming market discipline.\n\nFinally, in a system of market-based discipline, hedge-fund managers themselves have both the incentive and the responsibility to manage risk effectively, to develop consistent methods for valuing assets and liabilities, and to provide timely and accurate information to their investors, creditors, and counterparties.\n\nThus far, the market-based approach to the regulation of hedge funds seems to have worked well, although many improvements can still be made (Bernanke, 2006). In particular, risk-management techniques have become considerably more sophisticated and comprehensive over the past decade. To be clear, market discipline does not prevent hedge funds from taking risks, suffering losses, or even failing--nor should it. If hedge funds did not take risks, their social benefits--the provision of market liquidity, improved risk-sharing, and support for financial and economic innovation, among others--would largely disappear.\n\nConclusion\nI have argued today that, in many situations, regulation that relies on the invisible hand of market-based incentives can complement direct government regulation. For market-based regulation to work, the incentives of investors and other private actors must align with the objectives of the government regulator. In particular, private investors must be sophisticated enough to understand and monitor the financial condition of the firm and be persuaded that they will experience significant losses in the event of a failure. When these conditions are met, market discipline is a powerful and proven tool for constraining excessive risk-taking.\n\nReferences\n\nBenston, George J., Robert A. Eisenbeis, Paul M. Horvitz, Edward J. Kane, and George C. Kaufman (1986). Perspectives on Safe and Sound Banking: Past, Present, and Future, Regulation of Economic Activity Series. Cambridge, Mass.: MIT Press.\n\nBernanke, Ben (2006). “Hedge Funds and Systemic Risk,” speech delivered at the Federal Reserve Bank of Atlanta Financial Markets Conference, May 16.\n\nBoard of Governors of the Federal Reserve System and U.S. Department of the Treasury (2000). The Feasibility and Desirability of Mandatory Subordinated Debt, report to the Congress, December, www.federalreserve.gov/boarddocs/rptcongress/debt/subord_debt_2000.pdf.\n\nCalomiris, Charles, and Gary Gorton (1991). “The Origins of Banking Panics: Models, Facts, and Bank Regulation,” in R. Glenn Hubbard, ed., Financial Markets and Financial Crises. Chicago: University of Chicago Press, pp. 109-73.\n\nEvanoff, Douglas D., and Larry Wall (2000). “The Role of Subordinated Debt in Bank Safety and Soundness Regulation,” in The Changing Financial Industry Structure and Regulation: Bridging States, Countries, and Industries, proceedings of the Thirty-Sixth Annual Conference on Bank Structure and Competition. Chicago: Federal Reserve Bank of Chicago, pp. 480-93.\n\nGorton, Gary (1996). “Reputation Formation in Early Bank Note Markets,” Journal of Political Economy, vol. 104 (April), pp. 346-97.\n\nLang, William W., and Douglas D. Robertson (2002). “Analysis of Proposal for a Minimum Subordinated Debt Requirement,” Journal of Economics and Business, vol. 54 (January-February), pp. 115-36.\n\nFootnotes\n\n1. Federal Reserve Act, December 23, 1913. Return to text\n\n2. The least-cost-resolution requirement can be waived if a determination is made that adherence to it will create a systemic risk; however, this exception was intentionally made quite difficult to invoke. Return to text\n\n3. The potential advantages of creating market discipline through a mix of financial instruments, including subordinated debt, have been discussed for many years by banking economists, and a number of interesting proposals for increasing effective market discipline through restrictions on bank capital structures have been made; refer to Benston and others (1986), Evanoff and Wall (2000), Lang and Robertson (2002), and Board of Governors and U.S. Department of the Treasury (2000).  Many of these proposals involve a requirement that banks issue subordinated debt.  If markets are working well, the required yield on a bank’s subordinated debt should be a good indicator of the riskiness of the bank. Return to text\n\n4. The recent PWG work is at www.ustreas.gov/press/releases/hp272.htm. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070411a.htm",
        "title": "Financial Regulation and the Invisible Hand",
        "date": "4/11/2007"
    },
    {
        "content": "April 10, 2007\n\nGovernor Frederic S. Mishkin\n\nAt Bridgewater College, Bridgewater, Virginia\n\nThank you for inviting me to talk with you today.1 I would like to direct my remarks to the macroeconomic mission of the Federal Reserve System. I wish to note that these remarks reflect only my own views and not those of the Federal Reserve Board or of anyone else associated with the Federal Reserve System.\n\nIn a democratic society like our own, the ultimate purpose of the central bank is to promote the public good by pursuing a course of monetary policy that fosters economic prosperity and social welfare. In the United States, as in virtually every other country, the central bank has a more specific set of objectives that have been established by the government. This mandate was originally specified by the Federal Reserve Act of 1913 and was most recently clarified by an amendment to the Federal Reserve Act in 1977.\n\nAccording to this legislation, the Federal Reserve's mandate is \"to promote effectively the goals of maximum employment, stable prices, and moderate long-term interest rates.\" Because long-term interest rates can remain low only in a stable macroeconomic environment, these goals are often referred to as the dual mandate; that is, the Federal Reserve seeks to promote the two coequal objectives of maximum employment and price stability. In the remainder of my remarks today, I will describe how these two objectives are consistent with our ultimate purpose of fostering economic prosperity and social welfare. I will then talk about some important practical challenges in implementing these goals.\n\n(By the way, I wish that I could also discuss the Federal Reserve's role in promoting the stability of the financial system, another key objective of central banks, but unfortunately that would violate my own personal mandate of finishing this speech in the allotted time.)\n\nNow with respect to the first objective, the rationale for maximizing employment is fairly obvious. The alternative situation--high unemployment--is associated with human misery, including lower living standards and increases in poverty as well as social pathologies such as loss of self-esteem, a higher incidence of divorce, increased rates of violent crime, and even suicide. Furthermore, when unemployment is high, the economy has idle workers along with a reduced level of production and household income. And when factories are idle, firms generally choose not to invest in additional plant and equipment, which in turn has adverse consequences for subsequent labor productivity and economic growth. All these symptoms of high unemployment were observed during the economic devastation of the Great Depression during the 1930s, which is now just a fading memory. But even less severe recessions are associated with painful consequences for many individual workers and their families.\n\nWith respect to the second objective--that of price stability--there is now a broad consensus among policymakers, academic economists, and the general public in support of the principle that maintaining a low and stable inflation rate provides lasting benefits to the economy. In particular, low and predictable inflation promotes social welfare by simplifying the savings and retirement planning of individual households and by facilitating firms' production and investment decisions. Furthermore, an environment of overall price stability contributes to economic efficiency by reducing the variability of relative prices and by minimizing the distortions that arise because the tax system is not completely indexed to inflation.\n\nPrice stability also has important benefits in terms of equity. For example, an elevated inflation rate typically increases poverty because the poorest members of society do not have access to the sorts of financial instruments that would help protect them against inflation. By the way, these are not just theoretical arguments: The experience of the United States in the 1970s, and that of many other economies across a wide range of times and circumstances, demonstrates that high and unstable inflation generally detracts from the standard of living, hinders the process of capital formation and economic growth, and in some countries has even led to political and social instability. Such episodes also show that a full recovery from the adverse effects of severe inflation can take many years.\n\nAlthough I could spend all morning extolling the virtues of the dual mandate, let me now turn to describing some of the challenges that the Federal Reserve faces in implementing this mandate.\n\nThe first challenge is determining how to interpret the dual mandate. Of course, the Federal Reserve doesn't take a literal approach to the goal of maximum employment. In that case, our policies would need to be directed at getting everyone to work at least one hundred hours a week, and we would have to discourage senior citizens from retiring and young people from attending college instead of entering the labor force. Furthermore, every modern economy has a certain level of \"frictional\" unemployment, which reflects the transitory periods over which individuals remain voluntarily unemployed while searching for a new job. Partly for these reasons, Federal Reserve officials and other policymakers often refer to this aspect of the dual mandate as the goal of maximum sustainable employment, and they place particular emphasis on the word sustainable.\n\nSimilarly, in promoting the goal of price stability, policymakers have generally not taken this goal literally--by aiming at complete constancy of the price level--but instead have pursued policies aimed at maintaining a low and predictable inflation rate. In particular, at a congressional hearing in mid-1988, then-Chairman Greenspan defined price stability as an environment in which households and businesses \"can safely ignore the possibility of sustained, generalized price increases or decreases\" in making their saving and investment decisions.2\n\nNow let's turn to the practical challenges in conducting monetary policy to achieve the dual mandate.\n\nA central element in successful monetary policy is a strong commitment to a nominal anchor, that is, the use of monetary policy actions and statements to maintain low and stable inflation. During the 1980s and 1990s, the Federal Reserve succeeded in bringing inflation down from double-digit levels to the average rate of about 2 percent that has prevailed over the past decade. Moreover, when some measures of inflation were close to 1 percent in 2003, the Federal Open Market Committee's official statements specifically noted that any further substantial decline in inflation would be unwelcome, mainly because of the risk that a falling price level (which has not occurred since the Great Depression) could cause a significant disruption to economic activity and employment.\n\nIn recent years, the Federal Reserve has been quite successful in maintaining a nominal anchor. Not only has the inflation rate remained within a reasonably narrow range, but inflation expectations, as measured by spreads between inflation-indexed and non-inflation-indexed Treasury securities and by surveys of professional forecasters and the general public, have also been well anchored.\n\nMaintaining price stability is also essential for achieving the other element of the dual mandate, namely, maximum sustainable employment. First, as I have already emphasized, a low and predictable inflation rate plays a crucial role in facilitating long-term growth in employment and labor productivity. Second, although the economy will inevitably be buffeted by various shocks, in the majority of circumstances the appropriate monetary policy response to stabilize inflation also helps to stabilize employment and output fluctuations around their maximum sustainable levels. In other words, the two elements of the dual mandate are usually complementary.\n\nTo see how a commitment to price stability leads to appropriate policy actions to stabilize employment and output fluctuations, we need to understand that there are two key determinants of inflation: inflation expectations and the amount of slack in the economy.3 Maintaining a nominal anchor helps stabilize inflation expectations, which in turn means that rises or falls in inflation tend to be highly correlated with economic slack. Thus, stabilizing inflation also helps to stabilize economic activity around sustainable levels.\n\nTo see further how this process would work, consider a negative shock to aggregate demand (such as a decline in consumer confidence) that causes households to cut spending. The drop in demand leads, in turn, to a decline in actual output relative to its potential, that is, the level of output that the economy can produce at the maximum sustainable level of employment. As a result, future inflation will fall below levels consistent with price stability, and the central bank will pursue an expansionary policy to keep inflation from falling. The expansionary policy will then result in an increase in demand that raises output back up to potential output in order to return inflation to a level consistent with price stability.\n\nFor example, during the last recession the Federal Reserve reduced its target for the federal funds rate a total of 5-1/2 percentage points, and this stimulus not only contributed to economic recovery but also helped avoid an unwelcome further decline in inflation. In other cases, a tightening of the stance of monetary policy is needed to prevent an \"overheating\" of economic activity, thereby avoiding a boom-bust cycle in the level of employment as well as an undesirable upward spurt of inflation.\n\nA strong commitment to price stability helps reduce fluctuations in employment and output in other ways. First, when inflation expectations are well anchored, a central bank will not have to worry that expansionary policy to counter a negative demand shock will lead to a sharp rise in expected inflation--a so-called inflation scare--that will then push up actual inflation in the future. Thus, a strong commitment to a nominal anchor enables a central bank to be more aggressive in the face of negative shocks and therefore to prevent rapid declines in employment or output.\n\nMoreover, with a strong commitment to a nominal anchor, supply shocks to inflation, such as a rise in relative energy prices, are likely to have only a temporary effect on inflation. This result is exactly what we have seen in the United States. Because people are confident that the Fed will not allow inflation to remain high, the recent sharp run-up in oil prices did not lead to a sustained rise in longer-run inflation expectations. As a result, inflation rose temporarily but has now been falling back again. When inflation expectations are well-anchored, the occurrence of an adverse aggregate supply shock does not necessarily mean that the central bank must raise interest rates aggressively in order to keep inflation under control, and hence the commitment to price stability can help avoid imposing unnecessary harm on the economy and on the workers who are most vulnerable to a weakening of economic activity.\n\nNow that we see the benefits of maintaining a commitment to a nominal anchor, one might naturally think that there would also be benefits to establishing a similar sort of anchor for the maximum level of employment. But that thought would be incorrect.\n\nIn particular, although the Federal Reserve can determine and achieve the long-run average rate of inflation in keeping with its mandate of price stability, the level of maximum sustainable employment is not something that can be chosen by the Federal Reserve because no central bank can control the level of real economic activity or employment over the longer run. As I've already emphasized, monetary policy can certainly help improve the maximum sustainable employment of the economy by maintaining low and predictable inflation. But any attempt to use stimulative monetary policy to maintain employment above its long-run sustainable level will inevitably lead to an upward spiral of inflation and therefore will actually undermine the productive capacity of the economy, with severe adverse consequences for household income and employment.\n\nIndeed, the level of maximum sustainable employment is primarily driven by the fundamental structure of the economy, including factors such as demographics, people's preferences, the efficiency of labor markets, the characteristics of the tax code, and so forth. And many policies outside the control of the Fed can have a significant effect on the efficiency of the economy and hence on the maximum sustainable level of employment.\n\nAnother crucial challenge in implementing the dual mandate is that the level of maximum sustainable employment cannot be directly observed and is subject to considerable uncertainty. Indeed, economists do not even agree on the economic theory or econometric methods that should be used to measure the level of maximum sustainable employment. This challenge would be less formidable if the structure of the U.S. economy remained fairly constant over a sufficiently long period of time. In that case, a reasonably good estimate of the natural unemployment rate--the unemployment rate consistent with maximum sustainable employment--could be obtained from the actual long-term average rate of unemployment, and one could similarly estimate the path of potential output by fitting a trend to actual gross domestic product (GDP) data. In fact, however, such estimates can be highly misleading because the structure of the U.S. economy is highly dynamic and evolves almost continuously over time.\n\nIn particular, over the past few decades the natural unemployment rate and the path of potential output have apparently moved around quite substantially. If we do not recognize the potential for such shifts, they can pose serious pitfalls for the conduct of monetary policy. It is difficult to gauge these structural changes even with the benefit of hindsight, and it is even more difficult to recognize such developments when they occur. For example, most economists now agree that the natural unemployment rate shifted upward in the late 1960s and that potential output growth shifted downward after 1970. However, perhaps because these shifts were not generally recognized until much later, monetary policy in the 1970s seems to have been aimed at unsustainable levels of output and employment, and hence policymakers may have unwittingly contributed to a series of boom-bust cycles as well as accelerating inflation that reached double digits by the end of the decade. And although subsequent monetary policy tightening was successful in bringing inflation back under control, the toll was a severe recession in 1981-82, which pushed up the unemployment rate to around 10 percent.\n\nThe opposite problem occurred in the second half of the 1990s, when the common view among economists was that the natural rate of unemployment was near 6 percent. When the actual unemployment rate dropped close to 4 percent, and the economy was growing at a rate that many economists viewed as unsustainable, there were widespread calls for the Fed to raise interest rates to prevent an acceleration in inflation. Under Chairman Greenspan's leadership, however, the Federal Reserve resisted these views because it did not see inflation pressures developing and questioned whether the common estimate of the natural rate of unemployment was correct. The result was that the Fed did not tighten monetary policy, and the economy reaped the benefits of the new economy: very low levels of unemployment and a modest decline in the inflation rate.\n\nAttempting to anchor a particular rate of growth of output and employment in the longer term could therefore lead to seriously flawed policies. As I have already noted, an attempt by the Fed to achieve growth that is higher than the underlying growth rate of productive capacity would lead to unnecessary fluctuations of employment and inflation. And trying to achieve growth that is lower than the economy's true potential growth rate would create unnecessary unemployment and generate deflationary pressures that would subsequently have to be reversed.\n\nGiven the possible pitfalls of trying to anchor the level of output or employment, what is the best way for a central bank to minimize deviations of employment from its maximum sustainable level?\n\nTo be sure, central banks need to form some views about the economy's potential to produce on a sustained basis. After all, as I have already noted, the amount of slack in the economy is a key determinant of inflation. But, rather than focusing on fixed estimates of potential output or the natural rate of unemployment, central banks should take an eclectic approach in assessing the overall balance of economic activity relative to productive capacity. In other words, in pursuing the dual mandate, the central bank should recognize that a wide variety of indicators drawn from labor, product, and financial markets provide information about the overall balance of supply and demand in the economy. In addition, central banks should use information from various price indicators to tell them whether the economy is overheating or running well below productive capacity.\n\nIn some circumstances, a temporary tradeoff between the two elements of the dual mandate may exist. Unforeseen shocks to the economy--an adverse supply shock, for example--might lead to inflation that is temporarily above levels consistent with price stability at the same time that employment is growing more slowly than its maximum sustainable pace. In such a situation, returning inflation too quickly to levels consistent with price stability might unnecessarily exacerbate the economic weakness. Instead, while restoring price stability remains critical, the central bank should do so at a pace that does not do undue harm to the economy.\n\nFinally, central banks should respond aggressively to output and employment fluctuations on those (hopefully rare) occasions when the economy is very far below any reasonable measure of its potential. In this case, errors in measuring potential output or the natural rate of unemployment are likely to be swamped by the large magnitude of resource gaps, so it is far clearer that expansionary policy is appropriate. Furthermore, taking such actions need not threaten the central bank's credibility in its pursuit of price stability.\n\nIt should be clear from my remarks today that although as a Federal Reserve official I am legally obligated to fulfill the dual mandate, I am actually an enthusiastic supporter. The best way to achieve the mandate is for the Federal Reserve to have a strong commitment to a nominal anchor to promote price stability, but with a focus on keeping employment as close as possible to its maximum sustainable level.\n\nFootnotes\n\n1. I want to thank Andrew Levin, Brian Madigan, and Spencer Dale for their extremely helpful comments on and assistance with this speech. Return to text\n\n2. Alan Greenspan (1988), statement before the Committee on Banking, Finance, and Urban Affairs (746 KB PDF), U.S. Senate, July 13. Return to text\n\n3. Two other important factors are import prices, which are determined by world market prices and exchange rates, and relative energy prices, such as the price of oil. However, a central bank has no control over world market or relative oil prices, and so adding these factors into the analysis does not change the basic conclusion here. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/mishkin20070410a.htm",
        "title": "Monetary Policy and the Dual Mandate",
        "date": "4/10/2007"
    },
    {
        "content": "March 30, 2007\n\nChairman Ben S. Bernanke\n\nAt the Community Affairs Research Conference, Washington, D.C.\n\nThis year marks the thirtieth anniversary of the Community Reinvestment Act (CRA). Enacted in 1977, the CRA affirmed the obligation of federally insured depository institutions to help meet the credit needs of communities in which they are chartered, consistent with safe and sound operations. The act also charged the federal bank regulatory agencies, including the Federal Reserve, with implementing the CRA through regulations and with examining banks and thrifts to determine whether they meet their CRA obligations.\n\nThe CRA presents an interesting case study of a regulatory regime that has evolved to adjust to changes in the economic, financial, and social environment. Since the CRA's enactment, the implementing regulations have been substantially amended three times--in 1989, 1995, and 2005. In each case, changes to the regulations reflected both experience gained in the implementation of the law as well as ongoing developments in financial markets and the economy. In my remarks today, I will survey some milestones in the evolution of the CRA, beginning with a description of the economic and social concerns that prompted the passage of the act. With this brief history as background, I will comment on the challenges we face in ensuring that the CRA remains effective and relevant in the future.\n\nThe Origins of the Community Reinvestment Act\nPublic and congressional concerns about the deteriorating condition of America's cities, particularly lower-income and minority neighborhoods, led to the enactment of the Community Reinvestment Act. In the view of many, urban decay was partly a consequence of limited credit availability, which encouraged urban flight and inhibited the rehabilitation of declining neighborhoods. Some critics pinned the blame for the lack of credit availability on mainstream financial institutions, which they characterized as willing to accept deposits from households and small businesses in lower-income neighborhoods but unwilling to lend or invest in those same neighborhoods despite the presence of creditworthy borrowers.\n\nSeveral social and economic factors help explain why credit to lower-income neighborhoods was limited at that time. First, racial discrimination in lending undoubtedly adversely affected local communities. Discriminatory lending practices had deep historical roots. The term \"redlining,\" which refers to the practice of designating certain lower-income or minority neighborhoods as ineligible for credit, appears to have originated in 1935, when the Federal Home Loan Bank Board asked the Home Owners' Loan Corporation to create \"residential security maps\" for 239 cities that would indicate the level of security for real estate investments in each surveyed city.1 The resulting maps designated four categories of lending and investment risk, each with a letter and color designation. Type \"D\" areas, those considered to be the riskiest for lending and which included many neighborhoods with predominantly African-American populations, were color-coded red on the maps--hence the term \"redlining\" (Federal Home Loan Bank Board, 1937). Private lenders reportedly constructed similar maps that were used to determine credit availability and terms. The 1961 Report on Housing by the U.S. Commission on Civil Rights reported practices that included requiring high down payments and rapid amortization schedules for African-American borrowers as well as blanket refusals to lend in particular areas.\n\nBesides discrimination a variety of economic and institutional factors help to explain the relative unavailability of credit in lower-income neighborhoods.2 Thirty years ago, the secondary market for mortgages was rudimentary at best, which limited local loan originators' access to capital and reduced their ability to diversify credit risks geographically.3 Informational problems also inhibited lending in some urban areas. For example, relative to higher-income neighborhoods, lower-income areas tend to have fewer home sales and more-diverse housing structures, making accurate appraisal more difficult4. Similarly, credit evaluations tend to be more costly for lower-income borrowers, who are relatively more likely to have short or irregular credit histories.5 Informational barriers to lending were heightened by the absence of uniform national depositories of information on the credit experiences of consumers; at the time, the credit-reporting system consisted of hundreds of local credit bureaus, each of which maintained limited information on local residents.6 The high costs of gathering information, together with the difficulty of keeping information proprietary, may have created a \"first-mover\" problem, in which each financial institution has an incentive to let one of its competitors be the first to enter an underserved market. Without some coordination, the first-mover problem may result in no institution choosing to incur the costs of entry (Lang and Nakamura, 1993; Barr, 2005; and Ling, 1998).\n\nThe regulatory environment of the period was yet another factor limiting broad access to credit. State and federal rules prohibited interstate branching or acquisitions and in some cases restricted even intrastate branching, reducing competition and the ability of lenders to diversify geographic risk.7 Also, interest rate ceilings on mortgages in some locations effectively blocked lending to potential borrowers judged to pose higher risks, and interest rate ceilings on deposits (notably, the infamous Regulation Q) led to periodic episodes of disintermediation and reduced availability of mortgage credit (Chomsisengphet and Pennington-Cross, 2006; and McNeil and Rechter, 1980).\n\nTaken together, these social, economic, and regulatory factors contributed to the perception that banking institutions were failing to adequately serve the credit needs of some residents of their communities, a concern that led the Congress to enact the CRA. The CRA reaffirmed the long-standing principle that financial institutions must serve \"the convenience and needs,\" including credit needs, of the communities in which they are chartered. The obligation of financial institutions to serve their communities was seen as a quid pro quo for privileges such as the protection afforded by federal deposit insurance and access to the Federal Reserve's discount window (FFIEC, 1992). Indeed, the Bank Holding Company Act, passed in 1956, had already required the Federal Reserve Board, when ruling on proposed acquisitions by banks or bank holding companies, to evaluate how well the institutions involved were meeting community needs, consistent with the requirements of safety and soundness.\n\nThe CRA was only one of a series of laws passed during the 1970s intended to reduce credit-related discrimination, expand access to credit, and shed light on lending patterns. The CRA itself focused on the provision of credit to low- and moderate-income communities rather than on discrimination by race, sex, or other personal characteristics. Legislation that addressed discrimination in lending explicitly included the Equal Credit Opportunity Act and the Fair Housing Act. The Home Mortgage Disclosure Act was enacted to increase transparency in the mortgage lending market and to support public and private investment activity. From an economic perspective, the CRA can be interpreted as an attempt to rectify market failures--for example, by inducing banks to invest in building the knowledge and expertise necessary to lend profitably in lower-income neighborhoods. Similarly, to the extent that the CRA encouraged coordinated or simultaneous efforts by banks to lend in underserved areas, it had the potential to reduce the first-mover problem.\n\nThe debate surrounding the passage of the CRA was contentious, with critics charging that the law would distort credit markets, create unnecessary regulatory burden, and lead to unsound lending. Partly in response to these concerns, the Congress included little prescriptive detail in the law. Instead, the CRA simply directs the banking regulatory agencies to ensure that banks serve the credit needs of their local communities in a safe and sound manner. In effect, the agencies were left with considerable discretion and flexibility to modify the rules in light of changes in the economy and in financial markets (Garwood and Smith, 1993). At times, this discretion has been the source of some uncertainty on the part of regulated institutions concerned with compliance. However, the flexibility has proved valuable in allowing the CRA to remain relevant despite rapid economic and financial change and widely differing economic circumstances among neighborhoods.\n\nThe Evolution of the CRA\nFor more than a decade after its enactment, the CRA was a rather low-profile banking regulation, one that set minimal compliance requirements for depository institutions and attracted limited supervisory attention from the bank regulatory agencies.8 By the late 1980s, however, the issues surrounding access to credit were attracting renewed interest. In response to this interest, the Congress included in the Financial Institution Reform and Recovery Act of 1989 (FIRREA) an amendment to the CRA statute to require public disclosure of institutions' ratings and performance evaluations. FIRREA also expanded data collection and made public certain data reported under the HMDA. With the requisite data becoming available, advocacy groups, researchers, and other analysts began to perform more-sophisticated, quantitative analyses of banks' records in meeting the credit needs of their communities.\n\nFurther attention to CRA was generated by the surge in bank merger and acquisition activities that followed the enactment of the Riegle-Neal Interstate Banking and Branching Efficiency Act of 1994. As public scrutiny of bank merger and acquisition activity escalated, advocacy groups increasingly used the public comment process to protest bank applications on CRA grounds. In instances of highly contested applications, the Federal Reserve Board and other agencies held public meetings to allow the public and the applicants to comment on the lending records of the banks in question. In response to these new pressures, banks began to devote more resources to their CRA programs. Many institutions established separate business units and subsidiary community development corporations to facilitate lending that would be given favorable consideration in CRA examinations. Local and regional public-private partnerships and multibank loan consortia also gained more prominence as banks developed strategies for expanding and managing CRA-related activities.\n\nEven as these developments were occurring, extensive change was taking place in the financial services sector. During the 1980s and 1990s, technological progress significantly improved data collection and information processing, which led to the development and widespread use of credit-scoring models and the availability of generic credit history scores. Deregulation also contributed to the changes in the marketplace. Notably, the lifting of prohibitions against interstate banking was followed by an increased pace of industry consolidation. Also, the preemption of usury laws on home loans created more scope for risk-based pricing of mortgages. Securitization of affordable housing loans expanded, as did the secondary market for those loans, in part reflecting a 1992 law that required the government-sponsored enterprises, Fannie Mae and Freddie Mac, to devote a percentage of their activities to meeting affordable housing goals (HUD, 2006). A generally strong economy and lower interest rates also helped improved access to credit by lower-income households.\n\nBankers were also gaining experience in underwriting and managing the risk of lending in lower-income communities. After years of experimentation, the managers of financial institutions found that these loan portfolios, if properly underwritten and managed, could be profitable. In fact, a Federal Reserve study found that, generally, CRA-related lending activity was at least somewhat profitable and usually did not involve disproportionately higher levels of default (Avery, Bostic, and Canner, 2000; see also Board of Governors, 1993). Moreover, community groups and nonprofit organizations began to take a more businesslike, market-oriented approach to local economic development, leading them to establish more-formalized and more-productive partnerships with banks. Community groups provided information to financial institutions on the needs of lower-income communities for credit and services, offered financial education and counseling services to community members, and referred \"bankable\" customers to partner banks. Specialized community development banks and financial institutions with the mission of providing financial services and credit to lower-income communities and families emerged and grew.\n\nPolicy developments bolstered the infrastructure and funding of community development lending organizations. Notably, the passage of the Community Development Financial Institutions (CDFI) Act of 1994 created the CDFI Fund at the Department of Treasury. The expansion of CDFIs provided banks with access to new opportunities to finance community economic development. Other initiatives, such as the federal Low Income Housing Tax Credit and New Markets Tax Credit programs provided vehicles for investing in affordable housing development and economic revitalization in distressed communities.\n\nEven as CRA-related lending became more extensive and more market-based, concerns were expressed about the implementation of the law. Financial institutions complained about compliance costs (Elliehausen, 1998). Both bankers and community groups criticized the CRA examination procedures as emphasizing process over results, arguing that the examination criteria were too subjective and that a more-quantitative system for evaluating institutions' CRA performance should be developed. In response to these criticisms, President Clinton in 1993 directed the agencies that implement CRA to review and revise the regulations, with the goals of clarifying performance standards, making examinations and evaluations more consistent, and reducing the compliance burden.\n\nThe CRA regulations adopted in 1995 established for large institutions a three-pronged test based on performance in the areas of lending, investments, and services. While the regulations placed the greatest emphasis on lending, they encouraged innovative approaches to addressing community development credit needs. Several provisions were included to reduce compliance costs, among them a new rule that allowed small banks to meet their requirements by means of a streamlined examination focused on lending activities.9\n\nWhen the new regulations were adopted, the agencies committed themselves to reviewing the regulations in 2002 to assess their effectiveness. The promised review made use of extensive public comment and scholarly research on the efficacy of CRA programs. In their comments on the proposed revisions to the rules, bankers and community organizations generally agreed that the fundamental elements of the regulations were sound and that the agencies should maintain the overall structure of the 1995 regulations, although each group raised a number of specific issues. Findings from the research by Board staff members, in combination with the public comments, led the agencies to propose new definitions for \"small\" banks, which would be subject only to a lending test to assess compliance with the CRA, and for \"intermediate small\" banks, which would be subject to a lending test as well as a new and more-flexible community development test (Avery, Canner, Mok, and Sokolov, 2005). In addition, the research underscored the benefit of expanding the definition of \"community development\" to include activities benefiting middle-income communities in distressed rural areas and in disaster areas. The final rule was adopted in July 2005.\n\nIn each of the major regulatory revisions, the goal of the regulators has been to increase the effectiveness of the CRA in promoting the economic development of lower-income communities while reducing the associated compliance burden. Again, making progress toward achieving these goals has been made easier by the flexibility of the original statute, which has allowed the regulators to adapt the rules to changing market and economic circumstances and to give financial institutions the latitude to meet their CRA obligations in diverse and cost-effective ways.\n\nHas the CRA achieved its objectives? Research on the CRA has tended to find positive net effects, but the results are not uniform. A paper by Board staff members compared census tracts just above and below the low- and moderate-income threshold, finding that the tracts below the threshold had higher homeownership rates, higher growth in owner-occupied units, and lower vacancy rates than would have otherwise been predicted (Avery, Calem, and Canner, 2003). An analysis by Harvard's Joint Center for Housing Studies concluded that the CRA has expanded access to residential mortgages for lower-income borrowers, but that research also finds that the CRA's effect is diminishing as mortgage lending by nonbank institutions expands (Apgar and Duda, 2003). Yet another review concludes that the CRA has been effective in helping to overcome market failures and reduce discrimination at a relatively low cost, precisely because the CRA sets forth a flexible standard rather than a rule (Barr, 2005). However, some critical studies have argued that the CRA has been ineffective in addressing discrimination and market failures and that its social costs outweigh its benefits (see, for example, Hylton, 2006, and Barr, 2005).\n\nThe CRA is clearly far from perfect. Although its objectives are broad and ambitious, its net effects on lower-income neighborhoods are difficult to measure with precision.10 Addressing CRA responsibilities also imposes costs on financial institutions. It appears that, at least in some instances, the CRA has served as a catalyst, inducing banks to enter underserved markets that they might otherwise have ignored. At its most successful, the CRA may have had a multiplier effect, supplementing its direct impact by stimulating new market-based, profit-driven economic activity in lower-income neighborhoods.\n\nThe Future of the CRA\nAs we look forward, the CRA will have to continue evolving to reflect the ongoing changes in financial markets and in the economy more generally. I will conclude by flagging just a few of the issues that will remain important for the implementation and the effect of CRA.\n\nFirst, for some institutions the concept of the \"local community\" is no longer as clear as it was when the CRA was enacted. Today, some institutions are not identified with a particular community but are regional or national in scope, which inevitably makes the definition of the relevant assessment areas somewhat difficult. Moreover, to an increasing extent, banks use nontraditional avenues--the Internet, for example--to interact with customers, in some cases avoiding a bricks-and-mortar presence altogether. To date, defining \"local community\" for the purposes of CRA assessment has been manageable as most banks still lend in local communities where they have deposit-taking facilities or branches. However, if these trends continue, defining a \"local community\" may become increasingly difficult, and the concept eventually may require reconsideration by regulators or even the Congress.\n\nSecond, changes in the structure of the financial industry have resulted in many financial transactions that fell under the CRA umbrella in 1977 having become increasingly the province of nondepositories not subject to CRA, including companies owned by banks or bank holding companies. Holding companies' nonbank affiliates, for instance, can be included in the CRA assessment of the banking institution at the discretion of the bank but need not be. Most mortgages are now packaged by brokers, and nearly two in three mortgages are originated by nondepositories not covered by the CRA.11 Nonbank institutions, such as payday lenders, check cashers, and remittance agents, are important sources of financial services in low- and moderate-income communities. In some cases, nonbank service providers offer convenience to customers but at prices that have raised concerns (Carr and Schuetz, 2001, and Barr, 2004).\n\nSome observers have suggested extending the CRA to nonbank providers, but this proposal neglects a fundamental premise of the CRA legislation--that banks incur special obligations in exchange for the advantages conferred by their charters, such as deposit insurance. Of course, the CRA is not the only tool for addressing such issues, should it be determined that consumers are not adequately protected in their dealings with nonbanks. The CRA may nevertheless have some role to play; for example, a possible question to consider is whether increasing the focus on services by banking institutions might encourage them to compete more actively with nonbank providers in lower-income neighborhoods.\n\nThird, access to credit in lower-income communities is obviously much greater today than when the CRA was enacted. This greater access has had tangible benefits, such as the increase in homeownership rates (Joint Center for Housing Studies, 2006). However, recent problems in mortgage markets illustrate that an underlying assumption of the CRA--that more lending equals better outcomes for local communities may not always hold.12 Whether, and if so, how to try to differentiate \"good\" from \"bad\" lending in the CRA context is an issue that is likely to challenge us for some time. One possible strategy is to place more weight in CRA examinations on factors such as whether an institution provides services complementary to lending--for example, counseling and financial education.\n\nThe CRA was created to help ensure lower-income communities have access to credit and financial services. When it passed the legislation, the Congress could not have foreseen the extensive changes in financial markets and the economy that have occurred over the past thirty years; thus, the decision to write the statute broadly and with considerable flexibility appears wise in retrospect. In implementing the law, the banking agencies have tried to learn from market developments, from research, and from the comments of financial institutions, consumers, and other interested parties. The regulations have thus changed over time in response to the changing financial landscape and as we have learned more about what works and what doesn't. We do not know how the economy and the financial system will change in coming decades, but it is safe to assume that change will be rapid. Considerable creativity and flexibility will thus be necessary to ensure that the CRA continues to assist community economic development without placing an undue burden on financial institutions.\n\n\n\nReferences\n\nAmel, Dean F., and Daniel G. Keane (1986). \"State Laws Affecting Commercial Bank Branching, Multibank Holding Company Expansion, and Interstate Banking,\" Issues in Bank Regulation, vol. 10 (Autumn), pp. 30-40.\n\nApgar, William, and Mark Duda (2003). \"The Twenty-fifth Anniversary of the Community Reinvestment Act: Past Accomplishments and Future Regulatory Challenges\" (209 KB PDF), FRBNY Economic Policy Review, vol. 9 (June), pp. 169-91.\n\nAvery, Robert B., Raphael W. Bostic, and Glenn B. Canner (2000). \"CRA Special Lending Programs\" (115 KB PDF), Federal Reserve Bulletin, vol. 86 (November), pp. 711-31.\n\nAvery, Robert B., Kenneth P. Brevoort, and Glenn B. Canner (2006). \"Higher-Priced Home Lending and the 2005 HMDA Data\" (580 KB PDF), Federal Reserve Bulletin (vol. 92), www.federalreserve.gov/pubs/bulletin.\n\nAvery, Robert B., Paul S. Calem, and Glenn B. Canner (2003). \"The Effects of the Community Reinvestment Act on Local Communities\" (89 KB PDF) paper presented at \"Sustainable Community Development: What Works, What Doesn't and Why,\" a conference sponsored by the Board of Governors of the Federal Reserve System, March 27-28.\n\nAvery, Robert B., Glenn B. Canner, Shannon C. Mok, and Dan S. Sokolov (2005). \"Community Banks and Rural Development: Research Relating to Proposals to Revise the Regulations that Implement the Community Reinvestment Act\" (149 KB PDF), Federal Reserve Bulletin (vol. 91), www.federalreserve.gov/pubs/bulletin.\n\nBarr, Michael S. (2004). \"Banking the Poor,\" Yale Journal on Regulation, vol. 21 (Winter), pp. 121-237.\n\nBarr, Michael S. (2005). \"Credit Where It Counts: The Community Reinvestment Act and Its Critics\" (558 KB PDF), New York University Law Review, vol. 80 (May), pp. 513-652.\n\nBoard of Governors of the Federal Reserve System (forthcoming). The Effects of Credit Scores and Credit-Based Insurance Scores on the Availability and Affordability of Financial Products. Washington: Board of Governors.\n\nBoard of Governors of the Federal Reserve System (2000). The Performance and Profitability of CRA-Related Lending. Washington: Board of Governors, July.\n\nBoard of Governors of the Federal Reserve System (1993). Report to the Congress on Community Development Lending by Depository Institutions.Washington: Board of Governors, October.\n\nCarr, James H., and Jenny Schuetz (2001). Financial Services in Distressed Communities:  Framing the Issue, Finding Solutions (188 KB PDF). Washington: Fannie Mae Foundation.\n\nChomsisengphet, Souphala, and Anthony Pennington-Cross (2006). \"The Evolution of the Subprime Market\" (553 KB PDF), Federal Reserve Bank of St. Louis Review, vol. 88 (January/February), pp. 31-56.\n\nElliehausen, Gregory (1998). The Cost of Banking Regulation: A Review of the Evidence, Staff Study 171. Washington: Board of Governors of the Federal Reserve System, April.\n\nFederal Financial Institutions Examination Council (1992). \"A Citizens Guide to CRA,\" Washington: FFIEC, June, pp. 3-5.\n\nFederal Home Loan Bank Board (1937). \"Appraisal Methods and Policies,\" Federal Home Loan Bank Review, vol. 3 (4), pp. 110-13, 120.\n\nGarwood, Griffith L., and Dolores S. Smith (1993). \"The Community Reinvestment Act: Evolution and Current Issues,\" Federal Reserve Bulletin, vol. 79 (April), pp. 251-67.\n\nGramlich, Edward M. (2007).  America's Second Housing Boom. Washington: The Urban Institute, UI Press.\n\nHiller, Amy (2003). \"Redlining and the Home Owners' Loan Corporation,\" Journal of Urban History, vol. 29 (May), pp. 207-9.\n\nHylton, Keith (2006). \"Development Lending and the Community Reinvestment Act,\" Law and Economic Working Paper Series 06-07. Boston: Boston University School of Law.\n\nJoint Center for Housing Studies of Harvard University (2006). The State of the Nation's Housing. Cambridge, Mass.: JCHS.\n\nLacker, Jeffrey M. (1995). \"Neighborhoods and Banking,\" Federal Reserve Bank of Richmond Economic Quarterly, vol. 81 (Spring), pp. 13-38.\n\nLang, William W., and Leonard I. Nakamura (1993). \"A Model of Redlining,\" Journal of Urban Economics, vol. 33 (Spring), pp. 223-34.\n\nLing, David C., and Susan M. Wachter (1998). \"Information Externalities and Home Mortgage Underwriting,\" Journal of Urban Economics, vol. 44 (November), pp. 317-32.\n\nMcNeil, Charles R., and Denise M. Rechter (1980). \"The Depository Institutions Deregulation and Monetary Control Act of 1980,\" Federal Reserve Bulletin, vol. 66 (June), pp. 444-53.\n\nOffice of the Comptroller of the Currency, Board of Governors of the Federal Reserve System, Federal Deposit Insurance Corporation, Office of Thrift Supervision, and National Credit Union Association (2006). \"Federal Financial Regulatory Agencies Issue Final Guidance on Nontraditional Mortgage Product Risks,\" press release, September 29, www.federalreserve.gov/newsevents/press/bcreg/20070929a.htm.\n\nUnited States Commission on Civil Rights (1961). \"Report on Housing,\" Washington: Government Printing Office.\n\nU.S. Department of Housing and Urban Development (2006). \"HUD's Regulation of Fannie Mae and Freddie Mac,\" HUD, www.hud.gov/offices/hsg/gse/gse.cfm.\n\nFootnotes\n\n1. The Home Owners' Loan Corporation (HOLC) was a New Deal agency established in 1933 to help stabilize real estate that had depreciated during the Depression and to refinance mortgage debt of economically distressed homeowners.  It granted fifteen-year mortgage loans at 5 percent interest to some 1 million homeowners between August 1933 and June 1936, the period it was authorized to originate new loans (Hiller, 2003).  Return to text\n\n2. Debate continues on the relative importance of racial bias and economic factors for explaining redlining and similar practices (see Lacker, 1995).  Return to text\n\n3. The Federal Reserve's Flow of Funds accounts did not even record private securitization activity until the early 1980s, and purchases by federal housing agencies, which were focused on government-backed loans and lower-risk conventional loans, represented less than 1 percent of total outstanding home mortgage debt in the years preceding enactment of the CRA. Return to text\n\n4. Analysis of decennial census and Home Mortgage Disclosure Act data indicates that lower-income areas have about half the number of owner-occupied homes and home purchase loans in a given year than do higher-income areas.  Return to text\n\n5. Although information from the period before the CRA is not available, a review of credit records from one of the national credit-reporting agencies today supports this conjecture, as it finds that individuals in lower-income areas have, on average, substantially shorter credit histories and only about half as many credit accounts.  Also, nearly 40 percent of the individuals in lower-income census tracts cannot be scored, a proportion nearly three times that found in higher-income areas.  This information comes from analysis by staff members of the Federal Reserve Board in conjunction with a report to the Congress (Board of Governors of the Federal Reserve System, forthcoming).   Return to text\n\n6. The low-cost summary measures of credit history that have gained widespread market acceptance today did not emerge until 1989, when Fair Isaac and Company developed the FICO score.  Return to text\n\n7. For a listing of these rules, see Amel and Keane (1986).  Return to text\n\n8. Examinations were conducted to evaluate an institution's compliance in five performance areas, comprising twelve assessment factors.  The examination culminated in the assignment of a rating (Outstanding, Satisfactory, Needs to Improve, or Substantial Noncompliance) and a written report that became part of the supervisory record for the institution. Return to text\n\n9. The agencies sought to offer banks some flexibility in choosing an examination strategy that suited their business models.  The 1995 regulations gave banks the option to submit a strategic plan for complying with CRA in lieu of the standard approach to examination.  A community development test was offered to wholesale and limited-purpose banks as a standard for their compliance.  The agencies also required that examiners evaluate a bank's CRA record within a performance context that considered socioeconomic factors and market conditions within the institution's service area.  Return to text\n\n10. Distinguishing with certainty the effects that the CRA had on \"CRA-type\" activity from the effects of simultaneous regulatory and market changes over this period has not been possible.  It is highly likely that these factors have interacted with one another to affect consumers. Return to text\n \n11. The National Association of Mortgage Brokers reports that 68 percent of home loan originations involve mortgage brokers.  In 2005, 63 percent of mortgages were originated by mortgage companies.  (Of the mortgage companies, 70 percent were independent; the rest were affiliated with depository institutions.)  The remaining 37 percent were originated by depositories directly:  21.6 percent by commercial banks, 12.9 percent by savings institutions, and 2.5 percent by credit unions (see Avery, Brevoort, and Canner, 2006).  Return to text\n\n12. These concerns are reflected in the Interagency Guidance on Nontraditional Mortgage Product Risks (Office of the Controller of the Currency and others, 2006), as well as recently issued requests for public comment on the expansion of that guidance.  For further discussion of the emergence of the subprime mortgage market, see Gramlich, 2007. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070330a.htm",
        "title": "The Community Reinvestment Act: Its Evolution and New Challenges",
        "date": "3/30/2007"
    },
    {
        "content": "March 23, 2007\n\nGovernor Frederic S. Mishkin\n\nAt the Annual Macro Conference, Federal Reserve Bank of San Francisco, San Francisco, California\n\nUnder its dual mandate, the Federal Reserve seeks to promote both price stability and maximum sustainable employment.1 For this reason, we at the Federal Reserve are acutely interested in the inflation process, both to better understand the past and--given the inherent lags with which monetary policy affects the economy--to try to forecast the future. We economists have made some important strides in our understanding of inflation dynamics in recent years. To be sure, substantial gaps in our knowledge remain, and forecasting is still a famously imprecise task, but our increased understanding offers the hope that central banks will be able to continue and perhaps even improve upon their successful performance of recent years.\n\nToday, I will outline what I see as the key stylized facts that research has in recent years uncovered about changes in the dynamics of inflation and will present my view of how to interpret these findings. The interpretation has important implications for how we should think about the conduct of monetary policy and what we think might happen to inflation over the next couple of years. I will address these two issues in the final part of the talk. Before I continue, however, I should stress that the views I will express here are my own and are not necessarily those of my colleagues on the Federal Open Market Committee (FOMC).\n\nThe Empirical Evidence on Changes in Inflation Dynamics\nTo see what research has discovered about the evolution of inflation dynamics in recent years, let’s explore the following questions: (1) What is the available evidence on changes in inflation persistence in recent years? (2) What is the available evidence on changes in the slope of the Phillips curve? (3) What role do other variables play in the inflation process?\n\nInflation Persistence\nTo answer the first question, we need to measure how long the effects of a shock to inflation will last. Specifically, we need to know whether inflation tends to revert quickly to its initial level, or whether the effects of the shock persist--that is, lead to a changed level of inflation for an extended period. The most obvious way of measuring inflation persistence is to regress inflation on several of its own lags and then calculate the sum of the coefficients on lagged inflation. If the sum of the coefficients is close to 1.0, then shocks to inflation have long-lived effects on inflation. In other words, inflation behaves like a random walk, so that when inflation goes up, it stays up. If the sum of the coefficients drops well below 1.0, then a shock to inflation has only a temporary effect on inflation, and inflation soon reverts back to its trend level.\n\nThe evidence from these so-called autoregressions with U.S. data suggests that inflation may have grown less persistent over time. In particular, when autoregressions are run using rolling samples with a fixed width of, say, twelve years, the sum of the lagged coefficients often falls noticeably below unity as the sample advances to include the most recent data. Figure 1 illustrates this tendency, with the core personal consumption expenditures (PCE) price index as the measure of inflation. As you can see, the coefficient sum has declined to about 0.6 or so since the late 1990s, although the dotted lines indicate that the confidence interval for this estimate is wide and still includes 1.0. This result appears robust to the addition of other explanatory variables to the regression, although making such a modification does tend to reduce the extent of the estimated decline.2 Finally, it’s worth noting that this is not just a U.S. phenomenon, as some studies have found similar results for a number of other countries.3\n\n(I might proudly note in passing that the staff of the Federal Reserve System is responsible for much of this research.)\n\nRecent work by Jim Stock and Mark Watson (2007) provides an alternative and, to me, a quite intuitive way of thinking about inflation persistence. They estimate a model that decomposes inflation into two components. The first component, which can be thought of as the underlying trend, follows a random walk, so that shocks to this component persist indefinitely and thus affect the trend inflation rate going forward. The second component is a serially uncorrelated shock, meaning that such shocks are temporary and lead to only transitory fluctuations around the trend. Stock and Watson then allow the volatility of these two kinds of shocks, trend and temporary, to vary over time.\n\nStock and Watson find that the importance of the trend shocks relative to that of the temporary shocks started to rise at the end of the 1960s in the United States, peaked in the mid-1970s, stayed elevated over the next ten years, and then declined to a historical low (upper panel of figure 2). When the relative importance of the trend shocks became high, as it did in the 1970s and early 1980s, inflation became highly persistent. Under such conditions, if inflation went up, the trend component typically rose in tandem so that inflation stayed up. In contrast, when the temporary shocks were relatively more important, as was true before the 1970s and after the mid-1980s, a change in inflation tended to reflect a change in the temporary component, not the trend. As a result, fluctuations in inflation tended to fade away, implying that inflation persistence was much lower. In the context of the Stock-Watson model, then, when inflation rose to double-digit levels during the \"Great Inflation\" period of the 1970s and early 1980s, persistence became very high because the trend rate of inflation moved around a lot--that is, trend inflation became unanchored.\n\nAlthough the importance of the trend component has fallen markedly since the 1970s, the estimates produced by Stock and Watson suggest that it is still large enough to be economically meaningful. The bottom panel of figure 2 illustrates this point; as you can see, the estimated trend (the solid line) has drifted up a bit since the mid-1990s. Of course, such estimates are inevitably imprecise to some degree. For example, Cecchetti et al (2007) employ a slightly modified version of the Stock-Watson procedure in a recent paper and find that inflation persistence has fallen so low that their estimated trend for gross domestic product (GDP) price inflation has been almost perfectly flat at about 2.2 percent over the past few years. In contrast, Cogley and Sargent (2005), using a different modification of the procedure, obtain results more in line with the original Stock and Watson paper. Finally, follow-up work by staff members at the Federal Reserve Board suggests that the degree of persistence found with these techniques varies across different measures of inflation. Thus, the empirical evidence on this question can vary. Nonetheless, I think it fair to say that inflation has become less persistent over the past two decades but that the underlying trend may not yet be perfectly stable.\n\nWhen Cecchetti and his co-authors apply this type of analysis to other countries, the results are remarkably similar. In particular, in Canada, Italy, and the United Kingdom, the importance of the trend shocks began to rise in the late 1960s and early 1970s and then declined only in the mid-1980s. France followed a similar pattern, although there the rise began a bit earlier, in 1963. In short, all these countries experienced a similar \"Great Inflation,\" when trend inflation became unanchored. Germany and Japan had shorter periods of high persistence of inflation, with persistence beginning to decline around 1969 in Germany and around 1979 in Japan. Cecchetti and his co-authors conclude that the rise and subsequent decline of inflation persistence has thus been a worldwide phenomenon.\n\nSlope of the Phillips Curve\nIn traditional Phillips-curve equations, inflation depends on past values of inflation, an unemployment gap (the difference between the unemployment rate and an estimate of its natural rate), and variables such as the relative price of energy and import prices. When researchers estimate these equations, they typically find that the coefficient on the unemployment gap has declined (in absolute value) since the 1980s, often by a marked amount.4 In other words, the evidence suggests that the Phillips curve has flattened.\n\nThe finding that inflation is less responsive to the unemployment gap, if taken at face value, suggests that fluctuations in resource utilization will have smaller implications for inflation than used to be the case. From the point of view of policymakers, this development is a two-edged sword: On the plus side, it implies that an overheating economy will tend to generate a smaller increase in inflation. On the negative side, however, a flatter Phillips curve also implies that a given increase in inflation will be more costly to wring out of the system. We can quantify this latter consideration using the so-called sacrifice ratio--the number of years that unemployment has to be 1.0 percentage point greater than its natural rate to reduce the inflation rate 1.0 percentage point. Averaging estimates obtained from a comprehensive battery of equation specifications suggests that the sacrifice ratio may be 40 percent larger--that is, it may be 40 percent more costly to reduce inflation than it was two decades ago. Is this really bad news? I will return to this question later.\n\nRole of Other Variables in the Inflation Process\nEmpirical evidence suggests that inflation has also become less responsive to other shocks. The two oil price shocks in the 1970s were associated with large jumps in core inflation, whereas recent surges in energy prices have not had a similar effect. This reduced sensitivity manifests itself in traditional Phillips-curve models as a substantial decline in the estimated coefficient on the energy price term in these equations. Because this term equals the change in relative energy prices multiplied by the share of energy in aggregate output, energy price effects on inflation appear to have fallen by more than can be accounted for by the greater energy efficiency of the economy.5\n\nIn contrast, unpublished empirical work by the staff at the Federal Reserve Board suggests that, once we take the rising share of imports into account, the influence of import prices on core inflation in the United States has not changed much in the context of reduced-form forecasting models.6 At the same time, the influence of exchange rate movements on import prices--the so-called pass-through effect--may have fallen substantially, at least according to some studies.7 If so, then the influence of exchange rate fluctuations on domestic inflation may now be less than it once was, when one controls for changes in the volume of our foreign trade.\n\nWhat Interpretation Can We Give to Changes in Inflation Dynamics?\nIn interpreting these stylized facts about changes in inflation dynamics, we must first recognize that all of them are based on reduced-form relationships, and thus they are about correlations and not necessarily about true structural relationships. Because the explanatory variables in inflation regressions are themselves influenced by changes in economic conditions, changes in the underlying monetary policy regime are likely to be a source of changes in reduced-form inflation dynamics. This problem is especially acute for structural relationships involving expectations or other factors that are not directly observable and so cannot be included in reduced-form regressions. In such cases, we cannot use the reduced-form equations to disentangle the effects of such unobserved factors--which themselves may be driven by changes in monetary policy--from that of other influences.\n\nThe most important development in monetary economics that I have witnessed over my now-long career has been the recognition that expectations are central to our understanding of the behavior of the aggregate economy. Theory tells us that inflation expectations must be a key driving force behind inflation. This dependence has long been implicit in traditional Phillips-curve analysis, but expectations--now in explicit form--are also a central feature of the increasingly popular New Keynesian Phillips curves, in which current-period inflation is a function of expectations of next period’s inflation and resource utilization. Therefore, a natural first place to look for explanations of changing inflation dynamics is a possible change in the expectations-formation process.\n\nThe first stylized fact discussed earlier is that inflation persistence, which rose in the 1970s during the Great Inflation, has since declined to a much lower level. An intuitive way of thinking about this rise and fall in inflation persistence is that it resulted from an un-anchoring of trend inflation during the period of the Great Inflation, and a re-anchoring in recent years, as the work of Stock and Watson suggests. When we think about what drives trend inflation, inflation expectations--particularly long-run expectations--come to mind. A de-anchoring of inflation expectations would surely lead to trend inflation becoming unanchored, whereas an anchoring of inflation expectations at a particular level would necessarily lead to a stabilization of trend inflation and hence a decline in inflation persistence.\n\nDo indicators of inflation expectations support this story? They certainly do. Consider the measure of expected inflation for the coming twelve months reported by the Livingston survey. As illustrated by the dashed line in figure 3 , this measure of expected short-run inflation--adjusted by a constant factor to convert it from a CPI basis to a PCE basis--soared during the 1970s and then fell back markedly during the 1980s and early 1990s. Over the past few years, the short-run series has fluctuated around 2 percent, with its month-to-month movements correlated with swings in energy prices, as one might expect. An even more striking story is told by the survey-based measure of expected long-run inflation used in the FRB/US model, the dotted blue line in figure 3.8 When we look at this long-run series (which should be more closely related to expectations about policy objectives than the short-run measure), we see that the public’s expectations stood at a high level of 7-3/4 percent at the start of the 1980s, when this information first began to be collected. From that point on, however, expectations fell steadily over the 1980s and most of the 1990s as the process of re-anchoring continued. By 1998, this process was apparently completed, and since that time the public’s expectations--at least according to this particular measure--have been steady as a rock. Estimates of inflation compensation derived from indexed Treasury yields have also been remarkably stable in recent years. Figure 3 also reproduces the estimate of trend inflation produced by the Stock-Watson procedure (the solid line). As you can see, this series more or less tracks the survey-based measures of long-run expectations, suggesting that the anchoring of long-run inflation expectations in recent years may explain the finding of a marked decline in inflation persistence.\n\nAnchoring of inflation expectations is not a deus ex machina. It must come from somewhere, and since Milton Friedman’s adage that “[i]nflation is always and everywhere a monetary phenomenon” is still true, monetary policy must be the source of the change in the evolution of long-run inflation expectations. During the 1960s and 1970s, the Federal Reserve and a number of other central banks maintained a policy stance that, inadvertently or not, was too easy on average and that allowed both actual and expected inflation to drift up markedly over many years. Since the late 1970s, however, the Federal Reserve and many other central banks have increased their commitment to price stability, in both words and actions. The Fed pursued preemptive strikes against rises in inflation in 1994-95, 1999-00, and in 2004-06, as well as preemptive strikes against potentially deflationary forces in the fall of 1998 and in 2001-04. The result has been not only low and stable inflation but also, as we have seen, a strong anchoring of long-run inflation expectations.\n\nThe pursuit of more-aggressive monetary policy to control inflation and the achievement of anchored inflation expectations can also help explain the other stylized facts about inflation dynamics. With expectations of inflation anchored, any given shock to inflation--whether it is from aggregate demand, energy prices, or the foreign exchange rate--will have a smaller effect on expected inflation and hence on trend inflation. These shocks will then have a much less persistent effect on actual inflation. The recent experience with surging oil prices seems consistent with this story. The price of West Texas intermediate crude oil rose from about $30 per barrel in late 2003 to a peak of almost $75 per barrel in July of last year. During this episode, inflation appears to have been boosted only temporarily and by a strikingly small amount, in contrast to the 1970s when oil price shocks led to large, persistent increases in inflation.\n\nInterestingly, monetary policy could have worked to flatten the estimated Phillips curve even without these favorable expectational effects, simply by moving more aggressively to stabilize both inflation and output. A theorem from the literature on optimal control states that implementing a policy of adjusting some instrument to stabilize a particular variable has the effect, in the limit, of driving the correlation between the instrument and the targeted variable to zero. Thus, a monetary policy that more successfully stabilizes inflation and resource utilization could well lead to a smaller estimated coefficient on unemployment gaps in a traditional Phillips-curve equation.\n\nResearch carried out with quite different models of the macroeconomy by my former colleagues at Columbia, Jean Boivin and Marc Giannoni (2006), and by John Roberts (2006) at the Federal Reserve Board, supports these conclusions. In particular, their analyses suggest that changes in the way the Federal Reserve conducts monetary policy--including changes in both the parameters of monetary policy reaction functions and the volatility of shocks to those functions--may account for most of the reduction in the coefficients on resource utilization in traditional Phillips curves.9\n\nWhat is particularly attractive about highlighting a better anchoring of inflation expectations as probably the primary factor driving the changes in inflation dynamics is that this one explanation covers so many of the stylized facts--an application of Occam’s razor. Indeed, I have always become more confident in a theory if it can explain a number of very different facts.10 This is why I am so attracted to the view that inflation expectations are a key driving factor in the inflation process.\n\nOf course, many other factors also influence inflation, and some of these provide other possible explanations for the recent changes in inflation dynamics. For example, smaller coefficients on unemployment gaps and energy prices in traditional Phillips curve equations may reflect the influence of lower and less-variable inflation on the frequency with which firms change their prices. In the context of a low-inflation environment, firms may have concluded that they can leave their prices fixed for longer periods at little cost, causing inflation to be less responsive to shocks, particularly if they are transitory. In this way, monetary policy may have affected the slope of the Phillips curve without having affected the manner in which expectations are formed. Also, from the mid-1980s through the first years of this decade, energy price movements were smaller and more transitory. We see this in futures markets, in which oil prices in far-dated futures contracts moved much less than spot prices over this period, suggesting that people expected a quick reversal in any rise in oil prices. Such transitory shocks to energy price would presumably have a smaller effect on inflation than the more-persistent oil price shocks of the 1970s and early 1980s. Increased globalization and other sources of increased competition may also have lowered the sensitivity of domestic inflation to aggregate demand, thereby flattening the Phillips curve. However, the evidence on this last point is limited and inconclusive.11\n\nPolicy Implications\nThese stylized facts--that inflation has become less persistent and now responds less to aggregate demand and supply shocks--can lead to inappropriate policy advice. If we take these facts to be structural and attributable to factors other than monetary policy, we might interpret them as suggesting that the Federal Reserve could respond less to shocks and yet be confident that inflation would remain at a low level. In addition, the smaller coefficient on unemployment gaps in traditional Phillips-curve models, which seems to imply that the sacrifice ratio has gone up, might lead us to think that reducing inflation is very costly because it requires long periods of high unemployment. As a result, policymakers might decide that such an attempt would not be worthwhile and so would be less likely to try to reduce inflation if it were undesirably high. If used in model simulation exercises, the flatter Phillips curve might also suggest that getting inflation down to a particular desired level could take an inordinately long time.\n\nHowever, if we instead attribute the observed changes in inflation dynamics to better monetary policy and a resultant better anchoring of inflation expectations, then such policy conclusions are unwarranted. Under this alternative interpretation, the reason that inflation has become less persistent is that monetary policy, in carrying out its dual mandate of promoting both price stability and maximum sustainable employment, has been vigilant in maintaining a low rate of inflation on average. But if the monetary authorities were to become complacent and to think that they could get away with not reacting to shocks that, in their mistaken view, no longer have the potential to cause inflation to rise persistently, then inflation expectations would surely become unhinged again. In short, complacency that might arise from the current low inflation persistence might result in deja vu all over again and return us to an era like the Great Inflation. These are exactly the concerns expressed in Tom Sargent’s (2000) book on the rise and fall of U.S. inflation, in which he worries that a misunderstanding of the inflation process might again lead to a high-inflation equilibrium.\n\nIf inflation has indeed become less persistent because better monetary policy has anchored inflation expectations more solidly, the monetary authorities may find that they have less need to induce large swings in economic activity to control inflation. This is a key benefit of establishing a strong nominal anchor. Because the public has become confident that the Fed will do the right thing, expectations now behave in a manner that makes the economy more stable to begin with. If this hypothesis is correct, cyclical movements in interest rates need not be as great as was necessary when expectations were unanchored. However, these favorable circumstances will persist only so long as the monetary authorities continue to ratify the public’s expectations. Consistent with its dual mandate, the Fed must therefore continue to respond aggressively to shocks that have potentially persistent adverse effects on both inflation and real activity. Because long-run inflation expectations are a key driver of trend inflation, the monetary authorities need to monitor long-run inflation expectations closely. If they find that they are losing credibility with the markets, so that inflation expectations begin to drift and rise above (or fall below) a desired level, they must take actions to restore their credibility.\n\nAt the Federal Reserve, we understand the importance to the health of the economy of anchoring inflation expectations. This is why Federal Reserve officials continually reiterate our commitment to maximum sustainable employment and price stability, why we have been willing to make preemptive strikes against both inflationary and deflationary pressures, and why we remain vigilant about developments in the economy that could lead to persistent departures of inflation from levels that are consistent with price stability.\n\nMy view--that recent changes in inflation dynamics result primarily from better-anchored inflation expectations and not from structural change or simply the achievement of a persistent low rate of inflation--implies some very good news: Potentially inflationary shocks, like a sharp rise in energy prices, are less likely to spill over into expected and actual core inflation. Therefore, the Fed does not have to respond as aggressively as would be necessary if inflation expectations were unanchored, as they were during the Great Inflation era. Indeed, this can help explain why the recent sharp rises in energy prices have had a much more benign effect on the real economy than they did in the 1970s--a point that then-Governor Ben Bernanke made three years ago.\n\nAlthough solidly anchored inflation expectations are indeed highly desirable, they could pose a bit of a problem for monetary policy if they were at a level somewhat above or below the rate preferred by policymakers. Under such circumstances, the central bank would likely be interested in shifting the public’s expectations in a more favorable direction. Whether such adjustment would be easy or difficult is, unfortunately, quite uncertain because we do not understand the expectations-formation process very well. In some early models that used the rational expectations assumption, changing inflation expectations was relatively easy and thus implied sacrifice ratios that were extremely low, suggesting that the monetary authorities could shift inflation at little cost. However, the historical record suggests that permanently lowering inflation expectations may require keeping monetary policy tight for a substantial period, resulting in considerable output and employment losses for a time.\n\nOn the other hand, as Christy and David Romer (2002) have pointed out, the Federal Reserve in the 1970s overestimated the cost of reducing inflation because estimates of sacrifice ratios by Arthur Okun and other economists at that time proved to be too high.12 As a historical note, this provides one explanation for the Federal Reserve’s tolerance of such high inflation at the time. The disinflation after October 1979, carried out by the Federal Reserve under the leadership of Paul Volcker using words, procedures, and actions that were a sharp break from the past, produced a much lower cost of disinflation than policymakers had anticipated during the 1970s.\n\nI think these considerations leave us with fairly wide bounds on what the costs might be of permanently shifting long-run inflation expectations that are already anchored. On the one hand, the historical record gives us little reason to think the costs would be as minimal as the simplest models with rational expectations might suggest. On the other hand, overly pessimistic estimates have proved to be wrong in the past.\n\nImplications for Inflation Forecasts\nI have argued here that the most attractive explanation for recent changes in inflation dynamics is that expectations have become better anchored. Inflation is now less persistent and more likely to gravitate to a trend level that is determined by where long-run inflation expectations have settled. What implication does this have for forecasts of future inflation? In this framework, the first priority is to determine where inflation expectations may be anchored. This task is somewhat tricky. According to the latest reading from the Survey of Professional Forecasters (SPF), long-term inflation expectations are currently around 2 percent, as measured by the PCE price index that the FOMC has emphasized. The private forecasters appear to have held this view for many years, given that long-run expectations for CPI inflation in the SPF have been running at around 2-1/2 percent since the late 1990s, and that the average historical wedge between the CPI measure and the PCE measure of inflation has been about 50 basis points.13\n\nOf course, the views of professional forecasters may not be representative of what the average person or firm is thinking.If we turn to the financial markets, we find that inflation expectations extracted from a comparison of the yields between Treasury inflation-protected securities (TIPS) and standard Treasury securities seem consistent with an anchor at or perhaps a little above 2 percent in terms of PCE inflation.14 I say “seem consistent,” because we cannot be sure about at least one of the items used in the extraction, the premium paid to investors to compensate them for inflation risk. In the case of households, long-term inflation expectations from the Reuters/Michigan survey have been running much higher for a number of years, at around 3 percent. However, this figure seems overstated in light of the persistent bias found in the short-term inflation expectations reported by this survey. Correcting for the bias, these survey results are probably more in line with PCE inflation closer to 2 percent.15 As for firms, we unfortunately have no good data on their inflation expectations. More information on this score would be particularly welcome, but expectations in general is an area worthy of further study.\n\nTaken together, the data suggest to me that long-run inflation expectations are currently around 2 percent. That said, I think it should be clear that the evidence points to a range of estimates; moreover, this range is itself uncertain because of the assumptions needed to tease point estimates from the available data. So, although I think that 2 percent is a reasonable estimate of current long-run expectations, I don’t want to overstate the precision of this figure. We still face some uncertainty in this regard, and policymakers must be cautious about placing too much confidence in any one estimate.\n\nIf long-run expectations are in fact about 2 percent, where is actual inflation likely to be headed in the next year or two? While recognizing how embarrassingly wrong such prognostications often turn out to be, I think that we can be reasonably optimistic that core PCE inflation will gradually drift down from its latest twelve-month reading of 2-1/4 percent. This process may take a while in light of the recent rebound in prices for gasoline and other petroleum products. These price increases have boosted the cost of producing many non-energy goods and services, and as firms gradually pass on these higher costs to their customers, monthly readings on the change in core prices are likely to be higher than they otherwise would be. Once this process is completed, however, we might expect consumer price inflation to move into better alignment with long-run expectations and thus settle in around 2 percent. Of course, our understanding of the empirical links between our measures of expected inflation and actual inflation is sufficiently poor that things could well go awry with this forecast. Moreover, many things could happen in the coming months to alter the outlook, as the recent fluctuations in energy markets and swings in GDP growth illustrate.\n\nLooking to the medium term, I am less optimistic about the prospects for core PCE inflation to move much below 2 percent in the absence of a determined effort by monetary policy. For the most part, this assessment--which I should stress is subject to considerable uncertainty--flows from my view that long-term expectations appear to be well anchored at a level not very far below the current rate of inflation. If so, a substantial further decline in inflation would require a shift in expectations, and such a shift could be difficult and time consuming to bring about, as I noted earlier.\n\nAs I mentioned at the start, central bankers are acutely interested in the inflation process. This is why I have thought about the topic a lot and chosen to talk about it today. I hope you have found my musings on this subject useful.\n\nReferences\n\nBernanke, Ben (2004). \"The Great Moderation,\" speech delivered at the meeting of the Eastern Economic Association, Washington, D.C., February 20.\n\nBoivin, Jean, and Marc Giannoni (forthcoming) \"Has Monetary Policy Become More Effective?\" Review of Economics and Statistics.\n\nBorio, Claudio, and Andrew Filardo (2006).  \"Globalisation and Inflation: New Cross-Country Evidence on the Global Determinants of Domestic Inflation,\" unpublished paper, Bank for International Settlements, Basel, Switzerland (March).\n\nCecchetti, Stephen G., Peter Hooper, Bruce C. Kasman, Kermit L. Schoenholtz, and Mark W. Watson (2007). \"Understanding the Evolving Inflation Process,\" presentation at the U.S. Monetary Policy Forum, Washington, D.C., March 9.\n\nClarida, Richard, Jordi Gali, and Mark Gertler (2000). \"Monetary Policy Rules and Macroeconomic Stability: Evidence and Some Theory,\" Quarterly Journal of Economics, vol. 115 (February), pp. 147-80.\n\nCogley, Timothy, and Thomas Sargent (2005). \"Drifts and Volatilities: Monetary Policies and Outcomes in the Post WWII U.S.,\" Review of Economic Dynamics, vol. 8 (April), pp. 262-302.\n\nFriedman, Milton (1957). Theory of the Consumption Function. Princeton, N.J.: Princeton University Press.\n\nHanson, Bruce (1999). \"The Grid Bootstrap and the Autoregressive Model.\" Review of Economics and Statistics, vol. 81 (November), 594-607.\n\nHellerstein, Rebecca, Deirdre Daly, and Christina Marsh (2006). \"Have U.S. Import Prices Become Less Responsive to Changes in the Dollar?\" Federal Reserve Bank of New York, Current Issues in Economics and Finance, vol. 12 (September), pp. 1-7.\n\nHooker, Mark A. (2002). \"Are Oil Shocks Inflationary? Asymmetric and Nonlinear Specifications versus Changes in Regime,\" Journal of Money, Credit, and Banking, vol. 34 (May), pp. 540-61.\n\nIhrig, Jane, Steven Kamin, Deborah Lindner, and Jaime Marquez (forthcoming). “Some Simple Tests of the Globalization and Inflation Hypothesis,” International Finance Discussion Papers. Washington: Board of Governors of the Federal Reserve System.\n\nLevin, Andrew T., and Jeremy M. Piger (2004). \"Is Inflation Persistence Intrinsic in Industrial Economies?\" (1.0 MB PDF) ECB Working Paper Series 334. Frankfurt, Germany: European Central Bank, April.\n\nMarazzi, Mario, Nathan Sheets, Robert J. Vigfusson, Jon Faust, Joseph E. Gagnon, Jaime Marquez, Robert F. Martin, Trevor A. Reeve, and John H. Rogers (2005). \"Exchange Rate Pass-Through to U.S. Import Prices: Some New Evidence,\" International Finance Discussion Papers 833. Washington: Board of Governors of the Federal Reserve System, April.\n\nNason, James M. (2006). \"Instability in U.S. Inflation: 1967-2005,\" Federal Reserve Bank of Atlanta, Economic Review, vol. 91 (Second Quarter), pp. 39-59.\n\nOkun, Arthur M. (1978). \"Efficient Disinflationary Policies,\" American Economic Review, vol. 68 (May), pp. 348-52.\n\nO’Reilly, Gerard, and Karl Whelan (2005). \"Has Euro-Area Inflation Persistence Changed over Time?\" Review of Economics and Statistics, vol. 87 (November), pp. 709-20.\n\nRoberts, John M. (2006). \"Monetary Policy and Inflation Dynamics,\" International Journal of Central Banking, vol. 2 (September), pp. 193-230.\n\nRomer, Christina D., and David H. Romer (2002). \"The Evolution of Economic Understanding and Postwar Stabilization Policy,\" in Rethinking Stabilization Policy, proceedings of a symposium sponsored by the Federal Reserve Bank of Kansas City, Jackson Hole, Wyoming, August 29-31, pp. 11-78.\n\nRudebusch, Glenn D. (2005). \"Assessing the Lucas Critique in Monetary Policy Models,\" Journal of Money, Credit, and Banking, vol. 37(April), pp. 247-72.\n\nSargent, Thomas J. (2000). The Conquest of American Inflation. Princeton, N.J.: Princeton University Press.\n\nSekine, Toshitaka (2006). \"Time-Varying Exchange Rate Pass-Through: Experiences of Some Industrial Countries,\" BIS Working Paper 202. Basel, Switzerland: Bank for International Settlements, March.\n\nStock, James, and Mark Watson (2007). \"Why Has U.S. Inflation Become Harder to Forecast?\" Journal of Money, Credit, and Banking, vol. 39 (February), pp. 3-34.\n\nWilliams, John C. (2006). \"The Phillips Curve in an Era of Well-Anchored Inflation Expectations,\" unpublished working paper, Federal Reserve Bank of San Francisco, September.\n\nFootnotes\n\n1. I want to thank Michael Kiley, Jean-Philippe Laforte, Deborah Lindner, David Reifschneider, John Roberts, and Jeremy Rudd for their extremely helpful comments and assistance on this speech. Return to text\n\n2. Two recent studies that report a marked decline in inflation persistence in the United States are Nason (2006) and Williams (2006). Their findings should be treated carefully, however, as analysis by the staff of the Federal Reserve Board suggests that modest changes in methodology, such as lengthening the sample period, correcting for small-sample bias, or changing the particular price index used in the analysis, can alter both the magnitude and the statistical significance of the estimated decline in persistence. Return to text\n\n3. Levin and Piger (2004) find significant declines in inflation persistence since the 1980s for the major European economies as well as for Japan, Canada, Australia and New Zealand. However, O’Reilly and Whelan (2005) find little or no evidence for a recent decline in persistence for the euro area as a whole. Return to text\n\n4. Studies that present evidence of a marked decline in the sensitivity of U.S. inflation to unemployment and other measures of resource utilization include Roberts (2006) and Williams (2006). Unpublished work by staff at the Federal Reserve Board indicates that this result generally holds across a variety of regression specifications, estimation methods, and data definitions. Other studies find similar declines in many foreign industrial economies; see, among others, Borio and Filardo (2006) and Ihrig and others (forthcoming). However, the empirical evidence on this question is such that the exact magnitude, timing, and statistical significance of these changes remain a subject of debate. Return to text\n\n5. Hooker (2002) finds that oil price movements during the 1980s and 1990s had little or no effect on core inflation, in contrast to a substantial influence in previous decades. Recent empirical work carried out at the Federal Reserve Board confirms Hooker’s findings using data through 2006; however, some of this work also hints that the surge in energy prices since 2004 may have had a larger influence on core inflation. Return to text\n\n6. Interestingly, Ihrig and others (forthcoming) find that the sensitivity of inflation to movements in import prices has fallen in several foreign industrial economies. Return to text\n\n7. For evidence on U.S. exchange rate pass-through effects, see Marazzi and others (2005). Hellerstein, Daly, and Marsh (2006) also find that pass-through has declined in the United States, although by a considerably smaller amount. Empirical results for other developed countries are reported in Sekine (2006). Return to text\n\n8. The FRB/US series splices data from two surveys of expected long-run inflation--the Hoey survey of financial market participants from 1981 through 1989, and the Survey of Professional Forecasters from 1990 on. After splicing, a 50-basis-point constant factor is subtracted from the series to put it on a PCE price index basis, on the assumption that survey respondents reported their expectation for CPI inflation, not PCE inflation. (The average wedge between CPI inflation and PCE inflation was about 50 basis points from 1980 through 2006.) I have made a similar adjustment to the Livingston survey data plotted in figure 3 to put it on a PCE price basis. Return to text\n\n9. Clarida, Gali, and Gertler (2000) make a similar argument, but for a dissenting view, see Rudebusch (2005). Return to text\n\n10. This is why Milton Friedman’s Theory of the Consumption Function is one of my favorite pieces of empirical research. What is extraordinary about this book is that it has only a handful of regressions but nonetheless shows that the basic idea of permanent income has to be right because the theory explains numerous facts derived from numerous studies. This book is not widely read by graduate students nowadays, but it should be. Return to text\n\n11. See Borio and Filardo (2006) and Ihrig and others (forthcoming) for opposing views on this issue. Borio and Filardo provide evidence that global output gaps may be just as important as conventional domestic output gaps in the determination of inflation; moreover, they argue that these effects have been rising over time. However, Ihrig and her co-authors find that Borio and Filardo’s results are sensitive to small changes in specification; they also find little support for an independent role of global output gaps and no evidence that globalization can account for falling coefficients on domestic gaps. Return to text\n\n12. Okun (1978) estimated that a 10 percent reduction in real GDP for one year would reduce the inflation rate only 1.0 percentage point. By the mid-1980s, however, Federal Reserve Board staff estimates of the sacrifice ratio were roughly half as large. Return to text\n\n13. Recently, the Survey of Professional Forecasters (SPF) has begun to report long-run expectations for the PCE price index as well as the CPI. Long-run expectations for PCE price inflation were 2 percent in the latest release, the same rate as indicated by the FRB/US estimate since the late 1990s. (The FRB/US estimate since 1991 derives from the SPF reading on expected long-run CPI inflation less an average historical wedge between the CPI measure and the PCE measure of inflation of about 50 basis points.) Interestingly, long-run expectations for CPI inflation in the SPF ticked down to 2-1/3 percent in the latest survey, suggesting that private forecasters may have lowered their estimate of the average CPI-PCE wedge to about 30 basis points.  Such a revision would be in line with current Board staff estimates, which indicate that the wedge has declined in recent years and now stands at about 30 basis points. Return to text\n\n14. Inflation compensation--the difference between nominal and indexed Treasury yields--has recently been about 2-1/2 percent. However, two adjustments are necessary to translate inflation compensation into an estimate of expected long-term inflation on a PCE price basis. First, because the difference between nominal and indexed yields equals the sum of inflation expectations and an inflation term premium, we must subtract an estimate of the premium. Given that term premiums on nominal Treasury yields are extremely low at present, the inflation term premium is probably less than 25 basis points currently, and could even be zero. Second, because the inflation series used to index the Treasury securities is the CPI, not the PCE price index, we must subtract an estimate of the wedge between the two measures of inflation. If we use the same 30 basis points wedge expected by private forecasters in the latest reading from the SPF, indexed and nominal yields appear to be consistent with expected long-term PCE inflation of about 2 percent or a little higher. Return to text\n\n15. According to the Reuters/Michigan survey, long-term inflation expectations of households are currently around 3 percent, as they have been for the past few years. This survey does not specify a price index. However, expectations from the companion one-year-ahead expectations have come in about 75 basis points higher than actual PCE inflation since 1990, suggesting that there may be a systematic bias in the responses to the Reuters/Michigan survey relative to this measure of inflation. If this bias also applies to longer-run inflation expectations, then household expectations may be in line with PCE inflation running in the vicinity of 2-1/4 percent in the long run. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/mishkin20070323a.htm",
        "title": "Inflation Dynamics",
        "date": "3/23/2007"
    },
    {
        "content": "March 22, 2007\n\nGovernor Randall S. Kroszner\n\nTo the 2007 Credit Markets Symposium at the Charlotte Branch of the Federal Reserve Bank of Richmond, Charlotte, North Carolina\n\nI am delighted to be the opening speaker at this Credit Markets Symposium.  The time certainly is ripe for an open dialogue among market participants, risk-management professionals, and policymakers.  Credit markets have been evolving very rapidly in recent years.  New instruments for transferring credit risk have been introduced and loan markets have become more liquid.  Asset managers have become an important force in a wider range of credit markets.  Taken together, these changes have transformed the process through which credit demands are met and credit risks are allocated and managed.\n\nAs I will discuss, I believe these developments generally have enhanced the efficiency and the stability of the credit markets and the broader financial system by making credit markets more transparent and liquid, by creating new instruments for unbundling and managing credit risks, and by dispersing credit risks more broadly.   Interestingly, this is not the first time that we have witnessed innovations in derivatives markets that have led to these types of benefits.  Later, I will very briefly draw some parallels between recent developments in credit derivatives and the development of agricultural futures in the mid-nineteenth century.\n\nFor all their benefits, these developments have posed some significant challenges to market participants.  The complexity of some instruments creates difficulties with respect to valuation, risk measurement, and risk management.   Also, the trading of certain instruments has at times run well ahead of developments in the infrastructure necessary for clearing and settling those trades.  Although impressive progress is being made toward addressing these challenges, I believe they need continuing attention from market participants and policymakers.\n\nRecent Developments\nThe evolution of the credit markets has been spurred by the astonishing growth of new credit instruments, particularly credit derivatives.  The notional amount of credit derivatives outstanding has doubled each year for the past five years; it totaled $20 trillion at the end of June 2006, according to statistics compiled by the Bank for International Settlements (BIS).\n\nThe bulk of credit derivatives outstanding consist of single-name credit default swaps, or single-name CDS, which reference the obligations of a single obligor.1  Derivatives are sometimes faulted for their complexity, but that charge cannot be leveled against single-name CDS:  The risk of a single-name CDS is essentially that of simply buying or selling short a bond.  Single-name CDS make up 70 percent of all credit derivatives, according to the BIS.  The bulk of CDS trading is in the investment-grade segment of the corporate credit market, although CDS trading involving high-yield names has been expanding quickly.  Most recently, CDS that reference asset-backed securities have been a high-growth part of the market.\n\nSingle-name CDS can be used as building blocks to construct credit derivatives referencing portfolios of issuers.  These so-called multiname CDS make up the remaining 30 percent of credit derivatives, according to the BIS.  Some multiname CDS are quite straightforward, such as an index of actively-traded names in a particular market segment.  Some, however, are indeed quite complex; they include certain structured credit derivatives that unbundle the risk of a portfolio of names into tranches with different seniorities and dramatically different risks.\n\nCredit derivative indexes are currently the fastest-growing and most liquid area of the credit markets.  They were created first in the most actively traded market segments:  investment-grade and high-yield names in both North America and Europe.  Recently, newer indexes have been created in other market segments, including securities backed by commercial mortgages, subprime residential mortgages, and European leveraged loans.2\n\nAmong the more complex credit derivatives, the credit index tranches stand out as an important development.  A credit index tranche exposes an investor to a particular slice of the losses due to defaults among names in one of the credit derivative indexes.  The so-called equity tranche bears a disproportionate share of the total credit risk of the underlying index.  For example, an equity tranche might be exposed to the first 3 percent of losses.  Other tranches would cover the remaining percentiles of loss, and the sum of all tranches reproduces the exposure of the entire index.  As I will discuss in a few minutes, one factor behind the complexity of credit index tranches is that the loss exposure depends not only on the level of defaults but also on the correlation of defaults across issuers.\n\nAnother instrument in the credit markets, similar to a credit index tranche, is the collateralized debt obligation, or CDO.  A CDO pools a portfolio of fixed-income assets into a tranched liability structure that is familiar from other securitization markets.  For example, banks have long used a similar liability structure to fund their credit card loans to consumers.  The most common types of collateral for CDOs are asset-backed and corporate securities and syndicated loans.  CDOs backed by loans are referred to as collateralized loan obligations, or CLOs.\n\nThe growth of CLOs has certainly had an effect on the market for syndicated loans.  Of course, syndicated loans are not a new instrument.  They have been around since the 1970s.  But recently, the secondary-market liquidity of syndicated loans has improved dramatically, in part because of the demand for loans by CLOs.  This improved liquidity has transformed loans from buy-and-hold investments into traded assets.  Market participants are now working to standardize documentation for trading credit default swaps referencing loans. These so-called loan CDS have already started to trade in small amounts.\n\nThe development of these new instruments and markets has been facilitated by nonbank institutional investors and has in turn helped such investors expand their participation in credit markets.  Data on participation by nonbank institutional investors are difficult to come by, but such data are available for the syndicated loan markets.3  Historically, loan syndications, which include both loan commitments and term loans, were predominantly funded by banks.  But in the period from 2001 through 2006, when the amount of term loans to U.S. nonfinancial corporations rose from $100 billion to more than $350 billion, the nonbank share of such lending rose from less than one-half to roughly two-thirds.\n\nThe institutional investors in these loans include mutual funds, insurance companies, pension funds, and hedge funds.  In the mid-1990s, mutual funds were the most important type of institutional investor in these markets.  More recently, identifying the ultimate sources of institutional demand has become more difficult.  For example, from 2001 through 2006, about two-thirds of institutional term loans were purchased as collateral for the issuance of CLOs.  Although institutional investors undoubtedly are the predominant investors in CLOs, little is known about the holdings of the various types of institutions.  Also, while much is being made of the increasingly important role of hedge funds in credit markets, hedge funds, in turn, are increasingly managing assets on behalf of endowments, pension funds, and other institutional investors.4\n\nAsset managers are clearly playing an increasingly important role in credit markets, whether they are managing mutual funds, CLOs, or hedge funds.  The decisions of those asset managers are increasingly being driven by the preferences of institutional investors.  And the decisions of the asset managers have a large influence on the pricing of credit, even for types of credit whose pricing historically was driven by the decisions of banks.\n\nBenefits of Recent Developments\nThe new instruments, markets, and participants I just described have brought some important benefits to credit markets.  I will touch on three of these benefits: enhanced liquidity and transparency, the availability of new tools for managing credit risk, and a greater dispersion of credit risk.\n\nIn listing these benefits, I am struck by the strong resemblance of what is happening right now in credit markets to what happened historically in other markets when derivatives were introduced.  For example, the markets for agricultural commodities in the United States reaped some of these benefits when standardized futures contracts for wheat and other commodities were introduced in the middle of the nineteenth century.5\n\nHistorically, the secondary markets for corporate bonds, loans, and asset-backed securities were illiquid and not transparent.  Liquidity in these [msg1] markets has improved over time but is still quite poor.  For example, half of outstanding corporate bonds did not even trade once in the first three months of 2006.6   Individual bonds tend to be somewhat liquid immediately after they are issued, but trading activity declines quickly thereafter as investors put the bonds into buy-and-hold portfolios.  Also, shorting corporate bonds is very difficult.\n\nThe dramatic improvement in credit market liquidity has been spurred by credit derivatives.  One way to measure the improvement in liquidity is with bid-ask spreads.   For investment-grade corporate bonds--a relatively liquid part of the bond market--the bid-ask spread averaged 64 basis points last year.7  The bid-ask spreads for single-name investment-grade CDS, however, are typically only 10 basis points or less, and the usual bid-ask spread for investment-grade credit indexes is just 2 basis points.\n\nThe liquidity of the secondary market for loans has also improved in recent years.  Trading volume totaled $239 billion in 2006, up from $102 billion in 2000.8  A key factor driving the improvement in secondary-market liquidity is the expanded participation of nonbank institutional investors.  These investors are active managers of credit risk, and consequently they appear to place a higher value on liquidity.\n\nAlong with liquidity, transparency in credit markets has also improved over time. Corporate bond markets are more transparent thanks to a regulatory change that took effect in 2002.  Dealers must now report nearly all corporate bond trades to the NASD within fifteen minutes, and the NASD immediately reports the trade data to the market.  For asset-backed securities and loans, price transparency is available from specialist vendors who aggregate and disseminate dealers’ prices.  For example, in the syndicated loan market, one vendor currently aggregates data from more than seventy traders to price nearly 6,000 loans daily.  Prices of many credit derivatives, including single-name CDS, credit derivative indexes, and credit index tranches, are widely available on services such as Bloomberg or Reuters.  Complex credit derivatives such as CDO tranches are an exception to all this:  They remain largely illiquid and nontransparent.\n\nEnhanced liquidity and transparency should promote better risk management by market participants and facilitate broader participation in credit markets.  Liquid markets make it easier to access historical price data and thus permit better measurement of credit risks.  Measuring a risk more accurately allows it to be priced more accurately.  A more transparent market with more accurate pricing is attractive to a wider array of investors.  In effect, better liquidity and transparency have lowered the cost of entry into the credit markets.\n\nIn addition to enhanced liquidity and transparency, the recent developments in credit markets have equipped market participants with new tools for taking on, hedging, and managing credit risk.  These new tools allow investors to more easily customize their credit risk portfolios.  Investors can now construct a diverse portfolio at much lower transaction costs compared with purchasing a portfolio of corporate bonds or even single-name CDS.  As derivatives, indexes can readily be used either to reduce or to take on more credit risk.  In contrast, for corporate bonds, the lack of a well-developed securities lending market makes it difficult to shed credit risk by short selling.\n\nThe enhanced transparency and liquidity of credit markets and the development of new instruments for customizing the risk characteristics of credit exposures have resulted in a wider dispersion of credit risk.  Although significant participation by nonbank institutional investors has long been a hallmark of U.S. credit markets, these developments have facilitated greater risk-bearing by entities other than banks and other highly regulated depository institutions.  On its face, a wider dispersion of credit risk would seem to enhance the stability of the financial system by reducing the likelihood that credit defaults will weaken any one financial institution or class of financial institutions.\n\nSome have expressed concern about the transfer of risk by banks and other heavily regulated depository institutions to more lightly regulated or unregulated entities. Some specific concerns are quite legitimate.  For example, as I will discuss in a few moments, if banks transfer credit risk to other entities through mechanisms that expose the banks to counterparty risks to those entities, the transfer is fully effective only if the banks manage those counterparty risks prudently.\n\nHowever, some other concerns about the transfer of credit risk outside the banking system seem to be based on questionable assumptions.  For example, some observers believe that credit risks will be managed more effectively by banks because they generally are more heavily regulated than the entities to which they are transferring credit risk.  But those unregulated or less regulated entities should in principle be subject to more-effective market discipline than banks because, without a safety net supporting them, their creditors have stronger incentives to monitor and limit their risk-taking.  In fact, while many focus on the dangers of risk transfer to highly leveraged entities that might be vulnerable to a sharp widening of credit spreads, a significant portion of the risks that are being transferred outside the banking system are being transferred to institutional investors that are far less leveraged than banks.\n\nChallenges Associated with Recent Developments\nThe benefits from the development of new instruments and markets that I have described will be fully realized only if market participants address various risk-management challenges posed by the use of these instruments.  I will discuss three challenges: limiting counterparty credit risk, modeling default correlation, and improving the infrastructure for clearing and settling credit derivative trades.\n\nBut before I discuss the risk-management challenges, I want to emphasize that the fundamental risks in credit markets have not been changed by the new instruments that are now being traded.  Credit risks may be traded among market participants, but that does not eliminate the risk.  Investors in credit markets are still exposed to the risk that issuers may default on their obligations.  And in the event of default, investors are still exposed to the further risk that the recovery rate will be lower than expected.   The management of default and recovery risks presents nothing fundamentally new to investors.  Instruments like single-name CDS and credit derivative indexes simply replicate the sort of credit exposures that have always existed.\n\nFrom the perspective of financial stability, I believe that the most important risk-management challenge in credit markets is the management of counterparty credit risk.  I mentioned earlier that institutional investors, including hedge funds, are now more active in credit markets.  I also noted that when banks trade credit risk using new instruments such as credit derivatives or CDOs, they must take care to limit any counterparty credit risk that might result.\n\nBanks commonly rely on collateral to mitigate counterparty credit risk on over-the-counter derivatives.  Credit derivatives are no exception.  Also, banks universally require their hedge fund counterparties to post collateral to cover current exposures and, with some exceptions, to cover potential exposures from future market movements.  But given the growing role of hedge funds in credit markets, it is appropriate to ask whether dealer banks have enough collateral to protect them against a stress scenario that goes well beyond the recent benign experience in credit markets.\n\nDefault correlation is a distinctly new aspect of credit risk.  The value of credit index tranches and CDO tranches is sensitive not only to the number of defaults among a set of issuers but also to the correlation of defaults.  The more senior tranches suffer losses only in a scenario with many correlated defaults.  The value of these senior tranches falls when default correlation rises.  The value of a first-loss tranche rises when default correlation rises.  And somewhere in the middle of the capital structure is a tranche whose value is roughly insensitive to correlation.  The mathematical relationship between tranche value and correlation depends on the particular model that is used to forecast defaults.  This dependency exposes dealers and investors to so-called correlation risk if their models or forecasts of default correlation turn out to be incorrect.\n\nCorrelation risk is challenging to measure and manage, and fairly recently some market participants learned a hard lesson about those challenges.  In May 2005, a widening of credit spreads for several automotive companies led to sharp movements in the prices of credit index tranches that seemed to catch market participants by surprise.  Their models of correlation risk seemed to have lagged behind the development of the new tranched credit products.  Since then, much energy has been devoted to building better models of correlation risk, but I believe it is fair to say that much work remains to be done before this risk can be considered to be fully understood.\n\nThese risk-management challenges have not gone unnoticed by market participants themselves.  In 2005, a private-sector group, the Counterparty Risk Management Policy Group II, or CRMPG II, chaired by E. Gerald Corrigan, produced a report highlighting many of these issues.9  That report also made a number of useful recommendations to market participants on how they could address some of the challenges I have mentioned here.  It is encouraging to find market participants taking a leading role in making the improvements in risk management that will be needed to fully reap the benefits of innovations in credit markets.\n\nThe third and final challenge I will discuss is that of infrastructure.  The very rapid growth of trading credit derivatives had until recently outpaced the development of the infrastructure necessary to clear and settle those trades.  Post-trade processes were largely manual, with attendant inefficiencies and risks.  By early 2005, credit derivatives dealers had huge backlogs of unconfirmed trades, even though they had greatly increased their back-office resources.  Unconfirmed trades increase the potential for material inaccuracies in trade records, which can cause mismeasurement and mismanagement of market risks and counterparty credit risks.\n\nMarket infrastructures can be improved only through collective actions by market participants.  In 2005, some stimulus for the necessary collective actions was provided by both the private sector and the public sector.  CRMPG II called attention to the growing backlogs and the risks that they posed to market participants and called for the convening of an industry roundtable to address them.10  Prudential supervisors then took the lead.  In September 2005 they called fourteen leading dealers to the Federal Reserve Bank of New York, where the supervisors collectively made clear their concerns about the risks posed by the growing backlogs.\n\nThe supervisors wisely avoided any temptation to design their own improvements to the market infrastructure.  Instead, they simply insisted that the backlogs be reduced and left it to the dealers who had been at the meeting (and who became known as the Fed 14) to figure out with other market participants how best to achieve that objective.  The market participants recognized that automation was the key; manual processes simply are not scalable.  Both dealers and asset managers embraced use of the Depository Trust & Clearing Corporation’s Deriv/Serv electronic confirmation service.  The results have been dramatic.  Between September 2005 and December 2006, aggregate credit derivatives confirmations outstanding thirty days or more at the fourteen dealers declined 92 percent despite continued rapid growth in trading volumes.  With the encouragement and support of supervisors, a larger group of dealers and asset managers are now turning their attention to addressing backlogs in the equity derivatives markets.\n\nConclusions\nCredit markets undoubtedly will continue to evolve in the years to come.  In the short run, rapid change can pose significant challenges to market participants. Cooperative initiatives, such as CRMPG II and the Fed 14, can contribute greatly to ensuring that those challenges are met successfully by identifying effective risk-management practices and by stimulating collective action when it is necessary, notably in achieving improvements in market infrastructures.  The recent success of such initiatives strengthens my confidence that future innovations in the market will serve to enhance market efficiency and stability, notwithstanding the challenges that inevitably accompany change.\n\nFootnotes\n\n1.  A CDS provides the purchaser with protection against the cost of defaults or other credit events that reduce the market value of underlying reference obligations (usually bonds or loans). Reference obligations and obligors are known as “names.” Return to text\n\n2.  A leveraged loan is commonly defined as a syndicated loan, typically to a riskier borrower, with an interest rate of at least libor plus 125 basis points. Return to text\n\n3.  Standard and Poor’s Leveraged Commentary and Data, “Leveraged Lending Review,” 2006:Q3. Return to text\n\n4.  One industry survey forecasts that institutions will account for roughly half of hedge fund inflows in 2007, up from virtually zero in 2000 (“Hedge Funds,” International Financial Services, London, March 2006, p. 3, chart 7). Return to text\n\n5.  See Randall S. Kroszner (1999), “Can the Financial Markets Privately Regulate Risk? The Development of Derivatives Clearinghouses and Recent Over-the-Counter Innovations,” Journal of Money, Credit, and Banking, vol. 31 (August, part 2), pp. 596-618. Return to text\n\n6.  Calculated from NASD, TRACE Corporate Bond Data. Return to text\n\n7.  Calculated from NASD, TRACE Corporate Bond Data. Return to text\n\n8.  Reuters Loan Pricing Corporation, www.loanpricing.com/analytics/pricing_service_volume1.htm .  Return to text\n\n9.  Counterparty Risk Management Policy Group II (2005), Toward Greater Financial Stability:  A Private Sector Perspective, July 27, www.crmpolicygroup.org . Return to text\n\n10.  Counterparty Risk Management Policy Group II, Toward Greater Financial Stability, p. 19. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20070322a.htm",
        "title": "Recent Innovations in Credit Markets",
        "date": "3/22/2007"
    },
    {
        "content": "March 22, 2007\n\nVice Chairman Donald L. Kohn\n\nAt the Conference on Credit Risk and Credit Derivatives, Washington, D.C.\n\nGood afternoon. I am pleased to participate in the Board’s conference on credit risk and credit derivatives.  Song Han, Matt Pritsker, and Hao Zhou have worked hard to put together a stimulating program of cutting-edge research in this area.1  Your conference focuses on improving our understanding of credit risk and credit derivatives, but I will begin my talk by taking a step back and discussing a wider range of asset markets, in which our understanding is also limited.  Then I will examine how the research in this conference can help sharpen our focus on this broader range of asset markets.\n\nAt the Federal Reserve, we have considerable interest in credit risk and credit derivatives. As these markets develop and become more complete, they facilitate risk transfer and diversification, thereby increasing the resilience of our financial system.  With participants coming to rely more on these markets to manage risk, we have focused increasingly on their liquidity and structure.  We have worked closely with the private sector to strengthen the clearing and settlement infrastructure and to understand how these markets will function under stress.\n\nBut my emphasis today will be not the structure or mechanics of credit markets but rather the information contained in the prices we observe in these markets.  We at the Federal Reserve use this information in nearly every area of our responsibility.  For example, in our roles as bank supervisors and protectors of financial stability, we monitor the credit spreads of financial institutions as early warning signs of possible financial stress.  In our role as monetary policy makers, we analyze information from credit-risk markets to get readings on the cost of capital to businesses and on forward-looking indicators of the health of the corporate sector that can have implications for future macroeconomic developments.\n\nExtracting Information from Asset Prices\nAs a consequence, the staff at the Federal Reserve puts considerable effort into research on asset prices and into reporting the results of that research to policymakers.  One reason we do so is to try to understand the expectations that households, businesses, and market participants have about the future.  Expectations are critical to understanding the economy and developments in the financial system.  Of course, we look at a great deal of data from the nonfinancial side of the economy, such as gross domestic product (GDP) growth, the unemployment rate, and changes in the prices of goods and services.  These data certainly reflect expectations but not always in a transparent way.  And, these data take some time to compile and so are never available in real time.\n\nBecause financial asset prices embody expectations about the future, they also contain forward-looking information about prospective developments, and many are available continuously and instantaneously.  We pay attention to an extensive range of asset prices, including those of Treasury securities (both nominal and real), corporate debt and equities, and derivatives.  Although it is not easy, we use these asset prices to tease out information about expectations that help us to interpret and predict the pace of economic activity and prices.\n\nThe price of an asset reflects the future cash flows that investors expect to receive from owning that asset.  The price also reflects a risk premium, which is the excess expected return over the risk-free rate that investors require for holding risky assets in their portfolios.  Each asset’s risk premium depends on the asset’s risk--as measured by the possible variability of its cash flows from their expected level--and on investors' risk aversion, which represents the extent of investors’ appetite for risk.  And we try to measure and understand the elements of the risk premium, in addition to the embedded expectations.\n\nIn equity markets, corporate earnings are the future cash flows that affect equity prices.  Given the market’s outlook for corporate earnings--as embodied, for example, in the predictions of market analysts--we make a crude estimate of the risk premium on equities as the difference between the ratio of trend earnings to price and a real long-term Treasury yield.  On occasion, we go further and use structural economic models to decompose the risk premium into components related to risk and to investors’ risk appetite.\n\nIn the credit market, the relevant future cash flows are the coupons on corporate debt, less an allowance for expected losses from future defaults.  In one exercise, we forecast future defaults with a simple regression model and estimate the credit-risk premium as the difference between corporate yields and Treasury yields that is in excess of what would be required to compensate investors for their estimates of expected credit losses. We also estimate a term structure of credit-risk premiums by repeating this analysis using debt that has different maturities.  To judge what market prices of risk might be telling us, we try to understand how the resulting credit-risk premiums relate to other sources of information, such as the strength of business balance sheets, historical levels of risk premiums, and premiums observed in related markets, like that for equities.  This approach helps us to assess the current attitudes and expectations of market participants as well as possible future movements in risk premiums under alternative scenarios.\n\nWe look to prices in Treasury markets and in markets for interest rate options and futures to infer investors’ expected future path of monetary policy and their uncertainty about that path.  To do so, we need to model term premiums to tease out the links between long-term interest rates and investors’ expectations of future short-term rates.  We also compare the Treasury yield curve with the prices on Treasury inflation-protected securities to infer expected inflation, a key variable tracked by monetary policy makers.\n\nAlthough we use a variety of techniques for extracting information from asset prices, what we can learn has limits.  First, asset prices are tough to work with.  They change rapidly and are subject to short-run technical factors--swings in prices that are not related to fundamental and persistent shifts in supply and demand.  Second, and perhaps even more important, how asset prices embody risk and investors’ risk attitudes is complicated and varies over time.  We must use models to extract information on risk and risk preferences from prices, and because all models are simplifications of reality, we have to recognize that the results are only approximations of the underlying attitudes and circumstances and thus are subject to error.\n\nAsset-Pricing Puzzles\nResearchers are well aware of the difficulties of decomposing an asset’s required return into components that are related to expected future cash flows, risk aversion, and risk.  Moreover, risk preferences that should be related in a predictable way across markets often do not appear to be so.  For example, the risk preferences required to fit consumption data from the goods market are inconsistent with the risk preferences implied by prices in the equity market.\n\nThis problem is well known to most economists and to everyone in this room and is known as the \"equity-premium puzzle.\"  The equity premium is defined as the return that an investor expects to earn on a broad equity index in excess of the return on a U.S. Treasury security.  Although theory suggests that the equity premium should be related to investors’ risk preferences as well as the fundamental volatility of the corporate sector, it is difficult to find plausible risk preferences that can rationalize the high level of the historical equity premium.  Also, we observe that required returns appear to vary over time, but we do not understand all the reasons for the fluctuation.  Both of these problems complicate our interpretation of what implications, if any, movements in equity markets have for the macroeconomy.\n\nThe equity-premium puzzle is not the only aspect of the behavior of financial asset prices that is difficult to reconcile with economic theory or experience in related markets.  A second puzzle is the \"credit-spread puzzle.\"  The spread between a corporate bond and a similar-maturity Treasury bond compensates an investor for the risk that the bond’s issuer will default and recoveries on the defaulted bond will be low.  Credit-risk spreads vary substantially over the cycle, and right now they are on the low side of historical experience.  However, over long periods, actual percentage losses on corporate bonds have been well below historical averages of credit spreads at all maturities, especially in the high-grade, short-maturity segment of the market.  Again, it is difficult to reconcile this observation with standard models of investor preferences.  Other explanatory factors, such as the different tax treatment of corporate and Treasury bonds, appear to explain only part of the puzzle.\n\nA third puzzle concerns the behavior of financial market volatility.  Volatility and measures of expected volatility derived from options prices vary over time but not in ways that are easy to link to economic fundamentals or to the variation of expected returns in asset markets.  Moreover, the relationship between financial market volatility and the volatility of macroeconomic variables such as GDP is not well understood.\n\nA fourth puzzle related to the pricing of risk concerns the term premium, which is the additional compensation that investors require to hold longer-term securities.  We estimate from the Treasury market that the term premium has declined substantially in recent years to unusually low levels, which has contributed to the inversion of the yield curve.  But we do not understand why, and consequently we do not know to what extent we are seeing a permanent decline in the term premium--perhaps due to a general reduction in the volatility of economic activity and inflation over the past twenty-five years.  Or we may be seeing a temporary decline due to the influence of recent macroeconomic conditions or special factors affecting the demand for long-term bonds.\n\nIn addition to these well-known puzzles, are also a large number of puzzles across all asset markets that I will group under the common theme of risk harmonization.  Risk-harmonization puzzles concern whether a given risk is priced the same way in all markets in which that risk is traded.  In the absence of transaction costs, broadly defined, the law of one price should hold--there should be no risk-free arbitrage--and all risks should be priced the same way in all markets.\n\nIn fact, risk harmonization is limited because transaction costs, viewed in a broad way, are quite material in many markets.  A broad notion of transaction costs includes not only the direct fees paid when transacting and trading but also the full set of risks that are involved when arbitraging among markets.  These include various types of basis risk, which is the risk that long and short positions exposed to the same risk in different markets might not offset each other.  Another important risk is model risk, which is the risk of misjudging an apparent price anomaly when trading owing to not having the correct model.  Some of the conference papers focus on markets in which risk harmonization appears to be incomplete.\n\nHow the Common Lens of Credit Risk Improves Our Understanding\nPapers in this conference contribute to our overall understanding of how credit risk, as well as other risks such as those associated with volatility and liquidity, are priced in financial markets.  One of the contributions of the conference is that it views many of the asset-pricing puzzles through the common lens of credit risk.  This approach holds the hope of addressing the various puzzles in an internally consistent way that helps us to understand how the puzzles may be related.\n\nOne strand of the literature on the equity-premium puzzle attempts to explain the puzzle with a somewhat controversial refinement of standard risk preferences.  The first paper in the conference examines the plausibility of these preferences by analyzing whether they can also explain the average pattern of credit spreads in the bond market.  The results are mixed.  Part of the credit-spread puzzle is explained, lending some credence to these preferences, and suggesting the two puzzles are related, but a part remains unexplained.  And the paper identifies better modeling of the situation in which firms are forced into default as one research direction that may help to explain the remaining part of the credit-spread puzzle.  This research direction is pursued in a separate paper in the conference.\n\nA second paper at the conference also studies the relationship between equity prices and credit risk but does so from a different perspective by asking whether credit risk is appropriately priced in the stock markets.  To address this question, it focuses on companies that are heavily exposed to systematic risk of financial distress--that is, they are relatively likely to experience financial distress during future downturns--and then studies whether the stocks earn a positive premium for this risk.  The main finding is that investors in those companies do not earn a premium for distress risk in the stock market.  This result suggests a possible failure of risk harmonization in the stock market, which in turn raises the deeper question of identifying why this failure in risk harmonization is not arbitraged away.\n\nIn short, papers in the conference deepen our knowledge of some of the asset-pricing puzzles, but they also highlight new aspects of the puzzles that remain to be explained.\n\nHow Credit Derivatives Markets Can Improve Our Understanding\nThis leads to my last topic, which is how credit derivatives markets provide new ways for us to uncover market perceptions of risk.  Like all derivatives (such as options and swaps), credit derivatives allow for credit risk to be unbundled and traded independently from other types of risk, which makes it easier to price and measure the different types of risk.  Here I will focus on two examples.\n\nThe first involves our ability to infer the market's perception of default risk.  In the bad old days, about ten years ago, the best way to infer credit risk was from the prices of corporate bonds, but bond prices are contaminated by differences in coupons, taxes, option-like features, bond covenants, and the illiquidity of the corporate bond market itself.  All of these features meant that the modeling error involved in the process resulted in credit-risk measures that were noisy and potentially biased.\n\nNow, instead of looking to the bond market to measure default risk, we are increasingly turning to the market for credit default swaps, or CDS.  CDS are more standardized than corporate bonds, and, over time, they have also become more liquid.  They therefore provide us with new, and in many cases more precise, measures of credit risk.  These measures in turn can sharpen our measures of the pricing puzzles.  In addition, because the CDS market helps us to strip out the credit-risk component from bond prices, that market also gives us a clearer picture of how important non-credit-risk components of bond prices, such as liquidity, are priced.\n\nThe second example involves the pricing of default correlation.  Default correlation measures the tendency of firms to default at the same time.  Suppose a bank makes a set of loans that appear to be safe when looked at individually.  Whether the loans are likely to default at nearly the same time can represent the difference between whether the bank remains healthy or has the potential to become insolvent.  For this reason, the modeling of default correlations, and how correlations change with economic conditions, is one of the most important inputs into measures of portfolio credit risk at banks.  Default correlations and how they are modeled are also important to bank regulators and are heavily emphasized within the Basel II capital standards.\n\nCollateralized debt obligations, or CDOs, are one of a number of financial instruments whose prices are sensitive to the pattern of default correlations.  As a result, the prices of these instruments provide us with a forward-looking picture of the market’s perception of default correlations and an indication of how the risks of changes in correlation are priced.  Of course, as some of the papers in the conference demonstrate, the pricing of correlation-sensitive instruments is, putting it generously, somewhat less than straightforward.  For that reason, there is substantial model risk involved in making inferences from these prices.  Nevertheless, the prices of these instruments provide a blurry view of default correlations that I expect will improve through time as credit derivatives markets continue to grow and mature.\n\nCredit derivatives, like all derivatives, are in zero net supply, and, abstracting from the very important issue of counterparty credit risk, they neither add to nor subtract from the stock of financial risk in the economy.  They do, however, provide new and more-efficient ways for sharing and hedging the risks that do exist, and they facilitate the transfer of those risks to those who are most willing to evaluate and bear them.\n\nAs a consequence, as the credit derivatives market continues to develop and deepen, my guess, and it is just a guess, is that cleaner measures of credit risk will, all else being equal, reduce the costs of arbitraging between markets and will improve the harmonization of risk across markets--one of the asset-pricing puzzles I highlighted.\n\nTwo of the other puzzles I described earlier are the credit-spread and equity-premium puzzles.  At least a part of these puzzles may be due to imperfect risk sharing among active market participants.  If this is indeed part of the puzzle, then financial innovations such as credit derivatives may, again, all else being equal, reduce long-run average risk premiums in both the equity and credit markets, over time, by facilitating risk sharing among currently active market participants, provided that participants adequately understand and manage the risk of these products.  That said, time will tell whether my speculations on this point are correct.\n\nConclusion\nMy message to you today has been that the Federal Reserve places a lot of emphasis on understanding financial asset prices to help it meet its public policy objectives.  But in doing so, we are handicapped by the extent to which we do not understand important aspects of how financial assets are priced.  Your work as researchers in this field--a portion of which is show-cased at this conference--has been helpful in beginning to explain some of the puzzles, and more recent techniques and ideas together with the data series being generated in new markets hold the promise of more progress in the future.\n\nSo, I will not keep you from your work any longer.  Your contributions are important to the nation’s central bank.  Please, go solve some puzzles.\n\nFootnotes\n\n1.  Mike Gibson, Song Han, Matt Pritsker, and Hao Zhou, of the Board's staff, contributed to these remarks.  Return to text",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20070322a.htm",
        "title": "Asset-Pricing Puzzles, Credit Risk, and Credit Derivatives",
        "date": "3/22/2007"
    },
    {
        "content": "March 12, 2007\n\nGovernor Randall S. Kroszner\n\nTo the National Association for Business Economics 2007 Annual Washington Economic Policy Conference, Arlington, Virginia\n\nI am pleased to be here today at this meeting of the National Association for Business Economics. My subject this afternoon will be inflation dynamics. Since the mid-1980s, we have seen important improvements in these dynamics--inflation is now much lower and more stable than it once was, and it appears to be less closely correlated with movements in other economic factors than it was during the 1960s and 1970s (see table). Moreover, we have seen these improvements not only in the United States but in other countries as well. Questions of intense interest to many of you as well as to us at the Federal Reserve are, What caused these changes in the inflation process? and What are their implications for monetary policy?\n\nHaving spent many years as a University of Chicago professor, my first reaction to these changes is to think “money.” As Milton Friedman famously said many years ago, “Inflation is always and everywhere a monetary phenomenon.” Unfortunately, given the lack of a stable relationship between money growth and inflation, the pure monetarist view has taken a beating since then. However, Friedman was right that inflation is, ultimately, something that central banks determine, at least on average, over time.\n\nMy second reaction is to think about another factor that Friedman emphasized--expectations. Views about the inflation process vary, but expectations are at the heart of almost all of them. And in any model in which expectations are important, monetary policy will also be important. So monetary policy, if not money itself, remains a central determinant of inflation dynamics. Accordingly, one of my principal themes today will be that expectations are important in the inflation process and that the improved conduct of monetary policy, by influencing the formation of expectations in a favorable manner, may account for many of the changes in inflation dynamics that we observe. At the same time, I am wary of ascribing all of the changes in dynamics to monetary policy. We should not place too much faith in any one framework, and so we need to keep an open mind about other possible explanations for the recent changes in inflation dynamics.\n\nBefore proceeding further, let me say that the views I will express today are my own and are not necessarily shared by the other members of the Board of Governors of the Federal Reserve System or the Federal Open Market Committee.\n\nThe Expectational Approach to Thinking about Inflation\nNow almost forty years old, the expectational approach to inflation dynamics--developed simultaneously by Friedman and recent Nobel prize winner Edmund Phelps--is still the dominant framework for thinking about inflation. Let me begin with a quick review of what Friedman and Phelps said forty years ago and then discuss very briefly how it relates to current thinking about the inflation process.\n\nIn Friedman’s framework as expressed in his 1967 presidential address to the American Economics Association, inflation is related to inflation expectations as well as the level of resource utilization. Friedman explained that for a variety of real-world reasons, wages and prices might not always adjust immediately to changes in the money supply. If they did not so adjust, monetary policy could affect resource utilization. The reason that Friedman’s work, and that of Phelps, was so revolutionary was that it overturned the earlier belief that monetary policy could have a permanent influence on resource utilization in favor of a new view that monetary policy could affect real activity only temporarily.\n\nIn the early 1970s, Robert Lucas expanded on the ideas of Friedman and Phelps and noted that shifts in the way a central bank conducts monetary policy imply changes in the way the public forms its expectations.\n\nOver the past thirty years, economists have taken these observations to heart in trying to explain the behavior of overall inflation. One standard approach starts with the notion that many wages and prices adjust only gradually to changes in costs and demand. That assumption about the microeconomic behavior of price setters has recently been bolstered by some research that has looked at the data underlying the consumer price index to assess how often prices change (Bils and Klenow, 2004). The research finds that, indeed, prices for many goods and services appear to adjust only gradually, with the typical firm changing the price of a typical item about once every four months.\n\nWhen wages and prices adjust only infrequently, expectations are important, because firms and households must take into account the demand and supply conditions that will prevail until they again reset their prices. All sorts of expectations will matter, but central among them are inflation expectations: If wages and prices in general are rising over time, then when firms have a chance to reset their prices, they will generally set them higher than they would if the overall price level was holding steady.\n\nBecause the new approaches to understanding inflation are grounded in the behavior of individual decisionmakers, they have the solid theoretical foundations that are valuable for policy analysis.1 Moreover, some evidence indicates that empirical models based on this research do fairly well at forecasting. Of course, time will tell about their usefulness in the day-to-day operations of monetary policy. But the new research does demonstrate the continuing value of the expectations-focused approach that Friedman and Phelps championed forty years ago.\n\nChanges in Inflation Dynamics\nAs I noted earlier, the inflation process seems to have changed in a number of ways in recent years, both in the United States and in other countries. I would like to review these changes and then consider what they may tell us about the underlying processes driving inflation.\n\nOne notable change is that movements in inflation now appear to tell us much less about future inflation than was the case, say, thirty years ago. Here I am talking about predictions of inflation using only information on past inflation, without taking into account any other information. The evidence suggests that, at the peak of U.S. inflation in the late 1970s and early 1980s, the best such “univariate” forecast of inflation--into the indefinite future--was a simple average of inflation over the past few quarters (Stock and Watson, 2007; Cecchetti and others, 2007). In that period, sharp increases in inflation were reversed only slowly. By contrast, shocks to inflation since roughly the mid-1980s have tended to be short-lived, so that the best forecast of future inflation would be a very long average of past inflation. Thus, when inflation moves above its recent long-run average, most of the upswing will likely be quickly reversed, although this result is not guaranteed. That’s a remarkable change in the behavior of inflation. The international evidence indicates that the longevity of inflation shocks has been attenuated in many other countries as well (Cecchetti and others, 2007). Moreover, the timing of the switch from largely permanent to mostly transitory movements in inflation is remarkably similar across the United States and these other countries.\n\nAnother apparent change in the inflation process has been a reduction in the correlation between inflation and unemployment (Atkeson and Ohanian, 2001; Roberts 2006). Now, this relationship was always loose, as most of the historical variation in inflation has reflected influences aside from movements in unemployment or other measures of resource utilization. Still, in the 1960s and 1970s, a reasonably strong empirical relationship between inflation and unemployment could be found for the United States, with inflation tending to rise in periods when unemployment was low and vice-versa. Starting in the 1980s, however, this correlation began to weaken noticeably. In fact, some researchers now find no relationship at all, whereas others tend to find one that is of reduced economic importance.2 Again, similar shifts have been observed in other countries, and these results are not sensitive to whether we are looking at core inflation or total inflation (Borio and Filardo, 2006; Ihrig and others, forthcoming).\n\nNext on the list of changes is the influence of energy prices. During the 1970s, fluctuations in energy prices appear to have had a significant influence on core inflation--that is, on the growth rate of consumer prices excluding food and energy. But since the early 1980s, the inflationary effect of movements in prices for gasoline, natural gas, and other energy goods seems to have declined considerably, even after allowance is made for a secular decline in the energy intensity of the U.S. economy (Hooker, 1996). Indeed, some estimates even suggest that energy price shocks have no effect whatsoever on core inflation. From a cost-accounting perspective, estimates of a zero effect seem too improbable to be taken literally: Recent swings in energy input costs have been sufficiently large that they should have had a noticeable effect on the prices of other goods and services, even allowing for their relatively small share in overall costs. I will return later to possible explanations for the sharp drop in the estimated effects of movements in energy prices.3\n\nFinally, one of the most striking changes in the U.S. economy in recent decades has been the reduction in the economy’s volatility. The standard deviation of quarterly growth of real (that is, inflation adjusted) gross domestic product for the United States since the mid-1980s has been about half that experienced during the 1960s and 1970s. The volatility of inflation has fallen to a similar degree; moreover, the reduction in volatility for both output and inflation is widespread across countries. Of course, a smaller volatility of real GDP is not a change in inflation dynamics. But if monetary policy has been an important factor behind the drop in the economy’s volatility, then the expectational mechanisms may be very similar to those affecting inflation dynamics.4\n\nExpectations, Monetary Policy, and Changing Inflation Dynamics\nAs Lucas pointed out, because expectations matter for inflation, monetary policy matters for inflation, too. And the historical record supports the notion that, starting with Chairman Paul Volcker, U.S. monetary policy has been more focused on low and stable inflation than was the case in the 1960s and 1970s (Romer and Romer, 2002). So it is natural to ask, can changes in the conduct of monetary policy in the United States (and elsewhere) help to account for the changes we’ve seen in inflation dynamics?\n\nThe strongest case for a link between monetary policy and changes in inflation dynamics is in the greater stability of inflation. Inflation is clearly under the long-run control of the Fed, and the relative stability of inflation clearly reflects the action of monetary policy. Thus, if the central bank wants to keep inflation low on average over time, it can surely do so. The case for monetary policy contributing to reduced volatility of inflation is also fairly straightforward: The central bank can stabilize inflation by raising and lowering interest rates to lean against inflationary disturbances.\n\nOnce we take account of the role of expectations, the stabilizing effects of monetary policy become even greater: If economic decision makers come to realize that the Fed is doing more to stabilize inflation, then shocks that push up inflation will lead to smaller increases in inflation expectations than in the past. Because current inflation is affected by inflation expectations, the smaller increase in expected inflation will lead to a smaller increase in actual inflation as well. And because many shocks that may lead to inflation, such as unexpected surges in spending, also cause movements in output and employment in the same direction, the maintenance of price stability promotes the stability of the real economy.\n\nThis experience of low and stable inflation, coupled with the Fed’s clear statements of commitment to maintaining this performance, has no doubt contributed to the stability of long-run inflation expectations in the past decade or so. This stability has been remarkable. By one measure--from the Philadelphia Fed’s Survey of Professional Forecasters--long-run inflation expectations have barely budged since 1998. Other measures have varied a bit more, but overall, the movements in the expectational indicators have been quite small.\n\nBetter monetary policy may also help explain the apparent decline in the sensitivity of inflation to resource utilization. We might interpret the reduced statistical correlation between unemployment and inflation as evidence of a decline in the direct effect of resource utilization on inflation. But given that the conduct of monetary policy was changing at the same time, it may be premature to draw such a conclusion. Consider the following thought experiment. Suppose that the Federal Reserve managed to stabilize inflation perfectly. That outcome would eliminate any empirical correlation between inflation and unemployment even if there really was an underlying relationship between inflation and resource utilization operating through the influence of the latter on, say, marginal labor costs.5 As this example illustrates, the correlation between unemployment and inflation may have no bearing on whether these variables are truly linked structurally.\n\nI hasten to add that I am not advocating that the Fed stabilize inflation perfectly--this is simply an illustrative example. So let’s consider another alternative: Suppose that the Fed is willing to accept some temporary deviation of inflation from its desirable level to moderate an accompanying weakness in real activity, as might occur in the face of an adverse productivity shock. In this instance, the most likely correlation between inflation and unemployment would be positive--that is, under these conditions the relationship between inflation and unemployment would be the exact opposite of the predictions of the old-fashioned Phillips curve. And again, this result could arise even though the structure of the economy was such that an increase in resource utilization would tend to put upward pressure on production costs and thus prices.\n\nFrom this perspective, the declining correlation of resource utilization with inflation may be an indication of the success of monetary policy in pursuing its dual mandate of price stability and maximum sustainable growth: Because the Fed is trying to stabilize both inflation and real activity, then, when faced by shocks that push these variables in the same direction, the Fed will want to try to offset both adverse developments to the extent that it can. Thus, I see the reduced correlation between inflation and unemployment as an indication of the success of monetary policy in this dimension.\n\nFurther evidence that better monetary policy and accompanying expectational effects have promoted a more stable economy is provided by the rather muted inflationary effects of the recent sharp increases in crude oil prices. In the 1970s, inflation moved up sharply with increases in crude oil prices. Moreover, not only did overall inflation move up, but core inflation, wages increases, and inflation expectations moved up as well. In response to the resulting high inflation, the Fed was obliged to raise interest rates, and the economy weakened. The contrast with recent performance is quite stark. True, overall inflation moved up with energy prices, and some of the pickup in core inflation last year probably reflected the transitory effects of the pass-through of increased energy costs. However, that pass-through was a mere ripple compared with the behavior of the 1970s. Similarly, when gasoline prices surge, surveys of household inflation expectations still move up, but not for long. By contrast, in the 1970s, survey expectations moved up sharply and remained elevated in the wake of the two oil shocks. Indicators of long-term inflation expectations did not exist in the 1970s, but in the current period, the stability of these expectations has been remarkable. As I shall discuss, other explanations for these changes exist, but in my view, the effects of monetary policy are the most plausible.\n\nWhat can the international experience tell us about the likely sources of the changes in inflation dynamics? First, we need to acknowledge that many of the changes we have seen in U.S. inflation dynamics have also occurred in other countries. That fact suggests that at least some of the explanations of the change in inflation dynamics should be common across countries rather than country-specific. If monetary policy is central to these changes, it must be the case that many countries have made similar changes to monetary policy.\n\nAs I noted in a speech last fall, one possible reason for such common changes in monetary policy may have been greater currency competition (Kroszner, 2006). In broad terms, the idea is that increased globalization, deregulation, and innovation raised the returns to low inflation--and increased the penalties for high inflation--relative to results obtained twenty or thirty years ago. For example, deregulation has led to an opening of capital markets, and hence financial globalization, which has in turn boosted innovation and helped to increase global competition by shrinking barriers of time and distance. Accordingly, trade and financial linkages between countries have tightened tremendously in recent years.\n\nMeanwhile, substantial financial innovations--including advances in electronic payment systems and trading systems as well as more widespread credit card networks and increased use of mutual funds--have facilitated the movement of wealth around the globe. As a result, deregulation, globalization, and innovation have made it easier for citizens to move their wealth out of nominal assets in their local currency and thereby avoid any inflation tax should their government show signs that it might resort to inflationary tactics to finance spending.6 At the same time, the public’s understanding of the costs of inflation has increased, in part because of experiences of high inflation in many countries in the 1980s. Almost everywhere, public opinion eventually turned against allowing inflation to continue. This public pressure has reinforced the trend against inflationary policies.\n\nIncreased competition among currencies, driven by the confluence of factors that I just described, has limited the ability of governments and central banks to pursue high-inflation policies. Moreover, currency competition has raised the costs of poor policy and thus increased the incentives of the monetary authorities to maintain low inflation.\n\nMany of these arguments will apply with greater force in developing economies, where the costs of poor policies have been demonstrated quite clearly. Nonetheless, I think that currency competition has played at least some role in disciplining policy in the United States and other developed countries.\n\nOther Explanations for the Change in Inflation Dynamics\nOf course, monetary policy may not be the whole story, and we need to resist embracing any single explanation too wholeheartedly. There may be other reasons for the changes in inflation dynamics. For example, the reduced sensitivity of core inflation to oil and natural gas prices likely also reflects both the increased energy efficiency of the economy and the fact that shocks to the prices of these goods since the mid-1980s have, at least until the latest episode, been viewed as mostly temporary. In contrast, the rise in oil prices during the 1970s was probably seen at the time as largely reflecting a permanent shift in global demand/supply balances.\n\nAnother factor that might help to account for some of the changes in inflation dynamics is globalization. Because national markets have become more open to international trade, domestic firms and workers face more competition and have less market power than in the past. This development could help to account for any reduced sensitivity of U.S. inflation to domestic resource utilization. In fact, one recent study even purports to show that foreign output gaps are more important in explaining domestic inflation in industrialized countries than domestic factors (Borio and Filardo, 2006). However, this result has been challenged by the Federal Reserve staffers, who find that estimates to this effect are fragile.7 That said, this is an issue that merits close monitoring as globalization continues.\n\nOther factors may also be at work, such as the deregulation of the 1980s and the faster productivity growth we have seen over the past decade. But I think that even after we have given these factors their appropriate due, the evidence still suggests that better monetary policy explains much (albeit not all) of the changes in inflation dynamics that have occurred. In fact, it is interesting to speculate on the degree to which better monetary policy might account for some of the structural factors I have listed. Consider faster productivity growth. High and variable inflation likely creates a distraction for firms--managers must pay attention to the damage that inflation can do to their balance sheets. They thus divert their attention from improving products and services to financial management. Such distraction likely hurts the productivity of firms. Although I don’t think low inflation is the only factor behind the rebound in productivity growth in the United States--after all, other countries did not see such an acceleration in output per hour as inflation came down--I think it has played a role.\n\nPolicy Implications\nA review of the possible causes of the changes in inflation dynamics naturally leads to the question of their implications for the conduct of monetary policy. In today’s economy, it is very difficult to know whether any given change in output or employment will have inflationary consequences. One lesson that is fair to draw, however, is that resource utilization generally does not tell us much about the future course of inflation over the next year or two. Rather, the near-term inflation outlook is more likely to be dominated by cost factors, such as productivity growth and the price of raw materials, than by the tightness of labor and product markets. Furthermore, the weak relationship between inflation and the unemployment rate means that it is probably more difficult than ever to gauge the economy’s productive potential--and hence estimate so-called output gaps--especially in real time. In light of these uncertainties, prudent policymakers should take an eclectic approach and base their policy decisions on both a wide variety of indicators and views about how the economy may work and avoid a narrow focus on economic slack.\n\nMy earlier comments also underscored the central importance of expectations to the successful conduct of monetary policy. In particular, the Federal Reserve and many other central banks appear to have succeeded in anchoring long-run inflation expectations--an achievement that has contributed to macroeconomic stability and eased the task of monetary policy. However, bad luck or other factors could cause expectations to begin to drift again. If so, the Federal Reserve will need to respond appropriately. A problem of this sort is probably fixed most easily if it is detected early, and thus policymakers should closely monitor the available indicators of expectations to head off any trouble as soon as possible.\n\nThe final lesson I draw is a cautionary note: The stability of inflation could lead to complacency. As long as inflation expectations are well anchored, actual inflation will have a natural tendency to revert to the anchor of long-run inflation expectations. Under such circumstances, policymakers may be tempted to relax their resolve in responding to potentially inflationary developments. Such relaxation could be costly, however. Inflation expectations have become well-anchored because the public has become confident that the Federal Reserve will do the right thing. But this belief will persist only as long as we on the Federal Open Market Committee continue to ratify the public’s expectations that inflation will remain low and stable. Thus, complacency would be a threat to the credibility that the Federal Reserve has worked so hard to acquire, and its loss would likely mean the reversal of many of the favorable inflation developments seen over the past two decades.\n\nOne message that I hope has been clear is that there is much we don’t know about the inflation process. Policymakers would of course like to be 100 percent confident that they have the right way of looking at the world. But I think we always need to be open to the possibility that other forces may be at work or that other interpretations better explain what we’ve observed. We need to approach our task with a certain degree of humility and an open mind.\n\nStill, I think we can be fairly certain that low and stable inflation has been brought about by guarding against looming inflation risks, and continuing in this vein seems sensible to me. Above all, we must continue to conduct policy in such a way as to keep inflation low and stable--an approach that also promotes full employment and maximum sustainable real growth of the economy.\n\nReferences\n\nAtkeson, Andrew, and Lee H. Ohanian (2001). “Are Phillips Curves Useful for Forecasting Inflation?” Federal Reserve Bank of Minneapolis, Quarterly Review, vol. 25 (Winter), pp. 2-11.\n\nBernanke, Ben S. (2007). “Globalization and Monetary Policy,” speech delivered at the Fourth Economic Summit, Stanford Institute for Economic Policy Research, March 2.\n\nBils, Mark, and Peter J. Klenow (2004). “Some Evidence on the Importance of Sticky Prices,” Journal of Political Economy, vol. 112 (October), pp. 947-85.\n\nBorio, Claudio, and Andrew Filardo (2006). “Globalization and Inflation: New Cross-Country Evidence on the Global Determinants of Domestic Inflation,” unpublished paper, Bank for International Settlements, March.\n\nCecchetti, Stephen G., Peter Hooper, Bruce C. Kasman, Kermit L. Schoenholtz, and Mark W. Watson (2007). “Understanding the Evolving Inflation Process,” paper prepared for the U.S. Monetary Policy Forum 2007 (February), http://research.chicagogsb.edu/gfm/events/conferences/index.aspx.\n\nChristiano, Lawrence J., Martin Eichenbaum, and Charles L. Evans (2005). “Nominal Rigidities and the Dynamic Effects of a Shock to Monetary Policy.” Journal of Political Economy, vol. 113 (February), pp. 1-45.\n\nHooker, Mark A. (1996). “What Happened to the Oil Price-Macroeconomy Relationship?” Journal of Monetary Economics, vol. 38, pp. 195-213.\n\nIhrig, Jane E., Steven B. Kamin, Deborah Lindner, and Jaime Marquez (forthcoming). “Some Simple Tests of the Globalization and Inflation Hypothesis,” International Finance Discussion Papers. Washington: Board of Governors of the Federal Reserve System.\n\nIhrig, Jane E., Mario Marazzi, and Alexander D. Rothenberg (2006). “Exchange-Rate Pass-Through in the G-7 Countries,” International Finance Discussion Papers 851. Washington: Board of Governors of the Federal Reserve System, January.\n\nKroszner, Randall S. (2006). “The Conquest of Worldwide Inflation: Currency Competition and Its Implications for Interest Rates and the Yield Curve,” speech delivered at the Cato Institute Monetary Policy Conference, Nov. 16, www.federalreserve.gov/newsevents/speech/kroszner20061116a.htm.\n\nRoberts, John M. (2006). “Monetary Policy and Inflation Dynamics,” International Journal of Central Banking, vol. 2 (September), pp. 193-230.\n\nRomer, Christina D., and David H. Romer (2002). “The Evolution of Economic Understanding and Postwar Stabilization Policy,” (439 KB PDF) in Rethinking Stabilization Policy, symposium sponsored by the Federal Reserve Bank of Kansas City, August 29-31. Kansas City: the Reserve Bank, pp. 11-78.\n\nStock, James H., and Mark W. Watson (2007). \"Why Has U.S. Inflation Become Harder to Forecast?\" Journal of Money, Credit, and Banking, supplement to vol. 39 (February), pp. 3-33.\n\nThomas, Charles P., and Jaime Marquez (2006). “Measurement Matters for Modeling U.S. Import Prices,” International Finance Discussion Papers 883. Washington: Board of Governors of the Federal Reserve System, December.\n\nU.S. Department of the Treasury, Board of Governors of the Federal Reserve System, and U.S. Secret Service (2006). The Use and Counterfeiting of United States Currency Abroad, Part 3. Washington: Department of the Treasury, September, www.federalreserve.gov/newsevents/press/other/20061025a.htm.\n\nWoodford, Michael (2003). Interest and Prices. Princeton: Princeton University Press.\n\nFootnotes\n\n1. The academic literature refers to this new generation of macroeconomic models as dynamic stochastic general equilibrium models. Christiano, Eichenbaum, and Evans (2005) is one of the most prominent examples of this new approach. Return to text\n\n2. Atkeson and Ohanian (2001) argue that the unemployment rate no longer has any ability to forecast inflation, while Roberts (2006) argues that the correlation has fallen but is still nonzero. Return to text\n\n3. One area in which the pattern of smaller correlations with inflation does not hold is import prices. After adjusting for the rising share of imports in domestic price increases, we see little indication of a reduction in the effect of import prices on U.S. inflation. We have some evidence, however, of a reduced effect of exchange rates on import prices (Ihrig, Marazzi, and Rothenberg, 2006), although this result may be sensitive to specification (Thomas and Marquez, 2006). Return to text\n\n4. One key element of the inflation process that I have not yet mentioned is labor costs. Recent developments in labor markets make it difficult to assess changes in the role of labor costs in the inflation process. For example, since the mid-1990s, incentive-based employee stock options have become an important form of compensation. This development has created measurement difficulties: The government’s principle measure of labor compensation accounts for such options at the time they are exercised (thereby conflating them with capital gains), rather than recording them at their expected value at the time of issuance. As a result, the published compensation data provide a distorted picture of labor costs. Return to text\n\n5. Woodford (2003) includes results of this sort. Return to text\n\n6. For emerging-market countries that had experienced high inflation, another aspect of globalization fostering currency competition is the large amount of physical dollars now present in these countries, which allows citizens to conduct transactions and store liquid wealth without holding the local currency. Over one recent period, the fraction of U.S. currency estimated to be held in foreign countries rose dramatically, from less than one-fifth in 1980 to as much as two-thirds in the late 1990s, and today the total nominal amount is in the neighborhood of $400 billion, or somewhat more than one-half (U.S. Department of the Treasury and others, 2006). Return to text\n\n7. As noted in Bernanke (2007), Ihrig and others (forthcoming) find that these results are sensitive to details of specification. Return to text\n\n\n\nReturn to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20070312a.htm",
        "title": "The Changing Dynamics of Inflation",
        "date": "3/12/2007"
    },
    {
        "content": "March 09, 2007\n\nVice Chairman Donald L. Kohn\n\nAt the U.S. Monetary Policy Forum, Washington, D.C.\n\nPresented identical remarks\n\nThis was an interesting paper with an important objective--to contribute to our understanding of inflation dynamics, with a particular focus on the so-called Great Inflation of the 1970s.1 The better we understand history, the less likely we are to repeat it, even on a reduced scale. Of course, the basic problem in tackling the inflation of the 1970s is that we have one observation and many competing theories. The authors’ approach is to consider the experiences of other major industrial countries during that period with the hope that cross-country differences will provide a way to discriminate among the potential causes.\n\nThe authors’ principal empirical finding is that the onset and, to a lesser degree, the end of the Great Inflation were closely synchronized across a number of countries. On the basis of this timing and other findings, they reject many of the theories about why inflation rose in the period from the late 1960s through the end of the 1970s. According to the authors, the Great Inflation did not stem solely from adverse structural changes to the economy, or from policymaker misunderstandings about the nature of the inflation process, or from errors in gauging aggregate resource utilization. Instead, they say, changes in monetary policy preferences, perhaps accentuated by political influences, are the key part of the story.\n\nI certainly agree with the authors that high inflation could not have emerged and then persisted without an accommodative policy regime and that a period of tight policy was required to produce the sustained moderation in inflation that followed. However, their conclusions do not fit easily with my premise, which is that policymakers did not seek the result they obtained. In thinking about the story told by the paper, I also wonder why political forces and central-banker preferences would necessarily shift at the same time in so many countries. So I am left with the question of why policy was conducted the way it was.\n\nMy impulse when it comes to selecting from the list of causative factors in the paper is to say “all of the above.” Policymakers in the 1970s--the Federal Reserve among them--were dealt a very bad hand that, for a variety of reasons, they played poorly. The second half of the 1960s saw rising inflation driven by excess demand--a situation not countered sufficiently by a Federal Reserve that perhaps was more subject to political pressures than it should have been. By the early 1970s, politicians as well as the Federal Reserve recognized that inflation was excessive but seemed worried about the cost of reducing it. Those concerns gave rise to the Nixon wage and price controls (which ran from 1971 to 1974).\n\nInto this mix were added some very adverse supply shocks that elevated the costs of disinflation. Productivity growth slowed appreciably in the early 1970s; labor force developments raised the nonaccelerating-inflation rate of unemployment (NAIRU); and oil prices spiked. Moreover, as inflation remained elevated, inflation expectations rose and institutions adapted--for example through cost-of-living adjustments in labor contracts--thus further escalating the presumed cost of disinflation.\n\nNot only were the output costs of disinflation seen to be high but also the monetary policy needed to bring inflation down was consistently miscalculated--economists during the 1970s persistently overestimated both the speed and magnitude of the slowdown in real activity and inflation that would result from a given rise in the federal funds rate. Part of this miscalculation reflected a judgment that the economy and financial markets were fragile and that small changes in market interest rates would have major effects on aggregate spending (for example, as a result of disintermediation induced by ceilings on the interest rates that banks and thrifts could pay).\n\nBut, in addition, the size of the output gap was not correctly perceived. I agree with the authors that this misperception could not have been the whole story; surely consistent surprises on inflation should have been a strong clue that something was amiss. But I was not entirely persuaded by the authors’ arguments seeking to minimize this factor, especially those that rely on “new estimates of real-time output gaps,” a bit of an oxymoron given that you cannot really produce a new real-time estimate of a constructed series like the output gap. Perhaps the gap series produced by the Council of Economic Advisers was viewed skeptically by some contemporary observers, but it was the “official” series published by the Commerce Department, and it was referred to by the Federal Open Market Committee (FOMC) in its policy deliberations. It does not surprise me that forecasters took several years to catch up to the adverse developments in trend productivity and the demographic factors that boosted the NAIRU; in the 1990s, we took a while to realize the implications of favorable movements in both variables even though we were aware from the experience of the 1970s that such changes were possible. Moreover, the oil price shocks, analysis of which was not part of the standard tool kit of economists forty years ago, complicated matters considerably.\n\nFor a variety of reasons, then, forecasters consistently underpredicted the future level of inflation, seeing considerably more disinflation from a particular policy stance than in fact occurred. This underprediction was true for the Board staff’s outlook prepared for each meeting of the FOMC, in which inflation forecast errors persisted for some time, and it was also true for the bond market, as realized real interest rates, even at longer maturities, were very low or negative.\n\nThese experiences formed the backdrop for the FOMC’s October 6, 1979, adoption of a policy targeting the monetary aggregates, a policy whose particular characteristics can be seen as originating from three observations about the 1970s. First, inflation was far too high; it was corroding the economic system; and it was the central bank’s responsibility to bring it under control. Second, under the circumstances, economic forecasts had proved to be essentially worthless as a basis for setting monetary policy, so the central bank had to try a different approach for re-establishing price stability--namely, targeting the medium-term growth of the money supply. The third observation was that part of the problem with policy had been its excessive gradualism, possibly to some degree a reaction to political and public pressure. Because interest rates would probably have to move quite a bit to bring inflation down and no one at the time knew by how much, rate movements needed to be less discretionary; one way to achieve this was by changing the focus of policy to rates of money growth.\n\nThe memory of the 1970s and the subsequent disinflation are very much alive at the Federal Reserve and, I suspect, at every other central bank. What major lessons have we drawn from that era? First, low and stable inflation--effective price stability--is a necessary condition for the economy to realize its full potential for sustained increases in living standards. I doubt that high inflation was the proximate cause of the 1973 productivity slowdown, which persisted for a decade after 1983 to 1986, the period the authors give for the end of the Great Inflation. And I agree with them that low inflation was not the only cause of the Great Moderation, but I am confident that we would not have experienced more than two decades of nearly uninterrupted growth if the Federal Reserve had not brought inflation down in the early 1980s and kept it low thereafter. Low inflation reduces distortions from signals in market prices and facilitates longer-term planning. Low and stable inflation also anchors inflation expectations; in turn, anchored expectations make it easier for the central bank to control inflation with smaller variations in real activity.\n\nThat brings me to my second major lesson: Expectations are critical to policy success. Expectations about future policy help to determine the financial conditions that affect spending and inflation. In most situations, policy will need to be conducted so that expected real interest rates are positive; a policy that pushes expected real rates below zero would be appropriate only in special circumstances, such as when real activity is expected to be persistently weak and inflation undesirably low. Likewise, inflation expectations are critical: Increases in expectations of inflation elevate the cost of returning to price stability, and unanchored expectations make it very difficult to understand where the economy is and where it is going.\n\nAs a consequence, I do not agree with the authors’ assertion that central banks pay too much attention to inflation expectations. Those expectations may not be as much of a leading indicator of the inflation trend as I would like--although I am not sure that I find the paper’s conclusion, that the trend leads expectations, all that persuasive. Indeed, if I accept the authors’ assertion that post-1984 expectations have been a lagging indicator of the trend, then, when expectations rise and stay elevated, I should weight that observation heavily, not lightly. Policy cannot be conducted based solely on such a lagging or coincident indicator of inflation, but, of course, it is not. We pay close attention to those factors that are influencing the outlook for inflation, as for example, the level of resource utilization highlighted in our recent announcements. But a clear lesson of the 1970s is that a central bank must keep a very close eye on sustained movements in inflation expectations.\n\nI think a third lesson is humility--we should always keep in mind how little we know about the economy. Monetary policy operates in an environment of pervasive uncertainty--about the nature of the shocks hitting the economy, about the economy’s structure, and about agents’ reactions. The 1970s provide a sobering lesson in the difficulty of estimating the level and rate of change of potential output; these are quantities we can never observe directly but can only infer from the behavior of other variables. We cannot effectively implement policy without some reference to the likely level of potential, given that demand-supply pressures are an integral part of the monetary transmission mechanism. But we must be realistic about the accuracy of our estimates of potential while always doing our best to improve them.\n\nEven today, we face a number of sources of uncertainty about the nature of the inflation process. Interestingly, we can draw several examples from the paper. First, any estimate of the persistent component of inflation is inherently uncertain. Comparing the trend inflation estimates for the gross domestic product of the United States in the current study with related estimates based on Stock and Watson’s recent paper on the topic reveals an important difference: In the latter, the standard deviation to the innovation of the permanent component of inflation from 1997 onward is not zero but rather permits a small but significant amount of drift in the trend from its late-1990s trough.2 Other trend-extraction exercises imply either a stable trend or a trend that has drifted up over the past several years; likewise, the behavior of estimated trend inflation over this period differs according to the inflation concept used. Although these differences are small relative to the overall variation in the trend over the past forty years, they are assuredly large enough to be meaningful to a monetary policy maker.\n\nIn addition, we do not yet have a consensus structural model of inflation dynamics that satisfactorily explains all the important aspects of the empirical data. As the paper demonstrates, a standard workhorse model--a sticky-price business cycle model with a drifting inflation target and a New Keynesian Phillips curve--cannot mimic one important feature of the inflation process. I will leave it to the modelers to debate the seriousness of this deficiency and to propose how it might be rectified. However, it is clear to me that our understanding of the inflation process still has far to go.\n\nThe issue of expectations illustrates our ignorance. As I have already indicated, inflation expectations are among the most important variables policymakers monitor, but we do not have answers to our most basic questions about them: Are available measures suitable indicators of true inflation expectations by households and businesses? How are expectations formed--and in particular what are the respective roles of central bank talk, central bank actions, and actual inflation outcomes? And how do expectations influence price and wage setting? In short, although I believe that inflation expectations are critical to assessing the inflation outlook, I cannot be sure (particularly in real time) that our expectational measures are accurate and so cannot know what precise role expectations play in wage and price dynamics.\n\nIn sum, then, I found reading the paper to be useful and instructive because it reminded me not only of what we have learned but also of what we still do not know.\n\nFootnotes\n\n1.  Alan Kackmeister and Jeremy Rudd, of the staff at the Board of Governors, assisted in the preparation of these comments.  The views expressed here are my own and are not necessarily those of the Board of Governors or its staff. Return to text\n\n2.  James H. Stock and Mark W. Watson (2007), \"Why Has U.S. Inflation Become Harder to Forecast?\" Journal of Money, Credit, and Banking, supplement to vol. 39 (February), pp. 3-33. Return to text",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20070309a.htm",
        "title": "Comments on “Understanding the Evolving Inflation Process” by Cecchetti, Hooper, Kasman, Schoenholtz, and Watson",
        "date": "3/9/2007"
    },
    {
        "content": "March 09, 2007\n\nGovernor Randall S. Kroszner\n\nTo the U.S. Monetary Policy Forum, Washington, D.C.\n\nI’m pleased to be here today to participate in this discussion about liquidity and monetary policy. Apparently many others are having a similar discussion--a quick search of LexisNexis turned up 2,795 separate articles in the past six months alone that mentioned the word \"liquidity\" in the context of its abundance in financial markets. The use of the term liquidity in these articles spans a wide variety of meanings--perhaps 2,795 of them!\n\nRather than grapple with the definition of the rather slippery concept of liquidity in my short remarks here, I will focus on aspects of global financial markets that many analysts associate with liquidity and that are particularly relevant for monetary policy--namely, the relatively low level of long-term interest rates in both real and nominal terms and the resulting relatively flat slope of yield curves around the world.1 I want to emphasize that, given the openness of capital markets, explanations of these phenomena should have an international component. I will argue that the flatness of the term structure reflects both the global balance of saving and investment, through its effect on real interest rates, and currency competition, through its effect on expected inflation and inflation risk. I should note at the outset that the views I express here are my own and not necessarily those of my colleagues in the Federal Reserve System.\n\nOf course, flat and even inverted yield curves in advanced economies are nothing new. We know that the short end of the yield curve is dominated by monetary policy and cyclical factors. To abstract from the potential effects of cyclical factors on the yield curve, consider the pattern of forward rates many years into the future, at which point the effects of current cyclical shocks would be expected to no longer be important.2 Such forward rates reflect not only market expectations of future short-term interest rates but also term premiums to compensate for the risk associated with commitments to extend credit so far in the future, including the risk of future inflation. The short-term forward rate ten years ahead estimated from the yield curve for U.S. Treasury coupon securities has ranged between 4-3/4 percent and 5-3/4 percent over the past year, nearly 175 basis points (that is, about 1-3/4 percent) below its average level since 1990. Far-forward rates in many other advanced economies have also declined on balance over the past decade, and they are currently around the low end of their historical ranges.3\n\nLow far-forward rates have both a real and a nominal component. I believe that relatively low real forward rates are likely driven at least in part by what some have called a global saving glut.4 That is, high rates of saving relative to investment in other parts of the world have resulted in a relatively large stock of funds seeking investment outlets around the globe. The resulting strength of demand has kept real long-term interest rates lower than they might otherwise have been in many countries, and particularly in the United States because it is a relatively attractive place to invest. In addition, the lower variability in real macroeconomic variables over the past two decades relative to the previous three decades, often dubbed the \"Great Moderation,\" may well have trimmed real term premiums relative to their past levels.\n\nLet’s now consider possible reasons for relatively low nominal far-forward rates and, as a consequence, relatively flat (or in some cases) inverted yield curves. I will emphasize three significant changes in most countries around the world that point to a reduction in the compensation required by investors for the possible effects of future inflation on the returns to holding long-term bonds: Namely, declines in (1) actual inflation, (2) inflation volatility, and (3) expectations of long-term inflation, both survey based and \"market based.\" These worldwide effects have been particularly pronounced in emerging markets.\n\nWhat I have elsewhere referred to as \"currency competition\" may be a driving force behind these changes in inflation outcomes and expectations and may be reflected in low nominal long-term interest rates and the flattening of yield curves.5 I’d like to briefly review what I mean by currency competition. A confluence of four factors--deregulation, globalization, financial innovation, and the public’s increased understanding of the costs of inflation--has fostered a form of competition among currencies. That competition, in turn, has led to improved central bank performance and thereby contributed to the recent conquest of worldwide inflation.\n\nThe factors that drive currency competition are closely related and mutually reinforcing in many respects.\n\nIncreased competition among currencies, driven by the confluence of factors that I just described, has improved central bank performance by changing the ability and the incentives of governments and central banks to pursue high-inflation policies. In effect, currency competition has raised the costs of poor policy and thus promoted central bank incentives to maintain low inflation.\n\nThis change has led many governments to be more willing to adopt institutional changes to improve central bank governance that have bolstered central bank credibility for maintaining low inflation. Such changes have included greater central bank independence, which has typically been granted in conjunction with a mandate including the achievement of low and stable inflation as one of the goals of monetary policy. That is, policy is credible because the central bank’s objectives are clear to the public and because the central bank can be held accountable for failing to achieve its objectives. When citizens are more aware of the costs of inflation, and when governments are less able to reap benefits from high inflation, institutional reforms that make central banks more credible and independent are more likely to be adopted and sustained.7\n\nLet me make it clear that I think that currency competition has been a much larger force in emerging-market economies, in which the costs of poor policies have been demonstrated quite clearly, than in advanced economies. Still, at least some degree of currency competition may be at work in the United States as well: One could argue that competition faced by the dollar, particularly abroad, is likely greater now than before the introduction of the euro because of the scale of euro-denominated financial markets and the financial institutions in the euro area, but I would not want to push that argument too far.\n\nThe forces behind currency competition that have bolstered incentives for central banks to maintain low inflation and so have helped anchor inflation expectations are likely to persist and perhaps strengthen. The ease with which funds move across capital markets should continue to ensure that the responses to inflationary central bank policies will be swift and significant. The resulting incentives provided by currency competition should continue to foster relatively low far-forward nominal interest rates in many countries. As long as capital markets remain open and people remain aware of the costs of high inflation policies, I believe that the forces behind the low level of long-term interest rates and hence the general flatness of yield curves around the globe will tend to persist for some time.\n\nFootnotes\n\n1. For more discussion, see Alan Greenspan (2005), \"The Federal Reserve Board’s Monetary Policy Report to the Congress,\" statement before the Committee on Banking, Housing, and Urban Affairs, U.S. Senate, February 16, www.federalreserve.gov/boarddocs/hh/. See also Randall S. Kroszner (2006), \"Why Are Yield Curves So Flat and Long Rates So Low Globally?\" speech delivered at a meeting of the Institute of International Bankers, New York, June 16, www.federalreserve.gov/newsevents/speech/2006speech.htm. Return to text\n\n2. If you could borrow and lend at the same rate as the U.S. Treasury, then you could lock in a three-month loan ten years from now by borrowing for ten years and three months and simultaneously lending the same principal for ten years. The difference between the interest you pay and the interest you earn on the transaction determines the implied forward rate ten years from today. Return to text\n\n3. Far-forward rates in yen are up almost 2 percentage points from their historic lows in the second quarter of 2003, but they are at a level that is obviously still low in a historical context. Return to text\n\n4. See Ben S. Bernanke (2005), \"The Global Saving Glut and the U.S. Current Account Deficit,\" Sandridge Lecture delivered at a meeting of the Virginia Association of Economists, Richmond, Va., March 10, www.federalreserve.gov/newsevents/speech/2005speech.htm. Return to text\n\n5. See Randall S. Kroszner (2006), \"The Conquest of Worldwide Inflation: Currency Competition and Its Implications for Interest Rates and the Yield Curve,\" speech delivered at the Cato Institute Monetary Policy Conference, Washington, November 16, www.federalreserve.gov/newsevents/speech/2006speech.htm. Return to text\n\n6. For emerging-market countries that had experienced high inflations, another aspect of globalization fostering currency competition is the amount of physical dollars now present in these countries that allow citizens to conduct transactions and store liquid wealth without holding the local currency. Over one recent period, the fraction of U.S. currency estimated to be held in foreign countries rose dramatically, from less than 20 percent in 1980 to as much as two-thirds in the late 1990s, and today the total nominal amount is in the neighborhood of $400 billion. See U.S. Department of the Treasury, Board of Governors of the Federal Reserve System, and U.S. Secret Service (2006), The Use and Counterfeiting of United States Currency Abroad, Part 3, report to the Congress (Washington: Department of the Treasury), October 25, www.federalreserve.gov/newsevents/press/other/20061025a.htm. Return to text\n\n7. In a paper with Douglas Irwin, I documented a similar dynamic at work in the gradual reversal of protectionist policies in the United States in the 1930s and 1940s. See Douglas A. Irwin and Randall S. Kroszner (1999), \"Interests, Institutions, and Ideology in Securing Policy Change: The Republican Conversion to Trade Liberalization after Smoot-Hawley,\" Journal of Law and Economics, vol. 42 (October), pp. 643–73.Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20070309a.htm",
        "title": "Liquidity and monetary policy",
        "date": "3/9/2007"
    },
    {
        "content": "March 06, 2007\n\nChairman Ben S. Bernanke\n\nBefore the Independent Community Bankers of America's Annual Convention and Techworld, Honolulu, Hawaii(via satellite)\n\nThe subject of my remarks today is the regulation and supervision of two large financial companies: the Federal National Mortgage Association (known familiarly as Fannie Mae) and the Federal Home Loan Mortgage Corporation (or Freddie Mac).  Fannie Mae and Freddie Mac were created by acts of the Congress and are thus known as government-sponsored enterprises, or GSEs.  The Congress chartered these two companies with the goal of expanding the amount of capital available to the residential mortgage market, thereby promoting homeownership, particularly among low- and middle-income households.  Although they retain their government charters, Fannie and Freddie were converted (in 1968 and 1989, respectively) to private, publicly traded, for-profit companies.1\n\nFannie and Freddie are regulated by the Office of Federal Housing Enterprise Oversight (OFHEO), with additional oversight by the Department of Housing and Urban Development (HUD).  The regulatory framework under which the GSEs operate has two principal objectives: first, to support the GSEs’ mission of promoting homeownership, especially access to affordable housing; and second, to ensure that these two companies operate in a financially prudent manner.  For various reasons, including recent problems with accounting and internal controls at the GSEs, a consensus appears to exist that the regulatory and supervisory framework needs to be strengthened, and the leaders of the banking committees in the Congress have expressed optimism that agreement on legislation can be reached this year.\n\nThe Federal Reserve Board concurs that a stronger regulatory framework for the GSEs is needed, and we hope to see a bill passed this year that addresses the important public policy issues raised by the operations of these entities.  Because of its responsibility to help ensure financial and economic stability, the Federal Reserve Board must be concerned with any potential financial difficulties at the GSEs that might have broader systemic implications.  In addition, the Federal Reserve Board recognizes the great value that the Congress attaches to the GSEs’ affordable-housing mission.  In my remarks today, I will offer some thoughts on how GSE regulatory reform could reduce the systemic risks posed by these organizations while increasing their institutional focus on promoting access to affordable housing.\n\nPublic Policy Issues Raised by GSE Operations\nI will begin by discussing GSE operations and some issues of public policy raised by these activities.  Broadly speaking, Fannie Mae and Freddie Mac each run two lines of business.  Their first line of business involves purchasing mortgages from primary mortgage originators, such as community bankers; packaging them into securities known as mortgage-backed securities (MBS); enhancing these MBS with credit guarantees; and then selling the guaranteed securities.  Through this process, securities that trade readily in public debt markets are created.  This activity, known as securitization, increases the liquidity of the residential mortgage market.  In particular, the securitization of mortgages extended to low- and middle-income home purchasers likely has made mortgage credit more widely available.\n\nThe GSEs’ second line of business is the main focus of my remarks today.  It involves the purchase of mortgage-backed securities and other types of assets for their own investment portfolios.  This line of business has raised public concern because its fundamental source of profitability is the widespread perception by investors that the U.S. government would not allow a GSE to fail, notwithstanding the fact that--as numerous government officials have asserted--the government has given no such guarantees.  The perception of government backing allows Fannie and Freddie to borrow in open capital markets at an interest rate only slightly above that paid by the U.S. Treasury and below that paid by other private participants in mortgage markets.  By borrowing at this preferential rate and purchasing assets (including MBS) that pay returns considerably greater than the Treasury rate, the GSEs can enjoy profits of an effectively unlimited scale.  Consequently, the GSEs’ ability to borrow at a preferential rate provides them with strong incentives both to expand the range of assets that they acquire and to increase the size of their portfolios to the greatest extent possible.\n\nThe GSE portfolios have been the subject of much controversy.  First, analysts disagree about whether the GSE portfolios serve any public purpose.  The GSEs themselves argue that their purchases of MBS provide additional support to the mortgage market, particularly during periods of financial stress.  In contrast, research at the Federal Reserve Board and elsewhere has found that the GSE portfolios appear to have no material effect on the cost or availability of residential mortgages.2  At the margin, the GSEs finance their purchases of MBS by issuing equal amounts of debt, and thus the net supply to the market of housing-related debt is unchanged by GSE purchases.  Thus, standard economic reasoning does not predict large effects from these purchases on the mortgage market.3  Indeed, contrary to what would be expected if GSE portfolios lowered the funding costs of mortgages, over the past decade or so the spread between yields on thirty-year fixed-rate mortgages and Treasuries of similar duration has tended to rise in periods in which the GSEs have increased the share of the single-family residential mortgages held in their portfolios and to fall when the GSE share has fallen.4  All that being said, for the purpose of the policy recommendations that I will make today, I will stipulate that GSE portfolios may serve to enhance liquidity and reduce costs in the mortgage market in some circumstances.  In particular, the GSE portfolio purchases may create benefits for home purchase mortgages extended to lower-income households, to low- and moderate-income first-time homebuyers, and to buyers of homes in lower-income neighborhoods.  These are all mortgage markets in which the private sector might have greater difficulties making mortgage credit more widely available and thus for which the case for government support may be stronger.\n\nA second element of the controversy surrounding the GSE portfolios arises from the fact that they are not only large but also potentially subject to significant volatility and financial risk (including credit risk, interest-rate risk, and prepayment risk) and operational risk.  Many observers, including the Federal Reserve Board, have expressed concern about the potential danger that these portfolios may pose to the broader financial system; that is, the GSE portfolios may be a source of systemic risk (Greenspan, 2005a).  Systemic risk is the risk that disruptions occurring in one firm or financial market may spread to other parts of the financial system, with possibly serious implications for the performance of the broader economy.\n\nFinancial crises are extremely difficult to anticipate, and each episode of financial instability seems to have unique aspects, but two conditions are common to most such events.  First, major crises usually involve financial institutions or markets that are either very large or play some critical role in the financial system.  Second, the origins of most financial crises (excluding, perhaps, those attributable to natural disasters, war, and other nonfinancial events) can be traced to failures of due diligence or \"market discipline\" by an important group of market participants.\n\nBoth of these conditions apply to the current situation of Fannie Mae and Freddie Mac (Eisenbeis, Frame, and Wall, forthcoming).  The two GSEs are certainly large, having a dominant presence in U.S. mortgage markets and a substantial role in other financial markets, particularly in public debt and derivatives markets.  Beginning in the mid-1990s, the GSEs began to rapidly increase the quantity of mortgages and other assets that they purchased and retained in their portfolios.  From the end of 1990 until the end of 2003, the combined portfolios of Fannie Mae and Freddie Mac grew more than tenfold, from $135 billion to $1.56 trillion, and the share they hold of outstanding residential mortgages increased from less than 5 percent to more than 20 percent.5  Moreover, to finance their own holdings of MBS and other assets, in 2005 the two GSEs together issued almost $3 trillion in debt.  Today, the two companies have $5.2 trillion of debt and MBS obligations outstanding, exceeding the $4.9 trillion of publicly held debt of the\n\nU.S. government (Lockhart, 2007).  The activities of the GSEs are not confined to debt markets; because the GSEs engage in extensive hedging activities, these companies are among the most active users of derivative instruments.  Thus, by any measure, the GSEs have a significant presence in U.S. financial markets.6\n\nIn most situations, policymakers can rely on market forces to constrain the risk-taking behavior of privately owned financial organizations.  Market discipline is effective because, normally, the creditors of private firms have powerful incentives to monitor the risk-taking and risk-management activities conducted by these organizations.  In particular, if creditors believe that an organization is taking on increased risk, they will reduce their exposure to the organization or demand greater compensation for bearing the additional risk.  These market responses act as a brake on an organization’s risk-taking behavior and consequently reduce the likelihood that the company will fail.\n\nUnlike other private firms, however, the GSEs face little or no market discipline from their senior debt holders because of the belief among market participants that the U.S. government will back these institutions under almost any circumstances.  As a result, increased risk-taking by the GSEs does not significantly increase their cost of funding or reduce their access to credit, as it would for other private firms.  Indeed, as I have already noted, GSE debt trades at a narrow spread over U.S. Treasury debt and at spreads below those of other highly rated financial institutions, including the largest U.S. bank holding companies.7  Moreover, the spread of GSE debt over Treasuries has been remarkably unresponsive to the recent problems of the GSEs (including the turnover of senior management and the inability of either company to provide current financial statements), suggesting that investors’ faith in an implicit government guarantee remains unshaken.\n\nAs I have also noted, their low cost of borrowing gives GSEs an advantage over market participants in profitably financing the acquisition of just about any market-priced asset (other than U.S. Treasuries), and it creates a strong incentive for these companies to look for new types of assets to acquire and to find new lines of business to enter.8  These ingredients--the large presence of the GSEs in financial markets, the lack of market discipline exercised by investors in GSE senior debt, and the incentives for continued portfolio growth--led the Federal Reserve Board to conclude that while the GSEs do not seem to pose an immediate risk of financial difficulty, their portfolios continue to represent a potentially significant source of systemic risk.\n\nSome observers have suggested that the systemic risks raised by GSEs are not qualitatively different from those posed by the largest bank holding companies, which are also a sizable presence in financial markets and enjoy some government guarantees (notably deposit insurance).  However, this comparison is invalid for several reasons.  First, uninsured deposits and other uninsured debt of bank holding companies--which are the marginal sources of funding for these organizations--pay rates of interest that are higher than both Treasury and GSE rates and that are sensitive to the financial condition of the firm.  This behavior of banks’ cost of funds suggests that debt holders do not believe that their investments will be fully protected if the bank gets into trouble, and consequently these debt holders exert market discipline on the firm.9\n\nSecond, because of both regulatory requirements and the force of market discipline, banks hold much more capital than GSEs hold.  The very largest bank holding companies generally hold equity capital equal to 6 percent or more of assets, and the largest regional banks generally have capital ratios of about 8 percent.  (As I am sure you are keenly aware, community banks often have a capital-to-assets ratio exceeding 10 percent.)  In comparison, the GSEs hold capital equal to roughly 3.5 percent of assets.10  The justification for the low capital holdings of GSEs relative to banks is unclear.  The largest banks are more diversified than the GSEs; and although banks likely assume greater credit risks, they probably are less subject to interest-rate risk than are GSEs.11  Moreover, the recent experience of the GSEs suggests that they are subject to at least as much operational risk as the large banks.\n\nMeasures to Reduce the Systemic Risk of GSE Portfolios\nI have argued today that the size and the potentially rapid growth of GSE portfolios, combined with the lack of market discipline faced by GSEs, raise substantial systemic risk concerns.  How should this issue be addressed?  In recent years, the Federal Reserve Board has laid out three essential elements for the effective regulation of the GSEs that we believe would mitigate those concerns while promoting more effectively the important public purposes that they serve (Greenspan 2004; 2005b; 2006).  First, the GSE regulator should have the broad authority necessary to set and adjust GSE capital requirements in line with the risks posed by the GSEs.  Second, the GSEs should be subject to a clear and credible receivership process, a process that would establish that both shareholders and debt holders of a failed GSE would suffer financial losses.  Third, the GSEs’ portfolios should be anchored firmly to a well-understood public purpose approved by the Congress.\n\nThe concentrated and potentially volatile nature of the GSEs’ portfolios, together with the lack of market discipline on GSE activities, makes ensuring adequate capital--the first element--especially important.  To ensure the safety and soundness of the GSEs and to reduce systemic risks, the GSE regulator should have capital authority that is on a par with that of the bank regulators.  For example, the GSE regulator should have clear authority to establish and modify both the minimum and the risk-based capital standards for the GSEs.  Moreover, the GSE regulator should be able to adjust capital requirements quickly and as needed to address developing or foreseeable concerns, rather than being required by cumbersome procedures to wait until after the damage has been done.  A strong capital base would significantly reduce the implicit subsidy and incentive problems that now distort GSE investment decisions (Lucas and McDonald, 2006), while also increasing their safety and soundness.\n\nThe establishment of a clear and credible GSE receivership process, the second element, is needed to create market discipline for these companies.  Reform legislation should establish (1) a well-defined and mandatory process for placing a GSE in receivership and (2) a method for resolving a GSE once it is placed in receivership.  Both parts are necessary for the receivership process to be meaningful and credible.  Market participants should clearly understand that, once certain conditions arise, regulatory forbearance will be impermissible and a GSE receivership will be established.  Importantly, the GSE receivership process should include a mechanism for ensuring that both the shareholders and creditors of a failed GSE will bear financial losses.  Only if GSE debt holders are persuaded that the failure of a GSE will subject them to losses will they have an incentive to exert market discipline.\n\nThird, the GSE portfolios should be anchored to a clear and well-defined public purpose.  Tying the portfolios to a purpose that provides measurable benefits to the public would help to ensure that society in general--not just GSE shareholders--receives a meaningful return in exchange for accepting the risks inherent in the portfolios.  Moreover, defining the scope and purpose of the portfolios in this way would reduce the potential for unbridled growth in those portfolios while avoiding the imposition of arbitrary limits or caps.\n\nAffordable Housing and the GSE Portfolios\nWhat public purpose should be served by the GSE portfolios?  An obvious and worthy candidate is the promotion of affordable housing.12  The Congress has frequently expressed the priority it attaches to affordable housing through, for example, the provision of various housing programs and tax incentives aimed at increasing the availability of moderately priced homes and rental housing.  The Congress has also determined that financial institutions have a role in providing credit to low- and moderate-income households.  Most notably, the Community Reinvestment Act (CRA) obligates insured depository institutions to help meet the credit needs of their entire local communities, including low- and moderate-income borrowers and neighborhoods, consistent with the institutions’ safe and sound operation.13\n\nAlong similar lines, in 1992 the Congress established an affordable housing mission for Fannie Mae and Freddie Mac by directing HUD to create specific mortgage purchase goals for these GSEs.  However, evidence that Fannie and Freddie have had beneficial effects on the supply of affordable housing (over and above the benefits of their securitization activities for the mortgage market as a whole) has been difficult to find.14  After conducting several studies of the effects of GSEs on the mortgage market and establishing the GSEs’ disappointing results, HUD in 2004 raised the numerical goals that these institutions must reach to fulfill their affordable housing mission.  As noted by HUD, \"With respect to these public purposes, Congress does not simply expect the GSEs to strive toward achievement of these purposes but rather to lead the mortgage finance industry and to ensure that citizens throughout the country enjoy access to the public benefits provided by these federal entities.\"15\n\nThus, a standard for determining the public benefit of Fannie’s and Freddie’s portfolios seems readily available: Do the GSE portfolios support affordable housing?  At the present time, Fannie and Freddie appear to fail this test.  Indeed, by OFHEO’s estimation, less than 30 percent of the GSEs’ current portfolio holdings are oriented toward affordable housing (Lockhart, 2007).\n\nA straightforward means of anchoring the GSE portfolios to a clear public mission would be to require Fannie and Freddie to focus their portfolios almost exclusively on holdings of mortgages or mortgage-backed securities that support affordable housing.  The evolution of mortgage markets since the GSEs were created strongly suggests that a concentration on affordable-housing products would provide the greatest public benefit.  Markets for highly rated assets--including most residential mortgages and the pools of MBS backed by such mortgages--have become extremely deep and liquid, with more than $25 trillion in outstanding instruments.  These markets are international in scope, and market participants include thousands of banking organizations, insurance companies, pooled investment vehicles, institutional investors and, increasingly, foreign governmental authorities.  Given the size and depth of the secondary market for most residential mortgages, the GSEs’ purchase and retention of highly rated mortgages and of their own MBS are unlikely to do much to enhance liquidity in the secondary markets for these assets or to promote affordable housing.  On the other hand, the vast size of the market for highly rated assets greatly increases the potential for rapid growth of GSE portfolios and, consequently, systemic risk.\n\nIn contrast, the market for affordable-housing products--particularly mortgages extended to households with below-median-income--is less deep and liquid than the broader market for residential mortgages.  GSE portfolio purchases might add significant liquidity to the secondary markets for such assets, thereby reducing costs and increasing credit availability to prospective home purchasers.  In addition, increasing the presence of the GSEs in the market for affordable housing could help banks fulfill their CRA obligations by providing them with greater opportunities for securitizing such loans.  In all, from a social perspective, focusing the GSE portfolios on affordable housing could provide benefits that might offset some of the risks that these more-targeted portfolios might pose to financial markets and to taxpayers.  The key principle here is that the GSEs’ senior debt--which investors view as government-backed--should be used only to finance assets (such as affordable-housing mortgages) that have, in the view of the Congress, a clear and measurable public benefit.16  Such an approach would set some functional limits on the size of the portfolios and on the range of assets that GSEs would be allowed to purchase, while preserving the ability of these companies to operate profitably.\n\nTo be clear, I am not advocating a change in the exposure of GSEs to subprime loans.17  Orienting the GSEs’ portfolios more toward affordable housing is an approach which can succeed under the current GSE credit standards.  Indeed, the credit risks associated with an affordable-housing portfolio need not be any greater than mortgage portfolios generally, so long as the GSEs continue to adhere to sound underwriting practices.  Moreover, a renewal of the GSE affordable-housing mission might stimulate the development of innovative approaches to measuring and managing the credit risks associated with such mortgages.\n\nConclusion\nLegislation to strengthen the regulation and supervision of GSEs is highly desirable, both to ensure that these companies pose fewer risks to the financial system and to direct them toward activities that provide important social benefits.  Financial safety and soundness can be enhanced by giving the GSE regulator capital powers comparable to those of bank supervisors and by creating a clear and credible receivership process that leads debt holders to recognize that they would suffer financial losses should a GSE fail.  Finally, the Federal Reserve Board believes that the GSEs’ investment portfolios should be firmly anchored to a measurable public purpose, such as the promotion of affordable housing.  I believe that this approach provides a reasonable balance of social costs and benefits for the GSE portfolios.  In particular, this approach would re-focus the GSEs on the affordable housing objectives given to them by the Congress.\n\nReferences\n\nAmbrose, Brent W., Michael LaCour-Little, and Anthony B. Sanders (2004). \"The Effect of Conforming Loan Status on Mortgage Yield Spreads: A Loan Level Analysis,\" Real Estate Economics, vol. 32 (Winter), pp.541-69.\n\nDepartment of Housing and Urban Development (2004a). \"HUD’s Housing Goals for the Federal National Mortgage Association (Fannie Mae) and the Federal Home Loan Mortgage Corporation (Freddie Mac) for the Years 2005-2008 and Amendments to HUD’s Regulation of Fannie Mae and Freddie Mac,\" 24 CFR Part 81 (Docket No. FR 4790-F-03), Federal Register, vol. 69 (November 2), pp. 63579-628.\n\nDepartment of Housing and Urban Development (2004b). \"Regulatory Analysis for the Secretary of HUD’s Final Rule on HUD’s Regulation of the Federal National Mortgage Association (Fannie Mae) and the Federal Home Loan Mortgage Corporation (Freddie Mac),\" HUD, Office of Policy Development and Research, October 2004.\n\nEisenbeis, Robert A., W. Scott Frame, and Larry D. Wall (forthcoming). \"An Analysis of the Systemic Risks Posed by Fannie Mae and Freddie Mac and an Evaluation of the Policy Options for Reducing Those Risks,\" Journal of Financial Services Research.\n\nFrame, W. Scott, and Lawrence J. White (2005). \"Fussing and Fuming over Fannie and Freddie: How Much Smoke, How Much Fire?\" Journal of Economic Perspectives, vol. 19 (Spring), pp. 159-84.\n\nGerardi, Kristopher, Harvey S. Rosen, and Paul Willen (2006). \"Do Households Benefit from Financial Deregulation and Innovation?  The Case of the Mortgage Market,\" Public Policy Discussion Papers 06-6. Boston: Federal Reserve Bank of Boston, June.\n\nGreenspan, Alan (2006). Letter to Senators Elizabeth Dole, Chuck Hagel, and John Sununu, January 3.\n\nGreenspan, Alan (2005a). \"Government-Sponsored Enterprises,\" remarks delivered at the Conference on Housing, Mortgage Finance, and the Macroeconomy, Federal Reserve Bank of Atlanta, May 19, www.federalreserve.gov/boarddocs/speeches/2005/default.htm.\n\nGreenspan, Alan (2005b). \"Regulatory Reform of the Government-Sponsored Enterprises,\" statement before the Committee on Banking, Housing, and Urban Affairs, U.S. Senate, April 6, www.federalreserve.gov/boarddocs/testimony/2005/default.htm.\n\nGreenspan, Alan (2004). \"Government-Sponsored Enterprises,\" statement before the Committee on Banking, Housing, and Urban Affairs, U.S. Senate, February 24, www.federalreserve.gov/boarddocs/testimony/2004/default.htm.\n\nInside Mortgage Finance Publications (2006). The Housing-Related GSEs: Data on Their Business, Market, Investments, & Expenditures. Bethesda, Md.: IMFP.\n\nJaffe, Dwight M., and John M. Quigley (2007). Housing Subsidies and Homeowners:  What Role for Government-Sponsored Enterprises?\" paper presented at the NBER/Kellogg School of Management Conference on Measuring and Managing Federal Financial Risk, held at Northwestern University, Evanston, Ill., February 8 and 9.\n\nLehnert, Andreas, Wayne Passmore, and Shane Sherlund (forthcoming). \"GSEs, Mortgage Rates, and Secondary Market Activities,\" Journal of Real Estate Finance and Economics.\n\nLockhart, James B. III (2007). \"GSE Reform: A Priority for 2007,\" paper presented at the National Association of Affordable Housing Lenders Legislative Conference, Washington, D.C., February 1.\n\nLucas, Deborah, and Robert McDonald (2006). \"An Options-based Approach to Evaluating the Risk of Fannie Mae and Freddie Mac,\" Journal of Monetary Economics, vol. 53 (January), pp. 155-76.\n\nOffice of Federal Housing Enterprise Oversight (OFHEO) 2006. \"Mortgage Markets and the Enterprises in 2005,\" www.ofheo.gov/Research.asp.\n\nPassmore, Wayne, Gillian Burgess, Diana Hancock, Andreas Lehnert, and Shane Sherlund (2006). \"Federal Reserve Research on Government-Sponsored Enterprises,\" in Proceedings of the 42nd Annual Conference on Bank Structure and Competition on \"Innovations in Real Estate Markets: Risks, Rewards, and the Role of Regulation\"  (Federal Reserve Bank of Chicago, May 17-19), pp. 90-101.\n\nPassmore, Wayne, Shane Sherlund, and Gillian Burgess (2005). \"The Effects of Housing Government-Sponsored Enterprises on Mortgages Rates,\" Real Estate Finance, vol. 33 (Fall), pp. 427-83.\n\nFootnotes\n\n1.  Frame and White (2005) provide a general description of GSEs and the associated policy debates. Return to text\n\n2.  See Lehnert, Passmore, and Sherlund (forthcoming).  There is also much debate about whether GSE activities more generally lower the cost of mortgage credit, with estimates of the GSEs’ overall effect on fixed-rate mortgages ranging  from zero to around 25 basis points (see Ambrose, LaCour-Little, and Sanders, 2004; Geradi, Rosen and Willen , 2006, Passmore, Sherlund, and Burgess, 2005; and Passmore, Burgess, Hancock, Lehnert, and Sherlund, 2006).  Return to text\n\n3.  The corporate debt issued by the GSEs has somewhat different risk characteristics than MBS--less exposure to prepayment risk, for example.  However, the GSEs attempt to hedge most of the risks of MBS that they hold through callable debt and the use of derivatives, so those risks ultimately end up in the broader market as well. Return to text\n\n4. As discussed later, the combined portfolios of the GSEs grew more than tenfold between 1990 and 2003; their market shares peaked in 2003 at slightly more than 22 percent.  GSE market shares have fallen over the past three years to about 14 percent, partly as the result of the companies’ accounting and internal control problems, which resulted in agreements with OFHEO to limit portfolio growth (presumably on a temporary basis).  The spread between mortgage rates and Treasury yields rose during the second half of the 1990s, peaked in late 2002 and 2003, and has declined since, implying that higher mortgage spreads are associated with higher GSE market shares.  Lehnert, Passmore, and Sherlund (forthcoming) find that a similar result obtains over shorter periods; using monthly data, they show that the mortgage spread appears to rise when GSE purchases rise and contract when GSE purchases contract.  Return to text\n\n5.   Data on the GSEs’ portfolios can be found at www.ofheo.gov/Research.asp.     Return to text\n\n6.  See OFHEO (2006) and Inside Mortgage Finance Publications (2006). I should also note that in 2003, Fannie and Freddie together accounted for almost 70 percent of all mortgage securitizations and about 75 percent of all mortgage-backed securities outstanding. The GSEs’ market shares of mortgage securitizations and outstanding MBS have fallen since 2003, reflecting the growth of private securitizations.   Return to text\n\n7.  The funding advantage of the GSEs relative to large bank holding companies has varied significantly over time, ranging from 20 to 45 basis points.   Much of this variation reflects changes in the credit risk premiums embedded in the debt of large bank holding companies, which mainly reflect systematic credit risks.  In the past several years, this credit premium has narrowed substantially. Return to text\n\n8.  Under normal market conditions, the substantial profits of the GSEs would attract competitors.  But competitors, which are subject to market scrutiny when conducting their business, cannot provide a check on the GSEs because they cannot compete with the GSEs’ low cost of funds. Return to text\n\n9.  Legal provisions such as prompt corrective action and the least-cost-resolution requirement probably contribute to the perception of bank debt holders that their investments are not guaranteed if the bank gets into financial trouble.  Recently, the Federal Deposit Insurance Corporation (FDIC) has proposed to require that large banks maintain depositor records in a common format to help ensure that those creditors do not escape losses in the event of a bank failure.  Also, the ability of bank holding companies to increase their government-backed funding is limited.  By law, banks cannot make additional banking acquisitions if the resulting firm would control more than 10 percent of U.S. insured deposits. Return to text\n\n10.  This comparison excludes the temporary 30 percent additional capital currently required by OFHEO because of the operational risks posed by the GSEs’ recent accounting problems. Return to text\n\n11.  Interest rate risk depends on the degree of hedging, which is at the discretion of firm management.  However, because the GSEs’ portfolios are concentrated in the mortgage-backed securities market and thus are subject to rapid changes in market value, the risk profile of the GSEs can change rapidly in response to unexpected movements in interest rates.  In contrast, banks hold a more diversified mix of liabilities and assets, many of which are less sensitive to unexpected changes in interest rates, suggesting that banks as a general matter are less prone to interest rate risk.  Regardless, little social benefit is gained by encouraging the concentration of the substantial interest rate risks associated with long-term mortgages into only two government-sponsored organizations, when such risk could be easily distributed across tens of thousands of entities, both domestic and foreign, through the process of mortgage securitization. Return to text\n\n12.  According to the Department of Housing and Urban Development (HUD), \"The generally accepted definition of affordability is for a household to pay no more than 30 percent of its annual income on housing\"   (www.hud.gov/offices/cpd/affordablehousing ).  Presumably, this definition would apply only for low- and moderate-income households.  One difficulty of discussing GSE reform and affordable housing is that there is no straightforward link between this definition of affordable housing and the activities of the GSEs.  Better data and methods are needed to measure accurately the GSEs’ contributions to affordable housing. Return to text\n\n13. In addition,  the Congress in 1934 established the Federal Housing Administration (FHA) and gave it the mission of helping to increase the availability of affordable housing by extending the provision of mortgage insurance to many lower-income and liquidity-constrained households.  More recently, the Congress required the Federal Home Loan Banks (FHLBs) to establish an affordable housing fund. Return to text\n\n14.  As one recent study stated: \"A substantial literature has now developed analyzing the efficacy of HUD housing goals for promoting home ownership among lower-income families.  The consensus conclusion is that the affordable housing goals (AHGs) have achieved very little in terms of increasing homeownership among low-income families\" (Jaffe and Quigley, 2007, p. 12). Return to text\n\n15.  Department of Housing and Urban Development (HUD), (2004a), p. 63581.  Italics are quotes taken by HUD from Senate Report, No. 102-282, at 34 (1992).  For reviews of HUD’s studies comparing the GSEs to the primary mortgage market, see HUD (2004a and 2004b).  Return to text\n\n16.  Subordinated debt might be used to fund any assets allowed by Congress and held by Fannie and Freddie that did not have a strong and easily measurable link to making housing more affordable.  In principle, subordinated-debt spreads should be more responsive to market developments than senior-debt spreads, particularly if issuance was mandatory and the Congress establishes a clear and credible receivership process for the GSEs.  Fannie and Freddie took a step in the direction of enhancing GSE market discipline when they issued subordinated debt in 2000. But during the recent period of accounting problems, neither GSE has chosen to issue subordinated debt, likely because doing so would have been too costly.  Only recently has Freddie Mac resolved its problems sufficiently so that subordinated debt could be issued.   Return to text\n\n17.  The role of the GSEs in subprime lending and whether to increase or decrease their exposure to such lending are important questions for Congress.  The GSEs have been only indirect players in the subprime market, purchasing mainly AAA-debt that has been collateralized with subprime mortgages and leaving the higher risk components of the subprime mortgages to be funded by other market participants. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070306a.htm",
        "title": "GSE Portfolios, Systemic Risk, and Affordable Housing",
        "date": "3/6/2007"
    },
    {
        "content": "March 05, 2007\n\nGovernor Randall S. Kroszner\n\nAt the America’s Community Bankers Government Affairs Conference, Washington, D.C.\n\nI am delighted to be with you today to share some of my views on the vital role that community banks play in the U.S. economy. I am particularly pleased to be part of this excellent America’s Community Bankers conference for a very personal reason: one of my first jobs was with a community bank. My experiences at the teller's window provided a useful, in-the-trenches introduction to some of the issues I now face as a Federal Reserve Governor.\n\nCommunity banks play an important role in the United States economy, as they have throughout our history. Indeed, the roots of commercial banking in the United States can be traced to the development of community banks soon after the founding of the republic. The U.S. banking industry has, of course, changed dramatically over the past 200 years with the emergence of large, geographically diversified banking organizations that have the ability to exploit economies of scale and scope and to compete in global markets. More recently, this evolutionary process has accelerated, and the past two decades have witnessed dramatic changes in the structure of the banking industry and the business environment in which banks operate. These changes have brought with them new challenges for community bankers. Nonetheless, many community banks continue to thrive by providing traditional relationship banking services to members of their communities. Their local presence and personal interactions give community bankers an advantage in providing financial services to those customers for whom, despite technological advances, information remains difficult and costly to obtain.\n\nThese close ties, however, represent a two-edged sword, exposing community banks to risks even as they provide a valuable and profitable market niche. For example, the close ties of community banks to their customers bring not only potential benefits in screening and monitoring borrowers, but also the potential for conflicts of interest. I would like to spend the next few minutes discussing both the benefits of and the challenges facing community banking.\n\nThe Special Role of Community Banks: Relationship Finance\nThe earliest banks in the United States and elsewhere relied on close relationships with their customers to obtain information about creditworthiness and to monitor loan performance. For example, in an analysis of early nineteenth century banking in New England, Naomi Lamoreaux found that many bank directors lent the bulk of the funds under their control to themselves, their relatives, friends, and business partners. At the time, information systems were quite primitive, and it was difficult and costly to obtain data about potential borrowers who were not family members or those with whom they had a close relationship (Lamoreaux, 1994).\n\nRelationship finance continues to be at the heart of community banking. I believe that the most significant characteristics of community banks are: 1) their importance in small-business lending; 2) their tendency to lend to individuals and businesses in their local areas; 3) their tendency to rely on retail deposits for funding; and 4) their emphasis on personal service (Critchfield, et. al., 2004; Petersen and Rajan, 1994). Hence, successful community banking today depends importantly on the same characteristics that formed the foundations of the U.S. banking industry two centuries ago--personal interactions among bankers, their customers, and their communities.\n\nThe conventional paradigm of relationship finance is based on the premise that small businesses and households tend to be informationally opaque--an economist’s term that may be even more opaque than these small businesses and households. What I mean by this phrase is that “hard” systematic data about the creditworthiness of such potential borrowers tend to be quite difficult to obtain. Thus, the efficient supply of credit to these agents depends on “soft” information that may be generated through close interactions with the borrower and knowledge of the community--that is, classic relationship lending.\n\nIn contrast, larger banks tend to have a comparative advantage with larger, more mature firms for whom quantitative, or “hard,” information, such as longer, more-detailed credit histories, is relatively easy to obtain (Petersen and Rajan, 1994). The distinction between community banks and large banks, however, is clearly changing. Technology has lowered the cost of information processing and facilitated the growth of information bureaus, making information about households and small businesses more readily available. This information has allowed some institutions to substitute credit scoring for more costly traditional techniques in the underwriting of some types of consumer and small-business loans, particularly credit card loans. These products are quite different from traditional unsecured personal or business loans, which rely heavily on information obtained from personal interactions and relationships.\n\nIndeed, there can be no doubt that community banks today face considerable challenges. Rapid technological changes in the production of financial services, improved production and dissemination of financial information about consumers and businesses, deregulation, the increasing geographic scope of some banking activities, and the increased importance of nonbank providers of financial services are some of the factors altering the financial marketplace. While many of these changes may have increased financial system efficiency and lowered costs for consumers, they also present new and sometimes difficult challenges for community banks.\n\nThese changes have coincided with a significant consolidation of the banking industry and a pronounced decline in the number of community banks. If we define a community bank as any bank or thrift organization that has total real assets of less than $1 billion, in 2002 dollars, the number of community banks has declined about a third over the past decade. Most of this consolidation has been due to mergers. A Federal Reserve Board staff study reports that between 1994 and 2003 there were more than 3,500 bank and thrift mergers (Pilloff, 2004). In more than 90 percent of these mergers, the target institution had less than $1 billion in total assets, and in about half of those cases the acquiring organization also had assets of less than $1 billion. Acquiring organizations were typically larger than their targets; about 50 percent of the transactions involved an acquirer that was at least ten times as large as the target institution. Although merger activity has slowed since the 1990s, there were still at least 200 mergers each year between 2000 and 2006.\n\nDespite these changes, community banks continue to fill an important niche in banking, providing relationship loans in specific business and economic sectors (most notably small-business and agricultural lending), personalized service, and a local presence. Indeed, we have a considerable body of evidence that points to the value and viability of these institutions. For example, despite the decline in numbers that I just mentioned, there are still more than 7,000 community banking organizations, accounting for about 95 percent of all banks and thrifts in the United States today. Furthermore, new community banks continue to be formed. During the period 2000 through 2005, about 120 new community banks were chartered, on average, each year. And during the first three quarters of 2006, 137 new community banks were chartered. These figures suggest that many people continue to believe that community banks have an important niche to fill and, as a result, are willing to invest in the future of community banking.\n\nA variety of indicators suggest that as a whole, today’s community banks are doing well. The average return on equity (ROE) at community banks, for instance, is quite solid and has improved over the past few years. Net interest margins for community banks are, on average, higher than those for larger banks, and recently this difference has been widening. And, as a result of stable interest margins and improved cost controls, the return on assets (ROA) for community banks as a whole has remained well above industry standards for strong performance.\n\nLooking at other performance measures, we see that loan quality and capital ratios remain strong at community banks. On average, their nonperforming-loan-to-asset ratios have been low and comparable to those of the largest banking organizations, and their capital ratios have remained high. In addition, community banks continue to demonstrate a healthy ability to attract deposits, with core deposits funding about 70 percent of assets as of year-end 2005.\n\nCommunity Banks and Small Businesses\nThe thesis that I have put forward--namely, that relationship finance is important in sustaining the role that community banks play and in accounting for their strong performance--has a number of implications for small businesses’ use of community banks and the proximity of banks and their customers. The Board’s Survey of Small Business Finances gives us a window on the relationship between community banks and small businesses and the evolution of the banks’ role. The survey data are also helpful in contrasting the characteristics of small businesses that use community banks with those of small businesses that use larger depository institutions.\n\nBy way of background, the Board conducts the small-business survey every five years. Our most recent data are for year-end 2003 and were obtained from interviews conducted during 2004 with more than 4,200 small businesses, a representative sample drawn from more than 6.3 million small enterprises in the United States. The data were made available to the public in June 2006, but to my knowledge no one has yet used them to assess the role that community banks play in small-business finance.\n\nNot surprisingly, the vast majority of small businesses obtained some type of financial service from a bank or thrift in both 1998 and 2003. Over that period, the share of small businesses obtaining services from nondepository institutions increased substantially, from about 40 percent of firms in 1998 to about 54 percent in 2003. Nondepository sources typically supply loans and financial management products, including such services as check clearing, provision of letters of credit, cash management, and credit card processing. Although the fraction of small businesses obtaining these types of services from nondepositories has increased, the proportion obtaining them from banks increased as well. Both of these changes reflect the fact that between 1998 and 2003 small businesses substantially increased their demand for these types of services.\n\nOur survey data provide evidence that community banks continue to play an important role in providing financial services to small businesses, even in an increasingly competitive marketplace. Among the small businesses that reported using a bank or thrift, in both 1998 and 2003 about a third used a community bank.\n\nOne of the traditional strengths of community banks--indeed, a practical requirement for successful relationship lending--is local presence. Although our survey results suggest that community banks increasingly face competition from both larger banks and nondepository institutions, they also illustrate the importance of proximity for many small businesses. In 2003, the median distance between a small business’s headquarters and its bank or thrift was three miles, about the same as in 1998. For lending relationships, the median distance was four miles. Clearly, proximity and convenience, two of the characteristics that epitomize relationship finance at community banks, are important factors underlying the choice of financial service providers by small businesses.\n\nComparisons of the characteristics of the community and larger banks used by small businesses reveal differences that are consistent with the idea that community banks specialize in relationship finance. On average, the distances between small businesses and the banks from which they obtained financial services were smaller for community banks than for larger banks. Small businesses were also more likely: 1) to have longer relationships; 2) to obtain a larger number of services; and 3) to conduct business in person with community banks than with larger banks. And when asked why they used a specific institution, business owners using community banks were substantially more likely to mention the importance of relationships.\n\nOur survey results also allow us to consider whether small businesses that use community banks have different characteristics than those that use large banks. Of the firms that used banks in 2003, one-fifth used community banks exclusively, two-thirds used large banks exclusively, and the remainder (14 percent) used both.\n\nAs described above, relationship lending is most important to firms lacking hard information, and those firms, in general, are expected to be the smaller firms. Consistent with this view, our survey indicates that the average firm that used community banks exclusively was smaller in terms of number of employees, sales, and assets than the average firm that used large institutions exclusively or that used both community banks and large banks. In addition, firms that used only community banks used fewer services overall than other types of firms--which is consistent with their smaller size--but at the same time obtained slightly more services from each bank than did firms that used only larger banks. This suggests that community-bank-only firms are more likely to cluster their purchases at a single institution than are other firms, behavior that is consistent with the importance of relationship finance.\n\nOverall, in my judgment, our survey data confirm the view that, despite changes in the competitive environment, community banks continue to play an important role in providing relationship finance to small businesses in their local banking markets.\n\nPotential Conflicts of Interest\nThe very relationships that underlie relationship finance, and that constitute the primary source of community bankers’ comparative advantage in meeting the financial needs of small businesses and households, also present challenges. As I mentioned earlier, relationship finance dates back to the very earliest banking institutions in our country. Although the extreme form of relationship lending that characterized early nineteenth century New England banking had the potential to undermine the safety and soundness of these early banking institutions, Naomi Lamoreaux’s research suggests that market forces acted to minimize the adverse effects. In an environment characterized by easy entry into banking, bank directors apparently had strong incentives to monitor lending and minimize risks because their business success depended crucially on maintaining their unsullied reputations. Of course, this salutary outcome was by no means inevitable, as evidenced by experiences in a number of developing countries where this type of lending behavior has had quite pernicious effects; rather, the outcome was the result of the particular circumstances and competitive environment prevailing in New England at the time (Lamoreaux, 1994).\n\nWhen it comes to more recent times, evidence on the extent to which conflicts of interest resulting from close ties between banks and their potential borrowers may have adversely affected bank performance is limited to studies of the largest banking organizations. One example is my own research with Phil Strahan that looked at linkages between the boards of directors of banks and nonfinancial firms. Such board linkages can be beneficial because they improve information flow between the linked firms; however, they can also lead to conflicts of interest because the individual who serves as the link has fiduciary responsibilities to both firms, and those firms’ interests can sometimes diverge.\n\nIn our research, Phil and I focus on board linkages between large U.S. banks and Fortune 500 nonfinancial firms. This focus is driven largely by data availability. I would be very interested in hearing from you about the extent to which community bankers sit on the boards of nonfinancial firms and your views on the benefits and costs that result from these linkages. Our research finds that board linkages are quite prevalent among the largest banking organizations--more prevalent than among large nonfinancial firms--but that these linkages tend to involve firms in which shareholder-creditor conflicts of interest are least likely to arise. We also find that the connected firms are more likely to borrow from their connected bank, and that when they do, the loan terms are similar to the terms on loans to unconnected firms. Thus, contrary to the results of studies of some other countries, where it has been found that connections were misused, our results suggest that the avoidance of conflicts of interest explains both the allocation and behavior of bankers in the U.S. corporate governance system (Kroszner and Strahan, 2001, 2002).\n\nOn balance, evidence from both community banks in early nineteenth century New England and larger banks in more recent times suggests that, as a whole, U.S. banks have managed their relationships well and have avoided engaging in systematic conflicts of interest. Nonetheless, the potential for problems is there, and it is important that bankers remain vigilant and regularly review the corporate governance mechanisms they have in place to deal appropriately with challenges that can arise with relationship finance.\n\nConclusion\nIn sum, it is clear that community banks face many challenges today, as they have in the past. Technological developments in information production and dissemination, changes in the management strategies of larger banks and other institutions, deregulation, and changes in the delivery of financial services have reduced some of the advantages that community banks could once offer their customers. As a result, community banks have lost market share to larger banks and to nondepository institutions. At the same time, a large portion of small businesses and households continue to value and avail themselves of the relationship lending, personal service, and local proximity that community banks offer their customers. Community banks today are generally healthy and profitable, and new ones are being chartered every year.\n\nAlong with the benefits of relationship finance, however, come some risks, including the potential for conflicts of interest. This particular risk appears to have been mitigated significantly in the United States, although it is important for bankers to remain vigilant and to regularly review the corporate governance mechanisms they have in place. I expect that community banks will continue to exploit their traditional advantages and expertise, to be valuable members of their communities, and to adapt to their changing environment in ways that allow them to remain strong and viable competitors.\n\nReferences\n\nCritchfield, Tim, Tyler Davis, Lee Davison, Heather Gratton, George Hanc, and Katherine Samolyk (2004). \"The Future of Banking in America--Community Banks: Their Recent Past, Current Performance, and Future Prospects,\" Federal Deposit Insurance Corporation Banking Review, vol. 16, no. 3, pp. 1-56.\n\nKroszner, Randall S., and Philip E. Strahan (2001). “Bankers on Boards: Monitoring, Conflicts of Interest, and Lender Liability,” Journal of Financial Economics, vol. 62, pp. 415-452.\n\nKroszner, Randall S., and Philip E. Strahan (2002). “Throwing Good Money after Bad? Board Connections and Conflicts in Bank Lending,” Working Paper 02-12, The Wharton School, University of Pennsylvania.\n\nLamoreaux, Naomi (1994). Insider Lending: Banks, Personal Connections, and Economic Development in Industrial New England, Cambridge University Press.\n\nPetersen, Mitchell A., and Raghuram G. Rajan (1994). “The Benefits of Lending Relationships: Evidence from Small Business Data,\" Journal of Finance, vol. 49, pp. 3-37.\n\nPilloff, Steven J. (2004). Bank Merger Activity in the United States, 1994-2003, Staff Study 176, Board of Governors of the Federal Reserve System.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20070305a.htm",
        "title": "Community Banks: The Continuing Importance of Relationship Finance",
        "date": "3/5/2007"
    },
    {
        "content": "March 05, 2007\n\nGovernor Kevin Warsh\n\nAt the Institute of International Bankers Annual Washington Conference, Washington, D.C.\n\nThank you to the Institute of International Bankers for inviting me to speak about liquidity in U.S. financial markets. Certainly, trading activity in recent days has brought additional attention to the subject of market liquidity. It is not my purpose, however, to opine on these very recent market moves--a comprehensive understanding of which may depend on consequent market developments and the fullness of time. I would only note that while premiums on riskier assets rose some last week, markets are functioning well amid higher volatility, market discipline appears effective as investors are reviewing their positions, and overall liquidity does not appear to be in short supply. The balance of my remarks will focus on financial market liquidity from a somewhat broader and longer-term perspective.\n\nIn recent quarters, we witnessed very strong credit markets, bulging pipelines for leveraged loan and high-yield bond issuance, and near-record low credit spreads. Structured fixed-income products proliferated, and the investor universe expanded to match new supply. Global investment flows were proven noteworthy for the lack of home-country bias. Managers of private pools of capital--in all of its forms, private equity firms, alternative asset management companies, hedge funds, and investment banks--increased funding from many sources and through many structures. Due in no small measure to strong credit markets, leveraged transactions increased and the market for corporate control became increasingly robust.\n\nFund managers of private pools of capital seized upon this opportunity to acquire more-permanent sources of capital: extending lock-up periods; using retail platforms and co-investment funds to increase ‘stickiness’ of contributed capital; securing greater financing flexibility from prime brokers; accessing the private placement markets; and selling public shares of limited and general partnership interests to new investors; to name just a few.\n\nKey questions remain: Is liquidity at strong and sustainable levels, justified by economic fundamentals? What is likely to be the liquidity trend going forward? In today’s remarks, I will first propose a definition of market liquidity based on what I believe is its most fundamental characteristic. I will then discuss the primary sources of liquidity in the U.S. capital markets, and attempt to interpret signals from financial asset prices in this environment. I will conclude by discussing implications for the economy and policymakers.1\n\nLiquidity: What is It?\nThe traditional concept of liquidity relates to trading: An asset’s liquidity is defined by its ability to be transformed into another asset without loss of value. This definition is sufficiently general to encompass many ideas. Some assets, such as “money” are used to trade goods and services without diminution in value, and therefore are highly liquid. Indeed, when different measures of the money supply were established, it was with an eye toward determining the liquidity of the underlying assets; as an example, components of M1 were considered more liquid than those in M2. It is in this sense that some observers view the stock of money as a measure of liquidity, and changes in these measures as roughly equivalent to changes in liquidity. I doubt, however, that traditional monetary aggregates can adequately capture the form and structure of liquidity many observe in the financial markets today. Instead, market observers are more likely to be referring to liquidity in broader terms, incorporating notions of credit availability, fund flows, asset prices, and leverage.\n\nAs noted, ‘liquidity’ in the sense of “trading liquidity” reflects the ability to transact quickly without exerting a material effect on prices. Liquidity is optimally achieved when myriad buyers and sellers are ready and willing to trade. The trading is enhanced by market-makers and speculators alike. Underlying this concept is that while buyers and sellers have different views on the most likely outcomes--that is, after all what generates trading--they largely can agree on the distributions of possible outcomes for which they demand risk-based compensation.\n\nConsider liquidity, then, in terms of investor confidence. Liquidity exists when investors are confident in their ability to transact and where risks are quantifiable. Moreover, liquidity exists when investors are creditworthy. When considered in terms of confidence, liquidity conditions can be assessed through the risk premiums on financial assets and the magnitude of capital flows. In general, high liquidity is generally accompanied by low risk premiums. Investors’ confidence in risk measures is greater when the perceived quantity and variance of risks are low.\n\nThis view highlights both the risks and rewards of liquidity. The benefits of greater liquidity are substantial, through higher asset prices and more efficient transfer of funds from savers to borrowers. Historical episodes indicate, however, that markets can become far less liquid due to increases in investor risk aversion and uncertainty. While policymakers and market participants know with certainty that these episodes will occur, they must be humble in their ability to predict the timing, scope, and duration of these periods of financial distress. Recall the market turmoil related to events in Asian financial markets in 1997 and following the Russian bond default in the summer of 1998. Investors flocked to “on-the-run” Treasuries, and risk spreads for high-yield corporate and emerging market bonds spiked. Chairman Greenspan described these episodes as an apparent collapse in investors’ understanding of possible future risks, despite what appeared to be mild imbalances, which led to “disengagement” by traders.2\n\nTherefore, I wish to advance a simple proposition: Liquidity is confidence. That is, powerful liquidity in the U.S. capital markets is evidenced when the economic outcomes are believed to be benign. When the “tail” outcomes are either highly improbable or, at the very least, subject to reasonably precise measurement, the conditions are ripe for liquidity to be plentiful. When fund flows are strong and growing, there is little reason to expect trading positions to become inalienable. My goal in proffering this proposition is to improve the discourse by reducing the different notions of liquidity to its most fundamental feature. This exercise may also serve as a healthy reminder: If unmoored from fundamentals, confidence can give way to complacency, complacency can undermine market discipline and liquidity can falter unexpectedly. If, to the contrary, confidence is justified by real economic determinants, liquidity can flourish.\n\nOf course, some might disagree with this definition of liquidity. They may argue that any excess liquidity in financial markets results from too little capital investment, here and abroad, which may arise from a lack of confidence in future economic outcomes. For example, high cash balances at U.S. corporations can be interpreted as indicating a lack of confidence in investment prospects. Previously, however, I argued that while the build-up of cash since 2002 has been unusual, the most pressing determinant was not uncertainty about the profit potential of capital investment.3 Instead, corporate cash positions are explained more significantly by profits retained at foreign subsidiaries, and a sharper focus by investors and ratings agencies on companies’ abilities to finance short-term liabilities internally.\n\nCurrent Sources of Market Liquidity\nLet me discuss sources of liquidity of the U.S. financial markets. By my proposed definition, we must ask what forces have increased liquidity (read: confidence) in the United States over the course of the last couple of decades. I will turn, first, to two key drivers of liquidity: rapid financial innovation and strong economic performance. A third important source of liquidity--resulting from the excess savings of emerging-market economies and those with large commodity reserves--has also found its way to the United States in pursuit of high risk-adjusted returns. We must judge the extent to which each of these three liquidity drivers are structural or cyclical, more persistent or more temporary. Understanding the sources of liquidity--and the causes thereof--should help inform judgments about the level and direction of market liquidity. In so doing, we may better understand its implications for the economy and policymakers alike.\n\nFirst, liquidity is significantly higher than it would otherwise be due to the proliferation of financial products and innovation by financial providers. This extraordinary growth itself is made possible by remarkable improvements in risk-management techniques. Hewing to my proposed definition, we could equally state that financial innovation has been made possible by high levels of confidence in the strength and integrity of our financial infrastructure, markets, and laws. Moreover, remarkable competition among commercial banks, securities firms, and other credit intermediaries have helped expand access to--and lower the all-in-cost of--credit. Interest rate risk and credit risk exposures are now more diversified.\n\nLook no further than dramatic growth of the derivatives markets. In just the past four years, notional amounts outstanding of interest rate swaps and options tripled, and outstanding credit default swaps surged more than ten-fold. These products allow investors to hedge and unwind positions easily without having to transact in cash markets, expanding the participant pool.\n\nSyndication and securitization also lead to greater risk distribution. Commercial and industrial (C&I) lending potential has expanded with the adoption of syndication practices, allowing credit risks to be spread across a greater number of participating banks and nonbank lenders. Perhaps an even more significant support for the expansion of C&I loans is the rapid growth of collateralized loan obligations (CLOs)--special purpose entities that buy C&I loans with funds raised from investors seeking different risk exposures. CLOs allow loans to be financed primarily with high-rated debt securities issued to institutions like mutual funds, pension funds, and insurance companies. Indeed, in recent years, the share of syndicated C&I term loans funded by institutional investors is estimated to have exceeded that funded by commercial banks.\n\nFor CLO structures to be effective, they invariably must include a more risky equity tranche. Even the most sophisticated financial products are not immune to the physical Law of Conservation of Matter--the risk must rest somewhere. Hedge funds reportedly have served as willing buyers of these riskier positions, and we are all aware of their phenomenal growth. Now, more than 4,000 hedge funds hold assets of about $1-1/2 trillion. As important as the participation of hedge funds, the derivative products themselves allow credit risk to be hedged, which has the beneficial effect of further increasing the pool of other investors as well. The increase in financial product and provider innovation appears to be quite persistent; future trends, however, are likely to be significantly influenced by legal, regulatory, and other public policies.\n\nThe second factor, perhaps equally persistent, supporting strong investor confidence in U.S. markets has been our economy’s strong macroeconomic performance. Researchers have documented the so-called “Great Moderation” in which the U.S. economy has achieved a marked reduction in the volatility of both real gross domestic product (GDP) and core inflation over the past twenty years or so. In theory, reduced volatility, if perceived to be persistent, can support higher asset valuations--and lower risk premiums--as investors require less compensation for risks about expected growth and inflation. In this manner, confidence appears to beget confidence, with recent history giving some measure of plausibility to the notion that very bad macroeconomic outcomes can be avoided. The Great Moderation, however, is neither a law of physics nor a guarantee of future outcomes. It is only a description--an ex post explanation of a period of relative prosperity. If policymakers and market participants presume it to be an entitlement, it will almost surely lose favor.\n\nLet us look closer at the correlation between confidence and outcomes. Asset prices do appear somewhat correlated with volatility associated with the real economy and inflation. For example, equity valuations for U.S. corporations increased more in the past twenty years than in the two decades prior to the Great Moderation. The price-earnings ratio for S&P 500 firms averaged 14 from 1960 to 1984 and rose to an average of 18 from 1985 to 2006. In addition, term premiums on long-term U.S. Treasury securities are estimated to have declined substantially since the late 1980s.4 Moreover, this decline is significantly associated with a reduction in uncertainty about long-run inflation and about short-term interest rates.5\n\nThird, liquidity in U.S. markets also increased significantly in recent years due to increased international capital flows. These flows to the United States from global investors lead to higher liquidity by increasing capital available for investment and facilitating greater transfer and insurability of risk. A recent report by McKinsey & Company estimated that aggregate international capital flows amounted to $6 trillion in 2005--almost triple the volume a decade earlier--and that one-quarter of the worldwide volume flowed through the United States.\n\nPart of the increased international capital financial flows is a result of excess savings in some emerging-market and oil-exporting countries relative to domestic investment--the phenomenon Chairman Bernanke referred to as the “global saving glut.”6 Rapidly aging populations in a few large countries, such as China, Germany, and Japan, generated high savings. Also, some of the fastest growing economies, especially in Asia, pursued export-driven growth strategies, thereby accumulating large reserves of foreign-denominated assets. In addition, high prices of oil and other commodities in recent years shifted income from importing nations to exporters, and research suggests that the bulk of these “windfalls” has been saved rather than invested.7\n\nOn net, the savings of less developed countries has been deployed to purchase substantial volumes of financial assets in markets in the most developed nations, most notably the United States and the United Kingdom. Estimates from the International Monetary Fund indicate that the group of the most advanced economies in the world swung from being net purchasers of foreign financial assets on the order of $80 billion in 1997 to being net sellers of domestic financial assets to foreigners of about $570 billion in 2006.8\n\nIt is no accident that international excess capital flowed primarily to strong and stable economies and those with highly-developed financial markets. In a world of funds increasingly without borders, we would expect investors to seek out the best risk-adjusted returns. Sound, transparent regulatory and legal frameworks in the United States, United Kingdom, and some other advanced economies have helped contribute to the attractiveness of these markets. In addition, top-notch infrastructure allows for efficient clearance and settlement procedures for transactions in the most sophisticated financial markets, all of which promote investor confidence and continued sources of liquidity.\n\nImplications for the Economy and Challenges for Policymakers\nGenerally, high levels of liquidity offer substantial benefits to our financial system and overall economy through higher financial asset prices and a more efficient means to channel funds between savers and borrowers. Strong liquidity may also help to prevent imbalances in certain markets from spreading because of the greater dispersion of risks.\n\nThe U.S. economy continues to demonstrate extraordinary resilience, no doubt supported by the ability of financial markets to absorb substantial shocks. Financial markets have been buffeted by a number of significant events, including a spate of corporate accounting scandals, the bond rating downgrades of Ford Motor Co. and General Motors Corp. to speculative-grade status, the failure of Refco, (at the time the largest broker on the Chicago Mercantile Exchange), and the imposition (and pullback) of capital controls in Thailand. But the effects on broader markets appear to have been remarkably contained. Even the episode last year involving the hedge fund, Amaranth, which accumulated losses of $6 billion in a few short weeks, seemingly had little impact beyond its direct stakeholders.\n\nIt is hard to know with certainty when investors’ confidence will be stirred--but not shaken--by these events. It is harder still to know precisely why. I have argued that solid fundamentals--effective and dynamic products and markets to disperse risk, stable economic performance, and robust and attractive market infrastructures--are key underpinnings for strong liquidity and correspondingly strong investor confidence. Surely, policymakers must be vigilant to maintain output stability and low and anchored inflation expectations. In addition, policymakers need to encourage sound risk management by private participants as the first line of defense against financial instability. In particular, we should promote policies that encourage stakeholders to engage in ex ante practices, protocols, and principles--including those recently set forth by the President’s Working Group on Financial Markets--to accomplish that objective.\n\nOf course, investor confidence and liquidity can shift. In the aftermath of a financial shock, if buyers and sellers of credit can no longer agree on the distribution of possible outcomes, their ability to price transactions will be severely limited. While we cannot--and often should not--prevent all shocks or predict how they will reverberate through the financial system, we can attempt to create conditions that would lead investors to most quickly rebuild their confidence. That is most likely to occur when underlying fundamentals are solid.\n\nMonetary policy is no less challenged by the level and prospects for liquidity. We policymakers must ask whether liquidity conditions are obscuring signals from financial asset prices that we would otherwise use to gauge the performance of the real economy.9 Liquidity conditions could, in theory, lead to lower-than-justified risk premiums that stimulate aggregate demand or otherwise generate excessive inflationary pressures. Of course, inferences from market prices are always imprecise, because prices depend on expected growth, the variation surrounding that expected path, and investor risk aversion, none of which we can precisely observe. Market liquidity may further confound the inference challenges. Allow me to comment, nonetheless, on a few key indicators.\n\nLook at the current configuration of Treasury yields across the maturity spectrum. Typically, investors require compensation for the greater exposure to interest rate risk from holding longer-term securities, leading to an upward-sloping yield curve. Since about mid-2006, the yield curve has been about flat to downward-sloping. Currently, the two-year rate slightly exceeds the ten-year Treasury rate, which stands just above 4-1/2 percent. A negatively sloped yield curve has, in the past, served as a reasonably good predictor of economic recessions.\n\nBut, there are compelling reasons to suspect that level of liquidity is affecting the slope of the yield curve, and lessening its predictive power. The same factors that are contributing to liquidity--low uncertainty about inflation and output--are also driving down term premiums and, hence, long-term Treasury yields. Thus, to the extent that low long-term Treasury yields and the negative slope of the yield curve reflects a lower term premium, rather than a lower expected short rate, it is less likely to signal future economic weakness.\n\nHigh liquidity could also obscure some information we glean from corporate bond prices. What if the current level of liquidity caused lower risk premiums than could be justified by actual credit risks? Might a misallocation of resources result? Many commentators have pointed to the low spread of corporate yields relative to Treasuries as a sign of investors “reaching for yield” due to perceived excess liquidity. Risk spreads, however, appear less exceptional given the remarkable strength of the corporate sector. We can decompose risk spreads for corporate bonds into a series of forward spreads over a sequence of time periods. Forward spreads include compensation investors require for expected credit losses and a risk premium, and it would be reasonable to expect that investors would have a stronger conviction about expected credit losses in the near term than at future horizons. Currently, forward risk spreads one to two years ahead are quite low by historical standards, consistent with very liquid balance sheets, multi-decade low leverage ratios, and robust profitability. In sharp contrast, one-year forward risk spreads five or ten years ahead are higher relative to their averages of the previous ten years. I take some comfort from these implied forward spreads to suggest that investors may not be unduly sanguine about potential credit losses beyond the near-term.10 Of course, too much precision cannot be put on assessments of risk premiums. This is an area worthy of continued analysis.\n\nSome market participants tell me that the very low bond default rates seen recently, realized and expected, are themselves a reflection of liquidity. That is, excess market liquidity may have allowed less than creditworthy firms to refinance their obligations, thereby only deferring their financial difficulties. Other observers note the rise in the second half of last year in the share of new bond issuance that is rated highly speculative, and an increase in purchase and debt-multiples for leveraged buy-outs, suggesting some pick-up in risk-taking that may be indicative of overconfidence. This possibility cannot be ruled out. Others have pointed to the low levels of stock market volatility in recent months (prior to last week) as indicative of pressures from excess liquidity. Naturally, one would expect that high levels of liquidity would lead to lower volatility as investors quickly force asset prices back to their fundamental values. But, recent levels are not unprecedented; they were equally low during much of the 1960s. And, of course, volatility itself can be volatile. There may be good fundamental reasons for risk and risk premiums to be relatively low and for liquidity and confidence to be reasonably strong. Even so, the pace of change in the capital markets by credit buyers and sellers reminds us to constantly revisit assumptions underlying the financial and economic environment.\n\nIf liquidity conditions and risk premiums of the last several quarters were the sole basis by which to judge the stance of monetary policy, it would be hard to conclude that monetary policy has been restrictive. Of course, the assessment of the stance of monetary policy also depends on a variety of other important factors.\n\nConclusion\nIn summary, liquidity has risen significantly, with important benefits to our financial system and economy. An important source of strength has been financial innovation, and while we have yet to see how some new products will play out in a more stressful environment, there almost certainly will remain a greater dispersion and insurability of risks. Stable output and price stability have also been important contributors to liquidity and investor confidence by helping to anchor views about longer-term economic outcomes. And solid fundamentals may help to ease any changes in liquidity should they occur. Hence, job number one for the Federal Open Market Committee is to choose a course for policy to best keep the macroeconomy on an even keel. This attention to our dual mandate--to maintain stable prices and maximum sustainable employment--supports investor confidence in the economy and the considerable benefits conferred by liquidity.\n\nFootnotes\n\n1.  As usual, I will be expressing my opinions on these issues--opinions that do not necessarily correspond with those of my colleagues on the Board of Governors of the Federal Reserve System or the Federal Open Market Committee (FOMC). Nellie Liang and Michael Palumbo of the Board staff provided valuable contributions to these remarks. Return to text\n\n2.  “New Challenges for Monetary Policy,” Chairman Alan Greenspan, Symposium sponsored by the Federal Reserve Bank of Kansas City, August 27, 1999. Return to text\n\n3.  “Corporate Cash and Economic Activity,” Governor Kevin M. Warsh, American Enterprise Institute, July 18, 2006.  Return to text\n\n4.  Kim, Don H. and Jonathan H. Wright (2005), “An Arbitrage-Free Three-Factor Term Structure Model and the Recent Behavior of Long-Term Yields and Distant-Horizon Forward Rates,” FEDS 2005-33. Return to text\n\n5.  An empirical link, however, between financial market volatility and output and inflation volatility is less established. Despite the very low levels of S&P 500 return volatility in recent months, the averages over longer periods have not changed much--volatility averaged close to 13 percent from 1985 to 2006 and between 1960 and 1984. One reason proposed for the lack of a direct relationship is that asset price volatility depends not only on the volatility of future cash flows but also the volatility of the discount rate that is applied to those cash flows, which does not appear to have declined in line with variation in forecasts of cash flows.  Return to text\n\n6.  Ben S. Bernanke, “The Global Saving Glut and the U.S. Current Account Deficit,” Homer Jones Lecture, April 14, 2005, and “Reflections on the Yield Curve and Monetary Policy,” Economic Club of New York, March 20, 2006. Return to text\n\n7.  “Recycling Petrodollars,” Matthew Higgins, Thomas Klitgaard, and Robert Lerman, Current Issues in Economics and Finance, vol. 12, no. 9, Federal Reserve Bank of New York, December 2006. Return to text\n\n8.  These figures, as in table 1, refer to changes in current account balances for selected developed and emerging-market economies based on recent estimates in the International Monetary Fund’s World Economic Outlook (September 2006). Except for a statistical discrepancy and a typically small capital account balance, a country’s current account balance approximates its financial account balance--the difference between domestic net purchases of foreign financial assets and foreign net purchases of domestic financial assets. Return to text\n\n9.  “Financial Markets and the Federal Reserve,” remarks by Governor Kevin M. Warsh to the New York Stock Exchange, November 21, 2006.  Return to text\n\n10.  In addition, the level of far-forward credit spreads is broadly consistent with risk premiums evident in U.S. equity markets. The substantial stock price gains in recent years have been outpaced by the exceptional strength in corporate earnings that have posted double-digit annualized increases in every quarter since 2002. And, a measure of the long-run equity risk premium, the spread between the forward earnings (trend adjusted) to price ratio and a long-run Treasury rate is above its average of the past twenty years.  Return to text\n\np  projection by the International Monetary Fund\nNote: Components may not sum to totals because of rounding error.\nSource: World Economic Outlook, International Monetary Fund, September 2006.  Data for advanced economies come from table 26 in the statistical appendix; data for other emerging market and developing countries come from table 28. Return to table  Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/warsh20070305a.htm",
        "title": "Market Liquidity: Definitions and Implications",
        "date": "3/5/2007"
    },
    {
        "content": "March 02, 2007\n\nChairman Ben S. Bernanke\n\nAt the Fourth Economic Summit, Stanford Institute for Economic Policy Research, Stanford, California\n\nMy topic this evening is the implications of ongoing global economic integration “globalization” for short--for U.S. monetary policy. At the broadest level, globalization influences the conduct of monetary policy through its powerful effects on the economic and financial environment in which monetary policy must operate. As you know, several decades of global economic integration have left a large imprint on the structure of the U.S. economy, including changes in patterns of production, employment, trade, and financial flows. Other than by contributing to general economic and financial stability, monetary policy can do little to affect these structural changes or the powerful economic forces that drive them. However, to make effective policy, the Federal Reserve must have as full an understanding as possible of the factors determining economic growth, employment, and inflation in the U.S. economy, whether those influences originate at home or abroad. Consequently, one direct effect of globalization on Federal Reserve operations has been to increase the time and attention that policymakers and staff must devote to following and understanding developments in other economies, in the world trading system, and in world capital markets.\n\nA narrower question, but one that is critical for monetary policy makers, is whether the increased openness of the U.S. economy has in some way affected the ability of the Federal Reserve to meet its congressional mandate to foster price stability and maximum sustainable employment. On this issue, some analysts have argued that globalization hinders monetary policy--for example, by reducing the ability of the Federal Reserve to affect U.S. interest rates and asset prices or by diminishing the role of domestic factors in the inflation process.\n\nYou will not be surprised to hear that the Federal Reserve System is quite interested in the implications of globalization for the conduct and effectiveness of monetary policy. Members of the Board staff have conducted extensive research on the topic, and the Federal Reserve Bank of San Francisco--which is deeply engaged in Asia-Pacific issues--has been a leader in this area as well. The Federal Reserve Bank of Dallas has created a Globalization and Monetary Policy Institute, which will support the study of globalization’s effects on policy and the economy, and a number of other Reserve Banks have undertaken similar efforts.\n\nIn the remainder of my talk I will discuss two channels through which globalization may have affected the transmission and effectiveness of U.S. monetary policy. First, I will consider whether the globalization of finance has weakened or otherwise affected the ability of U.S. monetary policy to influence domestic financial market conditions. Second, I will discuss what we know about the way international factors influence the determination of inflation, a key goal variable for monetary policy.\n\nGlobalization, Monetary Policy, and Financial Markets\nMonetary policy works in the first instance by affecting financial conditions, including the levels of interest rates and asset prices. Changes in financial conditions in turn influence a variety of decisions by households and firms, including choices about how much to consume, to produce, and to invest. Anyone who participates in financial markets these days is aware that these markets transcend national borders and are highly sensitive to economic and political developments anywhere in the world. Does financial globalization significantly reduce the influence of the Federal Reserve on financial conditions in the United States and thereby possibly make U.S. monetary policy less effective?\n\nCertainly, the financial environment in which U.S. monetary policy is made has been irrevocably changed by the remarkable increases in the magnitudes of financial flows into and out of the United States. A quarter-century ago, foreign holdings of U.S. financial assets were limited, and therefore, the influence of foreign investors and foreign financial conditions on U.S. financial markets was in most cases relatively modest. As I have already noted, that situation has changed markedly, as global financial markets have become increasingly integrated and both foreign and domestic investors have become more diversified internationally. Today, foreigners hold about one-quarter of the long-term fixed-income securities issued by U.S. entities of all types and more than half of publicly-held U.S. Treasury securities. Cross-border financial flows are enormous and growing: For example, in 2006, foreigners acquired on net more than $1.6 trillion in U.S. assets, while U.S. investors purchased more than $1 trillion in foreign assets. Given their scale, capital inflows and outflows certainly influence long-term U.S. interest rates and other key asset prices, both by affecting the underlying supply-demand balance between saving and capital investment and by helping to determine the premiums that investors receive for holding assets that are risky or illiquid.\n\nHow does all this affect monetary policy? It is helpful to think about the potential implications of globalized financial markets for monetary policy by beginning with the first stages of the monetary policy transmission mechanism. In particular, as you know, the Federal Reserve influences financial conditions through its ability to control the federal funds rate, the interest rate at which banks lend to each other overnight. Through the use of open-market operations and other techniques, the Federal Reserve can manage the supply of funds in the interbank market as needed to keep the federal funds rate close to its target, a capability that has not been affected by the increased international integration of financial markets. Although the federal funds rate does not itself have a major influence on economic activity, other short-term rates--such as Eurodollar rates--are determined largely by the current and expected future values of the funds rate, reflecting the close substitutability of alternative short-term sources of funding. The Federal Reserve’s ability to control the federal funds rate thus gives it a strong influence over other short-term dollar nominal interest rates and, to the extent that inflation is inertial or sticky in the short run, over short-term real interest rates as well.\n\nThe Fed’s ability to set the short-term interest rate independently of foreign financial conditions depends critically, of course, on the fact that the dollar is a freely floating currency whose value is continuously determined in open, competitive markets. If the dollar’s value were fixed in terms of another currency or basket of currencies, the Fed would be constrained to set its policy rate at a level consistent with rates in global capital markets. Because the dollar is free to adjust, U.S. interest rates can differ from rates abroad, and, consequently, the Fed retains the autonomy to set its federal funds rate target as needed to respond to domestic economic conditions.\n\nShort-term interest rates affect the domestic economy through a number of channels (for example, by affecting the cost of holding inventories), so monetary policy could influence economic activity to some degree even if its control were limited to the short end of the yield curve. Moreover, the pricing of some putatively long-term financial assets may be strongly influenced by shorter-term rates. Thirty-year fixed-rate mortgages provide one example. Because people move or refinance their loans, leading them to prepay their mortgages, and because the pattern of real mortgage payments is more front-loaded than that of nominal payments, the effective duration of a thirty-year mortgage may be closer to five years than to thirty years.\n\nNevertheless, the ability to influence longer-term interest rates and the prices of longer-term assets is an important component of the Fed’s toolkit for managing aggregate demand. What are the implications of increased global financial integration further along the transmission mechanism? The globalization of financial markets does at times make the Fed’s analysis of financial and economic conditions more complex. The behavior of long-term interest rates over the past few years is a case in point. Compared with historical averages, long-term nominal interest rates have remained relatively low in recent years--the phenomenon that the previous Chairman, Alan Greenspan, termed a “conundrum”--even as the Federal Reserve was withdrawing monetary accommodation by raising the target for the federal funds rate by more than 400 basis points. As a consequence, the Treasury yield curve has become inverted--that is, long-term rates have been lower than short-term rates, another historically unusual pattern. Developments in global capital markets have contributed significantly to these outcomes. For example, strong foreign demand for U.S. long-term debt has been one factor tending to reduce the term premium, the extra return that investors demand to hold longer-term bonds. All else equal, a smaller term premium implies a lower level of long-term interest rates. At the same time, increases in the net supply of saving in global capital markets--which, to a significant extent, are a product of the large current account surpluses of some emerging-market economies and of oil-producing nations--have resulted in lower real long-term interest rates both in the United States and abroad. Clearly, to understand and evaluate the behavior of the term structure and to assess the implications of current yields for the domestic economy, the Fed must take into account the various effects of foreign capital flows on U.S. yields and asset prices, a task that can be quite challenging.\n\nWith globalized financial markets comes increased financial interdependence. One statistical indicator of that interdependence is that the correlations between long-term interest rates in the United States and those in other industrial countries are high and appear to have risen significantly in the last few years. For example, from 1990 to 2006, the daily correlation between changes in ten-year swap rates in the United States and Germany averaged 0.42, a relatively high value. During the last three years of that period, however, that correlation rose to 0.65, an increase that is both economically and statistically significant. Similar results obtain for the correlation of U.S. yields with yields of other industrial countries, including Canada, the United Kingdom, and Japan. That interdependence suggests that monetary policy makers must pay attention to conditions abroad as well as at home.\n\nHowever, the greater analytical complexity and interdependence associated with the globalization of financial markets notwithstanding, both theory and--perhaps more persuasively--recent experience support the view that the Federal Reserve retains considerable leverage over longer-term rates and key asset prices, although the links from monetary policy decisions to longer-term rates are somewhat looser than those to short-term rates. In particular, by employing consistent and predictable policies, the Fed can help to shape market participants’ views of how future nominal short-term rates are likely to evolve and how they are likely to respond to economic developments. Because long-term nominal interest rates can be viewed as the sum of a weighted average of expected future short-term nominal interest rates plus a term premium, Federal Reserve policies and communications substantially influence the behavior of these rates. In analogous fashion, the Fed’s ability to influence real interest rates at shorter horizons provides a lever for affecting longer-term real yields. Like nominal long-term yields, real long-term yields can be viewed as an average of current and future expected short-term real rates, so that the effects of monetary policy on shorter-term real yields feed into longer-term yields as well. Well-anchored inflation expectations are also helpful in this regard: If expectations of long-term inflation are stable, then changes in long-term nominal interest rates translate into similar changes in long-term real rates.\n\nThe empirical literature supports the view that U.S. monetary policy retains its ability to influence longer-term rates and other asset prices. Indeed, research on U.S. bond yields across the whole spectrum of maturities finds that all yields respond significantly to unanticipated changes in the Fed’s short-term interest-rate target and that the size and pattern of these responses has not changed much over time (Kuttner, 2001; Andersen and others, 2005; and Faust and others, 2006). Empirical studies also find that U.S. monetary policy actions retain a powerful effect on domestic stock prices.1\n\nIf globalization has not constrained the ability of U.S. monetary policy to influence domestic financial conditions, why are long-term interest rates and key asset prices so correlated across economies? One possibility is that economic integration has increased the extent that economic shocks--to the oil market, for example--have global rather than purely local effects and that most of the world’s central banks are guiding their policy response in similar ways to such shocks. Recent research suggests another possibility, which is that U.S. monetary policy actions may have significant effects on foreign yields and asset prices as well as on domestic financial prices. For example, changes in U.S. short-term interest rates seem to exert a substantial influence on euro area bond yields (Ehrmann, Fratzscher, and Rigobon, 2005; Faust and others, 2006) and appear to have a strong effect on foreign equity indexes as well.2 In contrast, the effects of foreign short-term rates on U.S. asset prices appear to be relatively weaker. These cross-border effects of policy, and their asymmetric nature, are somewhat puzzling. One would expect a more symmetric relationship between the United States and the euro area, for example, as the two regions are of comparable economic size. It will be interesting to see if these relationships persist.\n\nI draw two conclusions on this issue. First, the globalization of financial markets has not materially reduced the ability of the Federal Reserve to influence financial conditions in the United States. But, second, globalization has added a dimension of complexity to the analysis of financial conditions and their determinants, which monetary policy makers must take into account.\n\nHas Globalization Affected the U.S. Inflation Process?\nOther than through its influence on financial markets, globalization may also have affected the operation of monetary policy by changing the relative importance of the various factors that determine the domestic inflation rate.3 As national markets become increasingly integrated and open, sellers of goods, services, and labor may face more competition and have less market power than in the past. In particular, prices and wages may depend on economic conditions abroad as well as on conditions in the local market. These linkages suggest that, at least in the short run, globalization and trade may affect the course of domestic inflation.\n\nInternational factors might affect domestic inflation through several related channels. First, the expansion of trade may cause domestic inflation to depend to a greater extent on the prices of imported goods--not only because imported goods enter the consumer basket or (in the case of imported intermediate goods) affect the costs of domestic production, but because competition with imports affects the pricing power of domestic producers. Second, competitive pressures engendered by globalization could affect the inflation process by increasing productivity growth, thereby reducing costs, or by reducing markups. Third, to the extent that some prices are set in internationally integrated markets, pressures on resource utilization in foreign economies could be relevant to domestic inflation.4\n\nSome analysts might object to the proposition that globalization affects the inflation process at all on the grounds that the structural changes that globalization engenders can affect only the relative prices of goods and services; in contrast, inflation--the rate of change of the overall price level--must ultimately be determined solely by monetary policy (Ball, 2006). Certainly, monetary policy determines inflation in the long run, and the central bank must take responsibility for the inflation outcomes generated by its policies. For example, the opening of trade with emerging-market economies that have low labor costs may reduce the relative prices of imported manufactures, but if the long-run inflation objective of the central bank is held constant, then the ultimate effect of the lower import prices on inflation will be nil as changes in other prices offset the effect of import prices.\n\nHowever, the conclusion that inflation is determined only by monetary policy choices need not hold in the short-to-medium run. In the shorter term, central banks do not usually offset completely the effects of shocks to supply or prices--of which a change in the relative price of imports is an example--in part because any monetary action made in response will take time to be effective. Consequently, such shocks may affect domestic inflation for a time. More subtly, a central bank following a strategy of “opportunistic disinflation” might react to a favorable shock to supply or prices by lowering its medium-term objective for inflation (Orphanides and Wilcox, 2002). In the case of a central bank pursuing such a strategy, foreign factors that depress domestic inflation may have a persistent effect so long as inflation exceeds the central bank’s long-term objective.5\n\nWhat, then, is the evidence for the view that globalization is affecting the inflation process in the United States? Of the various channels that have been suggested, probably the most intuitive is the idea that greater openness to trade has increased the influence of import prices on domestic inflation. In the major industrial economies over the past decade or so, import prices--particularly the prices of imported manufactured goods--have generally risen at a slower rate than other consumer prices, slowing overall inflation. The slower growth in import prices reflects to some extent rapid productivity gains in the production of manufactures, an important component of trade.6 Increased exports by low-cost emerging-market economies have also helped keep down the prices of imports received by the United States and other industrialized countries. Indeed, the share of U.S. non-oil imports coming from the emerging Asian economies has increased from 27 percent to 34 percent over the past decade or so.\n\nOverall, research indicates that trade with developing economies in particular has slowed the rate of growth of import prices faced by industrialized countries, with estimates of the reduction ranging widely from 1/2 to 2 percentage points. One study, for example, estimated that trade with China alone has reduced annual import price inflation in the United States by about 1 percentage point over the period 1993-2002 (Kamin, Marazzi, and Schindler, 2006)7. However, imported goods make up only part of what people consume, and so the effect on overall inflation is less than the deceleration in the prices of imports alone. Typical estimates of the short-term effect on the overall inflation rate of less-rapid increases in the prices of imports stemming from trade with China are in the neighborhood of 0.1 percent or less per year--a discernible but certainly not a large effect.\n\nThis result requires several qualifications. First, the direct effect of lower import prices on overall consumer price inflation could understate the overall effect, if lower import prices force competing domestic firms to restrain their prices as well. Research has generally found that import prices do affect the prices charged by domestic producers.8 For example, the International Monetary Fund found that, in a range of industrial economies, the prices of domestic products were restrained by competition from imports, the effect being larger with greater penetration (IMF, 2006). To the extent that import competition slows the rate of increase of domestic prices, the tendency of lower-cost imports to reduce domestic inflation will be enhanced.\n\nOn the other hand, not all aspects of globalization and trade reduce inflation. For example, globalization has been associated with strong growth in some large emerging-market economies, notably China and India, and this growth likely has contributed to recent increases in the prices of energy and other commodities. During 2003-05, for example, China alone accounted for nearly one-third of the growth in both global real gross domestic product (GDP) and oil consumption. It is difficult to assess the exact extent to which increased demand by developing countries has contributed to the run-ups in commodity prices in recent years, as these prices are also affected by supply conditions and other factors. However, one study estimated that, if the share of world trade and world GDP enjoyed by non-industrial countries had remained at its 2000 levels, then by 2005 real oil prices would have been as much as 40 percent lower, and real metals prices 10 percent lower, than they actually were (Pain, Koske, and Sollie, 2006). Accordingly, in the past several years, the effect of growth in developing economies on commodity prices has been a source of upward pressure on inflation in the United States and other industrial economies.\n\nWhen the offsetting effects of globalization on the prices of manufactured imports and on energy and commodity prices are considered together, there seems to be little basis for concluding that globalization overall has significantly reduced inflation in the United States in recent years; indeed, the opposite may be true. That said, the integration of rapidly industrializing economies into the global trading system clearly has had important effects on the prices of both manufactures and commodities, reinforcing the need to monitor international influences on the inflation process.\n\nGlobalization also may affect the inflation process through other channels. Some researchers, for example, have suggested greater openness to trade and the resulting increase in competition may have led to reduced markups of price over cost (Chen, Imbs, and Scott, 2004). However, the rise in profit rates in recent years seems inconsistent with the view that markups have declined (Bowman, 2003; Kohn, 2006).9 The competition fostered by trade should also promote productivity growth, reducing growth in costs and making the attainment of low inflation easier. That productivity growth is linked to the intensity of competition is plausible, and more-rapid productivity growth seems to help to explain the slowing of inflation in the United States in the mid-1990s. However, the fact that most other industrial countries did not experience the same increase in productivity growth as the United States during that period, even as they became more open to trade, suggests that the relationship between productivity and trade may be complex.\n\nIn a globalized economy, the level of resource utilization in the world economy is another potential influence on domestic inflation. Standard analyses of inflation based on the concept of a Phillips curve assign a role in inflation determination to the domestic output gap--the difference between the economy’s potential output and its actual production. According to this theory, the existence of slack in the economy makes it more difficult for producers to raise prices and for workers to win higher wages, with the result that inflation slows. These conventional analyses have considered only the possible link between domestic inflation and the domestic output gap. But in an increasingly integrated world economy, one may well ask whether a global output gap can be meaningfully defined and measured and, if it can, whether it affects domestic inflation. In other words, all else being equal, would a booming world economy increase the potential for inflationary pressures within the United States?\n\nIn principle, with the domestic determinants of inflation held constant, reduced slack in the global economy could increase domestic inflation for a time if it led to higher prices for some traded goods and services relative to the prices of goods and services that are not usually traded. For example, suppose that the United States produces personal computers both for export and for domestic use, and that more-rapid growth abroad increases the world demand for computers. Stronger global demand for computers raises the prices that U.S. producers can charge their foreign customers. Moreover, because all computer producers are facing a stronger global market, U.S. producers can charge more for their output at home as well. If producers of many goods face increases in worldwide demand, the net effect could be higher inflation in the United States, even though there may be no measurable effect on the prices of U.S. imports.\n\nThe idea is intriguing but again, unfortunately, the evidence is so far inconclusive. Early work, including some done at the Federal Reserve Bank of Boston, found no effect of global demand conditions on U.S. inflation, as did most of the subsequent research.10 Recently, however, several researchers affiliated with the Bank for International Settlements (BIS) have reported results favorable to the global output gap hypothesis (Borio and Filardo, 2006). Using data for sixteen industrialized countries (plus the euro area) for 1985-2005, they found significant effects of the global output gap on domestic inflation rates--indeed, effects that were generally larger than those of domestic output gaps and that were rising over time. This provocative result has in turn been challenged by Federal Reserve Board researchers, who find that the empirical support for a role for the global output gap does not survive modest changes in the way the data are analyzed. As domestic output gaps are difficult to measure, even with the benefit of hindsight, it is perhaps not surprising that measuring and assessing the effects of a global output gap have proved contentious. A clear resolution of the question of how global economic conditions affect domestic inflation may continue to elude us.\n\nOverall, global factors do seem to influence domestic inflation. Most directly, increasing trade with China and other developing countries has led to slower growth in the prices of imported manufactured goods. However, this effect has been offset in the most recent period by the increases in the prices for energy and commodities associated with the rapid growth in these emerging market economies. Other, more indirect channels may exist, including the possibilities that trade promotes productivity growth and thus lower costs and that global demand conditions influence domestic pricing decisions. However, more research is needed to pin down the significance of these indirect influences.\n\nConclusion\nI have foreshadowed my conclusions. Without doubt, ongoing global economic integration is a phenomenon of the greatest importance, one that will help shape the U.S. economy for decades. Globalization has not materially affected the ability of the Federal Reserve to influence financial conditions in the United States, nor has it led to significant changes in the process which determines the U.S. inflation rate. However, effective monetary policy making now requires taking into account a diverse set of global influences, many of which are not yet fully understood. The Federal Reserve will continue to place a high priority on understanding the effects of globalization on the U.S. economy in general and on the conduct and transmission of U.S. monetary policy in particular.\n\nReferences\n\nAndersen, T.G., Bollerslev, T., Diebold, F.X., Vega, C., (2005). “Real-Time Price Discovery in Stock, Bond and Foreign Exchange Markets,” unpublished paper, University of Pennsylvania.\n\nBall, Lawrence (2006). “Has Globalization Changed Inflation?” NBER Working Paper No. 12687 (November). http://www.nber.org/papers/w12687\n\nBernanke, Ben and Kenneth N. Kuttner (2005). “What Explains the Stock Market’s Reaction to Federal Reserve Policy?” Journal of Finance, vol. 60, pp. 1221-1257. www.federalreserve.gov/pubs/FEDS/2004/200416/200416abs.html\n\nBowman, David (2003). “Market Power and Inflation,” Board of Governors of the Federal Reserve System, International Finance Discussion Paper No. 783. www.federalreserve.gov/pubs/ifdp/2003/783/default.htm\n\nBorio, Claudio and Andrew Filardo (2006). “Globalisation and Inflation: New Cross-Country Evidence on the Global Determinants of Domestic Inflation,” unpublished paper, Bank for International Settlements, Basel, Switzerland (March).\n\nChen, Natalie, Jean Imbs, and Andrew Scott (2004). “Competition, Globalization, and the Decline in Inflation,” CEPR Discussion Paper No. 4695. www.cepr.org/pubs/dps/DP4695.asp\n\nEhrmann, Michael and Marcel Fratzscher (2006). “Global Financial Transmission of Monetary Policy Shocks,” working paper no. 616, European Central Bank. http://www.ecb.int/pub/pdf/scpwps/ecbwp616.pdf (683 KB PDF)\n\nEhrmann, Michael, Marcel Fratzscher and Roberto Rigobon (2005). “Stocks, Bonds, Money Markets, and Exchange Rates: Measuring International Financial Transmission,” working paper no. 452, European Central Bank. http://www.ecb.int/pub/pdf/scpwps/ecbwp452.pdf (953 KB PDF)\n\nEuropean Central Bank (2006), “Effects of the Rising Trade Integration of Low-Cost Countries on Euro Area Import Prices,” ECB Monthly Bulletin, Frankfurt, Germany (August), pp. 56-57. http://www.ecb.int/pub/mb/html/index.en.html\n\nFaust, Jon, John H. Rogers, Shing-Yi B. Wang, and Jonathan Wright (forthcoming). “The High-Frequency Response of Exchange Rates and Interest Rates to Macroeconomic Announcements,” Journal of Monetary Economics.\n\nGamber, Eduard N. and Juann H. Hung (2001). “Has the Rise in Globalization Reduced U.S. Inflation in the 1990s,” Economic Inquiry, vol. 39 (January), pp. 58-73.\n\nHausman, Joshua and Jon Wongswan (2006). “Global Asset Prices and FOMC Announcements,” Board of Governors of the Federal Reserve System, International Finance Discussion Paper No. 886. www.federalreserve.gov/pubs/ifdp/2006/886/default.htm\n\nHooper, Peter, Torsten Slok, and Christine Dobridge (2006). “Understanding U.S. Inflation,” Global Markets Research, Deutsche Bank (July 26).\n\nIhrig, Jane, Steven Kamin, Deborah Lindner, Jaime Marquez (forthcoming). “Some Simple Tests of the Globalization and Inflation Hypothesis,” Board of Governors of the Federal Reserve System, International Finance Discussion Papers.\n\nInternational Monetary Fund (2006). “How Has Globalization Changed Inflation?” World Economic Outlook, Washington, D.C.: IMF, April, pp. 97-134. www.imf.org/external/pubs/ft/weo/2006/01/index.htm.\n\nKamin, Steven B., Mario Marazzi, and John W. Schindler (2006). “The Impact of Chinese Exports on Global Import Prices,” Review of International Economics, vol. 14 (May), pp. 179-201.\n\nKohn, Donald (2006). “The Effects of Globalization on Inflation and their Implications for Monetary Policy,” speech delivered at the 51st Economic Conference sponsored by the Federal Reserve Bank of Boston, Chatham, Mass., June 16. www.federalreserve.gov/newsevents/speech/kohn20060616a.htm.\n\nKuttner, Kenneth N. (2001). “Monetary Policy Surprises and Interest Rates: Evidence from the Fed Funds Futures Market,” Journal of Monetary Economics, vol. 47 (June), pp. 523-44.\n\nNickell, Stephen (2005). “Why Has Inflation Been So Low Since 1999?” Bank of England Quarterly Bulletin, London, vol. 45, pp. 92-107.\n\nOrphanides, Athanasios, and David Wilcox (2002). \"The Opportunistic Approach to Disinflation,\" International Finance, vol. 5 (Spring), pp. 47-71.\n\nPain, Nigel, Isabell Koske, and Marte Sollie (2006). “Globalisation and Inflation in the OECD Economies,” Economics Department Working Paper No. 524, Organisation for Economic Co-operation and Development, Paris (November).\n\nRigobon, Roberto and Brian Sack (2004). “The Impact of Monetary Policy on Asset Prices,” Journal of Monetary Economics, vol. 51 (November), pp. 1553-75.\n\nRogoff, Kenneth (2003). “Globalization and Global Disinflation,” Paper prepared for the Conference on Monetary Policy and Uncertainty: Adapting to a Changing Economy, sponsored by the Federal Reserve Bank of Kansas City, Jackson Hole, Wyoming, August 28-30. http://www.kansascityfed.org/PUBLICAT/SYMPOS/2003/pdf/Rogoff2003.pdf (237 KB PDF)\n\nTootell, Geoffrey M.B. (1998). “Globalization and U.S. Inflation,” Federal Reserve Bank of Boston, New England Economic Review (July/August), pp. 21-33. http://www.bos.frb.org/economic/neer/neer1998/neer498b.htm\n\nYellen, Janet (2006), “Monetary Policy in a Global Environment,” speech delivered at the conference “The Euro and the Dollar in a Globalized Economy,” University of California at Santa Cruz, Santa Cruz, California, May 27. http://www.frbsf.org/news/speeches/2006/060527.pdf (346 KB PDF)\n\nFootnotes\n\n1.  The decline of U.S. stock markets to a hypothetical 100 basis point tightening in the federal funds target rate is estimated to be 5.3 percent by Bernanke and Kuttner (2005), 5.5 percent by Ehrmann and Fratzscher (2006), and 6.2 percent by Rigobon and Sack (2004). Andersen, Bollerslev, Diebold, and Vega (2005) find similar results. Return to text\n\n2.  Ehrmann, Fratzscher and Rigobon (2005) find that euro area stock markets drop by nearly 2 percent in response to a hypothetical 100 basis point tightening in the United States on the same day. The estimated effect of euro area monetary policy on U.S. stock markets is much smaller, about 0.5 percent. Ehrmann and Fratzscher (2006) and Hausman and Wongswan (2006) examine the effect of U.S. monetary policy announcement surprises on equity indexes of fifty countries. These studies find that, on average, a hypothetical 25 basis point rise in the federal funds rate is associated with a drop of about 1 percent in foreign equity indexes. Equity indexes in countries with a less flexible exchange rate regime respond more to U.S. monetary policy surprises. Return to text\n\n3.  Some material in this section reflects research described in Ihrig, Kamin, Lindner, and Marquez (forthcoming). For additional perspective on these issues, see Kohn (2006) and Yellen (2006). Return to text\n\n4.  This list is not exhaustive. For example, if domestic pricing power is affected by international competition, globalization could affect the relationship between inflation and resource utilization at home. Return to text\n\n5.  Rogoff (2003) provides an alternative theory of how globalization may affect the central bank’s inflation objective. He argues that deregulation and international integration have led to more flexible prices, so that any attempt by a central bank to stimulate the real economy by allowing inflation to rise unexpectedly will be less effective than it would have been in the past. Because central banks have less incentive to create unexpected inflation, their promises to keep inflation low are more credible, which in turn reduces the cost of keeping inflation low. Accordingly, in Rogoff’s analysis, globalization has led monetary authorities to maintain lower long-term inflation rates. A criticism of this story is that it implies that the Phillips curve is steeper today than in the past (that is, that inflation is more sensitive to slack in the economy), a prediction that does not accord with most empirical studies.  Return to text\n\n6.  The dollar prices of imports are also affected by changes in the value of the exchange rate. However, the effects of exchange-rate changes on the domestic prices of imported goods have been quite low in recent years. Return to text\n\n7.  The share of imports coming from China is relatively high for the United States, and so the effect of trade with China may be lower for other industrialized countries. For example, one analysis of trade between the United Kingdom and both China and India found that, over the period 1999-2002, the effect on import-price inflation was only about minus 0.5 percentage point annually (Nickell, 2005). Research by the European Central Bank, however, found that the euro area’s trade with a wide range of developing economies had reduced the rate of increase in import prices to the area about 2 percentage points annually over 1996-2005 (European Central Bank, 2006). Return to text\n\n8.  An exception is Kamin, Marazzi, and Schindler (2006), who find little effect on the prices of domestic U.S. producers of similar goods. Return to text\n\n9.  Bowman (2003) finds little evidence that competition has increased or that markups declined in industrial economies. Return to text\n\n10.  Work finding no role or, at best, a marginal one for a global output gap includes Tootell (1998), Hooper, Slok, and Dobridge (2006), Pain, Koske, and Sallie (2006), and Ball (2006). In contrast, Gamber and Hung (2001) find that, over 1976-99, a trade-weighted average of capacity utilization for thirty-five U.S. trading partners is a significant determinant of U.S. inflation.  Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070302a.htm",
        "title": "Globalization and Monetary Policy",
        "date": "3/2/2007"
    },
    {
        "content": "February 26, 2007\n\nGovernor Susan Schmidt Bies\n\nAt the Global Association of Risk Professionals Basel II Summit, New York, New York\n\n\n\nThank you very much for the invitation to speak today. As most of you know, we are in the middle of a particularly busy time with respect to Basel II implementation in the United States. Therefore, I plan to offer some thoughts on recent events relating to U.S. work on Basel II. I will also provide some additional information and context to help people as they think about how to comment on the U.S. proposals that are currently outstanding. But before I begin discussing recent events, I would like to reiterate briefly the Federal Reserve's reasons for why Basel II is important in the United States.\n\nReasons for Basel II\nBanking is a business in which banks take and manage risks. Bankers implicitly accept risk when providing financial services to customers and also take explicit risk positions that offer profitable returns relative to their risk appetites. One of the most important jobs of bank supervisors is to ensure that banks maintain an adequate capital cushion against losses, especially during times of financial instability or stress. Minimum regulatory capital requirements are an integral part of ensuring that banks have an adequate cushion. When developing minimum capital requirements, supervisors should continue to promote approaches that both minimize the negative consequences of risk taking by financial institutions and encourage improved risk-management practices, particularly at those institutions that could affect global financial stability.\n\nThe Federal Reserve's main reason for pursuing Basel II is the growing inadequacy of current Basel I regulatory capital rules for the large, internationally active banks that are offering ever more complex and sophisticated products and services. We need a more risk-sensitive capital framework for these particular banks, and we believe Basel II is such a framework. In addition, Basel II would promote risk-measurement and risk-management enhancements and improve market discipline, while giving supervisors a more conceptually consistent and more transparent framework for evaluating systemic risk, particularly through credit cycles. Basel II should establish a more coherent relationship between regulatory measures of capital adequacy and the day-to-day risk-focused supervision of banks, enabling examiners to better evaluate whether banks are holding prudent levels of capital given their risk profiles.\n\nFor similar reasons, U.S. supervisors support the 2005 Basel/International Organization of Securities Commissions (IOSCO) revisions to the 1996 Market Risk Amendment (MRA). Since adoption of the MRA, banks' trading activities have become more sophisticated and have given rise to a wider range of risks that are not easily captured in the existing value-at-risk (VaR) models used in many banks. For example, banks are now including more products related to credit risk, such as credit-default swaps and tranches of collateralized debt obligations, in their trading books. These products can create default risks that are not captured well by the methodologies required under the current MRA rule--which specifies a ten-day holding period and a 99 percent confidence interval--thereby creating potential arbitrage opportunities between the banking book and the trading book.\n\nRecent Events Relating to U.S. Basel II Implementation\nAs most of you know, there have been two important recent events related to Basel II implementation in the United States, both of which occurred on February 15. The first was the release of a report by the Government Accountability Office (GAO) on U.S. implementation of Basel II. The second was issuance of proposed Basel II supervisory guidance by the U.S. banking agencies.\n\nGAO Study\nThe Federal Reserve welcomes the recent GAO report on Basel II implementation in the United States. We believe this report will help move the U.S. Basel II process forward. While I do not intend to summarize the GAO report here, I would like to offer a few thoughts on the report's conclusions.\n\nThe Federal Reserve concurs with the report's finding that the Basel I capital rule is particularly inadequate for large banking organizations; the report states that the agencies should continue their efforts to finalize the U.S. Basel II capital rule and proceed with the parallel run and transition period to Basel II. We agree with the report's conclusion that finalizing the U.S. Basel II rule would generate crucial information to enable the agencies to make future assessments of the strengths and weaknesses of the Basel II rule for the U.S. banking system. We also agree with the GAO's finding that establishing Basel-II transitional floors will prevent a bank's regulatory capital requirements from declining precipitously during the transition period. And finally, we agree with the GAO that any further delay in the U.S. implementation of Basel II creates potential competitive disadvantages for U.S. banks when they are compared with foreign banks.\n\nThe GAO report also raised issues about transparency and ambiguity in the U.S. Basel II process. While the agencies are striving to be as transparent as possible in the Basel II implementation process--in some cases going well beyond our obligations under the Administrative Procedure Act--we recognize that some aspects of the Basel II proposals are ambiguous, as noted by the GAO report. The agencies expect to address this ambiguity substantially as we work to finalize the Basel II rule. Moreover, the experience we gain implementing the new regulatory capital framework during the transition period will help us address remaining uncertainties. Of course, we hope that comments on all aspects of our proposals will also help us identify and resolve potential areas of ambiguity. To meet the timetable for US implementation, it is important that the agencies move forward in considering the comments as promptly as possible. Therefore, I particularly encourage you in your comments to go beyond criticism of what has been proposed and specify your preferred solution, including alternative technical methodologies, language, and frameworks.\n\nU.S. Basel II Supervisory Guidance\nWe are very pleased that on February 15 the U.S. agencies released proposed supervisory guidance to accompany the Basel II notice of proposed rulemaking (NPR) issued last September. While documents that make up the proposed guidance have not yet been published in the Federal Register, they are available on the agencies' public websites. I would like to take a moment to thank the Federal Reserve staff and staff from the other banking agencies who worked very hard to produce the proposed guidance. Once again, I do not plan to summarize the guidance here. But I do believe a few points are worth highlighting.\n\nAs most of you know, the Basel II NPR outlines qualification requirements for institutions calculating regulatory risk-based capital requirements (Pillar 1) using the advanced internal ratings-based approach (IRB) for credit risk and the advanced measurement approaches (AMA) for operational risk--together known as the advanced approaches. The qualification requirements for the advanced approaches are written broadly to accommodate the many ways a bank may design and implement robust credit and operational risk measurement and management systems, and to permit industry practice to evolve.\n\nThe supervisory guidance relating to requirements for calculating risk-based regulatory capital under the advanced approaches updates and expands on proposed supervisory guidance issued in 2003 and 2004; both of those documents were companion pieces to the 2003 Basel II advance notice of proposed rulemaking. Likewise, the full set of guidance documents issued on February 15 is intended to provide additional information to help banks understand the qualification requirements laid out in the Basel II NPR. In most areas of risk management, it is our intention to have institutions retain their ability to choose which specific methods they employ. In other words, the guidance identifies an acceptable range of practice for banks. Within this range, a bank could use several types of approaches and methodologies. The proposed guidance offers some concrete examples of different approaches that might be considered acceptable practices.\n\nIn addition to supervisory guidance on requirements for institutions using regulatory risk-based capital requirements (Pillar 1), the agencies issued proposed guidance relating to supervisory review of capital adequacy (Pillar 2). Although this is the first guidance on Pillar 2 issued by the U.S. agencies, the supervisory review process described in the guidance is a continuation of the agencies' long-standing approach to bank supervision. However, some aspects of existing supervisory practices are being augmented or more clearly defined to support the proposed Basel II framework. Probably the most noteworthy change is a specific requirement, outlined in the NPR, for banks to develop an internal capital adequacy assessment process (ICAAP). The proposed guidance offers additional detail about the ICAAP, but in short, the ICAAP should identify and measure material risks, set capital goals that relate to risks, and provide governance and controls to ensure that internal capital assessments are subject to proper oversight. The ICAAP requirement is intended to underscore the agencies' existing view that the primary responsibility for assessing capital adequacy lies with banks. Supervisors are responsible for evaluating bank assessments of capital adequacy and for ensuring that the processes for developing those assessments are robust and satisfactory. Notably, the proposed ICAAP requirement is consistent with existing Federal Reserve supervisory guidance as reflected in Supervision and Regulation Letter 99-18.\n\nThe agencies believe that the proposed supervisory guidance documents are a necessary supplement to the Basel II NPR. The guidance includes standards to promote safety and soundness and to encourage the comparability of regulatory capital measures across banks. During the qualification process, a bank's primary federal supervisor will review the bank's risk-measurement and -management framework relative to the qualification requirements in order to determine whether the bank may apply the advanced approaches to calculate minimum regulatory capital.\n\nThe proposed supervisory guidance documents are companion guidance to the Basel II NPR; therefore, they do not address any public comments received since the NPR was issued. The guidance provides additional information that should help banks satisfy the NPR's qualification requirements. Importantly, the publication of these guidance documents for comment does not imply that the final outcome of the NPR has already been determined.\n\nAs part of the rulemaking process, the proposed guidance documents are subject to change on the basis of, among other things, public comments and the agencies' decisions regarding any final rule. As they did with other proposed Basel II documents, the U.S. agencies are requesting feedback on all the outstanding proposed supervisory guidance. It would be particularly helpful if reviewers looked at the NPR--whose comment period ends March 26--and the proposed supervisory guidance together in order to decide whether the documents provide them with enough information to determine whether they meet the U.S. Basel II qualification requirements. We welcome any and all comments, ranging from broader opinions about the proposed framework to feedback on the more technical aspects of the proposals.\n\nWe think institutions will find information in the proposed guidance helpful as they progress towards implementing risk-measurement and risk-management systems that are consistent with any final U.S. Basel II rule. However, since the proposed guidance is subject to change, we understand that lack of certainty about rules and guidance can be confusing and frustrating. We recognize that many institutions have been diligently preparing for Basel II implementation and we understand our obligation, as regulators, to support institutions wanting to adopt Basel II at the first available date. One of the reasons for providing proposed guidance at several stages in the Basel II implementation process is to help guide U.S. institutions that wish to adopt Basel II as soon as possible. We suggest that those institutions continue to move forward with their implementation, including the identification of gaps in their own preparation, while remembering that proposals could change.\n\nThe agencies remain open to considering the full set of possibilities for Basel II. We continue to seek comment on whether some form of the Basel II standardized approach should be adopted in the United States. When deciding whether our large, internationally active banks should have access to a standardized approach, the agencies will need to consider a number of important issues: whether such an approach would accommodate the risks those banks take, now and in the future; whether it would provide adequate risk sensitivity; whether taking the time needed to develop an appropriate U.S. version of the standardized approach would unduly delay the Basel II implementation process; whether the standardized approach would be useful on a transition basis in order to give some banks more time to prepare to use the advanced approaches; and, finally, whether the marketplace would find such an option for those banks meaningful and acceptable.\n\nWe are aware that some institutions have concerns, which they have expressed in formal comment letters, about areas in which the Basel II NPR diverges from the Basel II 2004 Mid-year Text. Some of their specific concerns relate to proposed safeguards in the Basel II NPR that go beyond those in the Mid-year Text, including the effect on risk sensitivity that may result from the proposed limit on declines in aggregate capital. The proposed definition of default and treatment of downturn LGDs (loss given default) are also areas that have received attention from commenters. Additionally, there has been some concern expressed about retention of the existing leverage ratio. The U.S. agencies remain sensitive to these and other issues raised by commenters so far, including the overall burden and costs associated with the current proposals, as well as any potential competitive effects that may arise. Once we have received all comments on the proposals, we intend to review all comments carefully to assess what is the best way forward.\n\nThe Federal Reserve hopes that everyone who reviews the proposals understands that they are intended to promote the stability of the U.S. financial system by ensuring the safety and soundness of the largest U.S. banks. Thus, as Chairman Bernanke has noted, the ability of Basel II to promote safety and soundness is the first criterion by which these regulatory capital proposals should be judged.\n\nProposed Revisions to the Market-Risk Rule\nAs you are likely aware, the Federal Reserve, the Office of the Comptroller of the Currency, and the Federal Deposit Insurance Corporation also have issued an NPR to amend their market-risk capital rules. The proposed amendments would implement changes to the Market Risk Amendment that were adopted by the Basel Committee and IOSCO in 2005. The Office of Thrift Supervision joined in the NPR and, when the rules are finalized, will be implementing a market-risk capital rule for the first time. The comment period on the market-risk NPR recently ended, and the agencies are reviewing the comments received in preparation for interagency discussions on the final rule. The current market-risk rule uses a principles-based approach to market-risk capital regulation. In crafting the market-risk NPR, the agencies strived to continue the existing principles-based approach by avoiding highly prescriptive rule proposals. Some comments received to date indicate that in certain areas we could be even less prescriptive and in other areas the comments ask for more clarity or specifics. We will consider all comments carefully in trying to reach a balance between a principles-based approach and appropriate clarity.\n\nLet me briefly review a few of the more important changes introduced in the market-risk NPR and then discuss some of the comments received on these proposed changes. The objectives of the proposed changes are to enhance the rule's risk sensitivity and to reflect changes in the composition of banks' trading books and changes in risk-management practices over the last ten years.\n\nOne principal change is a new definition of the positions covered by the rule. Positions covered by the current market-risk rule generally include all GAAP trading assets and liabilities. Under the NPR, positions covered would include only those GAAP trading assets and liabilities that either meet the rule's definition of a trading position or are hedges of other positions covered by the rule. Trading positions would be defined as positions held for short-term resale, held with the intent of benefiting from actual or expected price movements, or held to lock in arbitrage profits. While the proposed new definition in the NPR would result in potentially different definitions of a trading position for accounting, risk-management, and regulatory capital paradigms, these differences, while not ideal, are based on the manner in which trading positions are treated in these respective areas. For example, under the market-risk capital rule, capital is calculated on the basis of a ten-day holding period and a 99 percent confidence level--parameters that generally are appropriate only for liquid and actively managed positions. Further, ongoing changes in accounting standards are affecting the mix of assets and liabilities carried in trading books and the banking book. For regulatory capital purposes, the geography of the exposure on the balance sheet should be secondary to the inherent nature of risks that the financial instruments reflect.\n\nRecognizing that banks' trading books have evolved over the last ten years to include riskier credit and illiquid positions, the rule would impose a new capital requirement for incremental default risk on any portfolio of covered positions for which a bank models specific risk. Incremental default risk would be defined as the default risk of a covered position that is not reflected in the bank's VaR measure (because, for example, the risk extends beyond the ten-day holding period and the 99 percent confidence level). Incremental default risk would be measured consistent with a one-year time horizon and a one-tailed, 99.9 percent confidence level--which is comparable to the measures under IRB in the Basel II framework. Consistent with a principles-based approach, banks would have the discretion to use one or more models to measure incremental default risk, subject, of course, to supervisory review.\n\nThe industry continues to actively discuss with the Basel Committee and IOSCO supervisors how best to implement a capital measure for incremental default risk. Accordingly, under the proposed rule, banks would have until January 1, 2010, to develop appropriate methodologies to measure this risk.\n\nThe U.S. agencies are currently reviewing all comments on the market risk NPR. I will not try to summarize the comments received so far, but I would like to note that we are carefully considering the request that implementation of the market risk proposals be pushed back one year to January 2009.\n\nInternational Aspects of Basel II Implementation\nOf course, we continue to recognize that the national discretion allowed under Basel II means various countries will adopt different approaches to Basel II. Further, we recognize that these different approaches may create challenges for banking organizations that operate in multiple jurisdictions. Wherever possible, we are working to minimize these differences and difficulties. But we should remember that, even before Basel II, cross-border banking has always raised specific challenges, which supervisors from various countries have worked hard to address and mitigate.\n\nLet me assure all bankers here that the Federal Reserve is aware that the adoption of national versions of Basel II has resulted in heightened concerns about home-host issues. We are committed to working with other international supervisors to resolve these issues. Indeed, the Federal Reserve and other U.S. agencies have, for many years, worked with their international counterparts to limit the difficulties and burdens that have arisen as foreign banks have entered U.S. markets and as U.S. banks have established operations in other jurisdictions. Throughout the Basel II process, we have been engaged in dialogue with our international counterparts through various avenues, such as supervisory working colleges, to share ideas and tackle specific issues as they arise. Some of these issues are very institution- and country-specific and are, therefore, better addressed through individual conversations with an institution and its relevant supervisors.\n\nThe Federal Reserve continues to be an active participant in supervisory working groups for all large U.S. and foreign banking organizations in the United States. We are encouraged by the level of cooperation and pragmatism coming out of these efforts. For example, supervisory planning efforts for U.S.-based banking organizations for the upcoming year are reviewed with foreign supervisory authorities to ensure that, regardless of Basel II timing issues, information pertaining to ongoing supervisory judgments of risk-management practices is available. We have also been encouraged by the dialogue with our foreign colleagues regarding their desire to provide flexibility in transitional arrangements. I would add that we have benefited from the numerous meetings we have had and from the opportunity to speak at conferences such as this one. These exchanges help us identify critical global concerns to examine further. We will be evaluating all of this input closely as part of our rulemaking process. As always, we encourage bankers who have questions and concerns about home-host issues to communicate promptly with their regulators in all jurisdictions so that the issues can be addressed.\n\nAnother key point, when talking about differences across countries, relates to findings from the Basel II quantitative impact studies (QIS4 and QIS5) conducted in the last few years. Interestingly, the two exercises identified a number of similar issues, some in areas in which institutions were not able to provide adequate data (especially for downturn scenarios). In the United States, QIS4 was conducted before the release of the Basel II NPR, while in Europe, QIS5 was conducted only after the passage of European Union legislation implementing Basel II. Therefore, much of what U.S. supervisors learned in QIS4 is addressed in the NPR; for example, the NPR includes a proposed supervisory mapping function for downturn estimates of loss given default. European supervisors, on the other hand, did not have the benefit of information from QIS5 when they were drafting their rules. While I cannot speak for my European counterparts, no doubt some of the issues raised in QIS5 will be addressed during Basel II implementation in Europe.\n\nIt is possible that differences in Basel II implementation may mean that the U.S. version of Basel II is in some aspects more conservative than other countries' versions. In other areas, the U.S. proposals may be less conservative. And we also know that banks manage capital to meet regulatory minimums as well as to support strategic objectives, and for that reason successful banks hold capital well above regulatory minimums. Banks know that customers, counterparties, creditors, and investors consider capital as well as the financial performance and risk exposures of banks when making their decisions. Under today's relatively stable minimum regulatory capital requirements, banks hold excess capital to be able to respond to potential business expansion opportunities and to manage through market and credit risk cycles. It is likely that with a more risk-sensitive approach to capital, banks will have to consider whether their current cushion of actual capital above regulatory minimums is still appropriate.\n\nOn balance, the Federal Reserve believes that an appropriately conservative approach to capital adequacy serves the United States' interest in maintaining the safety, soundness, and resiliency of our banking system. However, we also recognize the impact that differences among countries can have and that it is worthwhile to minimize them whenever possible. As Chairman Bernanke noted this past fall, we intend to review and consider international differences before issuing a final Basel II rule.\n\nConclusion\nThis will be my last speech on Basel II as a member of the Board of Governors of the Federal Reserve System. I want to thank many of you in the audience for taking the time to meet with me and answer my many questions related to capital and risk management over the years. I encourage all of you to continue to push forward with continuous innovation in risk management and financial instruments.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20070226a.htm",
        "title": "An Update on Basel II Implementation in the United States",
        "date": "2/26/2007"
    },
    {
        "content": "February 21, 2007\n\nVice Chairman Donald L. Kohn\n\nAt the Exchequer Club Luncheon, Washington, D.C.\n\n\n\nThank you for the opportunity to speak here today.  The Exchequer Club has enriched policy development in Washington for nearly a half century by providing a forum for the discussion of important national economic and financial policy issues.  I am honored to participate in that discussion.\n\nI want to talk this afternoon about some aspects of financial stability, a topic that has received increased attention in recent years from central banks and regulators as well as the financial media.1  A number of developments have contributed to this trend.  Just a few years ago, the terrible events of 9/11 alerted us to the importance of operational resiliency in a dangerous world.  But other forces have also led central banks and other financial supervisors around the world to increase their emphasis on financial stability.  Perhaps most important, the financial system, once essentially bank-centered, has become more market-centered.  Of course, banks continue to be core participants in the financial system and to provide an indispensable window on market activities.  But the development of a relatively market-oriented system has been accompanied by a large number of new participants, many with global reach, and a much larger array of financial instruments.  This vastly expanded web of participants and instruments has increased the number of potential channels for the creation and transmission of financial shocks.  And, some of the new financial instruments and markets have not been tested under extended stressful conditions.  In addition, supervisory authority in a number of countries abroad has been consolidated and separated from central banks; these moves have forced the new regulators and their central bank colleagues to learn how to operate not only in a new financial landscape but also in a new regulatory environment.\n\nFinally, in today’s global economy, very settled financial market conditions--narrow risk spreads and low expected market volatility--coexist with unprecedented current account imbalances among nations and interest rates that are low by historical standards.  In such a world, it would be imprudent to rule out sharp movements in asset prices and a deterioration in market liquidity that would test the resiliency of market infrastructure and financial institutions.\n\nWhile these factors have stimulated interest in both crisis deterrence and crisis management, the development of financial markets has also increased the resiliency of the financial system.  Indeed, U.S. financial markets have proved to be notably robust during some significant recent shocks, such as the sharp decline in equity prices beginning in 2000 and the failure of some large firms, including Enron and Amaranth.  New computing and telecommunications technologies, along with the removal of legal and regulatory barriers to entry have heightened competition among a wider variety of institutions and made the allocation of funds from savers to investors more efficient.  Technology also has helped financial market participants better understand the risks embedded in assets and develop instruments and systems for managing those risks, both individually and on a portfolio basis.  Together, these developments have allowed suppliers and demanders of funds and the intermediaries that stand between them to diversify their risk exposures, reduce their vulnerability to sector- or region-specific shocks, and become far less dependent on specific service providers.  In short, market developments that have altered the character and transmission of financial shocks have at the same time spread risks more widely among a greater number and broader range of market participants and given them the tools to better manage those risks.\n\nThe Federal Reserve, in its roles as a central bank, a bank supervisor, and a participant in the payments system, has been working in various ways and with other supervisors to deter financial crises.  As the central bank, we strive to foster economic stability.  As a bank supervisor, we are working with others to improve risk management and market discipline.  And in the payments and settlement area, we have been active in managing our risk and encouraging others to manage theirs.\n\nIn every step we take to deter or manage financial crises, it is important that we recognize that we impose costs, and that our efforts can be most effective if we both enhance and are supported by market discipline.  Institutions and investors must be allowed to take risks and must be prepared to accept the consequences of their actions.  For its part, the government should limit its intervention to those circumstances that could lead to placing the system in serious danger and could spill over to the economy.  Otherwise, even the most well-intentioned government intervention can actually weaken the system by undermining the incentives for market participants to limit the risks they undertake.\n\nDeterring a Financial Crisis\nThe first line of defense against financial crises is to try to prevent them.  A number of our current efforts to encourage sound risk-taking practices and to enhance market discipline are a continuation of the response to the banking and thrift institution crises of the 1980s and early 1990s.  In 1989, more than 500 banks and thrifts failed, and it was not until 1993 that the annual number of bank failures dropped well below 100.  One of the most important reforms produced in reaction to this crisis was a tightened focus on bank capital.  This tightening began with Basel I, the international capital accord of 1988, which emphasized the importance of connecting bank capital and bank supervision to bank risk.  Supervisory efforts today to develop a more advanced set of international capital standards, in Basel II, are in large part aimed at strongly reinforcing that connection.\n\nIdentifying risk and encouraging management responses are also at the heart of our efforts to encourage enterprisewide risk-management practices at financial firms.  Essential to those practices is the stress testing of portfolios for extreme, or “tail,” events.  Stress testing per se is not new, but it has become much more important.  The evolution of financial markets and instruments and the increased importance of market liquidity for managing risks have made risk managers in both the public and private sectors acutely aware of the need to ensure that financial firms’ risk-measurement and management systems are taking sufficient account of stresses that might not have been threatening ten or twenty years ago.\n\nA second core reform that emerged from past crises was the need to limit the moral hazard of the safety net extended to insured depository institutions--a safety net that is required to help maintain financial stability.  Moral hazard refers to the heightened incentive to take risk that can be created by an insurance system.  Private insurance companies attempt to control moral hazard by, for example, charging risk-based premiums and imposing deductibles.  In the public sector, things are often more complicated.  However, the Federal Deposit Insurance Corporation Improvement Act of 1991, or FDICIA, took major steps toward reducing moral hazard in the banking system and limiting taxpayer losses by reinforcing the importance of strong capital.  Among the mechanisms in the act is the requirement that bank supervisors take prompt corrective action when depositories show signs of becoming troubled.  This step was reinforced by the least-cost requirements of FDICIA, which generally require the FDIC to resolve a failing institution in the manner least costly to the deposit insurance fund.  Importantly, this provision encouraged market discipline by putting uninsured depositors and other uninsured creditors at greater risk.\n\nFDICIA allowed for the relaxation of its least-cost mandate in situations posing a true systemic crisis.  But the conditions under which least-cost resolution can be relaxed are quite strict.  First, a least-cost resolution would have to create “serious adverse effects on economic conditions or financial stability.”  Second, any action under the exception must be recommended by at least two-thirds majorities of the boards of both the FDIC and the Federal Reserve and ultimately approved by the Secretary of the Treasury in consultation with the President.\n\nThe systemic-risk exception has never been invoked, and efforts are currently underway to lower the chances that it ever will be.  For example, last December, the FDIC published an important proposal to improve its ability to resolve a troubled large depository institution in a least-cost manner.2  The FDIC proposal seeks to ensure that the largest banks and the FDIC have in place data and other management systems that would aid the FDIC in quickly identifying insured deposits and allow the FDIC to regulate the outflow of uninsured deposits while that identification was being completed.  This proposal would, among other things, allow the FDIC to continue to protect insured depositors while making clear to uninsured depositors that they could suffer losses in the event of the failure of even a very large bank.  Thus, these changes would greatly enhance market discipline and help ensure that no bank is too big to fail.\n\nCrisis prevention has also been a focus of attention in the payments area.  At the Federal Reserve, we have improved the technological redundancy and security of our payments system and have encouraged other participants to do the same.  We have also sought ways to make the clearing and settlement infrastructure keep pace with the rapid growth and evolution of financial markets and instruments.  The over-the-counter (OTC) derivatives markets provide a good example.  Those markets, especially the newer markets for credit derivatives, have been growing very rapidly.  Until 2005, however, the confirmation of trades remained largely decentralized and manual.  The result was a huge backlog of unconfirmed trades, which, to the extent it resulted in inaccurate trade records, had the potential to exacerbate market participants’ market and credit risks.\n\nThe Federal Reserve Bank of New York has taken the lead, working with other domestic and international supervisors, in helping the OTC derivatives market develop a stronger clearing and settlement infrastructure.  Those efforts focused initially on credit derivatives.  At the urging of supervisors, market participants set goals and implemented policies for reducing the huge backlogs of unconfirmed trades.  As a result, confirmation backlogs were reduced 94 percent between September 2005 and November 2006.  Market participants also promptly ended the practice of assigning trades without the prior consent of the counterparty and developed cash settlement as an alternative to physical settlement of credit derivatives in the event of a default of a participant that is a reference obligor.  Moreover, market participants have now agreed to turn their attention to equity derivatives, for which large backlogs still exist.  They are also providing supervisors with data on backlogs of all types of OTC derivatives, which will allow the supervisors to monitor the industry’s progress across the board.\n\nManaging a Financial Crisis\nClearly, when it comes to a financial crisis, as with so many other potential risks, an ounce of prevention is worth many pounds of cure.  But experience tells us that, despite our best efforts at deterrence, true financial crises will occur from time to time.  Prudently managing these tail events is no easy task.  A large part of the difficulty arises from the fact that some policy responses may have important costs that need to be balanced against their possible benefits in reducing or ameliorating the adverse effects of a crisis.  In particular, intervening in the market process can increase moral hazard by weakening market discipline if private parties come to believe that policy actions will relieve them of some of the costs of their own poor decisions or even just bad luck.  And weaker market discipline can not only distort current resource allocation but also sow the seeds of a future crisis.\n\nIf, nonetheless, policymakers reach a judgment that action must be taken, the central bank and other authorities have a variety of instruments to use.  The degree of potential moral hazard created will depend on the instrument chosen.  Policy actions that work through the overall market rather than through individual firms create a lower probability of distorting risk taking.  Thus, a first resort in managing a crisis is to use open market operations to make sure aggregate liquidity is adequate.  Adequate liquidity has two aspects:  First, we must meet any extra demands for liquidity that might arise from a flight to safety; if such demands are not satisfied, financial markets will tighten at exactly the wrong moment.  This was, for example, an important consideration after the stock market crash of 1987, when demand for liquid deposits raised the demand for reserves held at the Fed; and again after 9/11, when the loss of life and destruction of infrastructure impeded the flow of credit and liquidity.\n\nSecond, we must determine whether the stance of monetary policy should be adjusted to counteract the effects on the economy of tighter credit supplies and other knock-on effects of financial instability.  As a result, meetings of the Federal Open Market Committee (FOMC), often in conference calls if the situation is developing rapidly, have been an element in almost every crisis response.  Those meetings allow us to gather and share information about the extent of financial instability and its effects on markets and the economy as we discuss the appropriate policy response.\n\nOther policy instruments that can be used to deal with financial instability--discount window lending, moral suasion aimed at convincing private parties to keep credit flowing, actions to keep open or slowly wind down troubled institutions--are, in my judgment, more likely than open market operations or monetary policy adjustments to have undesirable and distortionary effects.  Hence, they should be, and are, used only after a finding that broader instruments, like open market operations, are unlikely to prevent significant economic disruption.  And in my view, when relatively targeted policy interventions are employed, their use should be designed to minimize moral hazard.  For example, if the central bank concludes that it must lend to individual depository institutions, any such loans should, in most situations, be on terms sufficiently onerous to encourage a quick return to market funding.\n\nThe central bank will and must be involved in the management and resolution of financial crises.  Indeed, a major reason for the founding of the Federal Reserve in 1913 was the need to address periodic banking crises and financial panics, which had plagued the U.S. economy during the nineteenth and early twentieth centuries.  And the need remains today for Federal Reserve involvement in crisis management and resolution.  The Federal Reserve’s ability to conduct open market operations, make discount window loans, and provide funds intraday to key payments system participants are unique tools necessary to ensure that the financial system stays liquid.  Our monetary policy experience and responsibilities afford us valuable insights into how financial disruptions may be affecting the real economy.  Our dual role as both a payments system participant and a payments system supervisor helps us manage problems that arise in this key segment of the financial system.  Finally, for a variety of reasons, the Federal Reserve has developed extensive relationships with foreign central banks and bank supervisors.  These relationships have proved useful in past crises and will likely be even more valuable in an increasingly global financial and economic system.\n\nWhen managing a crisis, prudent decisionmaking depends on the best possible information acquired in the shortest possible period of time.  By information I mean more than just facts; I mean the informed analyses of the facts that help us understand the true financial condition of the distressed firms and markets and the potential for broader effects.  For example, if a major financial institution is facing serious problems, we need to know its most important on- and off-balance-sheet activities, its key lines of business, its most important counterparties, its most important market activities, and its net worth as well as how close it is to failure.  In addition, we must get a fix on the primary causes of the institution’s problems and on how long the causal factors are likely to last.\n\nOnce we understand as best we can the situation we face, we need information to help us assess whether the financial disruption has the potential to significantly spill over to the real economy.  The extent of spillover depends upon complex patterns of interdependencies between the immediate source of the financial disruption and other parties, and the speed and cost with which affected parties could obtain substitute providers for the financial services that have been disrupted.  As you can imagine, getting the needed information is likewise a very complex and uncertain task, especially when timeliness is of the essence.  Once such questions are answered, we would have to judge the possible effects of the financial shock on credit flows, payments and settlement systems, and asset prices and, more broadly, on uncertainty and confidence in the financial sector.  We would then go on to consider how these effects might influence consumption, investment, and employment.\n\nAlthough information when financial stability may be threatened is crucial, the regular and periodic collection of information in more normal times has limitations and potential costs, especially for sectors that do not have access to the public safety net.  For example, if it leads market participants to believe that the government will in some circumstances protect them from loss, the costs of the resulting increase in moral hazard and reduction in market discipline could exceed the benefits the information would provide to policymakers.  And such information may be of little value to policymakers in a crisis if private risk managers can change their positions so rapidly that any periodic information collection by supervisors about risk positions would be out-of-date.\n\nThe Federal Reserve’s activities as a bank supervisor provide us with important and sometimes critical information, expertise, credibility, and powers to both deter and manage financial crises.  Thus, I want to take this opportunity to emphasize and reinforce the case for central bank involvement in bank supervision made by Chairman Bernanke in a speech last month.  The deterrence and management of financial crises are of vital practical concern.  The uncertainties surrounding these tasks are greater than anyone would want, and the costs of failure are much greater than anyone could desire.  In my experience, the information, expertise, credibility, and powers that the Federal Reserve derives from its supervisory activities are extraordinarily helpful in our efforts to maintain economic and financial stability.  Perhaps we could be successful, as central banks in a number of countries are trying to be, without supervisory authority.  But I, for one, would not want to take that risk.\n\nConclusion\nFinancial stability was the first and perhaps the most important responsibility of the Federal Reserve.  To meet that responsibility, we must cooperate closely with colleagues here and abroad to adapt our techniques for preventing and managing situations that could undermine financial and economic stability.  As I have tried to emphasize today, that process of adaptation must also recognize that it is ultimately the decisions of private participants, not governments, on which we rely for financial stability.  As we interact with the private sector, we must preserve the incentives for innovation, the rewards for success, and penalties for failure that have made our financial markets the engines of rising living standards in a dynamic market economy.\n\n\n\n\n\nFootnotes\n\n1. Scott Alvarez and Myron Kwast, of the Board's staff, contributed to these remarks, which represent my own views and not necessarily those of other members of the Board or its staff. Return to text\n\n2. Federal Deposit Insurance Corporation (2006), \"FDIC Solicits Comments on Improvements to Determining Insured Deposits at Large Banks,\" press release 111-2006, December 5, http://www.fdic.gov/news/news/press/2006/index.html. Return to text",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20070221a.htm",
        "title": "Financial Stability: Preventing and Managing Crises",
        "date": "2/21/2007"
    },
    {
        "content": "February 06, 2007\n\nChairman Ben S. Bernanke\n\nBefore the Greater Omaha Chamber of Commerce, Omaha, Nebraska\n\nA bedrock American principle is the idea that all individuals should have the opportunity to succeed on the basis of their own effort, skill, and ingenuity. Equality of economic opportunity appeals to our sense of fairness, certainly, but it also strengthens our economy. If each person is free to develop and apply his or her talents to the greatest extent possible, then both the individual and the economy benefit.\n\nAlthough we Americans strive to provide equality of economic opportunity, we do not guarantee equality of economic outcomes, nor should we. Indeed, without the possibility of unequal outcomes tied to differences in effort and skill, the economic incentive for productive behavior would be eliminated, and our market-based economy--which encourages productive activity primarily through the promise of financial reward--would function far less effectively.\n\nThat said, we also believe that no one should be allowed to slip too far down the economic ladder, especially for reasons beyond his or her control. Like equality of opportunity, this general principle is grounded in economic practicality as well as our sense of fairness. To a significant extent, American economic success has resulted from the flexibility and adaptability of our dynamic market economy. Indeed, the ability of our labor and capital markets to accommodate and adapt to economic change has helped make possible the strong productivity performance of the U.S. economy over the post-World War II era, including the past decade. But this very dynamism sometimes creates painful dislocations, as when a shift in consumer demand, the advent of new technology, or new competition leads to the closing of a factory or causes a worker's skills to become obsolete. If we did not place some limits on the downside risks to individuals affected by economic change, the public at large might become less willing to accept the dynamism that is so essential to economic progress.\n\nThus, these three principles seem to be broadly accepted in our society: that economic opportunity should be as widely distributed and as equal as possible; that economic outcomes need not be equal but should be linked to the contributions each person makes to the economy; and that people should receive some insurance against the most adverse economic outcomes, especially those arising from events largely outside the person's control. Even when we accept these principles, however, important questions remain. For example, what is meant in practice by equality of economic opportunity? Some might limit the concept to the absence of overt discrimination against particular individuals or groups, while others might extend the term to encompass universal access to adequate housing, education, and health care. Another difficult question is how to balance the need for maintaining strong market-based incentives, which support economic growth and efficiency but may be associated with greater inequality of results, against the goal of insuring individuals against the most adverse outcomes, which may reduce inequality but also tends to diminish the strength of incentives. No objective means of answering these questions exists. One can only try to understand the various issues and tradeoffs involved and then come to a normative judgment based on that understanding.\n\nI raise these questions of ethics and values because they are inextricably linked with the topic of my talk today, which is the level and distribution of economic well-being in the United States. As I will discuss, the average standard of living in this country has improved considerably over time. However, by many measures, inequality in economic outcomes has increased over time as well, albeit at varying rates. In the remainder of my remarks I will review these trends. I will discuss what economic research has to say about the sources of rising inequality and briefly consider some implications for economic policy. I will not draw any firm conclusions about the extent to which policy should attempt to offset inequality in economic outcomes; that determination inherently depends on values and social tradeoffs and is thus properly left to the political process.\n\nTrends in the Level and Distribution of Economic Well-Being\nOn average, and by almost any measure, Americans have gained ground economically over time. For example, since 1947, the real (that is, inflation adjusted) hourly compensation of workers in the U.S. nonfarm business sector (a measure that includes both earnings and benefits) has increased more than 200 percent. In other words, the real reward for an hour of work has more than tripled over the past sixty years.1 Over the same period, real disposable income per capita has increased almost 270 percent, real consumption per capita has increased almost 280 percent, and real wealth per capita has risen 310 percent. We have also seen significant gains in other indicators of living standards, such as health and educational attainment. Thus, in absolute terms, the well-being of most Americans compares quite favorably with that of earlier generations and, indeed, with the well-being of most people in the world today.\n\nAlthough average economic well-being has increased considerably over time, the degree of inequality in economic outcomes has increased as well. Importantly, rising inequality is not a recent development but has been evident for at least three decades, if not longer.2 The data on the real weekly earnings of full-time wage and salary workers illustrate this pattern. In real terms, the earnings at the 50th percentile of the distribution (which I will refer to as the median wage) rose about 11-1/2 percent between 1979 and 2006. Over the same period, the wage at the 10th percentile, near the bottom of the wage distribution, rose just 4 percent, while the wage at the 90th percentile, close to the top of the distribution, rose 34 percent.3 In 1979, a full-time worker at the 90th percentile of the wage distribution earned about 3.7 times as much as a full-time worker at the 10th percentile. Reflecting the relatively faster growth of wages of higher-paid workers, that ratio is 4.7 today. The gap between the 90th and 10th percentiles of the wage distribution rose particularly rapidly through most of the 1980s; since then, it has continued to trend up, albeit at a slower pace and with occasional reversals.\n\nThe long-term trend toward greater inequality seen in real wages is also evident in broader measures of financial well-being, such as real household income.4 For example, the share of income received by households in the top fifth of the income distribution, after taxes have been paid and government transfers have been received, rose from 42 percent in 1979 to 50 percent in 2004, while the share of income received by those in the bottom fifth of the distribution declined from 7 percent to 5 percent. The share of after-tax income garnered by the households in the top 1 percent of the income distribution increased from 8 percent in 1979 to 14 percent in 2004 (Congressional Budget Office, 2006).5 Even within the top 1 percent, the distribution of income has widened during recent decades.6\n\nThe measures of inequality I have cited reflect \"snapshots\" of a single time period, usually a year. Consequently, they may not tell a complete story about the extent of inequality or its trend. For example, the fact that an older, more-experienced worker earns more than a newly hired employee will appear as wage inequality when measured at a given time; but as long as the new employee can expect to gain experience and someday earn a higher wage, inequality arising for this reason should not particularly concern us. Studies that track individuals' positions in the earnings distribution over time suggest that, in a given five-year period, almost half the population moves from one quintile of the distribution to another, and the percentage moving between quintiles increases over longer periods (McMurrer and Sawhill, 1996). However, economists disagree about whether income mobility has changed significantly over time (Lee and Solon, 2006). If it has not, then factors related to mobility cannot go far in helping to explain the upward trend in measures of short-term inequality.7\n\nThe Sources of Changes in the Level and Dispersion of Economic Well-Being\nWhat are the underlying sources of these long-term trends in wages, incomes, and other measures of economic well-being? Economists have established that, over longer periods, increases in average living standards are closely linked to the growth rate of productivity--the quantity of goods and services that can be produced per worker or per hour of work. Since 1947, hourly labor productivity in the U.S. nonfarm business sector has increased a robust 2-1/4 percent per year, and productivity growth has been close to or above that figure in most of the past ten years. This sustained productivity growth has resulted in large and broad-based improvements in the standard of living. When discussing inequality, we should not lose sight of the fact that the great majority of Americans today enjoy a level of material abundance--including the benefits of many technological advances, from air conditioning to computers to advanced medical treatments--that earlier generations would envy.\n\nThat being said, understanding the sources of the long-term tendency toward greater inequality remains a major challenge for economists and policymakers. A key observation is that, over the past few decades, the real wages of workers with more years of formal education have increased more quickly than those of workers with fewer years of formal education. For example, in 1979, median weekly earnings for workers with a bachelor's (or higher) degree were 38 percent more than those of high-school graduates with no college experience; last year, that differential was 75 percent. Similarly, over the same period, the gap in median earnings between those completing high school and those with less than a high-school education increased from 19 percent to 42 percent. To a significant extent, to explain increasing inequality we must explain why the economic return to education and to the development of skills more generally has continued to rise.\n\nEconomists have hypothesized that technological advances, such as improvements in information and communications technologies, have raised the productivity of high-skilled workers much more than that of low-skilled workers. High-skilled workers may have enjoyed this advantage because, for example, they may have been better able to make more effective use of computer applications, to operate sophisticated machinery, or to adapt to changes in workplace organization driven by new technologies. If new technologies tend to increase the productivity of highly skilled workers relatively more than that of less-skilled workers--a phenomenon that economists have dubbed \"skill-biased technical change\"--then market forces will tend to cause the real wages of skilled workers to increase relatively faster. Considerable evidence supports the view that worker skills and advanced technology are complementary. For example, economists have found that industries and firms that spend more on research and development or invest more in information technologies hire relatively more high-skilled workers and spend a relatively larger share of their payrolls on them (Autor, Katz, and Krueger, 1998; Bartel and Sicherman, 1999; Berndt and Morrison, 1995; Berman, Bound, and Griliches, 1994).\n\nAlthough skill-biased technical change appears to be an important cause of the rise in earnings inequality, it does not provide a complete explanation for that trend. The hypothesis cannot explain, for example, why the sharp rise in investment in information technology in the 1990s was not accompanied by a higher rate of increase in wage inequality. Nor can it explain why the wages of workers in the middle of the distribution have grown more slowly in recent years than those of workers at the lower end of the distribution, even though, of the two groups, workers in the middle of the distribution are typically the better educated (Autor, Katz, and Kearney, 2006; Autor, Levy, and Murnane, 2003).\n\nAnother challenge for the hypothesis of skill-biased technical change, at least in its basic formulation, is to explain the especially large wage gains seen at the top of the distribution. A possible link between technological change and the substantial increases in the wages of the best-paid workers is that some advances, such as those that have swept the communications industry, may have contributed to the rise of so-called \"superstars\"--a small number of the most-gifted individuals in each field who are now better able to apply their talents in what has increasingly become a global marketplace (Frank and Cook, 1995; Hausman and Leonard, 1997; Krueger, 2005; Manasse and Turrini, 2001; and Rosen, 1981). For example, two decades ago, the highest-paid player for the Boston Red Sox baseball team (and in the American League), Jim Rice, earned (in inflation-adjusted terms) just over $3 million. In 2004, the highest-paid player on the Red Sox (and in all of major-league baseball) was Manny Ramirez, who received $22.5 million for the season. The number of fans who can fit into Fenway Park has not increased much since Jim Rice's day. But presumably the Red Sox owners believed that Ramirez's higher salary was justified by the increases in broadcast and merchandising revenues he might generate as a result of the confluence of new distribution channels (such as Internet-based broadcasts of games) and a larger and wealthier potential global audience.8 The earnings potentials of superstar entertainers, investment bankers, lawyers, and various other professionals have likewise risen sharply as technological innovations and globalization have helped them leverage their talents over a wider sphere.\n\nThe compensation of chief executive officers of corporations is often singled out for particular scrutiny. Some economists have argued that the observed increases in CEO pay packages can largely be justified by economic factors, such as changes in the relationship between the CEO and the firm that have led to shorter and less-secure tenures for CEOs (Kaplan and Minton, 2006) and to a greater tendency to hire CEOs from outside the company (Murphy and Zabojnik, 2004). Others note that substantial increases in the size and scope of the largest corporations have raised the economic value of skilled corporate leadership (Gabaix and Landier, 2006). However, critics have responded that increases in CEO pay may have been amplified by poor corporate governance, including the substantial influence that some CEOs appear to have had over their own pay (Bebchuk and Fried, 2003). This debate will no doubt continue.\n\nBeyond the effects of technological change, the variety of economic forces grouped under the heading of \"globalization\" may also have been a factor in the rise in inequality, even as these forces have provided a major stimulus to economic growth and to living standards overall. Immigration--the flow of people across borders--is one aspect of the increased economic integration of the world economy. In recent decades, most immigrants to the United States have arrived with relatively low levels of skills. By itself, this pattern of immigration increases measured inequality because it leads to an increase in the relative size of the low-wage work force (Lerman, 1999). Standard economic reasoning also suggests that the immigration of such workers should reduce the relative wages of less-skilled domestic workers. Empirical analyses of individual cities or regions have found some evidence that corroborates this hypothesis, although in most cases the effect appears to have been small. A typical finding is that an increase of 10 percent in the share of immigrants in a city reduces the wages of lower-skilled natives 1 percent or less. This somewhat muted effect of low-skilled immigration on local markets may reflect the adaptability of U.S. labor and product markets, which has allowed native workers and firms to adjust with relatively little displacement (Card, 2005; Card and Lewis, 2005; and Lewis, 2004, 2005). However, studies that examine national data tend to find somewhat larger effects, with a 10 percent increase in the share of immigrants in the total population reducing the wages of low-skilled natives 3 percent to 5 percent (Borjas, 2006).\n\nInternational trade, another aspect of globalization, may also have differential effects on the economic well-being of U.S. workers even as it tends to raise real wages and incomes on average. For example, some empirical research suggests that, in the 1980s and 1990s, increased international trade reduced the profitability and hence the demand for labor in a number of industries that employed relatively more low-skilled workers (Borjas, Freeman, and Katz, 1997; Sachs and Shatz, 1994). Of course, trade has increased the potential markets for other domestic industries, leading to higher demand and thus higher real wages for workers in those industries. A related development has been the outsourcing abroad of some types of services and production activities. Because labor markets are adaptable, outsourcing abroad does not ultimately affect aggregate employment, but it may affect the distribution of wages, depending on the skill content of the outsourced work. At least until recently, most such activity appears to have involved goods and services that use relatively more low-skilled labor, which (all else being equal) would tend through the workings of supply and demand to slow the growth of wages of domestic low-skilled workers relative to those with greater skills (Feenstra and Hanson, 1996).9\n\nUnfortunately, much of the available empirical research on the influence of trade on earnings inequality dates from the 1980s and 1990s and thus does not address later developments. Whether studies of the more-recent period will reveal effects of trade on the distribution of earnings that differ from those observed earlier is to some degree an open question. Overall, I read the available evidence as favoring the view that the influence of globalization on inequality has been moderate and almost surely less important than the effects of skill-biased technological change.\n\nFinally, changes in the institutions that have shaped the labor market over the past few decades may also have been associated with some increase in wage inequality. For example, unions tend to compress the dispersion of pay for jobs in the middle of the skill distribution. Thus, the decline in private-sector union membership over the post-World War II period--particularly the sharp drop in the 1980s--has been associated with an increased dispersion of pay among workers with intermediate levels of skill (Freeman, 2005). The sources of the decline in union membership are much debated, and certainly long-run structural changes in the economy, such as the decline in manufacturing employment, have played a role. Whatever the precise mechanism through which lower rates of unionization affected the wage structure, the available research suggests that it can explain between 10 percent and 20 percent of the rise in wage inequality among men during the 1970s and 1980s (Card, 2001; DiNardo, Fortin, and Lemieux, 1996; Freeman, 1993).\n\nDeclines in the real value of the minimum wage, brought about by the combination of inflation and the fact that minimum wages are usually set in dollar terms, also affect the labor market. Some research suggests that this factor contributed to the relative decline in the wages of the least-skilled workers during the 1980s. Economists have also pointed out that, although higher minimum wages increase the wages of those who remain employed, they may also lead to reduced employment of low-skilled workers. Thus, the net influence of the minimum wage on earnings and income inequality, as opposed to the inequality of observed hourly wages, is ambiguous (Neumark, Schweitzer, and Wascher, 2005). In any case, the real value of the minimum wage, adjusted to include state minimum wages that are above the federal level, has been fairly flat in recent years, and so has the proportion of the labor force that is unionized. This suggests that these institutional factors have been less important sources of increasing wage inequality recently than they were in the 1970s and 1980s.\n\nSome Policy Implications\nWhat, if anything, should policymakers do about the trend of increasing economic inequality? As I noted at the beginning of my remarks, answering this question inevitably involves some difficult value judgments that are beyond the realm of objective economic analysis--judgments, for example, about the right tradeoff between allowing strong market-based incentives and providing social insurance against economic risks. Such tradeoffs are, of course, at the heart of decisions about tax and transfer policies that affect the distribution of income as well as countless other policy debates.\n\nPolicy approaches that would not be helpful, in my view, are those that would inhibit the dynamism and flexibility of our labor and capital markets or erect barriers to international trade and investment. To be sure, the advent of new technologies and increased international trade can lead to painful dislocations as some workers lose their jobs or see the demand for their particular skills decline.10 But hindering the adoption of new technologies or inhibiting trade flows would do far more harm than good, as technology and trade are critical sources of overall economic growth and of increases in the standard of living.\n\nA better approach for policy is to allow growth-enhancing forces to work but to try to cushion the effects of any resulting dislocations. For example, policies to facilitate retraining and job search by displaced workers, if well designed, could assist the adjustment process. Policies that reduce the costs to workers of changing jobs--for example, by improving the portability of health and pension benefits between employers--would also help to maintain economic flexibility and reduce the costs that individuals and families bear as a result of economic change. Of course, devising policies that accomplish these goals in the most effective way is not straightforward, nor can such policies deal with all of the negative effects of trade and technology on affected individuals. Displaced older workers present a particularly difficult problem, as these workers have greater difficulty than others in finding new jobs and experience a greater decline in earnings than other workers if they are re-employed (Munnell and others, 2006). Considerable debate and analysis of policy alternatives lie ahead, but these discussions will be well worth the effort.\n\nAs the larger return to education and skill is likely the single greatest source of the long-term increase in inequality, policies that boost our national investment in education and training can help reduce inequality while expanding economic opportunity. A substantial body of research demonstrates that investments in education and training pay high rates of return both to individuals and to the society at large (Acemoglu and Angrist, 2001; Becker, 1964; Card, 1999; Topel, 2004). That research also suggests that workers with more education are better positioned to adapt to changing demands in the workplace.\n\nIn assessing the potential of education and training to moderate inequality, one should keep in mind that the economically relevant concept of education is much broader than the traditional course of schooling from kindergarten through high school and into college. Indeed, substantial economic benefits may result from any form of training that helps individuals acquire economically and socially useful skills, including not only K-12 education, college, and graduate work but also on-the-job training, coursework at community colleges and vocational schools, extension courses, online education, and training in financial literacy. The market incentives for individuals to invest in their own skills are strong, and the expanding array of educational offerings available today allows such investment to be as occupationally focused as desired and to take place at any point in an individual's life.\n\nAlthough education and the acquisition of skills is a lifelong process, starting early in life is crucial. Recent research--some sponsored by the Federal Reserve Bank of Minneapolis in collaboration with the University of Minnesota--has documented the high returns that early childhood programs can pay in terms of subsequent educational attainment and in lower rates of social problems, such as teenage pregnancy and welfare dependency.11 The most successful early childhood programs appear to be those that cultivate both cognitive and noncognitive skills and that engage families in stimulating learning at home (Heckman, Stixrud, and Urzua, 2006).\n\nTo return to the themes I raised at the beginning, the challenge for policy is not to eliminate inequality per se but rather to spread economic opportunity as widely as possible. Policies that focus on education, job training, and skills and that facilitate job search and job mobility seem to me to be a promising means for moving toward that goal. By increasing opportunity and capability, we help individuals and families while strengthening the nation's economy as well.\n\nReferences\n\nAcemoglu, Daron, and Joshua Angrist (2001). \"How Large Are Human Capital Externalities? Evidence from Compulsory Schooling Laws,\" in Ben S. Bernanke and Kenneth Rogoff, eds., NBER Macroeconomics Annual. Cambridge, Mass.: MIT Press, pp. 9–59.\n\n\n\nAutor, David H., Frank Levy, and Richard J. Murnane (2003). \"The Skill Content of Recent Technological Change: An Empirical Exploration,\" Quarterly Journal of Economics, vol. 118 (November), pp. 1279–333.\n\nAutor, David H., Lawrence F. Katz, and Melissa S. Kearney (2006). \"The Polarization of the Labor Market,\" American Economic Review, vol. 96 (May), pp. 189–94.\n\nAutor, David H., Lawrence F. Katz, and Alan B. Krueger (1998). \"Computing Inequality: Have Computers Changed the Labor Market?\" Quarterly Journal of Economics, vol. 113 (November), pp. 1169–213.\n\nBartel, Ann P., and Nachum Sicherman (1999). \"Technological Change and Wages: An Interindustry Analysis,\" Journal of Political Economy, vol. 107 (April), pp. 285–325.\n\nBebchuk, Lucien A., and Jesse M. Fried (2003). \"Executive Compensation as an Agency Problem,\" Journal of Economic Perspectives, vol. 17 (Summer), pp. 71–92.\n\nBecker, Gary S. (1964). Human Capital: A Theoretical and Empirical Analysis with Special Reference to Education. New York: National Bureau of Economic Research.\n\nBerman, Eli, John Bound, and Zvi Griliches (1994). \"Changes in the Demand for Skilled Labor within U.S. Manufacturing: Evidence from the Annual Survey of Manufacturers,\" Quarterly Journal of Economics, vol. 109 (May), pp. 367–97.\n\nBerndt, Ernst R., and Catherine J. Morrison (1995). \"High-Tech Capital Formation and Economic Performance in U.S. Manufacturing Industries: An Exploratory Analysis,\" Journal of Econometrics, vol. 65 (January), pp. 9–43.\n\nBorjas, George J. (2006). \"Native Internal Migration and the Labor Market Impact of Immigration,\" Journal of Human Resources, vol. 41 (Spring), pp. 221–58.\n\nBorjas, George J., Richard B. Freeman, and Lawrence F. Katz (1997). \"How Much Do Immigrants and Trade Affect Labor Market Outcomes?\" Brookings Papers on Economic Activity, 1:1997, pp. 1–67.\n\nCard, David (1999). \"The Causal Effect of Education on Earnings\" in Orley Ashenfelter and David Card, eds., Handbook of Labor Economics, vol. 3A. New York: Elsevier, pp. 1801-63.\n\n___________ (2001). \"The Effect of Unions on Wage Inequality in the U.S. Labor Market,\" Industrial and Labor Relations Review, vol. 54 (January), pp. 296–315.\n\n___________ (2005). \"Is the New Immigration Really So Bad?\" Economic Journal, vol. 115 (November), pp. F300–23.\n\nCard, David, and Ethan G. Lewis (2005). \"The Diffusion of Mexican Immigrants during the 1990s: Explanations and Impacts,\" NBER Working Paper Series 11552. Cambridge, Mass.: National Bureau of Economic Research, August.\n\nCongressional Budget Office (2006). \"Historical Effective Federal Tax Rates: 1979-2004,\" www.cbo.gov/ftpdoc.cfm?index=7718&type=1.\n\nDiNardo, John, Nicole M. Fortin, and Thomas Lemieux (1996). \"Labor Market Institutions and the Distribution of Wages, 1973–1992: A Semiparametric Approach,\" Econometrica, vol. 64 (September), pp. 1001–44.\n\nFarber, Henry S. (2005). \"What Do We Know About Job Loss in the United States? Evidence from the Displaced Workers Survey, 1984-2004,\" Federal Reserve Bank of Chicago Economic Perspectives, vol. 29(Q2), pp. 13–28.\n\nFeenstra, Robert C., and Gordon H. Hanson (1996). \"Foreign Investment, Outsourcing, and Relative Wages,\" in Robert C. Feenstra, Gene M. Grossman, and Douglas A. Irwin, eds., The Political Economy of Trade Policy: Papers in Honor of Jagdish Bhagwati. Cambridge, Mass.: MIT Press, pp. 89–127.\n\nFrank, Robert H., and Philip J. Cook (1995). The Winner-Take-All Society: How More and More Americans Compete for Ever Fewer and Bigger Prizes, Encouraging Economic Waste, Income Inequality, and an Impoverished Cultural Life. New York: Penguin Books.\n\nFreeman, Richard B. (1993). \"How Much Has De-unionization Contributed to the Rise in Male Earnings Inequality?\" in Sheldon Danziger and Peter Gottschalk, eds., Uneven Tides: Rising Inequality in America. New York: Russell Sage Foundation, pp. 133–63.\n\n_________________(2005). \"What Do Unions Do? The 2004 M-Brane Stringtwister Edition,\" NBER Working Paper Series 11410. Cambridge, Mass.: National Bureau of Economic Research, June.\n\nGabaix, Xavier, and Augustin Landier (2006). \"Why Has CEO Pay Increased So Much?\" NBER Working Paper Series 12365. Cambridge, Mass.: National Bureau of Economic Research, July.\n\nGoldin, Claudia, and Robert A. Margo (1992). \"The Great Compression: The U.S. Wage Structure at Mid-Century,\" Quarterly Journal of Economics, vol. 107 (February), pp. 1-34.\n\nGrossman, Gene M., and Esteban Rossi-Hansberg (2006). \"The Rise of Offshoring: It's Not Wine for Cloth Anymore,\" in The New Economic Geography: Effects and Policy Implications, paper prepared for a symposium in Jackson Hole, Wyoming, August 24–26. Kansas City: Federal Reserve Bank of Kansas City, http://www.kansascityfed.org/publications/research/escp/archive.cfm.\n\nHausman, Jerry A., and Gregory K. Leonard (1997). \"Superstars in the National Basketball Association: Economic Value and Policy,\" Journal of Labor Economics, vol. 15 (October), pp. 586–624.\n\nHeckman, James J., Jora Stixrud, and Sergio Urzua (2006). \"The Effects of Cognitive and Noncognitive Abilities on Labor Market Outcomes and Social Behavior,\" Journal of Labor Economics, vol. 24 (July), pp. 411–82.\n\nKaplan, Steven N., and Bernadette Minton (2006). \"How Has CEO Turnover Changed? Increasingly Performance Sensitive Boards and Increasingly Uneasy CEOs,\" NBER Working Paper Series 12465. Cambridge, Mass: National Bureau of Economic Research, August.\n\nKennickell, Arthur B. (2006). \"Currents and Undercurrents: Changes in the Distribution of Wealth, 1989–2004,\" Finance and Economics Discussion Series 2006-13. Washington: Board of Governors of the Federal Reserve System, August.\n\nKrueger, Alan B. (2005). \"The Economics of Real Superstars: The Market for Rock Concerts in the Material World,\" Journal of Labor Economics, vol. 23 (January), pp. 1–30.\n\nKrueger, Dirk, and Fabrizio Perri (2006). \"Does Income Inequality Lead to Consumption Inequality? Evidence and Theory,\" Review of Economic Studies, vol. 73 (January), pp. 163–193.\n\nLee, Chul-In, and Gary Solon (2006). \"Trends in Intergenerational Income Mobility,\" NBER Working Paper Series 12007. Cambridge, Mass: National Bureau of Economic Research, January.\n\nLerman, Robert I. (1999). \"U.S. Wage-Inequality Trends and Recent Immigration,\" American Economic Review, vol. 89 (May, Papers and Proceedings), pp. 23–28.\n\nLewis, Ethan (2004). \"How Did the Miami Labor Market Absorb the Mariel Immigrants?\" Working Paper Series 04-3. Philadelphia: Federal Reserve Bank of Philadelphia, January.\n\n____________(2005). \"Immigration, Skill Mix, and the Choice of Technique,\" Working Paper Series 05-8. Philadelphia: Federal Reserve Bank of Philadelphia, May.\n\nManasse, Paolo, and Allessandro Turrini (2001). \"Trade, Wages, and ‘Superstars,'\" Journal of International Economics, vol. 54 (June), pp. 97–117.\n\nMcMurrer, Daniel P., and Isabel Sawhill (1996). \"Economic Mobility in the United States,\" Urban Institute, www.urban.org/publications/406722.html (accessed January 16, 2007).\n\nMoffitt, Robert A., and Peter Gottschalk (2002). \"Trends in the Transitory Variance of Earnings in the United States,\" Economic Journal, vol. 112 (March), pp. C68–73.\n\nMunnell, Alicia H., Steven A. Sass, Mauricio Soto, and Natalia A. Zhivan (2006). \"Has the Displacement of Older Workers Increased?\" paper prepared for the Eighth Annual Joint Conference of the Retirement Research Consortium, Washington, D.C., August 10–11.\n\nMurphy, Kevin J., and Jan Zabojnik (2004). \"CEO Pay and Appointments: A Market-Based Explanation for Recent Trends,\" American Economic Review, vol. 94 (May, Papers and Proceedings), pp. 192–96.\n\nNeumark, David, ed. (2000). On the Job: Is Long-Term Employment a Thing of the Past? New York: Russell Sage Foundation.\n\nNeumark, David, Mark Schweitzer, and William Wascher (2005). \"The Effects of Minimum Wages on the Distribution of Family Incomes: A Nonparametric Analysis,\" Journal of Human Resources, vol. 40 (Fall), pp. 867–94.\n\nPiketty, Thomas, and Emmanuel Saez (2003). \"Income Inequality in the United States, 1913-1998,\" Quarterly Journal of Economics, vol. 118 (February), pp. 1–39L.\n\nReed, Keith (2006). \"Japanese May Follow Matsuzaka to Boston,\" Boston Globe, December 15.\n\nRosen, Sherwin (1981). \"The Economics of Superstars,\" American Economic Review, vol. 71 (December), pp. 845–58.\n\nSachs, Jeffrey D., and Howard J. Shatz (1994). \"Trade and Jobs in U.S. Manufacturing,\" Brookings Papers on Economic Activity, 1:1994, pp. 1–69.\n\nTopel, Robert (2004). \"The Private and Social Values of Education,\" in Education and Economic Development, proceedings of a conference held at the Federal Reserve Bank of Cleveland, November 18-19, 2004, pp. 47–57, www.clevelandfed.org/research/conferences/2004/November/cbook.pdf.\n\nWeinberg, Daniel H. (1996). \"A Brief Look at Postwar U.S. Income Inequality,\" Current Population Reports, P60–191 (June). Washington: Census Bureau, www.census.gov/hhes/www/img/p60-191.pdf.\n\nFootnotes\n\n1. This result is calculated using the data on compensation per hour in the nonfarm business sector from the Bureau of Labor Statistics, deflated by the price index for personal consumption expenditures from the national income and product accounts. Return to text\n\n2. Goldin and Margo (1992) find that wage inequality has been increasing at varying rates since 1950. Piketty and Saez (2003) obtain a similar result for wage income since the mid-1950s; using a measure of total income excluding capital gains, they find that inequality was more or less constant from the early 1950s until the early 1970s and rose thereafter. By contrast, Weinberg (1996) found that family income inequality, as measured by the Gini index, had diminished somewhat between 1947 and 1968, but had increased between 1968 and the early 1990s, when his sample period ended. Return to text\n\n3. The data are weekly earnings of full-time wage and salary workers aged twenty-five and older and are derived from the Current Population Survey, published by the Bureau of Labor Statistics. The data are deflated by the price index for personal consumption expenditures. Return to text\n\n4. Other important measures of economic well-being include consumption and wealth. Some economists view consumption as a better measure of economic well-being than labor compensation or income because consumption is a more direct indicator of the standard of living and because, on the assumption that households consume according to the income they expect to receive over the longer term, it may also be a better measure of a household's long-term economic prospects. As with other measures of economic well-being, consumption has become increasingly unequal over time, although at any point in time it is less unequally distributed than wages or income (Krueger and Perri, 2006). Wealth--the difference between a household's assets (both physical and financial) and its liabilities--shows a slightly different pattern. As we know from the Federal Reserve's triennial Survey of Consumer Finances, wealth is distributed far more unequally than income, with about 33 percent of aggregate household net worth being held by the top 1 percent of families and with just 2-1/4 percent of wealth being held by the bottom half (Kennickell, 2006). However, the relative shares of wealth held by the richest and poorest have changed very little over the past decade or so, in contrast to the further widening in the distributions of wages, incomes, and consumption. Return to text\n\n5. In general, measures of economic well-being (such as disposable income) that take account of taxes paid and transfer payments received from the government show lower levels of inequality at any point in time than measures that exclude such payments; the difference reflects the progressive nature of our system of taxes and transfers. However, analysis of these broader measures indicates that, over time, tax and transfer policies have not materially altered the general tendency toward greater inequality. Return to text\n\n6. Data on the distribution of income in the top 1 percent comes from an update of Piketty and Saez (2003), available at http://elsa.berkeley.edu/~saez/TabFig2004prel.xls. Return to text\n\n7. Another respect in which snapshots of inequality may be misleading is that they are influenced by fluctuations in income that are transitory and that hence have a smaller effect on long-term economic well-being. One study suggests that, for men, about one-third of the rise in point-in-time earnings inequality during the 1970s and 1980s resulted from an increase in the transitory variation in wages (Moffitt and Gottschalk, 2002). The proper interpretation of these short-lived changes in earnings is not clear. To the degree that they are associated with factors such as job changes, moves, or entry into or exit from schooling, temporary variations in individual incomes could signify a healthy labor market. On the other hand, such variations would be of greater concern if they reflected a higher incidence of factors such as short periods of unemployment or loss of income due to health problems. Return to text\n\n8. Recently, the Red Sox paid $51 million simply for the right to negotiate with the Japanese pitcher Daisuke Matsuzaka; they later signed Matsuzaka for an additional $52 million over six years. Illustrating some of the effects of globalization on baseball economics, the city of Boston anticipates that increased interest by Japanese tourists will bring in some $75 million, aside from what the Red Sox will earn from signing Matsuzaka (Reed, 2006). Return to text\n\n9. For an opposing view, see Grossman and Rossi-Hansberg (2006). Return to text\n\n10. Surveys of workers displaced from jobs because their plant closed or moved show that rates of such job loss fluctuated only a little, on balance, from 1983 to 2003 despite the decline in unemployment over that period (Farber, 2005). A range of studies have yielded conflicting answers to the question of whether job stability and job insecurity have changed over time, although the editor of a set of papers presented at a conference at the Federal Reserve Bank of New York concluded that the relationship of employees and firms weakened some in the 1990s (Neumark, 2000). Return to text\n\n11. More information on the Early Childhood Research Collaborative and copies of its research papers can be obtained from the website of the Federal Reserve Bank of Minneapolis, http://www.earlychildhoodrc.org/. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070206a.htm",
        "title": "The Level and Distribution of Economic Well-Being",
        "date": "2/6/2007"
    },
    {
        "content": "January 18, 2007\n\nGovernor Susan Schmidt Bies\n\nAt the Eller College of Management Distinguished Speaker Series, Tucson, Arizona\n\nTo begin, I would like to thank Dean Portney and everyone at the Eller College of Management here at The University of Arizona for the invitation to speak this evening. I know that this audience includes both experienced business leaders as well as students preparing to apply the knowledge they have learned in the classroom to the real world. Accordingly, my remarks will first touch on the economic outlook. Then, I will offer some thoughts on the challenges facing consumers as they navigate today’s changing mortgage loan market.\n\nEconomic Outlook\nEconomic activity slowed in the middle part of last year. After rising at an annual rate of 3-1/4 percent over the previous two years, real gross domestic product (GDP) increased at an annual rate of 2.6 percent in the second quarter of last year. Real GDP growth in the second half of last year slowed after accounting for some measurement difficulties in the automotive sector. Despite the recent slowing in output, however, resource utilization remains relatively high by historical standards and thus continues to be a potential source of upward pressure on inflation.\n\nIn the aftermath of the 2001 recession, the Federal Open Market Committee (FOMC) eased monetary policy substantially. However, the degree of easing in place in 2003 and 2004 was clearly unsustainable. Since mid-2004, the FOMC has gradually moved monetary policy from an accommodative stance to a more neutral position. As a consequence, the elements now appear to be in place for some easing of resource utilization rates over the next year or so and a reduction in inflationary pressures. However, substantial uncertainty surrounds the near-term outlook. In determining the future path of interest rates, the FOMC will be guided by the incoming data on both output and prices, so let’s begin by reviewing recent developments.\n\nEconomic Activity\nThe slowdown in the growth of real GDP since last spring largely reflects a cooling of the housing market: Sales of both new and existing homes dropped sharply after their peak in the summer of 2005, the inventory of unsold homes has soared, and the number of single-family and multifamily housing starts has fallen nearly 30 percent since the beginning of last year. At the same time, homes are appreciating more slowly and in some markets prices are even declining.\n\nNonetheless, a variety of factors should help limit any remaining contraction in housing demand. For example, despite the 4-1/4 percentage point increase in short-term interest rates over the past three years, the interest rate on a thirty-year fixed-rate mortgage has increased only about 1/2 percentage point, and borrowing costs continue to be relatively low. The ongoing growth in employment and real incomes and the recent increase in the stock market wealth of households should also support the demand for housing. Indeed, the latest data on home sales and consumer homebuying attitudes suggest that the demand for housing may be stabilizing. While much of the downshift in the housing market appears to have occurred already, some further softening in housing starts may yet lie ahead as the inventory of unsold homes is reduced to appropriate levels.\n\nThat said, it is encouraging that the recent weakness in residential construction does not appear to have spilled over to other sectors. For instance, employment has been growing nicely in nonresidential construction, even as it has shrunk in the residential sector. In addition, consumer confidence currently stands a good bit above its long-run average and consumption is still being fueled by past house-price gains, which raised household wealth. This contrasts with previous slowdowns in the housing market, which have typically coincided with widespread economic weakness.\n\nAlthough the slowdown in the housing market has so far done little to reduce consumer outlays, other factors do appear to have had a damping effect. In particular, consumption likely was restrained earlier this year by the rise in energy prices, which took a large bite out of household budgets. The rise in energy prices over the past few years has also affected the auto sector, most notably by reducing the demand for gas-guzzling automobiles, such as sport-utility vehicles and trucks. As a result of this decreased demand and other factors, inventories of these types of vehicles rose in the second half of last year. In response, domestic automakers cut production. However, the latest data on auto sales and scheduled new-vehicle production suggest that the drag from the inventory correction in this sector may be easing.\n\nOffsetting influences on consumption should support continued growth of consumer spending. The slowing in the appreciation of house prices will likely reduce the impetus to consumer spending from this source, while recent increases in stock market values, declines in energy prices and strong employment and wage growth should spur consumption.\n\nIn the business sector, spending on nonresidential construction has been particularly robust. Expenditures on drilling and mining structures have increased rapidly in response to high prices for natural gas and crude oil. Investment in other types of structures, such as offices and commercial buildings, has also been strong. Data for October and November indicate that although the growth rate of nonresidential investment moderated substantially in the fourth quarter of last year, it still remained healthy by historical standards.\n\nSpending on equipment and software grew quite rapidly from mid-2004 to mid-2006, but appears to have advanced at a substantially slower pace in the second half of last year. The recent stepdown in the growth of business sales likely had a damping influence on capital spending, and, in fact, business sentiment has moved down in the past year. However, some backlog of unfilled orders for capital goods such as industrial machinery and other types of heavy equipment remains, which should help support spending in the near term. Moreover, the demand for information technology equipment is likely to be well maintained, in part because of the recent introduction of a new generation of microprocessing chips and more-efficient large servers.\n\nCurrent financial conditions are also supportive of business spending. Corporate balance sheets are strong and flush with cash, profits are strong, and broad stock price indexes are up appreciably since the beginning of last year. At the same time, yield spreads on corporate bonds across the ratings spectrum have been low, supported by the strong balance sheets and robust profits.\n\nInflation\nWhat are the implications of current economic activity on inflation prospects? Consumer prices excluding food and energy accelerated through last summer--the core inflation rate was 2.4 percent over the four quarters ending in the third quarter of last year, up from 2.0 percent for the same period a year before.1 The more recent monthly readings have been more favorable--core prices rose 2.2 percent over the twelve months through last November, only a touch higher than the 2.1 percent rise over the previous twelve months, and this morning’s latest figures on the Consumer Price Index (CPI) suggest that this pattern continued into December. Nevertheless, the risk to inflation continues to be on the upside until we see further confirmation in this trend toward moderation.\n\nIn thinking about the macroeconomic consequences of inflation, it makes sense to look at the core rate, which excludes the prices of energy and food, when the focus is on the short run. This is because temporary shocks to food and energy prices typically don’t translate into changes in inflationary pressure. However, if these shocks persist, they may have an effect on core inflation and, more generally, on the economic behavior of households and businesses. Core inflation can be affected when the price changes are propagated along the production chain--say from oil prices to the prices of chemicals and ultimately to the prices of goods made with those chemicals. In addition, shocks to food and energy prices may affect inflation expectations. Thus, we also pay attention to broader measures of inflation.\n\nAll things considered, the scene appears to be set for a deceleration in prices. One contributing factor is likely to be the slowing in economic activity I already discussed, which should ease the overall pressure on resources. Another important factor affecting the inflation outlook is household and business expectations for inflation. As best we can judge, inflation expectations appear to be well contained: Measures of longer-term inflation expectations, based on surveys and on a comparison of yields on nominal and inflation-indexed government debt, have remained within the ranges in which they have fluctuated in recent years. Finally, the recent decline in energy prices, if sustained, should reduce cost pressures along the production chain.\n\nBut a decline in the inflation rate is not assured. Outside of the housing and automotive sectors, growth remains sound and labor markets remain tight, especially for skilled workers. The unemployment rate began to decline in the second half of 2003 and by the fourth quarter of last year stood at a relatively low 4-1/2 percent. With labor markets comparatively tight by historical standards, unit labor costs have accelerated over the past year, and firms may pass on some of these higher costs to consumers. However, strong business profit margins, which are currently well above their historical averages, could act as a shock absorber if cost strains were to intensify. All told, in my judgment, inflation appears poised to decelerate in coming months as energy prices stabilize and resource pressures ease. But the risks to that outlook seem tilted toward the upside.\n\nThe Federal Reserve and Consumer Protection\nMy remarks on the economic outlook discussed developments in housing markets, but now I would like to delve into a specific area of mortgage lending: consumer awareness and understanding of changes in mortgage markets. Given its role as the central bank, a banking supervisor, and a consumer protection regulator for financial services, the Federal Reserve takes a multifaceted approach to understanding the consumer financial services market. Thus, we recognize the importance of homeownership in building household wealth and we monitor the safety and soundness of mortgage underwriting practices. We also write regulations that help ensure consumers understand the characteristics of financial services and products and that they receive the protections available for these services and products.\n\nSince the 1960s, Congress has enacted several laws to ensure that consumers receive comprehensive information and fair treatment in a broad range of financial transactions. These laws protect consumers in transactions involving credit and debit card accounts; automated teller machine transactions and other electronic fund transfers; deposit account activities; automobile leases; mortgages and home equity loans; and lines of credit and other unsecured credit. Congress has assigned the Federal Reserve a considerable role in ensuring that consumers are protected under these laws. For example, the Federal Reserve writes rules to implement the Truth in Lending Act, which Congress enacted to ensure that consumers receive clear information about credit terms and costs. The Federal Reserve is also responsible for rules that implement fair lending laws, such as the Equal Credit Opportunity Act, which prohibits discrimination in credit transactions. In addition, we work with other federal agencies to write joint rules, for example, to ensure that consumers’ financial privacy is protected.\n\nThe Federal Reserve also examines state member banks and foreign banking organizations and exercises its umbrella supervisory authority with respect to bank and financial holding companies to address compliance with consumer protection laws and regulations. The Federal Reserve is committed to vigorous enforcement of those laws.\n\nFurther, the Federal Reserve develops consumer education materials, recognizing that well-educated consumers are better able to protect themselves in financial transactions. Clearly, to choose wisely from the variety of products and providers available, consumers must have the financial knowledge to navigate today’s increasingly complex financial services marketplace.\n\nFor example, the Federal Reserve and the Office of Thrift Supervision just revised the Consumer Handbook on Adjustable Rate Mortgages (CHARM) to include additional information about nontraditional mortgage products. The CHARM booklet is an important means for delivering information about alternative adjustable rate mortgage products to consumers because creditors are required to provide a copy of the booklet to each consumer who receives an application for an ARM. Because the booklet is provided at this early stage in the process, it can be useful in encouraging consumers to ask brokers and lenders the right questions to decide if this type of loan is right for them. The booklet explains such features as payment shock and negative amortization. It also provides numerical examples that illustrate how consumers' payments can change and how their loan balances may increase over the term of the loan.\n\nThe Federal Reserve also published a consumer education brochure titled: Interest-Only Mortgage Payments and Option-Payment ARMs--Are They for You?. The brochure is designed to assist consumers who are shopping for a mortgage loan and is available in printed form and in electronic form on the Federal Reserve’s website, along with other consumer mortgage information\n\nFinally, the Federal Reserve engages in numerous outreach activities and conducts research to help us better understand the financial services market and consumer behavior and the best approaches for assisting consumers. We sponsor consumer and industry surveys, hold public hearings, discuss issues with our Consumer Advisory Council, and conduct consumer focus groups and other types of consumer testing, in addition to considering the public comments on proposed rules.\n\nRecently, the Federal Reserve has engaged in outreach to a broad group of interested parties regarding nontraditional mortgages. Our Consumer Advisory Council, which includes consumer advocates and industry representatives, provided us with valuable information about nontraditional mortgage products during development of the interagency guidance. In addition, the Federal Reserve held a series of four public hearings on home-equity lending during the summer of 2006. One of the principal purposes of the hearings was to gather information to inform the Federal Reserve’s review of its Truth in Lending rules. A significant portion of the hearings was devoted to discussing nontraditional mortgage products, and in particular, whether consumers receive adequate information about these products. The hearings explored research on consumer behavior in shopping for mortgage loans and included discussions about the challenges in designing disclosures to more effectively communicate loan terms and risks to consumers. We will consider this information in developing plans and recommendations for revising the Truth in Lending rules, which require all creditors to provide consumers with disclosures about loan terms.\n\nMany financial products are very sophisticated credit transactions, and the disclosures describing them are also complex and highly technical; which can be difficult for some consumers to understand. To address this issue, the Federal Reserve is focusing in a more rigorous way on determining the effectiveness of disclosures. We conduct extensive consumer testing to determine what information is most important to consumers, when it is most useful for consumers to receive it, what language and formats work best, and how disclosures can be simplified, and better organized, to reduce complexity and information overload. To that end, the Federal Reserve uses design consultants to assist in developing model disclosures that are most likely to be effective in communicating vital credit information to consumers. The Federal Reserve also uses consumer testing to assist in developing model disclosure forms, and we draw from research done by respected behavioral economists.\n\nFor example, when considering some regulatory changes related to the Electronic Fund Transfer Act, the Federal Reserve used consumer focus group testing to develop useful and timely disclosures for employees whose pay is received in the form of a payroll card (a card that works much like a traditional debit card). The Federal Reserve is also engaging in extensive consumer testing of credit card disclosures required by the Truth in Lending Act, and has found those interactions with consumers instructive in revising disclosures to be more meaningful to consumers in today’s complex and highly prolific credit card market.\n\nMortgage Lending\nI now want to discuss developments in mortgage loan products. Last September, the federal financial institution regulators issued supervisory guidance on nontraditional mortgages. These mortgages include interest-only loans that allow consumers to defer the payment of principal and make only interest payments for an initial period. They also include adjustable-rate mortgages (ARMs) with a payment option, allowing consumers to make “minimum payments” of less than the accrued interest. The result of such a loan structure is that the amount owed can increase even as the borrower makes payments, a circumstance known as negative amortization. Over time, the payments for these loans must be adjusted upward to catch up with fully indexed interest rates and/or paydowns (amortization) of the balance owed, often significantly, which sometimes results in what some call “payment shock.” Sometimes borrowers can refinance out of these loans, but that may cost them a serious prepayment penalty and reduce the amount of equity that they have built up in their home. The complexity of these products contrasts with traditional thirty-year fixed-rate loans, in which consumers have equal monthly payments that are sufficient to cover the accrued interest and pay down the principal, and with which borrowers are generally familiar.\n\nTo be clear, nontraditional mortgages certainly serve a useful purpose when used appropriately. These products have increased the range of financing options available to consumers and have grown in popularity over the past few years. Some consumers may benefit from these products’ more flexible payment options. For example, consumers with seasonal or irregular income or who expect their incomes to increase are more likely to be able to absorb the increase in payments in the near term of the loan.\n\nBut nontraditional mortgage loan products can be complex and may not be appropriate for everyone. Concerns have been raised that consumers are not getting clear and balanced information about the risks and features of interest-only and payment option ARMs. These products have been advertised and promoted based on their initially low monthly payments when compared with traditional mortgage products. By focusing on initial monthly payments without appropriate understanding of how the payments can vary over time or that minimum payments may actually increase the amount owed, many consumers have become financially overextended.\n\nMortgage lenders are no longer limited in the size of their mortgage lending activities by the amount of available funding on their balance sheets. Mortgages are now routinely sold to institutions who securitize mortgages by aggregating them with other loans into pools. These mortgage-backed securities are held by banks, pension funds, mutual funds, and other investors both in the United States and internationally. This diversity of funding sources makes it possible for lenders to structure products that meet the various needs of borrowers, and still be able to mitigate the risks they may take on if the loan were held on their balance sheets. As a result, the mortgage lending industry continually adapts to consumer requirements and investor preferences for loan securitization structures.\n\nA mortgage is generally the largest financial obligation a consumer enters into. The initial costs, the cost over the life of the mortgage, and the costs to prepay and refinance can be significant. Having a wide variety of mortgage types assists consumers in finding a way to finance the purchase of a home that best meets their own financial and lifestyle requirements. But the growing complexity of products makes it a bigger challenge for borrowers to understand the characteristics of competing products. This is where shopping among a couple of lenders and a couple of different products can help the consumer understand the features of different loan types. .\n\nWhen looking at different loan products, consumers should consider how long they plan to own their home, expectations of future income, their stage of life cycle, and broader financial obligations in choosing among mortgage alternatives. For example, if consumers want to pay off their mortgages before retirement, they must determine if the monthly payment will fully amortize the loan by the target date. If homeowners plan to move in three years, then they should compare prepayment penalties, if any, and interest rates among their mortgage alternatives.\n\nBut some consumers do not actively shop for a lender or a mortgage, and they do not compare loan terms in light of their personal circumstances. Instead they rely on one lender and accept the mortgage loan presented to them. Mortgages that are appropriate for one borrower, however, may create problems for another. For example, some mortgages have low initial interest rates and monthly payments. But increases come later as the “teaser period” ends and payments rise to begin pay down of principal. Borrowers whose budgets make the initial payment just manageable may have difficulty meeting these significant jumps in monthly payments and so are at risk of defaulting on their mortgages and losing their home.\n\nBy shopping among lenders and products, consumers are also better able to determine whether a given loan feature is really an advantage. For example, some lenders have recently been more willing to waive proof of income when making a mortgage. But, they may charge a higher interest rate to cover the risk that the borrower may not be able to afford the mortgage. The customer however may save interest cost by making the effort to provide copies of tax returns and pay stubs. Another example is that some lenders do not require their borrowers to pay their property tax and hazard insurance premiums into an escrow account. Consumers need to ask lenders if their monthly payments will include funds for such an escrow account. If not, borrowers need to separately set aside savings to meet these obligations.\n\nLenders, for their part, should make it possible for consumers to engage in shopping for a mortgage. As I noted, last September the federal financial institution regulators issued interagency guidance on nontraditional mortgage products, which included both a focus on safe and sound lending practices as well as on consumer awareness and consumer protection. The guidance focuses on the responsibility of lenders to provide consumers with clear and balanced information at crucial decision-making points about the benefits and risks of nontraditional mortgage products. Accordingly, the interagency guidance describes recommended practices for financial institutions in communicating with consumers while they are shopping, not just upon submission of an application or at loan consummation. Specifically, the guidance recommends that institutions’ promotional materials and descriptions of these products include information about, among other things, potential increases in consumers’ payment obligations and the potential consequences of increasing principal loan balances and decreasing home equity. The guidance also recommends that institutions alert consumers to the amount of any prepayment penalty that may be imposed if the consumer refinances the mortgage.\n\nConclusion\nI hope that my remarks today provided a useful snapshot of the current economic outlook. Outside of housing and the automotive sector, the economy is strong and employment is growing. The surge in housing construction that occurred in 2004-2005 was not sustainable, and we are now well along in adjusting housing construction to more comfortable levels.\n\nInflation continues to run at a pace above my comfort level. The Federal Reserve has raised interest rates to moderate inflation. Monetary policy works with long lags and so the economy is still reacting to our past actions. Given the strong growth in most sectors of the economy, the risks to inflation are still on the upside. In addition to giving some thoughts on the economy, I felt it was useful to offer a summary of the role that the Federal Reserve plays in consumer protection, using the current example of nontraditional mortgages. Clearly, the Federal Reserve believes in promoting consumer awareness and maintaining consumer protection so that users of financial products fully understand those products and their associated risks.\n\nFootnotes\n\n1. The numbers cited here refer to the price index for personal consumption expenditures (PCE), excluding food and energy, as published by the Commerce Department in the national income and product accounts. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20070118a.htm",
        "title": "Economic Outlook and Developments in Mortgage Markets",
        "date": "1/18/2007"
    },
    {
        "content": "January 17, 2007\n\nGovernor Frederic S. Mishkin\n\nAt the Forecaster’s Club of New York, New York, New York\n\nOver the past ten years, we have seen extraordinary run-ups in house prices. From 1996 to the present, nominal house prices in the United States have doubled, rising at a 7-1/4 percent annual rate.1 Over the past five years, the rise even accelerated to an annual average increase of 8-3/4 percent. This phenomenon has not been restricted to the United States but has occurred around the world. For example, Australia, Denmark, France, Ireland, New Zealand, Spain, Sweden, and the United Kingdom have had even higher rates of house price appreciation in recent years.\n\nAlthough increases in house price have recently moderated in some countries, they still are very high relative to rents. Furthermore, with the exception of Germany and Japan, the ratios of house prices to disposable income in many countries are greater than what would have been predicted on the basis of their trends. Because prices of homes, like other asset prices, are inherently forward looking, it is extremely hard to say whether they are above their fundamental value. Nevertheless, when asset prices increase explosively, concern always arises that a bubble may be developing and that its bursting might lead to a sharp fall in prices that could severely damage the economy.\n\nThis concern has led to an active debate among monetary policy makers around the world on the appropriate reaction to the run-ups in house prices that we have recently seen in many markets: Should central banks raise interest rates? And how should they prepare themselves to react if housing prices decline? These are the issues that I will address today. The views I will express are my own and not necessarily those of my colleagues on the Federal Open Market Committee.\n\nHome prices, like other asset prices, have important effects on output and inflation. Home prices affect the economy in two primary ways. First, when they begin rising, the expectation of further appreciation tends to become built into the market. That expectation boosts demand for homes, which stimulates new construction and aggregate demand. Of course, the sustained rise in prices can simultaneously sow the seeds of a market correction by making houses progressively less affordable relative to income, thereby limiting the demand for them and restraining additional construction. Second, higher home prices increase household wealth, thus stimulating consumer spending, another component of aggregate demand.\n\nBecause central banks are in the business of managing total demand in the economy so as to produce desirable outcomes on inflation and employment, monetary policy should accordingly respond to home prices to the extent that these prices are influencing aggregate demand and resource utilization. The issue of how central banks should respond to house price movements is therefore not whether they should respond at all. Rather, the issue is whether they should respond over and above the response called for in terms of objectives to stabilize inflation and employment over the usual policy time horizon. The issue here is the same one that applies to how central banks should respond to potential bubbles in asset prices in general: Because subsequent collapses of these asset prices might be highly damaging to the economy, as they were in Japan in the 1990s, should the monetary authority try to prick, or at least slow the growth of, developing bubbles?\n\nI view the answer as no.\n\nI will outline some conventional arguments for and against reacting to asset prices over and above their direct and foreseeable effects on inflation and employment. I will also discuss some additional reasons why central banks should not overly emphasize house prices in particular. Although I come down squarely on the side of those who oppose giving a special role to house prices in the conduct of monetary policy, I do think that central banks can take steps to ensure that sharp movements in the prices of homes or other assets do not have serious negative consequences for the economy.\n\nThere is no question that asset price bubbles have potential negative effects on the economy. The departure of asset prices from fundamentals can lead to inappropriate investments that decrease the efficiency of the economy. For example, if home prices rise above what the fundamentals would justify, too many houses will be built. Moreover, at some point, bubbles burst and asset prices then return to their fundamental values. When this happens, the sharp downward correction of asset prices can lead to a sharp contraction in the economy, both directly, through effects on investment, and indirectly, through the effects of reduced household wealth on consumer spending.\n\nDespite the clear dangers from asset price bubbles, the question remains as to whether central banks should do anything about them. Some economists have argued that central banks should at times \"lean against the wind\" by raising interest rates to stop bubbles from getting out of hand. They argue that if a bubble has been identified, then raising interest rates will produce better outcomes. For instance, William White, of the Bank for International Settlements, has said that \"monetary policy might rather be used in a highly discretionary way to respond to growing imbalances that were judged by policymakers to threaten financial instability.\"2 Although central banks have generally not argued that interest rates should be raised aggressively to burst asset price bubbles, statements suggest some central bankers believe some leaning against the wind might be warranted. For example, in the second half of 2003 and the first half of 2004, a minority of members of the Monetary Policy Committee of the Bank of England argued for raising interest rates more than could be justified in terms of the Bank of England's objectives for inflation over its normal policy horizon. They said that such a move would help lower the probability of house prices rising further and make it less likely that a house price collapse would occur later. Mervyn King, the Governor of the Bank of England, did not advocate leaning against the wind but did suggest that, to prevent a buildup of financial imbalances, a central bank might extend the horizon over which inflation is brought back to target. Statements from officials at the European Central Bank also have suggested that the possibility of an asset boom or bust might require longer than the usual one to two years in assessing whether the price stability goal was being met.\n\nThe recent case of the Sveriges Riksbank, the Swedish central bank, is particularly interesting. I studied the Riksbank in a report on monetary policy written with Francesco Giavazzi for the Swedish parliament before I came to the Federal Reserve Board.3 We found that communications by the Riksbank suggested to market participants that it was actually adjusting monetary policy to lean against the wind of rapid increases in home prices. On February 23, 2006, the Executive Board of the Riksbank voted to raise the repo rate 25 basis points (0.25 percentage points). This monetary policy action was accompanied by a statement acknowledging that the inflation forecast was revised downward. In fact the Inflation Report published on the same day also showed that inflation forecasts had been revised downward and were below the 2 percent target at every horizon. The Executive Board's statement pointed out that \"there is also reason to observe that household indebtedness and house prices are continuing to rise rapidly.\"4 It then said: \"Given this, the Executive Board decided to raise the repo rate by 0.25 percentage points at yesterday's meeting.\" Not surprisingly, market participants took this statement to mean that the Riksbank was setting the policy instrument not only to control inflation but also to restrain house prices. A similar reference to house prices in explaining the decision to raise rates was made in the press release of January 20, 2006.\n\nThe above statements suggest that some central bankers advocate that asset prices, and in particular, house prices, should have a special role in the conduct of monetary policy over and above their foreseeable effect on inflation and employment. There are several objections to this view.\n\nA special role for asset prices in the conduct of monetary policy requires three key assumptions. First, one must assume that a central bank can identify a bubble in progress. I find this assumption highly dubious because it is hard to believe that the central bank has such an informational advantage over private markets. Indeed, the view that government officials know better than the markets has been proved wrong over and over again. If the central bank has no informational advantage, and if it knows that a bubble has developed, the market will know this too, and the bubble will burst. Thus, any bubble that could be identified with certainty by the central bank would be unlikely ever to develop much further.\n\nA second assumption needed to justify a special role for asset prices is that monetary policy cannot appropriately deal with the consequences of a burst bubble, and so preemptive actions against a bubble are needed. Asset price crashes can sometimes lead to severe episodes of financial instability, with the most recent notable example among industrial countries being that of Japan. In principal, in the event of such a crash, monetary policy might become less effective in restoring the economy's health. Yet there are several reasons to believe that this concern about burst bubbles may be overstated.\n\nTo begin with, the bursting of asset price bubbles often does not lead to financial instability. In research that I conducted with Eugene White on fifteen stock market crashes in the twentieth century, we found that most of the crashes were not associated with any evidence of distress in financial institutions or the widening of credit spreads that would indicate heightened concerns about default.5 The bursting of the recent stock market bubble in the United States provides one example. The stock market drop in 2000-01 did not substantially damage the balance sheets of financial institutions, which were quite healthy before the crash, nor did it lead to wider credit spreads. At least partly as a result, the recession that followed the stock market drop was very mild despite some severely negative shocks to the U.S. economy, including the September 11, 2001, terrorist attacks and the corporate accounting scandals in Enron and other U.S. companies; the scandals raised doubts about the quality of information in financial markets and ultimately did indeed widen credit spreads.\n\nThere are even stronger reasons to believe that a bursting of a bubble in house prices is unlikely to produce financial instability. House prices are far less volatile than stock prices, outright declines after a run-up are not the norm, and declines that do occur are typically relatively small. The loan-to-value ratio for residential mortgages is usually substantially below 1, both because the initial loan is less than the value of the house and because, in conventional mortgages, loan-to-value ratios decline over the life of the loan. Hence, declines in home prices are far less likely to cause losses to financial institutions, default rates on residential mortgages typically are low, and recovery rates on foreclosures are high. Not surprisingly, declines in home prices generally have not led to financial instability. The financial instability that many countries experienced in the 1990s, including Japan, was caused by bad loans that resulted from declines in commercial property prices and not declines in home prices. In the absence of financial instability, monetary policy should be effective in countering the effects of a burst bubble.\n\nMany have learned the wrong lesson from the Japanese experience. The problem in Japan was not so much the bursting of the bubble but rather the policies that followed. The problems in Japan's banking sector were not resolved, so they continued to get worse well after the bubble had burst. In addition, with the benefit of hindsight, it seems clear that the Bank of Japan did not ease monetary policy sufficiently or rapidly enough in the aftermath of the crisis.\n\nA lesson that I draw from Japan's experience is that the serious mistake for a central bank that is confronting a bubble is not failing to stop it but rather failing to respond fast enough after it has burst. Deflation in Japan might never have set in had the Bank of Japan responded more rapidly after the asset price crash, which was substantially weakening demand in the economy. If deflation had not gotten started, Japan would not have experienced what has been referred to by economist Irving Fisher as debt deflation, in which the deflation increased the real indebtedness of business firms, which in turn further weakened the balance sheets of the financial sector.\n\nAnother lesson from Japan is that if a burst bubble harms the balance sheets of the financial sector, the government needs to take immediate steps to restore the health of the financial system. This should involve structural improvements in the way banks operate, not bailing out insolvent institutions. The prolonged problems in the banking sector are a key reason that the Japanese economy did so poorly after the bubble burst.\n\nA third assumption needed to justify a special focus on asset prices in the conduct of monetary policy is that a central bank actually knows the appropriate monetary policy to deflate a bubble. The effect of interest rates on asset price bubbles is highly uncertain. Although some theoretical models suggest that raising interest rates can diminish the acceleration of asset prices, others suggest that raising interest rates may cause a bubble to burst more severely, thus doing even more damage to the economy. An illustration of the difficulty of knowing the appropriate response to a possible bubble was provided when the Federal Reserve tightened monetary policy before the October 1929 stock market crash because of its concerns about a possible stock market bubble. With hindsight, economists have viewed this monetary policy tightening as a mistake.\n\nGiven the uncertainty about the effect of interest rates on bubbles, raising rates to deflate a bubble may do more harm than good. Furthermore, altering the trajectory of interest rates from the path predicted to have the most desirable outcomes for inflation and employment over the foreseeable horizon has the obvious cost of producing deviations from these desirable outcomes.\n\nBecause I doubt that any of the three assumptions needed to justify a special monetary policy focus on asset prices holds up, I am in the camp of those who argue that monetary policy makers should restrict their efforts to achieving their dual mandate of stabilizing inflation and employment and should not alter policy to have preemptive effects on asset prices.\n\nA central bank that focuses intently on asset prices looks as if it is trying to control too many elements of the economy. Part of the recent successes of central banks throughout the world has been that they have narrowed their focus and have more actively communicated what they can and cannot do. Specifically, central banks have argued that they are less capable of controlling real economic trends in the long run and should therefore focus more on price stability and damping short-term economic fluctuations. By narrowing their focus, central banks in recent years have been able to increase public support for their independence. A central bank that expanded its focus to asset prices could potentially weaken its public support and may even cause the public to worry that it is too powerful and has undue influence over all aspects of the economy.\n\nToo much focus on asset prices might also weaken support for a central bank by leading to public confusion about its objectives. When my co-author and I conducted our evaluation of monetary policy in Sweden, I directly observed this problem. I heard over and over again in interviews with participants from different sectors of Swedish society that the statements about house prices by the Riksbank confused the public about what it was trying to achieve.\n\nMy discussion so far indicates that central banks should not put a special emphasis on prices of houses or other assets in the conduct of monetary policy. This does not mean that central banks should stand by idly when such prices climb steeply. Rather my analysis suggests that central banks can take steps to make it less likely that sharp movements in asset prices will have serious negative consequences for the economy.\n\nInstead of trying to preemptively deal with the bubble--which I have argued is almost impossible to do--a central bank can minimize financial instability by being ready to react quickly to an asset price collapse if it occurs. One way a central bank can prepare itself to react quickly is to explore different scenarios to assess how it should respond to an asset price collapse. This is something that we do at the Federal Reserve.\n\nIndeed, examinations of different scenarios can be thought of as stress tests similar to the ones that commercial financial institutions and banking supervisors conduct all the time. They see how financial institutions will be affected by particular scenarios and then propose plans to ensure that the banks can withstand the negative effects. By conducting similar exercises, in this case for monetary policy, a central bank can minimize the damage from a collapse of an asset price bubble without having to judge that a bubble is in progress or predict that it will burst in the near future.\n\nAnother way that a central bank with bank supervisory authority can respond to possible bubbles is through prudential supervision of the financial system. If elevated asset prices might be leading to excessive risk-taking on the part of financial institutions, the central bank, as in the case of the United States, can ask financial institutions if they have the appropriate practices to ensure that they are not taking on too much risk. Working through supervisory channels has the advantage not only of helping make financial institutions better able to cope with possible asset price declines but possibly also of indirectly restraining extreme asset prices if they have been stimulated by excessive bank financing. Also, reminding institutions to maintain risk-management practices appropriate to the economic and financial environment could potentially help lessen a buildup of excessive asset prices in the first place.\n\nEven if the central bank is not involved in the prudential supervision directly, it can still play a role through public communication, particularly if it has a vehicle like the financial stability reports that some central banks publish. In these reports, central banks can evaluate whether rises in asset prices might be leading to excessive risk-taking on the part of financial institutions or whether distortions from inappropriate tax or regulatory policy may be stimulating excessive valuations of assets. If this appears to be happening, the central bank's discussion might encourage policy adjustment to remove the distortions or encourage prudential regulators and supervisors to more closely monitor the financial institutions they supervise.\n\nLarge run-ups in prices of assets such as houses present serious challenges to central bankers. I have argued that central banks should not give a special role to house prices in the conduct of monetary policy but should respond to them only to the extent that they have foreseeable effects on inflation and employment. Nevertheless, central banks can take measures to prepare for possible sharp reversals in the prices of homes or other assets to ensure that they will not do serious harm to the economy.\n\nFootnotes\n\n1. House prices are measured with the repeat-transaction price index of the Office of Federal Housing Enterprise Oversight. Return to text\n\n2. William R. White (2004), \"Making Macroprudential Concerns Operational,\" speech delivered at the Financial Stability Symposium sponsored by the Netherlands Bank, Amsterdam, October 25-26 (www.bis.org/speeches/sp041026.htm). Return to text\n\n3. Francesco Giavazzi and Frederic S. Mishkin (2006), \"An Evaluation of Swedish Monetary Policy between 1995 and 2005\" report published by the Riksdag (Swedish parliament) Committee on Finance; refer to Sveriges Riksbank (2006), \"Assessment of Monetary Policy,\" press release, November 28, www.riksbank.com/templates/Page.aspx?id=23320. Return to text\n\n4. Sveriges Riksbank (2006). \"Repo Rate Raised by 0.25 Percentage Points,\" press release, February 23, www.riksbank.com/templates/Page.aspx?id=20502. Return to text\n\n5. Frederic S. Mishkin and Eugene N. White (2002), \"U.S. Stock Market Crashes and Their Aftermath: Implications for Monetary Policy,\" NBER Working Paper Series 8992. Cambridge, Mass.: National Bureau of Economic Research, June; also in William Curt Hunter, George G. Kaufman, and Michael Pomerleano, eds., Asset Price Bubbles: The Implications for Monetary, Regulatory, and International Policies. Cambridge, Mass.: MIT Press, pp. 53-80. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/mishkin20070117a.htm",
        "title": "Enterprise Risk Management and Mortgage Lending",
        "date": "1/17/2007"
    },
    {
        "content": "January 11, 2007\n\nGovernor Susan Schmidt Bies\n\nAt the National Credit Union Administration 2007 Risk Mitigation Summit\n\nGood morning. I would like to thank Vice Chairman Rodney Hood and the National Credit Union Administration for the invitation to speak at the 2007 Risk Mitigation Summit. Given the continuing challenges in risk management facing banks and credit unions, this event is certainly topical. Having once been a chief risk officer at a commercial bank, I find it particularly interesting to address this group in my current role as supervisor and central banker. I hope my past private-sector experience adds a useful perspective on our current regulatory and supervisory policies.\n\nToday I would like to focus on the topic of enterprise risk management. I am pleased to see more and more sessions at conferences devoted to risk management, analyzing its different facets and exploring ways to tailor it to specific institutions and situations. Indeed, there is a growing understanding that sound risk management should be an integral part of running any type of business. A key theme I would like to highlight today is that all financial institutions should seek ways to strengthen risk management, but that the specific methods for improving risk management should depend on the size and level of complexity of the institution.\n\nIn my remarks today I will offer some general remarks about enterprise risk management, or ERM, and then look at mortgage lending as a particular example. Of course, mortgage lending is but one area in which ERM has application--other current examples include information security, credit derivatives, and overall portfolio management. Based on some recent observations, mortgage lending certainly is an area in which we believe financial institutions and supervisors have learned some key lessons about risk management. These lessons demonstrate how sound risk management can also increase business efficiency and profitability.\n\nGeneral Thoughts on Enterprise Risk Management\nThe financial services industry continues to evolve to meet the challenges posed by emerging technologies and business processes, new financial instruments, the growing scale and scope of financial institutions, and changing regulatory frameworks. A successful enterprise risk-management process can help an organization meet many of these challenges by providing a framework within which managers can explicitly consider how the organization's risk exposures are changing, determine the amount of risk they are willing to accept, and ensure that they have the appropriate risk mitigants and controls in place to limit risk to targeted levels.\n\nOf course, ERM is a fairly broad topic, one that can mean different things to different people. For our purposes here today, I will define ERM as a process that enables management to deal effectively with uncertainty and the associated risk and opportunity, enhancing the capacity to build stakeholder value. Borrowing from ERM literature, I would say that ERM includes:\n\n\n\n\n\nSome of you are probably familiar with the ERM framework published more than two years ago by the Committee of Sponsoring Organizations of the Treadway Commission, or COSO. The COSO framework provides a useful way of looking at ERM and helps generate further discussion.\n\nIn the COSO framework, ERM consists of eight interrelated components, which are derived from the way management runs an enterprise and integrated with the management process: (1) internal environment, (2) objective setting, (3) event identification, (4) risk assessment, (5) risk response, (6) control activities, (7) information and communication, and (8) monitoring. Each of these components is described in more detail in the COSO literature.\n\nNotably, the COSO framework states explicitly that although its components do not function identically across entities, its principles should apply to institutions of all sizes. Small and midsize entities, for example, may choose to apply the framework in a less formal and less structured way and scale it to their own needs--as long as quality is maintained. This explicitness about the universal applicability of principles underscores the message from financial institution supervisors that sound risk management is expected of every institution, and that it should reflect an institution's size and level of complexity. As most of you know, running a smaller or less complex institution presents different types of challenges and requires a risk-management framework appropriately tailored to the institution. For example, many smaller organizations face the challenge of ensuring independent review of processes and decisions because their officers and staff members often have multiple responsibilities, which can result in conflicts of interest.\n\nFor smaller organizations, ERM can provide a framework to strategically assess how risks are changing. That is, risk should be considered as part of the annual budgeting and strategic planning processes. Very often most of the planning process focuses on \"the most likely\" outcome. Using a risk-management framework that considers other, less likely outcomes leads management and the board of directors to consider how the types of risks and the amount of risk are expected to change to implement the plan. While smaller organizations will not find it practical to try to quantify many of these changes, the direction of change in and of itself is very important in the planning process. For example, while a new product or sales initiative may be expected to increase profitability, if the level of risk is expected to rise significantly, management and the board should discuss whether the returns are sufficient to compensate for the higher risk exposures.\n\nIf the risk assessment indicates that returns are not expected to compensate for the higher risk exposures, an institution may want to consider risk mitigation. This entails an understanding of the key risk drivers and a determination of what could be done to minimize their impact. For example, a new loan product may increase interest rate risk if the asset/liability rate sensitivity increases. An institution could consider restructuring the duration of its investment portfolio, selling loans to other investors, or initiating a campaign to attract deposits with a repricing structure closer to the loan product--all of which could reduce the interest rate risk.\n\nInstitutions are also finding that technology and business process changes are a growing source of risk exposures--what we call operational risk. Operational risk data, which support the Basel II capital initiative, show that the second most prominent cause of losses are due to breakdowns in execution, delivery, and process management. Organizations who wish to mitigate these types of risk often use design review, quality management, or change control processes to identify potential sources of risk early in the design and implementation process. Quality control is generally less expensive to design into a new process than correcting an error or rebuilding the system after a problem has occurred. While the old saying of total quality control management that \"quality is free\" may not literally be true in all cases, most organizations have learned that designing quality into the process not only reduces development and operating costs, it also improves service quality and customer satisfaction.\n\nThe examples I have just given reflect interest rate and operational risks. An enterprise-wide risk-management approach can help management consider these various types of risk jointly. That is, an organization should be aware of whether the drivers of various risk types tend to make those risks move up and down together, or whether they move independently. If the risks are correlated, then, in the aggregate, risks could rise above the risk appetite of management and the board, and they may have to consider changes in the business plan. For example, a decline in interest rates may increase interest-rate risk if it causes fixed-rate loans to be prepaid unexpectedly. Additionally, the interest rate drop may stimulate a surge in new loan originations and that could cause lending staff to make errors as it copes with the increased workload. On the other hand, breakdowns in loan underwriting standards due to the retirement of an experienced loan manager who is replaced by an unproven lender can occur throughout the interest rate cycle.\n\nI have purposely chosen to describe ERM activities that are part of other processes. I think this is the practical way for smaller organizations to implement ERM. Small organizations cannot afford to have dedicated staff and quantitative models of all forms of risk. By adding steps to existing management practices, management can lower implementation costs, but more importantly can increase attention on risk management by staff throughout the organization. That is, whether someone is designing a new branch office, shipping tapes to a backup site for storage, developing the layout for a newspaper ad, or training new employees, they will consciously think about risk as one of the elements of that business activity. Increased risk awareness by staff throughout the enterprise is integral to managing risk successfully.\n\nHaving made some general points, I would now like to turn to the topic of mortgage lending to highlight the importance of ERM. While details of these recent observations pertain to mortgage lending, they can also be applied to risk management in general.\n\nRisk Management in Mortgage Lending\nEffectively managing the risk associated with mortgage lending involves much more than prudent underwriting. Experienced risk managers understand the need to carefully consider the risks should the housing market slow, interest rates change, or unemployment rise. These include the risks that borrowers will not have sufficient income in the future to manage substantial payment increases and that continued home price appreciation may not provide a sufficient equity cushion to minimize losses in foreclosure. In addition, an accumulation of portfolio concentrations could leave an institution exposed in a downturn. Lenders specializing in subprime loans, for example, have endured a string of bad news recently, including increasing loan delinquency and foreclosure rates and the shutdown of some lenders that could not operate profitably in a slower origination environment.\n\nIn a broader sense, mortgage lending can present many types of risk for the enterprise as a whole, including credit, market, reputational, legal, and compliance risks. Therefore, while mortgage lending has been a very profitable business for many financial institutions recently, they need to understand the full set of risks associated with their mortgage lending business, including the consequences of adverse outcomes. For this reason, mortgage lending should be folded into the broader ERM process at any organization.\n\nNontraditional Mortgage Products\nLast September, the federal banking agencies, including the NCUA, issued guidance on the risks associated with nontraditional mortgage lending. Supervisors are concerned that current risk-management practices may not fully address the entire set of risks inherent in nontraditional mortgages--risks that could be heightened by current market conditions.\n\nNontraditional mortgage loans are those that allow borrowers to defer repayment of principal and, in some cases, interest. Over the past few years, there has been a large increase in nontraditional mortgage products, including interest-only (IO) loans, for which the borrower pays no loan principal for the first few years of the loan, and payment-option adjustable-rate mortgages (option ARMs), for which the borrower has flexible payment options--and which could result in negative amortization. These types of mortgages are estimated to have accounted for about one-third of all U.S. mortgage originations in 2006, compared with less than one-tenth just a few years earlier.\n\nNontraditional mortgage products have been available for many years; however, they have historically been offered to higher-income borrowers. More recently, nontraditional mortgages have been offered to a wider spectrum of consumers, including consumers who may be less able to afford the jump in monthly payments common in these types of mortgages and may not fully recognize their embedded risks. Subprime borrowers are more likely to experience an unmanageable payment shock during the life of the loan, meaning that they may be more likely to default on the loan.\n\nSupervisors have also observed that lenders are increasingly combining nontraditional mortgage loans with \"risk layering\" practices--such as by not evaluating the borrower's ability to meet increasing monthly payments when amortization begins or when interest rates on adjustable rate mortgages rise due to indexing or at the end of a \"teaser\" rate period. We are also seeing more frequent use of limited or no documentation in evaluating an applicant's income and assets. Although some lenders may have used elements of nontraditional mortgage products successfully in the past, the recent easing of traditional underwriting controls and the sale of some types of nontraditional products to subprime borrowers may generate losses on these products greater than has been observed in the past. Additionally, information from other sources seems to indicate that more borrowers are purchasing real estate with no equity down payment by using simultaneous second liens. The greater prevalence of risk-layering practices and sales of nontraditional mortgage products to nonprime borrowers have occurred in the past few years as competition for borrowers and declining profit margins has prompted lenders to loosen their credit standards to maintain loan volume in a slowing environment.\n\nThe industry trends I have just described, taken together, were what led the Federal Reserve, NCUA, and the other banking agencies to issue guidance on nontraditional mortgage products last September. The guidance emphasizes that an institution's risk-management processes should allow it to adequately identify, measure, monitor, and control the full set of risks associated with these products. It reminds lenders of the importance of assessing a borrower's ability to repay the loan, both now and when amortization begins and interest rates rise. Nontraditional mortgage products warrant a bank having strong risk-management standards as well as adequate capital and loan-loss reserves. Further, bankers should consider the impact of prepayment penalties for ARMs. Lenders should provide enough information so that borrowers clearly understand, before choosing a product or payment option, the terms of and risks associated with these loans, particularly the extent to which monthly payments may rise and negative amortization may increase the amount owed above the amount originally borrowed.\n\nSubprime Mortgage Lending\nThe agencies' guidance on nontraditional mortgage products did not specifically address mortgage lending to subprime borrowers--although, as noted, nontraditional mortgage products are sometimes offered to subprime borrowers. Both lenders and supervisors are aware of the benefits of subprime lending to homeowners, and both have an interest in ensuring that the market remains viable over the longer term. To ensure that viability, it is important to maintain sound underwriting standards and product terms as well as sufficient consumer protection practices. Therefore, subprime mortgage lending continues to be an area that supervisors monitor closely.\n\nWhile overall mortgage delinquency rates remain low by historical standards, they have been increasing in recent months, especially in the subprime sector. Performance deterioration is most notable in the more recent vintages. Many industry observers believe the poor performance of more recently originated subprime loans is due primarily to looser underwriting standards, including limited or no verification of borrower income and high loan-to-value transactions. Subprime lending has certainly created homeownership opportunities for borrowers with weaker or less certain credit histories. But because of the increased risk profile, lenders need to be especially diligent in maintaining prudent underwriting standards and in promoting manageable loan terms and sufficient consumer disclosure practices. Further, as part of an ERM process, as lenders design more complex products they need to identify ways to clearly communicate the product features and risks to their customers.\n\nSubprime mortgages typically carry higher interest rates than prime loans. It is not uncommon to find margins of 600 basis points or more on adjustable rate subprime loans after the expiration of a teaser rate. Not surprisingly, some borrowers are unable to keep up with their mortgage payments once these payments fully adjust. In some cases, if alternative financing cannot be found, borrowers may be forced to sell their home or enter foreclosure. And given prepayment penalties, home price appreciation slowing significantly and capital market investors becoming more conservative, some borrowers may be having more difficultly in refinancing to avoid foreclosure.\n\nSupervisors are discussing what can be done to ensure that these types of loans are being originated in a safe and sound manner and that consumers are being provided with clear and balanced information so that they can fully understand the terms and risks of these products. Subprime loan underwriting, when done prudently, should reflect all relevant credit factors, including the borrower's ability to service the debt. In the current environment, risk managers should review policies governing the use of loans with limited or no documentation and simultaneous-second mortgages. Lenders that do not account for tax and insurance burdens in assessing borrower qualifications should understand the associated risks. It may even be prudent to escrow tax and insurance payments to ensure that the collateral is adequately protected from physical casualty losses as well as tax liens, or the lender should inform borrowers what should be set aside to meet the periodic insurance and tax payments on their homes if these payments are not already included in their total monthly mortgage payment.\n\nConclusion\nAll financial institutions need sound risk-management practices. An enterprise-wide approach is appropriate for setting objectives across the organization, instilling a culture attuned to risk, and ensuring that key activities and risks are being monitored regularly. Clearly, there is always an opportunity to improve upon enterprise risk-management strategies and strengthening the discipline to implement those strategies effectively. But vigilance is critical, too, since problems can sometimes quickly arise in a business line or unit that has presented no past difficulties. Accordingly, it is always helpful to evaluate the \"what if\" scenarios even for the most pristine of business units.\n\nBut the manner in which risk-management challenges are addressed can--and should--vary across institutions, based on their size, complexity, and individual risk profile. In many cases, it simply does not make sense for small organizations to adopt the most sophisticated risk-management practices; however, that does not absolve such smaller institutions of their responsibility to improve risk management. Additionally, as supervisors, we want to ensure that institutions are not only identifying, measuring, and managing their risks but are also developing and maintaining appropriate corporate governance structures appropriate for their business activities and risk taking. Our hope is that the guidance we offer on these various topics is becoming more consistent with financial institutions' own risk-management practices.\n\nToday I have used the example of mortgage lending to stress the importance of ERM, but there are obviously many other areas to which ERM applies. We believe that the recently issued guidance on nontraditional mortgage products contains helpful reminders and recommendations for institutions using those products, ensuring that they recognize the full set of risks involved.\n\nAs a final point, I would like to stress that supervisors at all five federal banking agencies, including the NCUA and the Federal Reserve, aim to implement the guidance as consistently as possible across institutions, since we do understand institutions' concerns about this issue. Of course, it is always a challenge to ensure that guidance is applied consistently throughout the industry, especially when institution-specific factors--such as portfolio concentrations and individual risk-management practices--might affect the manner in which the guidance needs to be applied to individual organizations. But we have already begun to undertake efforts across our agencies, including extensive communication and coordination, so that institutions are not subjected to needlessly differing treatment.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20070111a.htm",
        "title": "Enterprise Risk Management and Mortgage Lending",
        "date": "1/11/2007"
    },
    {
        "content": "January 08, 2007\n\nVice Chairman Donald L. Kohn\n\nAt the Atlanta Rotary Club, Atlanta, Georgia\n\nThank you for inviting me to fill in for Jack Guynn today to discuss the economic outlook for the new year. Those are big shoes to fill. Jack played a prominent and constructive role in the Federal Reserve System, as I know he did in the Atlanta community, bringing his vast experience and uncommonly good sense to bear on a wide variety of important policy issues.\n\nAs we enter 2007, the current economic expansion is now more than five years old. Although it got off to a slow start, the expansion was quite strong from mid-2003 through mid-2006. Over that period, a good deal of the slack in our nation's utilization of resources was taken up, and the unemployment rate reached its lowest level in five years. At the same time, however, core inflation--that is, inflation without potentially volatile food and energy prices--as measured by the price index for personal consumption expenditures, moved up from less than 1-1/2 percent to about 2-1/2 percent. To safeguard the gains made over the past quarter century in the achievement of price stability and to promote sustained economic expansion, the Federal Reserve in mid-2004 began removing the considerable monetary accommodation it had earlier put in place.\n\nMore recently, led by a sharp pullback in housing activity, economic activity decelerated in the second half of 2006 to a pace that was probably a bit below the long-term rate of growth in our nation's productive capacity. At the same time, decreases in energy prices have substantially reduced overall consumer price inflation of late, and core inflation has showed signs of slowing.\n\nMy expectations for 2007 quite naturally rest on an assessment of how recent trends are likely to play themselves out. How long will the decline in housing activity hold back overall economic growth? What about spillovers from housing to other sectors? What are we to make of the recent weakness in manufacturing activity? Will the recent good news on inflation persist? Before venturing some guesses on these questions, I need to issue two caveats. First, events will probably unfold differently than currently seems likely, and the range of uncertainty around any forecast is considerable. That uncertainty does not, however, diminish the value of having and discussing an outlook. Monetary policy must be based on our best estimate of future developments, and the effectiveness of policy is aided when the public understands the outlook of policymakers. But the uncertainty does underscore the value of monitoring the incoming information closely, as we always do, and of being prepared to adjust our expectations accordingly. Second, the views that I will express today are my own and not necessarily those of my colleagues on the Federal Open Market Committee (FOMC).1\n\nEconomic Activity\nThe deceleration in economic activity in the second half of 2006 was concentrated in the housing and motor vehicle sectors and in the production of related materials and supplies. The slowing of activity has been most acutely felt in the real estate market, where the sales of new and existing homes contracted sharply beginning in the fall of 2005. Residential construction has slowed dramatically as well. As of November, single-family housing starts had fallen about 30 percent from their peak in January 2006. Tentative signs have begun to emerge that the housing market may be stabilizing. Home sales appear to have flattened out since midyear, mortgage applications have been increasing, and consumers' perceptions of homebuying conditions, as reported in the Michigan survey, have improved. Nonetheless, even if the demand for housing is leveling off, housing activity may not yet have found a floor, given the sizable overhang of unsold houses.\n\nUncertainty about where we stand in the housing cycle remains considerable. In part, that is because this housing downturn has differed from some of those in the past in important ways. It was not triggered by a restrictive monetary policy and high interest rates; indeed, relatively low intermediate and long-term interest rates are helping to support the stabilization of this sector. But the current contraction in housing did follow an unusually large run-up in sales and construction and, even more so, in prices relative to the returns on other financial and real assets. Our uncertainty about what pushed home prices and sales to those elevated levels raises questions about how the market will adjust now that expectations of the rate of house price appreciation are being trimmed. And changes in the organization of the construction industry, with activity more concentrated in the hands of large, publicly traded corporations, may also affect the dynamics of prices and activity in response to the inventory overhang.\n\nIn my own judgment, housing starts may be not very far from their trough, but the risks around this outlook still are largely to the downside. Although house prices nationally have decelerated noticeably and appear to have fallen in some markets, they are still high relative to rents and interest rates. Building permits decreased substantially again in November, and inventories of unsold homes have only started to edge lower. We also do not know whether the possible stabilization that seems to be taking hold would be immune to a rise in longer-term interest rates should term premiums increase or the federal funds rate fail to follow the downward path currently built into market expectations. Even if starts stabilize at close to current levels, those levels are sufficiently low that overall construction activity would remain a negative for the growth of economic activity in the first half of this year.\n\nWhile the downturn in housing was steepening during the third and fourth quarters, domestic producers of cars and light trucks slashed output in an effort to reduce their elevated inventories, particularly of light trucks (minivans, SUVs, and pickups). In October, light motor vehicles were assembled at the slowest pace in more than eight years. However, production rebounded in the final two months of the year, and, with inventories having come down from their highs last summer, available monthly schedules suggest that vehicle manufacturers anticipate maintaining the pace of assemblies during the first quarter at about the average rate in November and December. Thus, with sales reasonably well maintained through December, the drag from this sector's inventory correction should be ending.\n\nAlthough much of the weakness in industrial production toward the end of 2006 can be readily traced to the housing and motor vehicle sectors, production in other manufacturing industries also softened from September through November, and this development raised concerns that the deceleration in economic activity was becoming more broadly based. Some of the industries reporting lower output late last year included those that produce intermediate goods for the housing and motor vehicle sectors, but others have only tenuous links to those two sectors. Evidently, other industries have experienced a small buildup of inventories, prompting production adjustments. These inventory and production developments may reflect in part a slowing in the growth of business capital spending that has become evident in recent data on orders and shipments of equipment other than high-tech and transportation equipment.\n\nIn my view, however, what we are seeing in the recent information on factory output and capital spending is not the leading edge of general economic weakness but instead an adjustment to a sustained pace of expansion that, necessarily, is less rapid than that from mid-2003 to mid-2006. A number of indicators continue to suggest that economic activity outside the housing and motor vehicle sectors is likely to post continuing healthy gains over coming quarters. Although several regional manufacturing surveys have suggested that the weakness in factory production extended into December, the national purchasing managers' survey rose a notch last month. Prices of many industrial commodities are typically sensitive to developments in the manufacturing sector, and these prices generally remain firm, as is consistent with sustained demand here and abroad. More broadly, last Friday's employment report suggested no signs of cumulating weakness either in manufacturing or private service-producing industries. New job creation has remained relatively brisk in recent months; over the fourth quarter, private businesses added an average of 119,000 jobs to payrolls each month--only a little below the pace of hiring earlier in the year. And the unemployment rate remained in the neighborhood of 4-1/2 percent.\n\nOn the whole, businesses seem to be reasonably upbeat. The Reserve Bank Districts, including Atlanta, report that most firms are anticipating good gains in sales over the coming year. The semiannual economic forecast of the Institute for Supply Management, released in mid-December, was optimistic--perhaps surprisingly so in light of the recent slowdown in industrial activity. Respondents indicated that capital spending in 2007 would increase at a robust pace similar to that for all of 2006. That businesses are beginning the year with a positive outlook is not surprising: Profits have been high, encouraging business expansion, and external funding for capital projects remains readily available on favorable terms. And as I noted, firms in their hiring decisions seem to be acting on plans to increase output.\n\nMost importantly, the data we have in hand suggest that consumer demand for nonhousing goods and services has been well maintained. The retail sales report for November was strong across the board, and surveys of consumer confidence show that, in the final months of 2006, households' views about business conditions and about their financial situations improved noticeably. Spending and attitudes have been supported over recent months in part by solid gains in household income and employment.\n\nOne caution is that some of the recent buoyancy in household attitudes and strength in consumer demand also may reflect the unwinding of earlier increases in gasoline prices, in which case part of the strong gain in spending in recent months may be transitory. Another cautionary note is that the strength in consumer spending throughout 2006 received a considerable boost from the earlier rise in household wealth. In the wake of the current slowdown in house price gains, I expect that, over time, households will find it necessary to build their net worth by holding back on consumption, and thus, consumer spending will rise a little less rapidly than income for a while. I do not anticipate that the gap between the growth rates in consumption and income will be large, however, and I believe that the recent data on consumer spending provide some very tentative evidence that the cooling of the housing market will have a limited effect on other forms of spending.\n\nOngoing gains in household consumption and in business capital spending to meet that expected demand and to take advantage of the cost-saving benefits of new technology form the foundation for a moderate pace of economic activity going forward. Continued solid economic expansion among our major trading partners should also provide support to production here at home. To be sure, as I already noted, a low level of housing starts and production adjustments in some manufacturing industries will remain a drag on growth in output in the near term, but these effects will wane during the first half of 2007 as excess inventories are worked off. When this process is completed, the rate of economic growth should pick up to something in the neighborhood of the growth rate of the economy's potential.\n\nInflation\nI believe that a path for output like the one that I have just described is likely to be associated with a gradual decline in core inflation from the elevated levels of last spring and summer. Importantly, measures of long-term inflation expectations are no higher than they were before the rise in core inflation. In addition, some of that earlier price acceleration probably resulted from the pass-through of sharp increases in energy prices in the first half of the year that have been partly reversed; increases in rents and imputed rents for owner-occupied housing may ease back as a portion of the oversupply of homes for sale is shifted into the rental market; and a period of below-trend economic growth should relieve some pressures on labor and product markets.\n\nCertainly, the recent data on consumer prices have been encouragingly consistent with the downward tilt to inflation that the FOMC has been expecting. However, we need to be cautious about extrapolating trends from a couple of months of data. The data themselves are noisy--subject to month-to-month variations that are unrelated to more-persistent developments. And we need to recognize that some of the very recent disinflation may represent one-time influences. Energy costs have moved down markedly in recent months, and those declines have fed through to prices for a number of intermediate goods and probably for some final goods as well. But futures markets anticipate that prices of crude oil will increase gradually, which suggests that, once the adjustment to the current level plays out, energy prices will no longer work to restrain total and core inflation. And if a portion of the weakness in goods prices reflects efforts by producers to forestall or correct inventory imbalances, that restraint on pricing will dissipate as firms' corrective actions take effect.\n\nSo, despite the recent favorable price data, I believe it is still too early to relax our concerns about whether the run-up in price pressures in the spring and summer of last year is truly unwinding and whether it is unwinding rapidly enough to forestall a pickup in inflation expectations. Even with the opening of some slack in the manufacturing sector and in homebuilding, labor markets generally seem to have stayed fairly tight, with the unemployment rate at only 4-1/2 percent. Although recent data indicate that labor costs were not rising as rapidly in 2006 as first estimated, labor compensation does appear to have increased more quickly over 2006 than over 2005. Last year's increase in compensation also appears to have outpaced overall consumer price inflation. That development in and of itself does not necessarily indicate an increase in inflationary pressures, especially if it represents a process in which real compensation begins to catch up with the rapid increases in labor productivity earlier this decade. What would be problematic would be a pickup in the growth of nominal hourly labor compensation that was passed through to prices over the next several quarters, or one that was not matched, over a sustained period, by a comparable pickup in the growth of productivity. Eventually, the resulting faster growth of unit labor costs would pose a serious threat to price stability.\n\nCore inflation is still higher than it was just a year ago, and, as I noted, some of the very recent decline may result from one-time changes in relative prices rather than an easing in underlying inflation pressures. A very gradual decline in the trend rate of inflation continues to be the most likely outcome, but that path is still by no means assured, and in my judgment such a decline remains critically important to the sustained prosperity of the U.S. economy.\n\nIn sum, conditions appear to be in place for a good year for the U.S. economy, one marked by growth that is moderate and sustainable and by inflation that will be lower than last year's. The economy appears to be weathering the downturn in housing with limited collateral effects, and inflation appears to be easing with the aid of lower energy prices, well-anchored inflation expectations, and competitive labor and product markets. I am a central banker to my core, so I know that somewhere, somehow, something will go wrong, but you will have to rely on the new president of the Federal Reserve Bank of Atlanta to explain to you next January just what happened and what the implications are for 2008.\n\nFootnotes\n\n1. John Stevens and Joyce Zickler, of the Board's staff, contributed to these remarks. Return to text",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20070108a.htm",
        "title": "The Economic Outlook",
        "date": "1/8/2007"
    },
    {
        "content": "January 05, 2007\n\nChairman Ben S. Bernanke\n\nAt the Allied Social Science Association Annual Meeting, Chicago, Illinois\n\nThe Federal Reserve, like many central banks, is engaged in a wide range of activities beyond the making of monetary policy. For example, the Fed plays a critical role in the U.S. payments system, both as an overseer and as a provider of wholesale and retail payments services; it has substantial responsibilities in the area of consumer protection, including rule-writing and enforcement; it promotes financial stability; and, together with other agencies, it supervises both large and small banking organizations.\n\nIn this talk I will consider the case for one of these activities--namely, the supervision of the banking system--being conducted, at least in part, by the U.S. central bank. In some countries, such as the United Kingdom and Japan, the responsibility for supervising banks (as well as financial institutions and financial markets more generally) has been assigned to a single financial supervisory agency, rather than to the central bank. In the euro area, even as the European Central Bank has assumed responsibility for monetary policy, some national central banks or other national authorities have retained substantial supervisory powers. Various other institutional arrangements exist around the world, including the more traditional model in which the central bank also serves as a supervisor of the banking system. In light of these alternative models, does it make sense for the Federal Reserve to supervise banking organizations?\n\nAt a conceptual level, the discussion of whether the central bank should also act as a supervisor has focused on issues of incentives and efficiency. With respect to incentives, one question that has been raised is whether assigning both macroeconomic and supervisory goals to a single institution might not lead to conflicts of interest. Some writers have argued, for example, that a central bank with supervisory responsibilities might, at times, be hesitant to impose appropriate monetary restraint out of concern for possible adverse effects on banks (Goodhart and Schoenmaker, 1995). On the other hand, if macroeconomic and supervisory goals are interdependent, a single agency responsible for both objectives might be better able to take those interdependencies into account than could multiple agencies, each charged with a single goal (Wall and Eisenbeis, 1999; Bernanke, 2001). For example, Alan Greenspan has argued along these lines that \"a single regulator with a narrow view of safety and soundness and with no responsibility for the macroeconomic implications of its decisions would inevitably have a long-term bias against risk-taking and innovation (Greenspan, 1994, p. 130).\"\n\nThe issue of the efficiency of combining central banking and supervision in one agency boils down to whether that combination entails significant economies (or diseconomies) of scope (Goodhart, 2000; Mishkin, 2001; Haubrich and Thomson, 2005). For instance, if supervisory activities yield information that is useful for carrying out monetary policy or other central bank functions (or vice versa) and this information cannot be obtained easily through interagency cooperation, then granting some supervisory authority to the central bank may lead to better outcomes. By the same token, if economies of scope are outweighed by the benefits of specialization--arising, for example, from increased organizational focus, the development of more specialized expertise, or a reduction in regulatory overlap--then the cause of efficiency may be better served by separating supervision from central banking.1\n\nAbstract arguments can take us only so far, however. Because financial markets, political systems, and regulatory objectives vary across countries and because initial conditions are influenced by historical accident, no single institutional structure seems likely to be best in all cases (Dixit, 1996). Therefore, I will avoid generalizations today and restrict my attention to the current arrangements in the United States.\n\nIn the U.S. context, the various issues that have been raised concerning institutional incentives seem unlikely to be determining factors in a consideration of the preferred regulatory structure; indeed, the side of the debate on which they would weigh is uncertain. In particular, U.S. monetary policy has been quite successful for some time, and I am not aware of any evidence that monetary-policy decisions have been distorted because of the Fed’s supervisory role. The issue of efficiency is less easy to dismiss, however. The U.S. bank regulatory system is complex and in some respects duplicative, and though removing supervisory authority from the Fed would leave much of this complexity intact, it would produce a slightly simpler allocation of supervisory responsibilities. We can thus narrow the question I posed at the beginning, of whether the Fed should supervise banks, to the question of whether the Fed’s supervisory role generates economies of scope of sufficient social benefit to outweigh any associated costs.\n\nIn the remainder of my remarks, I will discuss some economies of scope arising from the combination of bank supervision and other central bank responsibilities. Although I will touch on a number of complementarities between these activities, my focus will be the benefits of the Fed’s supervisory authority for the execution of one of its core functions: the prevention and management of financial crises. In particular, I will argue that the Fed’s ability to deal with diverse and hard-to-predict threats to financial stability depends critically on the information, expertise, and powers that it holds by virtue of being both a bank supervisor and a central bank.\n\nThe Federal Reserve’s supervisory authority\nAs a starting point, some background on the Federal Reserve’s supervisory authority may be helpful. The Federal Reserve shares the responsibility for regulating and supervising the U.S. financial system with a number of federal and state government agencies, including the other banking agencies, the Securities and Exchange Commission (SEC), and the Commodity Futures Trading Commission. The Fed, along with state authorities, supervises state member banks (that is, state-chartered banks that are members of the Federal Reserve System). In addition, it supervises the U.S. operations of foreign banks and, in some cases, the foreign operations of U.S. banks.\n\nThe Federal Reserve also serves as the umbrella supervisor of all bank holding companies and financial holding companies, which gives the U.S. central bank broad oversight responsibilities for these banking organizations. However, the bank and nonbank subsidiaries of such holding companies are often supervised by agencies other than the Fed. Supervisory responsibility is determined by the type of charter held by the supervised company and by the principle of functional regulation, under which the identity of the primary supervisor depends on the nature of the financial activity being carried out. For example, the commercial banking activities of holding-company subsidiaries with national bank charters are supervised by the Office of the Comptroller of the Currency (OCC), whereas securities activities in nonbank subsidiaries are under the jurisdiction of the SEC. The Fed cooperates with the OCC, the SEC, and other supervisors in determining the financial condition of the consolidated organization.\n\nThe Fed’s supervisory responsibilities require extensive engagement at both the policymaking and the operational levels. In the policy arena, the Fed provides technical support to the Congress on legislation related to banking and financial markets and sometimes advocates specific measures (such as prompt corrective action for troubled depositories); along with other agencies, it represents the United States in international forums to develop and negotiate various standards (such as bank capital standards); it collaborates with other agencies (or, in some cases, has sole authority) to develop regulations and supervisory guidance that implement the banking and consumer protection laws; and, working with other agencies as appropriate, it develops norms and practices for supervisors and examiners. At the operational level, the Federal Reserve System (including the twelve regional Federal Reserve Banks) works closely with other agencies to examine banking organizations and to ensure that they operate in a safe and sound manner and comply with the relevant laws and regulations.\n\nThe Federal Reserve’s supervisory activities give the institution access to a wealth of information about the banking system. For example, the Fed’s examination staff collects and analyzes information on each supervised organization’s management structure, lines of business, financial condition, internal controls, risk‑management practices, operational vulnerabilities, and much else. Moreover, the Fed’s supervisory activities provide it with a window onto financial institutions that it does not regulate and onto developments in the broader financial markets. I have already mentioned the Fed’s role as the umbrella supervisor, which affords it access to information (as well as direct or back-up examination authority) for all holding-company subsidiaries, nonbank organizations as well as banks. Its supervisory activities also allow the Fed to obtain useful information about the financial companies that do business with the banking organizations it supervises. For example, some large banks are heavily engaged in lending and providing various services to hedge funds and other private pools of capital. In the process of ensuring that banks prudently manage these counterparty relationships, Fed staff members, collaborating with their colleagues from other agencies, learn a great deal about the business practices, investment strategies, and emerging trends in this industry.2 Finally, many large banking organizations are sophisticated participants in financial markets, including the markets for derivatives and securitized assets. In monitoring and analyzing the activities of these banks, the Fed obtains valuable information about trends and current developments in these markets. Together with the knowledge obtained through its monetary-policy and payments activities, information gained through its supervisory activities gives the Fed an exceptionally broad and deep understanding of developments in financial markets and financial institutions.\n\nThe benefits of the Fed’s supervisory authority for its non-supervisory activities\nThe extensive information and the expertise that the Fed gains in the process of supervising banks are useful for carrying out many of the central bank’s non-supervisory activities. The benefits of supervisory information for monetary policy are perhaps the most debated. The research literature has focused on whether information drawn from bank examinations is helpful in assessing the economic outlook and thus in formulating monetary policy. The results have been mixed (Peek, Rosengren, and Tootell, 1999; Feldman and others, 2003). Some evidence suggests that supervisory information is likely to be most useful for monetary policymaking in times of financial stress. For example, the Federal Reserve’s experience suggests that such information was helpful in evaluating the availability of credit in the early 1990s, when some banks’ lending was constrained by their limited capital (Bernanke and Lown, 1991; Greenspan, 1994). More generally, monetary policy is certainly aided by the anecdotal information on regional economic conditions that Reserve Bank business and community contacts, including numerous bankers, provide. Many of these contacts are fostered by the extensive interaction between the Reserve Banks and the banks in their Districts.\n\nThe Federal Reserve’s oversight of the payments system is another critical central banking function. In contrast to the situation in some other countries, the Federal Reserve lacks explicit legal authority to oversee systemically important payments systems. Instead, the Federal Reserve’s powers in this area derive to a considerable extent from its bank supervisory authority. Notably, some key institutions providing clearing and settlement services hold bank charters that place them under Federal Reserve oversight.3 In its capacity as a bank supervisor, the Fed can obtain detailed information from these institutions about their operations and risk-management practices and can take action as needed to address risks and deficiencies. The Fed is also either the direct or umbrella supervisor of several large commercial banks that are critical to the payments system through their clearing and settlement activities.4\n\nAs I have mentioned, the Fed also has an operational role in the payments system. In particular, Fedwire--a system that the Fed operates for transmitting large-value payments--is a critical component of the U.S. financial infrastructure. The operation of Fedwire and related payments systems often involves large short-term credit exposures to system participants (reflecting so-called daylight overdrafts). The Fed also extends overnight credit to depository institutions through the discount window. The Fed’s management of credit risk associated with these various forms of short-term lending relies heavily on supervisory information. In normal times, this information often is obtained from other banking agencies. When a financial institution is under considerable stress, however, it is helpful to have available in-house supervisors who can independently assess the borrowing institution and evaluate its collateral.5\n\nI have described several ways in which the Fed’s supervisory authority assists it in performing its other functions. In my view, however, the greatest external benefits of the Fed’s supervisory activities are those related to the institution’s role in preventing and managing financial crises.\n\nBank supervision and the prevention and management of financial crises\nAlthough the Federal Reserve is today best known to the public as the agency responsible for monetary policy, the Fed was founded in 1913 largely in response to the periodic episodes of banking panics and other forms of financial instability that had plagued the U.S. economy during the nineteenth and early twentieth centuries (Friedman and Schwartz, 1963).6 Today, the Fed retains its key role in the prevention and mitigation of financial crises, for a number of reasons. First, the Fed has unique powers to provide liquidity to the financial system, through means that include open-market purchases, discount-window loans, and intra-day overdrafts. Second, as I have noted, the Fed is a key player in the payments system, functioning both as an overseer of systemically important clearing-and-settlement systems and as a major provider of payments services. Problems in the execution of payments--arising from institutions’ uncertainties about the financial condition of their counterparties, failures of the payments infrastructure, or both--have been central features of a number of U.S. financial crises. Even in situations in which the payments system has continued to function normally, the information the Fed has gleaned from its payment activities has been helpful for understanding and managing financial stresses. Third, the Federal Reserve has developed extensive international relationships and has worked closely with foreign central banks and, as a consequence of its responsibility for the supervision of cross-border banking operations, with foreign supervisors. These relationships have proved useful in past crises, and are likely to be even more critical in the future as the globalization of finance continues. Fourth, the Fed’s responsibilities for macroeconomic stability provide it with both a strong incentive and the knowledge to prevent or mitigate financial disruptions that threaten to spill over into the broader economy (Mishkin, 2000). Finally, the wide scope of the Fed’s activities in financial markets--including not only bank supervision and its roles in the payments system but also the interaction with primary dealers and the monitoring of capital markets associated with the making of monetary policy--has given the Fed a uniquely broad expertise in evaluating and responding to emerging financial strains.\n\nFinancial stability is strengthened both by taking steps to prevent financial crises and by managing crises effectively if they occur. To make crises less likely, over the years the Federal Reserve has worked effectively with the Congress, other supervisors, and financial market participants to develop statutory, regulatory, and other measures.7 The Fed has also worked with other supervisors and with financial institutions to support the development of practices that limit systemic risk. For example, over the past year and a half, the Federal Reserve Bank of New York has collaborated with the private sector and with other regulators to strengthen the infrastructure that supports the credit derivatives market. Very rapid growth in the volume of trading in that market in recent years had led to substantial backlogs of unconfirmed trades. As a result of the cooperative efforts of market participants and regulators, these backlogs have been cut sharply, reducing the risk that incomplete trading records could complicate the evaluation of firms’ positions and financial conditions in a crisis. A number of other cross-cutting initiatives are under way, including, for example, a coordinated review of how major firms use stress-testing techniques to measure their credit and market risks. As the financial system becomes more complex, more global, and more interconnected, public-private collaborations that transcend national borders and cut across types of institutions, markets, and financial instruments will become increasingly valuable. The Fed will continue to initiate and support such efforts.\n\nBecause of the Fed’s unique perspective on issues of financial stability--a perspective based on statutory authority, historical experience, and an appreciation of the links between financial and macroeconomic stability--the participation of the U.S. central bank in collective efforts to prevent financial crises seems highly desirable. The Federal Reserve’s supervisory authority is useful in helping the institution to more effectively identify issues and potential problems, by giving it a \"seat at the table\" that allows it to be heard on these issues, and by increasing its contact and influence with financial-market participants and other supervisors. In short, the Fed’s supervisory powers help both to give it a voice in policy discussions concerning financial stability and to increase the quality of its contributions to those discussions.\n\nWith strong private-sector institutions and good public policies in place, episodes of severe financial stress should be relatively infrequent. When financial problems do develop, however, the Fed and other policymakers face the important threshold question of whether public action is warranted; in particular, they must weigh the expected benefits of taking action against the possibility that such action will encourage excessive private risk-taking in the future (the moral hazard problem). To minimize the moral hazard problem, policymakers should act only in those cases in which it seems likely that inaction would risk systemic problems with the potential to damage the broader economy; and if they act, they should do so in ways that disturb market outcomes as little as possible.\n\nIn those cases when policymakers choose to respond to an actual or potential crisis, however, the actions taken should be informed, timely, and effective. From the Fed’s perspective, the ability to act effectively in a financial crisis is greatly enhanced by its ongoing supervisory role. Such events can involve significant and unpredictable interdependencies across institutions, markets, and the real economy and, in some cases, breakdowns in communications or in the financial infrastructure. In a situation of financial stress, the Federal Reserve’s supervisory function helps it to obtain timely and reliable information on conditions in the banking sector, the payments system, and the capital markets, while helping the Fed maintain the in-house expertise necessary to gather and evaluate such information quickly and to make sound judgments about possible policy responses. Moreover, the Fed’s ongoing relationships with the financial firms it supervises, as well as with other supervisors, can ease communications and improve cooperation in a crisis, leading to more effective crisis management.\n\nIt is true that, in some episodes of financial stress, important information was provided voluntarily by securities firms and other organizations over which the Federal Reserve has limited or no regulatory authority, or by other supervisors; moreover, some of the expertise that proved most valuable was derived from the Fed’s non-supervisory activities. Nevertheless, examination authority and a knowledgeable and experienced examination staff have often proved essential. After the September 11 terrorist attacks, for example, Federal Reserve examiners were sent to the backup sites of the large banking firms that had been affected. These staff members proved crucial in discerning what was happening in a highly confused and uncertain situation. The information they obtained helped the Fed assess the damage that had been done, think through the potential implications for financial markets, and evaluate possible remedial actions.\n\nPerhaps the most important remedial action taken by the Fed in the wake of the September 11 attacks was its provision of massive amounts of liquidity, which helped avoid logjams in the payments system and ensure that credit for the financial system and the economy would be available as needed. Methods of providing liquidity included open-market purchases, intra-day overdrafts that were well above normal levels, discount-window lending, the crediting of banks on regular schedules for checks in process of collection, securities lending, and currency swaps that allowed foreign central banks to provide dollar liquidity to financial institutions in their own countries (Ferguson, 2003). In undertaking these steps, the Federal Reserve benefited from its knowledge of the liquidity management practices of key institutions, their funding positions, and their financial conditions, as well as from its ability to evaluate the collateral provided by institutions requesting funds. This information and expertise, gained in part through its supervisory role, allowed the Fed to supply the needed liquidity efficiently and without undue risk.\n\nMost financial crises do not involve loss of human life and damage to physical infrastructure of the sort that occurred on September 11, 2001. But even in the case of purely financial events, the Fed’s supervisory powers give it the knowledge (and, in some cases, the influence) needed to work with other supervisors and the private sector to help resolve the situation. In the aftermath of the 1987 stock market crash, for example, the Fed drew on its supervisory experience to evaluate the funding and credit risks facing major banking organizations. On-site examiners obtained timely information on potentially significant lending losses and liquidity pressures. Besides issuing a public statement that the discount window was available to provide liquidity, Federal Reserve officials contacted senior managers of major banks--many of whom had ongoing relationships with the Fed as the result of supervisory interactions--to discuss the situation. These discussions encouraged the banks, when it was consistent with appropriate risk management, to make credit available to securities firms, allowing them to make necessary payments and avoiding possible payment gridlock (SEC, 1988; Greenspan, 1994).\n\nThe Fed undertook similar discussions with other supervisors and with financial firms in response to the failure of Drexel Burnham Lambert in 1990 and the collapse of Long Term Capital Management (LTCM) in 1998. As the condition of Drexel deteriorated, other firms became less willing to trade with it, making it difficult to wind down its positions in an orderly manner (Breeden, 1990). Because of the Federal Reserve’s ongoing supervisory relationships with the main clearing banks and its detailed knowledge of the payments system, the Fed was able to address the banks’ concerns and facilitate the liquidation of Drexel’s positions (Greenspan, 1994). In the case of LTCM, the Federal Reserve had the credibility with large financial firms to facilitate a discussion, from which emerged a private-sector solution that helped to avoid potential market disruptions (Greenspan, 1998).8\n\nA general point that emerges from many of these examples is that financial crises and panics often involve problems of coordination and collective action (Diamond and Dybvig, 1983). The job of the crisis manager in such cases is to assist in solving these coordination problems and to help all parties arrive at a cooperative solution. The market information, the wide-ranging expertise, and the relationships that the Fed has developed in part through its supervisory activities have been invaluable in allowing the central bank to act as an \"honest broker\" in such situations.\n\nI have focused today on the benefits of the Fed’s supervisory role for its other central bank responsibilities, but the reverse is also true: The Fed’s non-supervisory activities help it to be a better supervisor, which in turn promotes financial stability. For example, in its conduct of monetary policy, the Federal Reserve monitors financial market developments closely and interacts regularly with the primary dealers in government securities. The knowledge thus gained has improved the Fed’s ability to understand and evaluate banks’ financial-market activities. The Fed’s capital-market expertise is also useful in the regulatory process. For example, the Federal Reserve made substantial contributions to the revisions to the Market Risk Amendment to the Basel I Capital Accord, under which the capital charge for banks’ trading assets is more closely tied to the market risk of their overall portfolios.\n\nConclusion\nHow best to preserve financial stability is a complex subject with a long history, and my remarks today have touched lightly or not at all on some central questions--among them, how to determine when financial events justify public action and how the responsibilities for maintaining financial stability should be allocated among private and public actors. Also, I should be clear that my purpose today is not to make a comprehensive case that our current regulatory structure is ideal. To the contrary, we should always be looking for ways to make supervision more effective and less burdensome. My point today is a narrower one: that the supervisory authority of the Fed has significant collateral benefits in helping it carry out its responsibilities for financial stability. In particular, the information, expertise, and powers that the Fed derives from its supervisory authority enhance its ability to contribute to efforts to prevent financial crises; and, when financial stresses emerge and public action is warranted, the Fed is able to respond more quickly, more effectively, and in a more informed way than would otherwise be possible.\n\nYogi Berra reminded us that prediction is very hard, especially about the future. In that spirit, the Federal Reserve continues to work actively to prepare for the possibility of financial stress. For example, we have created cross-disciplinary teams of experts--including staff drawn from bank supervision--to monitor financial developments and to consider possible crisis scenarios and the appropriate policy responses. We have worked with other agencies both here and abroad to improve our abilities to communicate and coordinate in a crisis situation. We also continue to take measures to ensure that our communications, information systems, and policy processes will remain viable should critical infrastructure be disrupted. The Federal Reserve has established a strong record of promoting financial stability, and we will continue to work to ensure the vibrancy and resilience of the U.S. financial system.\n\nReferences\n\nBernanke, Ben (2001). \"Comment,\" in Frederic Mishkin, ed., Prudential Supervision: What Works and What Doesn’t, Chicago: University of Chicago Press, pp. 293-297.\n\n__________ (2006). \"Hedge Funds and Systemic Risk,\" speech delivered at the Federal Reserve Bank of Atlanta’s 2006 Financial Markets Conference, Sea Island, Georgia, May 16.\n\nBernanke, Ben and Cara Lown (1991). \"The Credit Crunch,\" Brookings Papers on Economic Activity, 2:1991, pp. 205-39.\n\nBreeden, Richard (1990). Testimony before the Committee on Banking, Housing, and Urban Affairs, United States Senate, March 2.\n\nDiamond, Douglas and Philip Dybvig (1983). \"Bank Runs, Deposit Insurance, and Liquidity,\" Journal of Political Economy, vol. 91 (June), pp. 401-19.\n\nDixit, Avinash (1996). The Making of Economic Policy: A Transaction-Cost Politics Perspective. Cambridge, Mass.: MIT Press.\n\nFeldman, Ron J., Jan Kim, Preston Miller, and Jason E. Schmidt (2003). \"Are Banking Supervisory Data Useful for Macroeconomic Forecasts?\" The B.E. Journal of Macroeconomics, vol. 3 (Contributions), article 3.\n\nFerguson, Roger (2003). \"September 11, the Federal Reserve, and the Financial System,\" speech delivered at Vanderbilt University, February 5.\n\nFriedman, Milton and Anna Schwartz (1963). A Monetary History of the United States, 1867-1960: Princeton, NJ: Princeton University Press.\n\nGreenspan, Alan (1994). Testimony before the Committee on Banking, Housing, and Urban Affairs, United States Senate, March 2. Available in Board of Governors, Federal Reserve Bulletin, vol. 80 (May), pp. 382-85.\n\n__________(1998). Testimony before the Committee on Banking and Financial Services, U.S. House of Representatives, October 1.\n\nGoodhart, Charles (2000). \"The Organizational Structure of Banking Supervision,\" Occasional Papers, no.1, Basel Switzerland: Financial Stability Institute, November.\n\nGoodhart, Charles and Dirk Shoenmaker (1995). \"Should the Functions of Monetary Policy and Banking Supervision Be Separated?\" Oxford Economic Papers, vol. 47 (October), pp. 539-560.\n\nHaubrich, Joseph G. (1996). \"Combining Bank Supervision and Monetary Policy,\" Federal Reserve Bank of Cleveland, Economic Commentary, vol. 11, pp. 1-9.\n\nHaubrich, Joseph G. and James B. Thomson (2005). \"Umbrella Supervision and the Role of the Central Bank,\" Federal Reserve Bank of Cleveland, Policy Discussion Paper no. 11, December.\n\nMcDonough, William J. (1998). Statement before the Committee on Banking and Financial Services, U.S. House of Representatives, October 1.\n\nMishkin, Frederic (2000). \"What Should Central Banks Do? (101 KB PDF)\" Federal Reserve Bank of St. Louis Review, vol. 82 (November/December), pp. 1-14.\n\n__________ (2001). \"Prudential Supervision: Why Is It Important and What Are the Issues?\" in Frederic Mishkin, ed., Prudential Supervision: What Works and What Doesn’t, Chicago: University of Chicago Press.\n\nPeek, Joe, Eric S. Rosengren, and Geoffrey M. B. Tootell (1999). \"Using Bank Supervisory Data to Improve Macroeconomic Forecasts,\" Federal Reserve Bank of Boston, New England Economic Review, September/October, pp. 21-32.\n\nSecurities and Exchange Commission, Division of Market Regulation (1988). The October 1987 Market Break: A Report, Washington: The Commission, February.\n\nWall, Larry and Robert A. Eisenbeis (1999). \"Financial Regulatory Structure and the Resolution of Conflicting Goals,\" Journal of Financial Services Research, vol.16 (December), pp. 223-45.\n\nFootnotes\n\n1.  Charles Goodhart has suggested that separating monetary-policy and supervisory functions may have the benefit of eliminating the risk that poor supervisory performance could adversely affect the central bank’s reputation for competence, thereby affecting its ability to perform its other responsibilities, including monetary policy (Goodhart, 2000). See also Haubrich (1996). Return to text\n\n2.  Bernanke (2006) explains the usefulness of the \"indirect regulation\" approach to hedge funds. This approach avoids the costs of direct regulation by relying on the self-interest of hedge-fund counterparties and investors to exert market discipline on these organizations. Return to text\n\n3.  These institutions include the Depository Trust Company (a state member institution) and the CLS Bank International (an Edge Act corporation).  Return to text\n\n4.  In principle, legislation to give the Federal Reserve direct oversight authority over systemically important payments systems could reduce the Fed’s need to rely on its bank supervisory powers in this area. However, the Fed’s supervisory activities also provide it with information about payments systems from the vantage points of system users (that is, the banks that make and receive payments), providing useful additional perspectives both on the functioning of the payments systems themselves and on the risks to banks and other financial institutions that arise in the process of clearing and settling transactions. Return to text\n\n5.  Indeed, the Congress, in the FDIC Improvement Act of 1991, imposed a heightened level of scrutiny on lending to troubled institutions. In particular, if the Federal Reserve extends credit to a bank more than 5 days after the bank becomes critically undercapitalized or to an undercapitalized bank for more than 60 days out of a 120-day period, the Federal Reserve Board may be liable for a portion of any consequent increase in FDIC resolution costs [12 U.S.C. § 347b(b)]. In such situations, the Fed needs the capacity to evaluate whether such loans are appropriate--a task requiring considerable information and expertise on the operation of troubled banks and the resolution process. Return to text\n\n6.  The first sentence of the Federal Reserve Act assigned the new central bank the responsibilities of furnishing an elastic currency, affording a means of rediscounting commercial paper, and establishing more effective supervision of banking. Each of these responsibilities was closely tied to the goal of preventing or ameliorating financial panics; in particular, the furnishing of an elastic currency and the rediscounting of commercial paper both involved the provision of liquidity to the financial system as needed to avoid seasonal or other financial stresses. Return to text\n\n7.  For example, the Federal Reserve has supported, and sometimes helped to craft, clarifications of the legal framework that underpins financial contracts, which are critical when contracts must be unwound in a crisis. In the regulatory and supervisory areas, the Fed has helped to develop risk-sensitive bank capital standards and encouraged banks to continue upgrading their methods for measuring and managing risk. Return to text\n\n8.  In both the Drexel and LTCM episodes, the Federal Reserve initially detected the firms’ problems through either market surveillance or its interactions with primary dealers, rather than through banking supervision. Once the problems were known, considerable information came from the troubled firms themselves (Breeden, 1990; McDonough, 1998). However, that information was supplemented, and to an extent validated, with information received from banking institutions under the Federal Reserve’s supervisory authority. In contrast, in the 1987 crash, when the crisis was not centered on a single firm but instead involved widespread concerns about securities firms’ access to bank credit, the Fed placed greater reliance on information obtained through its supervisory authority. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20070105a.htm",
        "title": "Central Banking and Bank Supervision in the United States",
        "date": "1/5/2007"
    }
]