[
    {
        "content": "December 15, 2006\n\nChairman Ben S. Bernanke\n\nAt the Chinese Academy of Social Sciences, Beijing, China\n\nThe emergence of China as a global economic power is one of the most important developments of recent decades. For the past twenty years, the Chinese economy has achieved a growth rate averaging nearly 10 percent per year, resulting in a quintupling of output per person. In overall size, China's economy today ranks as the fourth largest in the world in terms of gross domestic product (GDP) at current exchange rates and the second largest when adjustments are made for the differences in the domestic purchasing power of national currencies. This strong economic performance has resulted in improved living standards for the Chinese people. By some estimates, about 200 million Chinese have been brought out of poverty since the reforms began in 1978.1 Moreover, by 2004 life expectancy at birth in China had reached seventy-one years, the infant mortality rate had fallen to 26 per 1,000 live births, and the literacy rate of those aged fifteen or above had reached 90 percent.2 These are remarkable accomplishments.\n\nNonetheless, by most measures China remains a developing nation.3 In particular, although life in some urban centers is typical of a modern, affluent society, average household incomes and consumption remain quite low in rural and inland areas. Thus, China faces the double challenge of sustaining a high and stable overall rate of economic growth while stimulating economic development in parts of the country that have shared less fully in the economic boom. In my remarks today, I would like to offer a few thoughts on how China can continue to prosper and promote the economic welfare of its people.\n\nEconomic progress: Markets and productivity\nEconomists agree that the most important determinant of living standards in a country is the average level of productivity, or output per worker. On this score as well, China's record in recent decades is excellent. Between 1978, when the Open Door policy reforms began, and 1989, output per employed person in China grew vigorously, at an estimated average rate of about 6-1/2 percent per year. However, from 1990 to 2005, productivity grew at an even more impressive rate of 9 percent per year.4\n\nMany factors have contributed to this strong productivity performance, including high rates of capital investment, increasing openness to trade, and a strengthening of the educational system.5 However, in my view, the single most important cause of the ongoing expansion in productivity is that China has moved, gradually but steadily, away from central planning and toward a greater reliance on markets. In 1978, almost no prices in China were determined by the market, and most production was controlled or directed by the state. Since then, the government has reduced its direct intervention in the economy and scaled back state-owned enterprises, in the process allowing more scope for market forces. By 1999, according to one estimate, about 95 percent of retail business, together with more than 80 percent of trade in agricultural commodities and producer goods, was conducted at market-determined prices.6\n\nSubstantial experience has shown that modern economies, including those in early stages of development, are too complex to be managed effectively on a centralized basis. Prices set in free and competitive markets serve several critical functions, among them aggregating disparate information about supply and demand conditions and the relative scarcities of specific goods and services; directing resources to their most productive uses; and providing incentives to engage in cost reduction, innovation, and entrepreneurial activities.\n\nA free market for labor is particularly critical for sustained economic development. China has made substantial progress in this area over the past few decades, most notably by reducing barriers to the movement of workers among regions, sectors, and firms and by allowing greater flexibility in the determination of employment and wages.7 Indeed, the ongoing movement of workers from relatively low-productivity, low-wage jobs in agriculture to higher-productivity, higher-wage jobs in manufacturing and services has been a significant source of Chinese economic growth. The decline in the share of the population in rural areas, from more than 80 percent in 1970 to less than 60 percent in 2005, indicates the scale of the movement of labor out of agriculture in recent decades.\n\nDespite these shifts, differences in labor productivity among sectors remain large. For example, in 2005, estimated output per worker in agriculture and related sectors was about $800, whereas in industries such as manufacturing, utilities, and mining, output per worker was about $5,900, more than seven times as much.8 Moreover, a considerable portion of China's labor force (specifically, in agriculture and in inefficient state-owned enterprises) remains underutilized. Thus, substantial additional gains in productivity for the economy as a whole might be realized through the further reallocation of the labor force to more productive and growing sectors. In particular, small- and medium-sized enterprises are emerging as an engine of job creation in China--as they are in the United States--even as they promote innovation and help to create a more dynamic and diversified economy. The government can support the process of reallocating labor to more productive uses by continuing to reduce barriers to labor mobility, helping workers obtain the education and training they need to be productive in new occupations, and encouraging entrepreneurship and small-business development.\n\nAs significant as the reallocation of labor among sectors has been, more of the improvement in productivity in recent years has resulted from increasing efficiencies within the major sectors rather than from the reallocation of resources between sectors. Here again, markets and competition have played a vital role. In particular, the opening of the economy to international trade and investment, which has accelerated since China's accession to the World Trade Organization in 2001, has done much to harness market forces in the service of the country's development. Exposure to the competition of the global marketplace has forced Chinese producers--alone or in partnership with foreign firms--to increase their efficiency and improve the quality of their output. Notably, globally-engaged firms (or their affiliates) operating in China have helped to foster productivity growth in the country by introducing new technologies and managerial techniques, as well as by enhancing domestic competition.\n\nAlthough the expansion of the market-based portion of the economy has yielded impressive results, further gains could be achieved by allowing still greater scope for market forces. The energy sector presents one such opportunity. As you know, China's appetite for energy has grown rapidly: China's consumption of oil has risen by more than 50 percent since 2000, and the International Energy Agency estimates that Chinese oil usage has increased by about 400,000 barrels per day in 2006, representing nearly half of this year's growth in world oil demand. This rapid expansion in energy use reflects both overall economic growth and a relatively energy-intensive pattern of development. Greater use of market pricing in the energy sector, including the elimination of remaining price controls on fuels and the liberalization of electricity prices, would support cleaner, sustainable economic growth by promoting more-efficient use of energy by households and firms and by encouraging the development of new energy supplies.9\n\nSignificant benefits could also be obtained by allowing a larger role for market forces in guiding investment decisions. China's economic growth owes much to the extraordinary share of GDP that is devoted to investment in new capital, such as factories, equipment, and office buildings, which is partly financed by a very large amount of business saving.10 However, the rapid pace of investment growth raises concerns about whether new capital is being deployed in the most productive ways. In particular, some analysts have questioned whether China is getting an adequate return on its investment. For example, from 1990 to 2001, fixed investment as a share of GDP in China averaged about 33 percent, and the economy grew at an annual rate of 10 percent. Between 2001 and 2005, fixed investment's share of GDP rose to about 40 percent, but the economy's average growth rate remained about the same, suggesting a lower return to the more recent investment. Comparisons can also be drawn to the rapid development phases of other Asian countries, such as South Korea and Japan. Average annual growth was between 9 and 10 percent in South Korea during the 1982-91 period and in Japan during the 1955-70 period; but for both countries during the relevant years investment's share of GDP was about 30 percent, lower than it is in China today. In a few Chinese industries, heavy investments have continued even as signs of excess capacity have begun to appear--another possible indication of capital misallocation. An example is the steel industry, in which excess capacity appears to have tripled between 2002 and 2005 to reach more than 115 million metric tons.11 Allowing competitive financial markets to play a larger role in the allocation of capital would likely increase the returns to investment and reduce the risk that uneconomic investments could exacerbate the problem of nonperforming loans and contribute to future financial instability. Basing investment decisions on market signals also takes better account of the costs of inputs complementary to capital, such as labor (relatively abundant in China) and energy (relatively scarce).12\n\nChina has taken initial steps toward a greater reliance on markets for determining the allocation of investment--for example, by authorizing and beginning to liberalize the stock market. China has also strengthened its banking system by improving supervision, confronting the enormous problem of nonperforming loans, allowing domestic institutions to partner with foreign banks, and increasing the use of market-based criteria in bank lending. These trends are positive; however, a great deal more remains to be done, including broadening the range of financial instruments available to savers and borrowers; taking further steps to ensure that credit evaluation and extension are based on sound economic criteria; increasing competition in banking and finance; improving credit availability for consumers and smaller firms; removing the remaining controls on interest rates; and eliminating the use of quantitative and administrative measures to influence the amount and composition of capital investment. Finally, capital markets require an appropriate institutional foundation to function effectively: Well-defined property rights (including intellectual property rights), transparent accounting standards, good corporate governance, effective supervisory oversight of banks and markets, the consistent enforcement of contracts, and rules that allow for orderly bankruptcy proceedings for unprofitable firms all help to support efficient investment. China has made progress in developing these critical institutions, but continued focus on these areas would provide large economic benefits in the long run.\n\nMacroeconomic policy\nMoving from microeconomics to macroeconomics, I believe that China could benefit by improving its tools for managing the economy--notably, monetary and fiscal policies. Effective use of macroeconomic policy tools can help achieve low and stable inflation and increase economic stability by moderating the effects on growth of temporary fluctuations in global or domestic demand. Stable economic conditions reduce an important source of risk and consequently promote innovation and growth. As a central banker, I will focus my remarks on the development of monetary policy.13\n\nThe People's Bank of China (PBOC), the nation's central bank, is capable and well-respected around the world. However, monetary policy can work well only to the extent that financial markets are sufficiently developed to allow the monetary authorities' interest-rate decisions to affect economic activity in a reasonably predictable way. The development of a reliable \"monetary transmission mechanism\" is thus another reason to reform and strengthen China's banks and financial markets. The more interest-sensitive that borrowing and investment decisions are, the more monetary policy can affect investment and aggregate demand without the need for quantitative controls. Moreover, when funds are allocated on market principles, only those projects with an expected risk-adjusted return that is higher than the relevant market rate of interest will be undertaken. Thus, for example, if the central bank raises interest rates, the market will ensure that the resulting slowing of investment will be borne by the least promising projects, without the need for officials to make such judgments.\n\nThe effectiveness of monetary policy would also be enhanced by greater flexibility in the exchange rate. To maintain the current close link of the renminbi (RMB) to the dollar in the presence of capital inflows (arising from China's trade surplus or from foreign purchases of RMB-denominated assets), the PBOC must intervene in the exchange market to buy dollars with RMB. Increases in the domestic money supply will result unless the central bank offsets the effects of these purchases on the money supply by selling bonds to investors, primarily commercial banks, in exchange for RMB--a procedure commonly referred to as \"sterilization.\" If dollar purchases by the central bank were not routinely sterilized, the money supply might increase more than desired, possibly leading to an overheating of the economy and inflation.\n\nTo date the PBOC has been largely successful in its sterilization operations, but if it continues to use this strategy it will eventually encounter problems. If the exchange rate and the domestic interest rate are maintained near their current levels, the incentives for capital inflows--and thus the need for continued intervention and sterilization--will remain undiminished. Accordingly, the value of outstanding sterilization bonds will continue to grow rapidly. These bonds could substitute for other assets that might be held by banks and private investors, potentially interfering with the growth and development of private financial markets. Further, the perception that the RMB is undervalued (a point to which I will return) has fueled additional capital inflows, as investors expect to earn capital gains when the RMB ultimately appreciates. However, these speculative inflows only increase the need for exchange-market intervention and sterilization.\n\nIn the longer term, the continued growth and modernization of the Chinese economy will require a substantial loosening of current restrictions on the flow of financial capital into and out of the country. However, if fluctuations in the value of the RMB remain limited within a narrow range, permitting substantial capital mobility would almost entirely eliminate the PBOC's capacity to use monetary policy to stabilize the domestic economy, as any excess of Chinese over dollar interest rates, for example, would trigger large capital inflows. Further appreciation of the RMB, combined with a wider trading band and with the ultimate goal of a market-determined exchange rate, would allow an effective and independent monetary policy and thereby help to enhance China's future growth and stability.\n\nFrom an institutional perspective, China may find--as many countries have done-- that granting greater autonomy to the central bank, by insulating it to a degree from short-term political concerns, increases its ability to ensure price stability and support stable growth. Of course, central banks must remain accountable to governments over the longer term.\n\nTrade, capital flows, and the transition to domestically-led growth\nA central component of China's development strategy has been its openness to trade and capital inflows. For example, the value of goods exports and imports currently equals about two-thirds of China's GDP, a high level for a country of China's geographic size. China's accession to the World Trade Organization (WTO) has been particularly effective in stimulating trade: Since joining the WTO in 2001, China has seen the dollar value of its exports grow at an average rate of about 30 percent per year, compared with annual growth of about 12-1/2 percent over the five years before gaining WTO membership. China currently accounts for about 7-1/2 percent of world exports, up from 1-1/2 percent in 1985. About half of China's trade is so-called processing trade; that is, components and materials are imported (mostly from other Asian countries), processing or assembly is done in China, and the finished product is re-exported. Much economic research has shown that openness to trade promotes growth.14 Trade supports growth not only through the classical principle of comparative advantage, but also through the effects of trade on the intensity of market competition, transfers of technology and knowledge, and other factors.\n\nChina has also proved successful in attracting capital inflows, particularly foreign direct investment (FDI). The flow of FDI into China increased from about $2 billion in 1986 to $72 billion in 2005, making the country the third largest recipient of FDI in the world, after the United Kingdom and the United States.15 FDI benefits China's development by bringing with it new technologies, products, and business methods.16 The flow of foreign investments in Chinese securities has also increased significantly, reaching $22 billion in 2005.\n\nAs you know, China today is running substantial trade and current account surpluses. These external surpluses are caused in part by China's remarkably high saving rate.17 Because China's national saving rate is even higher than its rate of domestic investment, the country has excess funds to lend in the global capital market; it follows from the balance-of-payments accounts that China's net lending abroad (or its acquisition of foreign assets) equals the country's current account surplus. A large portion of this lending finances foreigners' purchases of Chinese net exports (the trade surplus). High household saving and the corresponding low level of consumption in China contribute to the trade surplus by depressing the demand for imports and by forcing domestic firms to look abroad for markets.\n\nTogether with the large trade and current account deficits of the United States, the Chinese external surpluses are contributing to the current pattern of global imbalances, which many economists and policymakers have argued are unsustainable in the long run. The Chinese leadership has recognized the importance of encouraging imports and achieving greater balance in China's international trade, having recently included these objectives in the Five-Year Plan for 2006-2010. In my view, not only would China's achieving smaller external balances contribute to global financial stability, it would also be in China's economic interest.\n\nAlthough China's extensive participation in the global trading and financial systems has been invaluable for the country's development, the ultimate purpose of economic growth is to improve living standards at home. Today, about half of China's GDP is devoted to investment and to producing net exports for the rest of the world, and thus only the remaining half is available for consumption, including government consumption. In particular, household consumption in China last year was only 38 percent of GDP, down from 45 percent in 2001. In comparison, household consumption was about 60 percent of GDP in India in 2004, according to the most recent available data. China's low share of consumption in GDP is, of course, the counterpart of its high national saving rate.\n\nPolicies aimed at increasing household consumption would clearly benefit the Chinese people, notably by improving standards of living and allowing the fruits of economic development to be shared more widely. Such policies, by reducing saving and increasing imports, would also serve to reduce China's current account and trade surpluses.18 Putting greater reliance on domestic demand rather than net exports to drive growth would also decrease China's vulnerability to fluctuations in global demand.\n\nHow can China direct a greater share of its output to domestic consumption? Again, increased flexibility in the exchange rate could help. As the Chinese trade surplus has continued to widen, many analysts have concluded that the RMB is undervalued.19 Indeed, the situation has likely worsened recently; because of the RMB's link to the dollar, its trade-weighted effective real exchange rate has fallen about 10 percent over the past five years.20 Allowing the RMB to strengthen would make imports of consumer goods (as well as capital goods) into China less expensive. Greater scope for market forces to determine the value of the RMB would also reduce an important distortion in the Chinese economy, namely, the effective subsidy that an undervalued currency provides for Chinese firms that focus on exporting rather than producing for the domestic market. A decrease in this effective subsidy would induce more firms to gear production toward the home market, benefiting domestic consumers and firms. Reducing the implicit subsidy to exports could increase long-term financial stability as well: If China invests too heavily in export industries whose economic viability depends on undervaluation of the exchange rate, a future appreciation of the RMB could lead to excess capacity in those industries, resulting in low returns and an increase in nonperforming loans.\n\nAlthough more flexibility in the exchange rate would be helpful, the most direct and probably the most effective way to reduce the external surpluses and increase the welfare of Chinese households is to take measures to reduce domestic saving relative to domestic investment.21 Why is domestic saving so high at present? The high saving rate of households, even very poor households, likely reflects the relatively thin \"social safety net\" in China. For example, only about 14 percent of the population is covered by health insurance, and pension plans (which, in any case, replace only about 20 percent of pre-retirement earnings) apply to only about 16 percent of the economically active population.22 Combined expenditures by the central government and local governments on education, health, pensions and relief, and social security amount to only about 4 percent of GDP, lower than most other countries at similar income levels. In the absence of a stronger social safety net, Chinese households save at high rates to protect themselves against risks such as unexpected medical expenses and poverty in old age.\n\nA sustained program of expanding social services has the potential for reducing saving and raising living standards in China and, at the same time, moderating China's external surpluses. In particular, increased government spending on health, education, and other types of social services would raise both household consumption and government consumption, and thus reduce national saving. As one means of helping to fund the increase in social expenditures, state-owned enterprises could be required to pay larger dividends to the government, a measure which I understand to be currently under consideration. Financial reforms that increase the access of households to mortgages, private insurance, and other forms of consumer finance would also support higher rates of consumption.\n\nAs I have noted, the Chinese leadership has made reducing the trade and current account surpluses an important policy objective. Further, recognizing that achieving these goals would be greatly facilitated by increases in consumption and in the social safety net, the government has instituted or proposed various reductions in taxes and increases in social spending. The government has also announced an effort to raise living standards in rural and inland areas. Obviously, it is too early to judge the success of these initiatives. However, these efforts are constructive and deserving of support.\n\nConclusion\nAs I have discussed today, China has made remarkable economic progress since the reforms began in 1978. Greater use of markets, together with expanding trade and capital flows, has helped to increase growth in output and productivity, which in turn has increased living standards significantly. A still-greater reliance on markets, particularly for allocating capital investment, would contribute to a continuation of strong economic growth. Further development of macroeconomic policy tools would also support healthy growth by keeping inflation low and by increasing economic stability.\n\nChina also faces some significant risks and imbalances. The principal risk arises from the likelihood that capital is not being allocated as efficiently as possible, the result of an undervalued exchange rate and of capital markets that, despite positive steps, remain distorted and underdeveloped. Misallocated capital will not pay the highest possible returns, leading potentially to slower growth and future financial stress. In addition, monetary policy may be constrained by the lack of a reliable monetary transmission mechanism and by the relative inflexibility of the exchange rate, which inhibit the central bank's ability to keep inflation low and to stabilize the economy.\n\nThe principal imbalance lies in the composition of Chinese GDP, which is heavily tilted toward investment and net exports and away from domestic consumption and government provision of social services. A better balance would improve the welfare of the Chinese people (by increasing consumption and social services) and would increase global financial stability (by reducing China's current account and trade surpluses). The Chinese government has undertaken to reduce this imbalance by taking measures to expand domestic demand, especially household consumption, and I hope that these efforts will be successful.\n\nChina's development and its opening up to the global economy have also benefited the United States in many ways. China is now the second-largest source of U.S. imports; these imports boost U.S. real incomes by allowing U.S. households to purchase consumption goods and U.S. firms to purchase intermediate inputs at lower cost. China is also a growing market for investment and exports by U.S. firms. Since China joined the WTO, U.S. exports to China have more than doubled. As China develops further, its households and firms will demand a greater variety of goods and services, enhancing opportunities for producers in industrial countries, including the United States. At the same time, trade--with all its benefits--does displace some workers and firms as patterns of production and consumption change. The policy challenge for the United States is to help those who are adversely affected by trade while preserving the broad gains that openness to trade provides for the economy as a whole.\n\nThe economic relationship between China and the United States is of extraordinary importance, and both countries have much to gain from interactions with each other. Serious challenges exist as well, requiring both countries to address such areas as energy, the environment, intellectual property rights, and global imbalances. Regarding global imbalances, I have discussed means by which China can reduce its contribution to the imbalances while encouraging domestic consumption. The United States must also do its part, in particular by increasing its own rate of national saving and by avoiding protectionism. With greater integration come greater interdependence and greater responsibility. I hope that our countries will work together in a spirit of cooperation to address these shared challenges.\n\nReferences\n\nAnderson, Jonathan (2006). \"The Complete RMB Handbook,\" 4th ed., UBS Investment Research, Asian Economic Perspectives, September.\n\nBergsten, Fred C., Bates Gill, Nicholas R. Lardy, and Derek Mitchell (2006). China, The Balance Sheet: What the World Needs to Know Now about the Emerging Superpower. New York: PublicAffairs.\n\nBrooks, Ray (2004). \"Labor Market Performance and Prospects,\" in Eswar Prasad, ed., China's Growth and Integration into the World Economy: Prospects and Challenges. Washington: International Monetary Fund, pp. 51-61.\n\nDunaway, Steven, Lamin Leigh, and Xiangming Li (2006). \"How Robust are Estimates of Equilibrium Real Exchange Rates: The Case of China,\" IMF Working Paper 06/220. Washington: International Monetary Fund, October.\n\nFrankel, Jeffrey A., and David Romer (1999). \"Does Trade Cause Growth?\" American Economic Review, vol. 89 (June), pp. 379-99.\n\nGoldstein, Morris (2004). \"Adjusting China's Exchange Rate Policies,\" paper presented at the International Monetary Fund's Seminar on China's Foreign Exchange System, held in Dalian, China, May 26-27.\n\nGoodfriend, Marvin, and Eswar Prasad (2006). \"A Framework for Independent Monetary Policy in China,\" IMF Working Paper, 06/111. Washington: International Monetary Fund, May.\n\nHoffman, Bert, and Louis Kuijs (2006). \"Profits Drive China's Boom,\" Far Eastern Economic Review, vol. 169 (October), pp. 39-43.\n\nKuijs, Louis (2005). \"Investment and Saving in China,\" World Bank Policy Research Paper 3633. Washington: World Bank, June.\n\n__________ (2006). \"How Will China's Saving-Investment Balance Evolve?\" World Bank Policy Research Paper 3958. Washington: World Bank, July.\n\nLardy, Nicholas R. (2002). Integrating China into the Global Economy. Washington: Brookings Institution Press.\n\n___________ (2006). \"China: Toward a Consumption-Driven Growth Path,\" Policy Briefs in International Economics, P806-6. Washington: Institute for International Economics, October.\n\nNational Bureau of Statistics of China (2006). China Statistical Yearbook, 25th ed., China Statistics Press.\n\nSachs, Jeffrey D., and Andrew M. Warner (1995). \"Economic Convergence and Economic Policies,\" Brookings Papers on Economic Activity, 1995:1, pp.1-95.\n\nShan, Weijian (2006). \"The World Bank's China Delusions,\" Far Eastern Economic Review, vol. 169 (September), pp.29-32.\n\nWacziarg, Romain (2001). \"Measuring the Dynamic Gains from Trade,\" World Bank Economic Review, vol. 15 (September), pp.393-429.\n\nWorld Bank (2006). \"On-Line China Country Profile,\" World Development Indicators, www.devdata.worldbank.org/wdi2006/contents/home.htm.\n\nZebregs, Harm (2002). \"Foreign Direct Investment and Output Growth,\" in Wanda Tseng and Markus Rodlauer, eds., China: Competing in the Global Economy. Washington: International Monetary Fund, pp. 89-100.\n\nFootnotes\n\n1.  Bergsten and others ( 2006), p. 3. Return to text\n\n2.  World Bank (2006).  Return to text\n\n3.  For example, according to 2005 data from the International Monetary Fund's World Economic Outlook database, China's per capita GDP is about one-sixth of that of the United States on a purchasing-power-parity basis and about one twenty-fifth that of the United States at current exchange rates.  Return to text\n\n4.  Productivity growth rates are calculated by Federal Reserve Board staff using Chinese data on output and employment. Productivity can be difficult to measure, and so these figures should be treated as approximate. A new method for surveying employment was introduced in 1990, resulting in a break in the series in that year. Return to text\n\n5.  See, for example, chapter 2 of Bergsten and others (2006) for a discussion of why China has grown so rapidly.  Return to text\n\n6.  Lardy (2002), p. 25.  Return to text\n\n7.  Brooks (2004) provides details of some of the labor market developments in China in recent years. Return to text\n\n8.  Calculations use current exchange rates and are based on output and employment data from the National Bureau of Statistics of China and the Chinese State Statistical Office; most data from Chinese national sources are obtained from the CEIC database. Return to text\n\n9.  The adverse effects on poor households of eliminating price controls on fuels would best be dealt with through direct cash grants. Return to text\n\n10.  World Bank researchers have concluded that business saving now finances the majority of business investment in China (Kuijs, 2005). Some have questioned this (Shan, 2006). For a discussion of the issues involved in the debate, see Hoffman and Kuijs (2006). Return to text\n\n11.  Based on production and capacity data provided by the National Bureau of Statistics. Return to text\n\n12.  Market-based allocations of capital do not take into account environmental and other external effects, unless additional mechanisms such as tradable pollution permits are employed. Return to text\n\n13.  Goodfriend and Prasad (2006) provides a good description of the banking and financial systems of China and of issues regarding the implementation of monetary policy. Return to text\n\n14.  See, for example, Sachs and Warner (1995), Frankel and Romer (1999), and Wacziarg (2001). Return to text\n\n15.  In 2005, the United Kingdom surpassed both China and the United States as the largest recipient of FDI in the world. In 2004, China ranked second after the United States. Return to text\n\n16.  For a discussion of the relationship between FDI and economic growth in China, see Zebregs (2002). Return to text\n\n17.  China's national saving rate was 41 percent in 2003 and 47 percent in 2005, according to IMF figures. As of 2003, the latest date for which detailed data are available, households accounted for 42 percent of domestic saving, with firms (including state-owned enterprises) and government accounting for 36 percent and 22 percent of domestic saving, respectively (National Bureau of Statistics of China, 2006). Return to text\n\n18.  See Lardy (2006), for example, for a careful discussion of the need for reorientation of Chinese growth toward consumption.  Return to text\n\n19.  See, for example, Goldstein (2004) and Anderson (2006). Dunaway and others (2006) provide a useful analysis of the sensitivity of the Chinese equilibrium real exchange rate to alternative approaches and assumptions commonly used. Return to text\n\n20.  The estimate is by Federal Reserve Board staff for the period from November 2001 to November 2006. Return to text\n\n21.  Kuijs (2006) documents the sources and composition of saving and investment in China.  Return to text\n\n22.  Bergsten and others (2006), p.28. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20061215a.htm",
        "title": "The Chinese Economy: Progress and Challenges",
        "date": "12/15/2006"
    },
    {
        "content": "December 01, 2006\n\nChairman Ben S. Bernanke\n\nAt the Fourth Conference of the International Research Forum on Monetary Policy, Washington, D.C.\n\nVice President Papademos, ladies and gentlemen, I would like to welcome you to the fourth conference of the International Research Forum for Monetary Policy. The forum is one outgrowth of the increased interaction between central banks and academic institutions that, in my opinion, benefits both groups. It is a true joint effort involving the European Central Bank, the Federal Reserve Board, the BMW Institute for German and European Studies at Georgetown University, and the Centre for Financial Studies at Goethe University in Frankfurt.\n\nYou have a very full agenda for the next two days. The topics represent a good mix of theoretical work as well as empirical work based on both calibration and econometric estimation. You are beginning with two papers that emphasize the importance of inflation expectations. Other papers examine such topics as the application of search theory, the functioning of mortgage markets, the theoretical analysis of optimal monetary policy, and empirical research on price-setting.\n\nThere is a lot of intellectual food on the table. Bon appetit!",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20061201a.htm",
        "title": "Welcoming remarks",
        "date": "12/1/2006"
    },
    {
        "content": "December 01, 2006\n\nVice Chairman Donald L. Kohn\n\nAt the Fourth Conference of the International Research Forum on Monetary Policy, Washington, D.C.\n\nTonight I will talk about one of the themes of this conference: uncertainty and its influence on the monetary policy process. Policymakers always face an uncertain economic environment, and from time to time I think it is useful to review the nature of the uncertainties we face and the prescriptions for dealing with them. Both of these tend to evolve over time, and we may find some lessons--or at least subjects for further research--in recent experience. Of course, the views I express tonight are my own and do not necessarily reflect the opinions of my fellow members of the Federal Open Market Committee (FOMC).1\n\nWhat are the basic sorts of uncertainty faced by central banks? In informal terms, we are uncertain about where the economy has been, where it is now, and where it is going. In gauging the past and current state of the economy, measurement difficulties are rife and so I will review some of the challenges that we face in that area. As for where the economy is headed, central banks confront many sources of uncertainty but tonight I will focus on one in particular, namely, our inadequate understanding of the public’s expectations. Finally, I will conclude with a few observations on the ways that central bankers cope with risk in its various forms.2\n\nMeasurement Uncertainty\nAn important source of our uncertainty about the recent past and the current state of the economy is that economic data typically come in with a considerable lag and are subject to substantial measurement errors and revision. Work by Athanasios Orphanides and other economists has helped to heighten economists’ awareness of this issue by exploring the extent to which faulty estimates of potential output may have contributed to the monetary policy errors of the 1970s.3 However, academic economists may still not fully appreciate the degree to which measurement uncertainty bedevils policymaking. These difficulties are especially pronounced at times like the present, when resource utilization and the rate of economic growth are probably not far from their long-run potential, inflation trends may be shifting, and policy interest rates are close to their historical averages in real (that is, inflation adjusted) terms.\n\nConsider our estimates of real economic activity. These estimates often change markedly with the receipt of just a few more days or weeks of data. For example, the Bureau of Economic Analysis released revised estimates in late July that showed persistently slower growth in real gross domestic product (GDP) in recent years. These more pessimistic data were then followed over the balance of the summer and into the fall by stronger-than-expected readings on current labor market conditions as well as by the announcement of upcoming benchmark revisions that will raise the level of payroll employment 1/2 percent. Taken together, these revisions have had important implications for our estimates of employment, productivity, labor costs, and related statistics.\n\nPrice data are subject to several measurement problems besides the well-known issues of quality changes and appropriate weights. For example, a significant portion of the personal consumption expenditures (PCE) price index is based on imputations of prices for important categories of household purchases, such as banking services, rather than on direct observations of market prices. This \"nonmarket\" component of the index is hard to replicate, tends to move in an erratic manner from month to month, and is subject to considerable revision--factors that reduce the usefulness of the overall index as a short-run indicator of price pressures.\n\nMeasures of labor compensation pose their own special problems. To begin, the available indicators often do not tell a consistent story. For example, the data in hand last week showed hourly compensation rising almost 7 percent over the past four quarters based on the national accounts measure, but only 3 percent as measured by the employment cost index. Beyond this, the compensation figures in the national accounts are subject to significant revision, as illustrated by the release of new data this week that suggests hourly compensation rose only 4-1/2 percent, not 7 percent, over the past year. Changes such as this make real-time estimates of unit labor costs and labor’s share of total income much less useful in our analyses than studies based on revised data might suggest. Finally, the existing wage data are not well suited for measuring certain concepts important to modeling and policymaking, such as marginal labor costs. For example, hourly compensation in the national accounts includes stock options at their exercise value rather than at their value at the time of issuance.\n\nPerhaps the most intractable problems surround the measurement of such key concepts as the equilibrium real interest rate, trend productivity, and potential output. We never observe these variables, which often figure prominently in our deliberations, but can only infer them from the behavior of other variables that are themselves subject to mismeasurement. As I just hinted, recent revisions to GDP and to labor input would seem to point to downward adjustments to estimates of trend productivity, but sorting out trend from cycle in the new data has been a challenge. These revisions may or may not also have implications for the level of the real federal funds rate consistent with longer-run macroeconomic stability. I will return later to the policy implications of this sort of measurement uncertainty.\n\nExpectations Uncertainty\nExpectations--which are critical to the decisions of households and firms--are an area in which measurement problems are compounded by questions about the behavior of private agents and hence of the economy. We have only limited information on households’ views of their income prospects or firms’ beliefs about their future sales. For example, in the United States we have some survey information on household expectations for their financial situation and their labor market prospects. These expectations are undoubtedly important in households’ estimates of their permanent income and hence in determining aggregate demand; indeed, econometric analysis suggests that these survey measures can help in predicting consumer spending. Nonetheless, our understanding of movements in household perceptions of permanent income is limited by a general paucity of data and associated research. Similar difficulties arise when considering firms’ assessment of future demand. Although we have some information on expected conditions from various surveys and from the earnings guidance provided by publicly traded companies, these indications are mostly qualitative, the quality is mixed, and research has not clarified their link to aggregate economic prospects.\n\nIn the case of inflation expectations, we do have a larger number of indicators at our disposal. Yet here, too, the reliability and usefulness of the existing data are less than we might like. For example, survey measures of households are based on small samples. In addition, household expectations do not refer to any specific index and focus on time horizons that may not correspond to those relevant for wage bargaining or financial planning. Moreover, the wide dispersion of views across households strongly suggests varying levels of sophistication in forming expectations, to a degree that raises questions about the link between measured expectations and behavior relative to common assumptions, at least for some households.\n\nMeasures of inflation compensation derived from nominal and indexed Treasury yields provide information that addresses some of the weaknesses in survey measures. For example, investors have strong incentives to ensure that inflation compensation reflects their beliefs about the prospects for a specific index, the consumer price index (CPI), over a fixed time horizon. Even here, however, we encounter important technical difficulties: These inflation compensation measures are \"contaminated\" both by an inflation risk premium and by differences in liquidity between the markets for nominal and indexed Treasury securities. More fundamentally, even a \"rational\" forecast of inflation from financial markets provides only part of the information needed to form monetary policy because it gives only a sense of where inflation is expected to go, not why it is going there. The latter question is often important for assessing the appropriate stance of policy.\n\nOf course, asset prices are a category for which expectations are extremely important and for which data are available on a large scale. The term structure of interest rates, the spread between private and public yields on debt, equity prices, and the exchange value of the dollar--to name a few--are of first-order macroeconomic importance and are directly related to expectations. But we still face the challenge of distinguishing the quantitative role of, say, time-varying term and risk premiums on the one hand from that of expectations (including any speculative component) regarding underlying fundamentals on the other. As an example, consider house prices. Some commentators have suggested that, over the past several years, households extrapolated previous gains in house prices in thinking about the likely return to real estate and, in doing so, created a speculative bubble that pushed home prices significantly above their \"fundamental\" level.4 However, others have argued that the rapid rise in home prices was fully justified by strong income growth and low interest rates.5 Distinguishing between these alternatives would be aided both by better measures of households’ expectations regarding the appreciation of their homes and by a better grasp of the determinants of those expectations. As I have noted elsewhere, our lack of understanding of the dynamics of asset price determination is a significant hurdle to giving them extra weight in setting monetary policy.6\n\nMuch of my discussion regarding expectations has so far focused on measurement issues, but the question of why economic conditions unfold as they do also raises a critically important question: How are expectations formed? The baseline assumption used in much research is that expectations are rational, in the sense that private agents use a fixed and known model of the economy to process all relevant information. This assumption is extremely useful because it is a benchmark that facilitates comparisons with other hypotheses about expectations formation, and it allows various questions to be considered without an extraneous focus on expectations. But this form of rational expectations seems to be of limited usefulness when the question at hand is the evolution of expectations and their effect on activity and inflation. For example, rational expectations models will often rule out the possibility that learning errors in households’ expectations of future labor market conditions can have an independent effect on aggregate demand. And, these models usually simply assume that \"irrational\" movements in asset prices are not an important factor in the macroeconomic outlook.\n\nOf course, research has led to some relaxation in the baseline assumption of rational expectations. One prominent example is the work in behavioral finance on how alternative assumptions regarding rationality can affect predictions for asset prices and saving behavior; another is the growing literature on the interaction of learning, inflation dynamics, and monetary policy. Nonetheless, this research has only begun to investigate how households and firms actually form their expectations, and the models we use for policy analysis, at most, only crudely embed the early lessons from this literature. As a result, uncertainty over how best to model expectations and hence how best to model the aggregate economy remains a central concern of policymakers.\n\nTo illustrate the effect of this sort of uncertainty on policy, consider the interaction of inflation dynamics and expectations--a subject of major study over the past thirty years. At one end of the spectrum of possible views is a policymaker who thinks that inflation expectations are rational and consistent with a New-Keynesian model of the economy, in which intrinsic sources of inflation persistence are not especially important. In this case, the policymaker might not be too worried that, say, a string of adverse supply shocks would create a severe conflict between the goals of price stability and of full employment. According to this worldview, if people expect the central bank to follow a price-stabilizing strategy, and the central bank ratifies that belief, then any undesired movement in inflation will be quite short lived. And restoring price stability in such a world will likely involve little cost in terms of real activity.\n\nAt the other end of the spectrum are policymakers who suspect that most firms and households form their expectations using something closer to simple rules of thumb based on recent history. Under this alternative worldview, a string of adverse supply shocks is dangerous because it has the potential to cause rising inflation to become embedded in expectations. Should this shift in expectations occur, the central bank would face a persistent inflation problem, one whose correction would likely require a prolonged period of tight monetary policy. In this less comfortable world, restoring price stability can involve a painful process of slow growth and elevated unemployment.\n\nOf course, these considerations are more than a theoretical curiosity and help to explain the intense focus of central banks on inflation expectations. The marked rise in energy prices over the past few years led until recently to a rate of overall consumer price inflation notably above core inflation. However, the available measures of expectations--whether from surveys or financial markets--have shown longer-term expectations increasing very little, if at all, throughout this period, providing some assurance about the inflation outlook. However, this is an ex post assessment. As a policymaker, I would have been more confident in my ex ante judgment about the risk of expectations moving higher if we had had a better understanding of the determinants of expectations regarding prices and of the links between these expectations and the subsequent performance of inflation.\n\nMore generally, the uncertainty we face about the process of expectations formation makes interpretation of the underlying correlations in the data challenging. This is no surprise: The rational expectations revolution begun by Robert Lucas more than thirty years ago started from the premise that it is impossible to move from reduced-form evidence to the underlying economic structure without understanding the evolution of expectations. Much of the macroeconomic literature over the past few years has focused on how alternative assumptions about expectations may explain the patterns of correlations in aggregate data. However, the empirical weaknesses of the rational expectations assumption have limited our progress in this area. The growing interest in research examining the evolution of expectations at the microeconomic level may provide better ways to discriminate between alternative hypotheses. In the meantime, policymakers must live with their uncertainty regarding how expectations are formed and how these expectations shape aggregate activity.\n\nCoping with Uncertainty\nGiven that uncertainty is pervasive, how should central banks deal with it? One obvious response has been to look for cost-effective ways to support both the development of more accurate and timely data and research to improve our understanding of the economy. Central banks also try to mitigate measurement problems by using data in a nuanced manner--for example, by looking at a multitude of alternative data series and by being cautious about the weight placed on short-run movements in various indicators. Realistically, however, such efforts can take policymakers only so far. Thus, risk is unavoidable, and central banks need to conduct policy in a manner that takes account of uncertainty in its various forms, as they strive to maximize public welfare. But what exactly does this mean?\n\nThe literature on this topic extends at least as far back as William Brainard’s original paper on uncertainty and policy almost forty years ago.7 Brainard’s analysis showed that if policymakers are uncertain about how real activity and inflation will be affected over time by monetary actions, they should be less aggressive in responding to changes in economic conditions than would be the case if they knew the true model of the economy. Subsequent research has largely supported Brainard’s conclusions and highlighted a corollary to it: Monetary policy should not respond too strongly to any one economic indicator, as the relationship between that indicator and the goals of policy--price stability and full employment--often differs across alternative models in important ways. More generally, this literature suggests that central banks should be cautious about boldly acting on the predictions and policy prescriptions of any one model, especially given that policymakers usually are unsure about the nature and persistence of the shocks hitting the economy.\n\nCentral bankers around the world certainly seem receptive to taking a gradualist and cautious approach to policy under most circumstances, as indicated by (among other things) their apparent tendency to smooth interest rates. The behavior of the Federal Reserve during the second half of the 1990s illustrates this approach to policy. During this period, incoming data suggested that trend productivity might be accelerating. However, the evidence for this unexpected development was far from conclusive; moreover, the short-run implications for inflation and employment of a sustained pickup in productivity growth were ambiguous. Staff analysis at the time supported Brainard’s conclusion that the appropriate response to heightened uncertainty about the economy’s true productive potential would be to reduce the importance of the estimated output gap in setting policy.8 Whatever the persuasiveness of this analysis, the FOMC did respond in a restrained manner to unusually robust real economic activity--as I believe was appropriate in light of the low and stable inflation that followed.\n\nOf course, gradualism and model averaging may not be appropriate in all circumstances. For example, it may be necessary for monetary policy to respond to what might be called \"tail events,\" along the lines suggested by recent work on \"robust control.\" To simplify greatly, this approach often amounts to choosing policy settings to minimize the maximum possible loss across different models of the economy, in contrast to the standard Bayesian approach, which (loosely speaking) seeks to minimize the average loss across models. Much of the research on robust control has been a bit technical and esoteric. But the notion that policymakers may at times base policy settings on especially pernicious risks has an important ring of truth.\n\nFor example, in 2003 the FOMC noted that a continued fall in inflation would be unwelcome largely because such an eventuality might potentially lead to persistently weak real activity with interest rates stuck at zero. Partly in response, the FOMC reduced the federal funds rate to an unusually low level and kept it there for an extended period, in a manner that perhaps would not have occurred in the absence of concerns about the \"worst case\" effects of deflation. This type of risk management--in which the central bank takes out some insurance against a bad but improbable event--has been an aspect of policymaking for some time and does seem to respond to extreme risks in a way reminiscent of the literature on robust control.9\n\nPolicymakers also seem to have absorbed another lesson from the recent literature, namely, the desirability of reducing the public’s uncertainty about how the central bank will respond to changes in economic conditions. To this end, central banks now strive to conduct policy in a predictable (albeit flexible) manner that is consistent with their stated objectives. On occasion, however, the goal of predictability may conflict with the concept of risk management, particularly when risk management requires taking steps to deal with an unusual or unprecedented risk. This conflict is probably unavoidable, and all that policymakers can do in such circumstances is to try to communicate as best they can the rationale behind their departure from standard practice.\n\nMost central banks also strive to follow at least the spirit of Bayesian thinking by taking an eclectic approach to forecasting and to policy analysis. To see this, consider the range of material that the staff supplies to the FOMC. In the case of the economic projections contained in the briefing document we call the Greenbook, the staff consults a variety of indicators and models and then judgmentally pools this information to produce the baseline outlook. The staff then supplements this analysis with various alternative scenarios intended to illustrate the primary risks to the outlook. Although these scenarios are usually constructed using a single model (FRB/US), the simulations actually encompass a wider range of views about the nature of the economy. For example, the simulations routinely consider alternative characterizations of such key aspects of the economy as the expectations formation process, wealth effects, and the sensitivity of inflation to changes in resource utilization and monetary policy. Finally, the staff provides the FOMC with estimated confidence intervals for the forecast and produces studies addressing such questions as the optimal design of policy under different types of uncertainty. Of course, there is always room for improvement and the staff continues to refine and expand this type of analysis.\n\nIn addition, the structure of the FOMC, like that of a number of foreign monetary authorities, may also provide Bayesian-like benefits in attempting to deal with uncertainty. Many of the individuals who participate in policymaking at the Fed have different views about the structure of the economy. These differences enter our discussions and, through the Committee’s deliberations, affect the course of policy, although, I admit, how we weigh these competing views to arrive at a decision can appear to be murky. Certainly, the process is one that a good Bayesian might find hard to recognize. Nevertheless, studies suggest that the decisions reached by committees are usually superior to those produced by individuals.10 In any event, I know that the heterogeneous viewpoints expressed by my fellow Committee members are intellectually stimulating and that they spur me to improve my own thinking about the economy and about the best course for monetary policy.\n\nThus policymakers and the public at large live in an uncertain world. For example, most of you are probably wondering when this speech will end. I thought about gradually drawing to a close at, say, a measured pace, but my risk-management instincts tell me just to stop. Thank you.",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20061201a.htm",
        "title": "Monetary Policy and Uncertainty",
        "date": "12/1/2006"
    },
    {
        "content": "November 30, 2006\n\nGovernor Susan Schmidt Bies\n\nAt the Institute for International Bankers Seminar on the Impact of Basel II on Financial Markets and Business Strategies, New York, New York\n\nGood afternoon. Thank you for the invitation to participate in this seminar. We at the Federal Reserve welcome all opportunities to discuss current regulatory proposals and to hear the banking industry's comments about them. Today I plan to provide an overview of developments in the United States relating to Basel II and the Market Risk Amendment, as well as offer some thoughts about cross-border implementation of the New Accord.\n\nThe Role of Regulatory Capital\nBanking is, and should be, a business in which banks take and manage risks. Bankers implicitly accept risk as a consequence of providing services to customers and also take explicit risk positions that offer profitable returns relative to their risk appetites. One of the most important jobs of bank supervisors is to ensure that banks maintain an adequate capital cushion against losses, especially during times of financial instability or stress. Minimum capital requirements are a major tool for ensuring an adequate cushion. When developing minimum capital requirements, supervisors should continue to promote approaches that minimize the negative consequences of risk taking by financial institutions, particularly those institutions that could affect global financial stability. That is what we are doing with the Basel II framework.\n\nThe Federal Reserve's main reason for pursuing the advanced approaches of Basel II is the growing inadequacy of current Basel I regulatory capital rules for the large, internationally active banks that are offering ever more complex and sophisticated products and services. We need a more risk-sensitive capital framework for these particular banks, and Basel II is such a framework. In addition, Basel II promotes risk-measurement and risk-management enhancements and improves market discipline, while giving supervisors a more conceptually consistent and more transparent framework for evaluating systemic risk, particularly through credit cycles. Basel II should establish a more coherent relationship between regulatory measures of capital adequacy and day-to-day risk-focused supervision of banks, enabling examiners to better evaluate whether banks are holding prudent levels of capital given their risk profiles.\n\nFor similar reasons, U.S. supervisors support the recent Basel/IOSCO revisions to the 1996 Market Risk Amendment (MRA). Since adoption of the MRA, banks' trading activities have become more sophisticated and have given rise to a wider range of risks that are not easily captured in their existing value-at-risk models. For example, more products related to credit risk, such as credit default swaps and tranches of collateralized debt obligations, are now included in the trading book. These products can create default risks that are not captured well in methodologies required by the current rule specifying a ten-day holding period and a 99 percent confidence interval--thereby creating potential arbitrage opportunities between the banking book and the trading book.\n\nRecent U.S. Developments\nAs most of you know, over two months ago the U.S. banking agencies issued a notice of proposed rulemaking (NPR) relating to Basel II. Concurrently, the agencies issued an NPR on revisions to the MRA. Notably, in the United States the Basel II NPR and the revised market-risk NPR may or may not necessarily apply to the same set of institutions. The proposed market risk NPR will continue to apply only to banks with significant trading activity, whether they are on Basel I as amended or moved to Basel II.\n\nThe U.S. banking agencies are eagerly awaiting comments on both proposals and expect the dialogue with all interested parties to expand as we explore whether the proposals meet our stated objectives and how the proposals can be improved. Specifically, I want to emphasize that you should not wait until the end of the comment period to submit comments; our staffs are working diligently to review them even now so that we will be ready to proceed in the development of a final rule. Therefore, even if you might have additional comments later, it is still helpful to submit now any you may currently have. We especially appreciate very detailed comments. Furthermore, it would be especially helpful to us if in your comments you could differentiate between those that are relevant during the Basel II transition period versus those that relate to features of the proposed framework that are more permanent in nature. In addition to the written comments, the comment period has afforded us more frequent interaction with industry groups, such as the Institute of International Bankers, which has been very helpful and informative. For our part, the agencies hope to issue a set of proposed supervisory guidance relating to Basel II soon that should be helpful as banks move forward with implementation. We at the Fed hope that everyone who reviews our proposals relating to Basel II and the MRA revisions understands that they are intended to promote the stability of the U.S. financial system by ensuring the safety and soundness of the largest U.S. banks. Thus, as Chairman Bernanke has noted, the ability of Basel II to promote safety and soundness is the first criterion by which these regulatory capital proposals should be judged.\n\nA key aspect of Basel II implementation in the United States relates to scope of application. In the United States, Basel II is expected to apply to only a handful of large, complex organizations, which is the principal reason why the U.S. agencies are proposing only the advanced approaches (A-IRB for credit risk and AMA for operational risk). Perhaps the main difference between the implementation of Basel II in the United States and the implementation in most other countries is that the U.S. banking agencies plan to retain a revised form of the existing Basel I-based capital rules for the vast majority of U.S. banks; most other countries are replacing Basel I entirely and will apply Basel II to the entire banking system. Notably, the initial U.S. Basel II proposals, issued in an advance notice of proposed rulemaking (ANPR) in August 2003, suggested that only the advanced approaches be used in the United States; comments on the Basel II ANPR did not indicate any opposition to the agencies' proposed approaches.\n\nIt should also be noted that, based on the latest information, apparently none of the large, complex organizations in other Basel-member countries plan to adopt the standardized approach for credit risk. Therefore, our proposal that large, complex U.S. banking organizations use the advanced approaches of Basel II seems generally consistent with approaches to be used by large, complex organizations in other countries.\n\nThe U.S. agencies remain open to considering the full set of possibilities for Basel II, so the Basel II NPR specifically seeks comment on whether some form of the Basel II standardized approach for credit risk should be adopted in the United States. When deciding whether our large, internationally active banks should have access to a standardized approach, the agencies will need to consider a number of important issues: whether such an approach would accommodate the risks those banks take, now and in the future; whether it would provide adequate risk sensitivity; whether it would be useful to have a transition period in order to give some banks more time to prepare to use the advanced approaches, and whether other transition arrangements are available; whether taking the time needed to develop an appropriate U.S. version of the standardized approach would unduly delay the Basel II implementation process; and, finally, whether the marketplace would find such an option for those banks meaningful and acceptable.\n\nIn developing U.S. proposals for Basel II implementation, the agencies did not think it would be appropriate to replace the existing Basel I capital rules for small, noncomplex banks in this country with the Basel II standardized rules. The agencies concluded, based in significant part on input from small community banking organizations, that the implementation costs of such an overhaul generally would exceed its regulatory benefit. Instead, the agencies intend to propose, through a notice of proposed rulemaking, a simpler, more modest set of revisions to our existing Basel I-based capital rules for smaller U.S. banks--revisions known as Basel IA.\n\nBasel IA\nAs noted, we anticipate that only one to two dozen institutions would move to the U.S. version of Basel II in the near term, meaning that the vast majority of U.S. institutions would continue to operate under Basel I-based rules. The U.S. Basel I framework has already been amended more than twenty-five times since its introduction, in response to changes in banking products and the financial services marketplace. In October 2005, the agencies issued an ANPR for Basel IA, which discussed possible changes to increase the risk sensitivity of the U.S. Basel I rules and to mitigate competitive distortions that might be created by introducing Basel II. We have reviewed comments on the ANPR and are working on a notice of proposed rulemaking, which we hope to have out very soon. In drafting all of these regulatory capital proposals, we continue to consider the balance between risk sensitivity and regulatory burden, since more risk-sensitive capital requirements generally imply greater burden. Thus, we are mindful that amendments to the Basel I rules should not be too complex or too burdensome for the large number of small- and mid-sized institutions to which the revised rules might apply. Indeed, a number of those commenting on the ANPR advocated leaving existing rules unchanged.\n\nAs noted, the agencies are taking into account potential competitive effects that Basel II might create between those institutions that would adopt Basel II and those that would not. Indeed, we recognize that many of the thousands of depository institutions that would remain under the current capital rules may be concerned about the potential uncertainty surrounding Basel II. As part of our efforts to analyze and address these concerns, the Federal Reserve published a series of research papers focused on potential competitive effects in areas such as small business lending, mergers and acquisitions, and residential mortgage markets. The agencies have been taking into account the issues raised in those papers in drafting the Basel IA NPR.\n\nWith regard to both the Basel II proposals and the proposed Basel I amendments, we understand the need for full transparency. For that reason, we expect the comment periods for the Basel II NPR and the NPR for the proposed Basel I amendments to have some overlap so that all interested parties may have adequate time to compare, contrast, and comment on both proposals. Accordingly, either of our proposals could change as a result of comments received or new information gathered. And as I stated earlier, we encourage you to submit your comments early rather than waiting until the end of the comment period.\n\nImplementing Basel II Across Countries\nHaving covered the status of U.S. Basel II proposals, I now want to offer a few thoughts about the implementation of Basel II around the globe. As you know, the U.S. agencies participate with other national supervisors in the Basel Committee on Banking Supervision and in other groups to identify differences in implementation and discuss possible ways to harmonize rules and thereby reduce burden on cross-border banking organizations. At the recent Accord Implementation Group meeting, there was a very fruitful exchange of ideas and dialogue among the member nations, as well as the EU and Committee of European Banking Supervisors, on approaches that countries are considering to address capital adequacy beyond regulatory minimums. Of course, we recognize that the national discretion allowed under Basel II means that there will be adoption of different approaches to Basel II by various countries. We recognize that this may create challenges for banking organizations that operate in multiple jurisdictions and are working to try to minimize both the differences and the difficulties wherever possible. But it is good to remember that cross-border banking has always raised specific challenges, even before Basel II, and supervisors from various countries have worked hard to address and mitigate those difficulties.\n\nLet me assure all bankers here that the Federal Reserve is aware that the process of adopting national versions of Basel II has heightened concerns about home-host issues. We are committed to working with other international supervisors in resolving home-host issues. Indeed, the Federal Reserve and other U.S. agencies have, for many years, worked with international counterparts to limit the difficulty and burden that have arisen as foreign banks have entered U.S. markets and as U.S. banks have established operations in other jurisdictions. Throughout the Basel II process, we have been engaged in dialogue with our international counterparts through various avenues such as supervisory working colleges to share ideas and tackle specific issues as they arise. Some of these issues are very institution- and country-specific, and are therefore better addressed through individual conversations with an institution and their relevant supervisors.\n\nThe Federal Reserve continues to be an active participant in supervisory working groups for all large U.S. and foreign banking organizations in the U.S. and we are encouraged by the level of cooperation and pragmatism coming out of these efforts. For example, supervisory planning efforts for U.S.-based banking organizations for the upcoming year are reviewed with foreign supervisory authorities to ensure that, regardless of Basel II timing issues, information pertaining to ongoing supervisory judgments of risk management practices is available. We have also been encouraged by the dialogue with our foreign colleagues regarding their desire to provide flexibility in transitional arrangements. I would add that we have benefited from the numerous meetings and exchanges we have had with groups such as yours to help us identify the most critical areas to examine for further convergence globally. We will be evaluating all of that input closely as a part of our rulemaking process. As always, we encourage bankers who have questions and concerns about home-host issues to communicate promptly with their regulators in all jurisdictions so that the issues can be addressed.\n\nAnother key point, when talking about differences across countries, relates to findings from recent Basel II quantitative impact studies (QIS4 and QIS5). Interestingly, the two exercises identified a number of similar issues, some in areas in which institutions were not able to provide adequate data (especially for downturn scenarios). In the United States, QIS4 was conducted before the release of the Basel II NPR, while in Europe, QIS5 was conducted only after the passage of EU legislation implementing Basel II. Therefore, much of what was learned by U.S. supervisors in QIS4 is included in the NPR, such as the supervisory mapping function for downturn estimates of loss given default. European supervisors, on the other hand, did not have the benefit of information from QIS5 when they were drafting their rules. While I cannot speak for my European counterparts, no doubt some of the issues raised in QIS5 will be addressed during implementation in Europe.\n\nIt is possible that differences in Basel II implementation may mean that the U.S. version of Basel II is in some aspects more conservative than other countries' versions. In other areas, the U.S. proposals may be less conservative. But, as I noted, national differences in capital regulation are not unique to the Basel II capital regime. Over the years, the U.S. agencies have consciously chosen to maintain a more conservative stance in some aspects of Basel I, as applied in the United States, compared with versions of Basel I adopted by other countries. For example, the U.S. banking agencies currently impose a supplemental leverage ratio, and risk-based capital is linked to our prompt-corrective-action framework. However, we do not believe that elements of conservatism in existing U.S. capital rules, relative to other countries' rules, have been a barrier to the financial success of our banks, nor have they constrained foreign banks from participating in our markets. On the contrary, we believe that capital strength and the resilience it demonstrates offer some competitive advantages and instill market confidence. That is a key reason that most of the world's largest banks hold capital in excess of minimum regulatory standards. Creditors and counterparties will always consider capitalization when assessing the risks associated with these banks. Indeed, many factors other than minimum regulatory capital requirements--including domestic and international tax policies, economies of scale and scope, risk-management skills, and the ability to innovate--also affect competition and profitability.\n\nOn balance, the Federal Reserve believes that an appropriately conservative approach to capital adequacy serves the United States' interest in maintaining the safety, soundness, and resiliency of our banking system. However, we also recognize the impact that differences among countries can have and that it is worthwhile to minimize them whenever possible. As Chairman Bernanke noted earlier this fall, before issuing a final rule we intend to review all international differences to assess whether the benefits of rules specific to the United States outweigh the costs. Of course, that will include a review of whether U.S.-specific rules are having an adverse impact on the competitiveness of U.S. banks vis-à-vis foreign counterparts.\n\nConclusion\nI believe that forums such as this one are very useful places to discuss actual and potential differences in capital requirements across countries. As supervisors, we must always strive to minimize these differences and reduce burden on bankers as they conduct business across national borders. However, bankers must also realize that national boundaries still matter, and that some differences in capital requirements across countries will remain. While there may be opportunities here and there to reduce burden through broad policy changes, generally the most productive way to address cross-border issues is on a case-by-case basis. In most cases, blanket assurances are neither feasible nor realistic. Therefore, I continue to encourage bankers to present issues to their supervisors so that the issues can be raised in bilateral or multilateral discussions of specific topics--the process by which issues have been addressed in the past.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20061130a.htm",
        "title": "A U.S. Perspective on Basel II Implementation",
        "date": "11/30/2006"
    },
    {
        "content": "November 28, 2006\n\nChairman Ben S. Bernanke\n\nBefore the National Italian American Foundation, New York, New York\n\nThank you for inviting me to speak today. I will take this opportunity to present an update on the economic outlook.\n\nThis month marks the fifth anniversary of the beginning of the current expansion. Frequently, the early stages of an expansion include a period of above-trend growth, as underutilized resources are put back to work. As slack in the economy is reduced, however, economic growth tends to moderate. Indeed, at that stage, some slowing of growth to a pace consistent with the rate of increase in the nation's underlying productive capacity is necessary if the expansion is to be sustained without a buildup in inflationary pressures. In my testimony to the Congress in July, as part of the Federal Reserve's semiannual monetary policy report, I noted that the U.S. economy had entered this transition phase, and that some moderation of economic growth over the remainder of the year seemed likely.\n\nThe deceleration in economic activity currently under way appears to be taking place roughly along the lines envisioned in the Federal Reserve's July report. As anticipated, the slowdown primarily reflects a cooling of the housing market. Most other sectors of the economy appear still to be expanding at a solid rate, and the labor market has tightened further.\n\nInflation, which picked up earlier this year, has been somewhat better behaved of late. Overall inflation was pushed up this spring by a surge in energy prices, but the recent declines in energy prices have largely reversed those effects. Price inflation for consumer goods and services excluding energy and food, the so-called core inflation rate, has also moderated a bit in the past few months. But the level of the core inflation rate remains uncomfortably high.\n\nOver the next year or so, the economy appears likely to expand at a moderate rate, close to or modestly below the economy's long-run sustainable pace. Core inflation is expected to slow gradually from its recent level, reflecting the reduced impetus from high prices of energy and other commodities, contained inflation expectations, and perhaps further reductions in the rate of increase of shelter costs and some easing in the pressures on capital and labor resources. However, substantial uncertainties surround this baseline forecast. The Federal Open Market Committee (FOMC), the committee that sets monetary policy, will continue to monitor the incoming data closely. In its latest statement, the FOMC reiterated its view that the upside risks to inflation are the predominant risks to the forecast and indicated that it is prepared to take action to address inflation if developments warrant.\n\nEconomic Activity: Recent Developments and Prospects\nAs I have just noted, the pace of economic activity has moderated over the course of the year. According to the latest estimates by the U.S. Department of Commerce, real gross domestic product (GDP) increased at an annual rate of 2.6 percent in the second quarter of 2006 and at a rate of only 1.6 percent in the third quarter. These figures are down noticeably from the 3-1/2 percent average pace of growth of the preceding two years. We will receive an updated estimate of third-quarter GDP growth tomorrow. At this juncture, information about economic activity in the fourth quarter is limited, and the range of plausible outcomes remains wide. But the indicators in hand suggest that real GDP growth this quarter is likely to be in the same general range that it was in the second and third quarters.\n\nHousing has played a significant role in the recent slowing of overall activity, and developments in this sector are likely to have an important influence on economic growth going forward as well. As you know, the correction in the housing market that is now in train follows a boom during the first half of this decade. Between 2000 and late 2005, the pace of construction of single-family homes rose more than 40 percent, and sales of both new and existing homes increased by a similar amount. Nationally, home prices increased about 60 percent over that period--an average figure that masks considerable variation in the rate of price appreciation across cities and regions, as home prices rose exceptionally rapidly in some \"hot\" locations but only modestly in others.\n\nNo real or financial asset can be counted upon to pay a higher risk-adjusted return than other assets year after year, and housing is no exception. Thus, a slowing in the pace of house-price appreciation was inevitable. Moreover, the sustained rise in prices, together with some increase in mortgage interest rates, sowed the seeds of the correction by making housing progressively less affordable. Declining affordability ultimately served to limit the demand for housing, leading to a deceleration in house prices and slowing home purchases.\n\nThe drop in home sales that began earlier this year has led homebuilders to curtail the rate of new construction. Indeed, single-family housing starts are down about 35 percent since their peak earlier this year. Obtaining a precise read on home prices is difficult: During a period of weak demand, potential sellers often choose to leave their homes on the market longer or even to remove them from the market, rather than accept price offers that are below their expectations. The timeliest data on house prices do not fully account for changes in the composition of home sales by location, size, and other characteristics. Moreover, the data do not capture hidden price cuts, as when builders try to stimulate sales through the use of \"sweeteners\" such as paying the customer's mortgage points or upgrading features of the house at no additional cost. Nevertheless, there can be little doubt that the rate of home-price appreciation has slowed significantly for the nation as whole. Some areas have continued to experience gains--albeit smaller ones than before--while other markets have seen outright price declines.\n\nNotwithstanding the sharp reduction in starts of new single-family houses, inventories of both new and existing homes for sale have increased markedly this year. For example, according to the most recent data, homebuilders currently have about 550,000 new homes for sale, roughly half again the number that has been typical during the past decade. Moreover, the official statistics likely understate the full extent of the inventory buildup, as many homebuilders have reported a sharp increase this year in the number of buyers canceling signed contracts. A home for which the sales contract is cancelled becomes available for sale once again but is not included in the official data on the inventory of unsold new homes. To reduce this inventory overhang, builders are likely to continue to limit the number of new homes under construction.\n\nAlthough residential construction continues to sag, some indications suggest that the rate of home purchase may be stabilizing, perhaps in response to modest declines in mortgage interest rates over the past few months and lower prices in some markets. Sales of new homes ticked up in August and increased a bit further in September. The University of Michigan's survey of consumers shows an increase in the share of respondents who believe that now is a good time to buy a home, from 57 percent in September to 67 percent in November. Meanwhile, an index of applications for mortgages for home purchases has been trending up since July. Although these developments are encouraging, we should keep in mind that even if demand stabilizes in its current range, reducing the inventory of unsold homes to more normal levels will likely involve further adjustments in production. The slowing pace of residential construction is likely to be a drag on economic growth into next year.\n\nGrowth in some manufacturing industries has also slowed of late, and data prepared by the Federal Reserve show that aggregate manufacturing production declined in September and October. The motor vehicle sector in particular has experienced weaker demand and an accompanying buildup in stocks of unsold cars and trucks over the past year. Energy prices have contributed to these developments, as consumers have responded to high prices at the pump by reducing their demand for less fuel-efficient vehicles. The decline in sales caused inventories of these vehicles to surge this past spring. Since then, automakers have cut production to reduce the overhang of inventories; on a seasonally adjusted basis, the pace of light vehicle assembly in October was about 10 percent below the pace in the second quarter. The growth of production in some other manufacturing industries, notably those closely tied to the housing and automobile sectors, has also been slowing. Elsewhere in the industrial sector, though, production in high-tech industries has been growing rapidly, and high prices for energy and other commodities have stimulated drilling and mining activity. The global economy continues to be strong, with cyclical recoveries under way in Europe and Japan and ongoing growth in the emerging-market economies; this growth abroad should support the continuing expansion of U.S. exports of goods and services.\n\nOutside of the housing and motor vehicle sectors, economic activity has, on balance, been expanding at a solid pace. Perhaps the clearest evidence of this broader economic strength comes from the labor market. Although the number of jobs in manufacturing and construction fell in October, most other sectors of the economy experienced solid job gains. Private employers in industries outside of manufacturing and construction added nearly 125,000 workers to their payrolls last month, following an average increase of 140,000 jobs per month during the preceding three months. With labor demand continuing to expand over the past several months, the national unemployment rate fell to 4.4 percent in October, its lowest level since May 2001.\n\nThe strength of the labor market and the associated increases in wage and salary income have supported consumer spending. The data in hand indicate that, the slowdown in housing notwithstanding, inflation-adjusted outlays for personal consumption increased in the third quarter at about the average rate seen since the current economic expansion began in late 2001. The latest retail sales figures suggest an increase in consumption at roughly that pace in the current quarter as well. Other factors that are positive for consumer spending include the recent declines in energy prices, which have boosted household purchasing power and consumer confidence; increases in stock prices, which have added to household wealth; and relatively low long-term interest rates.\n\nIn the business sector, capital investment has continued to expand at a healthy pace. Spending on nonresidential construction--a component of business investment--has been particularly robust, reflecting higher outlays for new office and commercial buildings as well as a rapid increase in expenditures on drilling and mining structures. Outlays for equipment and software, which grew briskly from mid-2004 through the early part of this year, have moderated somewhat, though order backlogs for capital goods such as industrial machinery and other types of heavy equipment remain substantial. Moreover, financial conditions continue to be favorable for investment spending, as profitability is high, the cost of capital is relatively low, and significant cash reserves remain on firms' books.\n\nOverall, the economy is likely to expand at a moderate pace going forward. A reasonable projection is that economic growth will be modestly below trend in the near term but that, over the course of the coming year, it will return to a rate that is roughly in line with the growth rate of the economy's underlying productive capacity. This scenario envisions that consumer spending--supported by rising incomes and the recent decline in energy prices--will continue to grow near its trend rate, and that the drag on the economy from the motor vehicle and housing sectors will gradually diminish. The motor vehicle sector may already be showing signs of strengthening; after having cut production significantly in recent months in response to the rise in the inventory of unsold vehicles, automakers appear to have boosted the assembly rate a bit in November, and they have scheduled further increases for December. The effects of the housing correction on real economic activity are likely to persist into next year, as I have already noted. But the rate of decline in home construction should slow as the inventory of unsold new homes is gradually worked down.\n\nLike all economic forecasts, this one is provisional, and risks exist in both directions. On the downside, the correction in the housing market could turn out to be more severe and widespread than seems most likely at present. A deeper correction would directly affect economic activity through additional cutbacks in housing starts and through its effects on employment in construction and housing-related industries. More indirectly, it might also impose greater restraint on consumer spending by reducing homeowners' equity and thus household wealth, and perhaps by affecting consumer confidence as well. Because consumption makes up more than two-thirds of aggregate expenditure, any significant effect on consumer spending arising from further weakness in housing would have important implications for the economy.\n\nOn the other hand, economic growth could rebound more vigorously than now expected. The solid rate of job growth, the decline in the unemployment rate, and the healthy pace of capital investment could be signals that underlying economic fundamentals are stronger than generally recognized. Moreover, to date there is little evidence that the weakness in housing markets is spilling over more broadly to consumer spending or aggregate employment. If these trends continue, growth in real activity might return to a pace that could intensify upward pressures on resource utilization.\n\nPotential Output\nIn my remarks today, I have alluded to the economy's underlying productive capacity--in the jargon of economists, \"potential output.\" The growth rate of potential output is the rate of growth that the economy can sustain in the long run. I will briefly discuss the factors determining potential output and the implications that the growth rate of potential output has for the economy and monetary policy.\n\nGrowth in potential output is determined to a large extent by two factors: the trend growth rates of the labor force (that is, the number of individuals available to work) and of labor productivity (that is, the amount of output that each worker can produce).\n\nWith regard to the labor force, research by the Board's staff highlights the role of demographic factors in determining the number of people available to work in the years just ahead. Most notably, the impending retirement of the baby boomers and the fact that women are no longer increasing their participation in the labor force at the rate they were in the past will tend to restrain the future growth rate of the U.S. labor force. All else being equal, these developments translate into a slower rate of growth of potential output. Estimates of the magnitude of the likely slowdown in labor force growth, particularly in the longer run, are subject to significant uncertainty. For example, to a degree that is hard to predict, improved health and increased longevity may increase the interest of older workers in remaining in the labor force, perhaps on a part-time basis, and an increasing scarcity of labor may prompt changes in labor-market institutions and employer behavior that facilitate the participation of older workers. But those adjustments are likely to take time, and some slowing in the growth of the labor force thus seems likely over the next few years at least.\n\nWith regard to productivity, I remain optimistic that the recent favorable trends will continue. The price of computing power continues to fall sharply, having declined by nearly half between 2000 and 2005. Increased computing power has contributed, in turn, to the development and growth of other commercially relevant technologies, such as biotechnology, and has led to improvements in efficiency, through better supply-chain management, for example. Moreover, whatever the pace of future technological progress, further diffusion of already-existing technologies and applications to more firms and industries should continue to increase aggregate productivity for a time.\n\nThat said, longer-run trends in the growth of productivity are very difficult to predict. During the first half of the decade, productivity in the nonfarm business sector increased at an unusually high average annual rate of about 3 percent. However, according to current estimates, productivity growth slowed in the second quarter of this year and came to a halt in the third quarter. Moreover, the strength of recent hiring raises the possibility of subpar productivity growth in the fourth quarter as well. When all is said and done, however, I expect that the latest numbers will turn out to have been a reflection of the typical volatility in the data and some cyclical response to the slowing in economic activity, not a signal of a sea change in the longer-run outlook for productivity growth.\n\nEven if productivity growth is sustained at a reasonably good rate, the slower expansion of the labor force will imply some moderation in the rate of growth of potential output over the next few years. In the very near term, that slower growth in the labor force needs to be taken into consideration when assessing the sustainability of given rates of expansion in economic activity. In the medium term, because the factors that affect potential output and thus aggregate supply also tend to affect aggregate demand, slower growth of potential output does not necessarily mean that inflation will be higher or that monetary policy will have to be tighter. Rather, the implications for monetary policy of a possible slowing in the growth of potential output depend on the extent to which such a slowing alters the balance of supply and demand in the economy. For example, as we saw in the second half of the 1990s, changes in expected productivity growth and potential output can significantly affect aggregate demand through their influences on income expectations and asset prices. The problem for policymakers is to identify, in real time, any changes in the prospective growth rate of potential output and to anticipate the accompanying effects on the balance of supply and demand.\n\nInflation and Monetary Policy\nOverall (or \"headline\") inflation has slowed significantly since earlier this year; indeed, in October the consumer price index fell by 1/2 percent for the second consecutive month. This improvement is largely the result of the recent declines in energy prices. The price of crude oil has fallen about one-fourth since its recent peak, reflecting some easing of geopolitical concerns and other factors. In particular, participants in crude oil markets--still mindful of the devastating effects on energy supplies of the hurricane season in 2005--appear in retrospect to have incorporated a substantial risk premium into spot prices earlier this year. In the event, no damaging storms occurred this hurricane season. As the good news about the weather unfolded, spot prices of crude oil fell from August through early October.\n\nReadings on the core inflation rate--that is, the inflation rate excluding the energy and food components--have improved modestly since the spring, but core inflation nevertheless remains uncomfortably high. Core CPI inflation over the most recent twelve months was 2.7 percent, up from 2.1 percent over the previous twelve months. Another measure of core inflation that we monitor, the price index for personal consumption expenditures excluding food and energy, is available only through September; that index was up 2.4 percent over the past year, compared with a 2.1 percent rise in the preceding twelve months.\n\nSeveral factors underlie the increase in core inflation over the past year, although the relative contributions are impossible to estimate precisely. Increased pressure on resource utilization as the economic expansion matured and slack was reduced has likely played some role. The sharp increases in energy and materials costs figured in the rise in core inflation as well, as some suppliers of non-energy goods and services may have been able to pass through their higher input costs into final prices.\n\nMore-rapid increases in shelter costs also boosted core consumer inflation over the past year. In the broad measures of consumer prices that we follow, substantial weight is given to an item called owners' equivalent rent (OER). OER is a measure of the price of the dwelling services enjoyed by people who own their homes; conceptually, if homeowners were to rent their homes from themselves, OER would be the market rent that they would pay. Economic statisticians estimate OER by using prices in the rental housing market. Over the past twelve months, the consumer price index for OER has risen about 4 percent, compared with 2-1/4 percent during the preceding twelve months. The acceleration in OER may reflect in part a shift in demand toward rental housing as families judged homeownership to have become less financially attractive of late. The most recent monthly increases in OER generally have been smaller than those earlier in the year, and further slowing may occur as the supply of rental units increases and the demand for owner-occupied housing stabilizes. However, the future evolution of this measure is difficult to know with any certainty.\n\nLooking forward, core inflation seems likely to moderate gradually over the next year or so. Some of the factors that pushed up core inflation in the recent past--in particular, energy prices and shelter costs--appear likely to be more neutral in the coming year, and inflation expectations remain contained. Moreover, if, as seems most probable, the economy grows at a rate modestly below its potential for a time, pressures on resource utilization should ease a bit.\n\nHowever, as with the outlook for economic activity, there are substantial uncertainties about the inflation forecast. In the case of inflation, the risks to the forecast seem primarily to the upside. Given the current level of inflation, a failure of inflation to moderate as expected would be especially troublesome.\n\nOne factor that we are watching carefully is labor costs, which depend on both the compensation received by workers and labor productivity. Although the available indicators give somewhat different signals, it seems clear that labor costs--which account for roughly two-thirds of firm's total costs--have been rising more quickly of late. Some part of this acceleration no doubt reflects the current tightness in labor markets. For example, anecdotal reports suggest that businesses have been finding it difficult to recruit well-qualified workers in certain occupations.\n\nWhat implications does the pickup in labor costs have for price inflation? One possible outcome is that increases in labor costs will largely be absorbed by a narrowing of firms' profit margins and not be passed on to consumers in the form of higher prices. The fact that the average markup of prices over unit labor costs is currently high by historical standards suggests some scope for this outcome to occur. If higher labor costs are mostly absorbed by firms and not passed on, then workers will see the gains in their nominal compensation per hour of work translated into greater real compensation per hour; in the process, workers would capture a greater share of the fruits of the high rate of productivity growth seen in recent years. The more worrisome possibility is that tight product markets might allow firms to pass all or part of their higher labor costs through to prices, adding to inflation pressures. The data on costs, margins, and prices in coming months may shed some light on which of these two scenarios is likely to be the better description of events.\n\nDuring the early part of this decade, the Federal Reserve sharply eased the stance of monetary policy to help bring the economy out of recession and to foster a durable economic expansion. Once the expansion had clearly gained firm footing, the FOMC began a process of normalizing interest rates that involved seventeen consecutive increases in overnight rates of 25 basis points each. In August of this year, and again in September and October, the Committee left interest rates unchanged so as to assess the effects of its previous policy actions, and because of indications that economic growth was moderating and that inflation pressures might be diminishing somewhat. At the same time, the Committee has continued to emphasize the upside risks to inflation and the high costs that would be associated with a failure of inflation to moderate gradually as expected. Needless to say, we will continue to monitor the inflation situation closely. Whether further policy action against inflation will be required depends on the incoming data and in particular on how these data affect the FOMC's medium-term forecasts of both inflation and output growth.\n\nI have focused today on the near-term prospects for the economy and the risks to the economic outlook. However, in reviewing the economic developments of recent years, one cannot help but be impressed by the dynamism and resilience of the U.S. economy. I have confidence, therefore, that however events play out in the short term, in the longer term the economy will grow at a healthy pace, raising living standards in the process. The Federal Reserve will continue to play its part by implementing policies designed to achieve its mandate of fostering price stability and maximum sustainable employment.",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20061128a.htm",
        "title": "The Economic Outlook",
        "date": "11/28/2006"
    },
    {
        "content": "November 21, 2006\n\nGovernor Kevin Warsh\n\nAt the New York Stock Exchange, New York, New York\n\nThank you for inviting me to speak about the role of financial markets and market discipline in Federal Reserve policymaking. As chief financial officers and business leaders, you work assiduously to incorporate real-time information about your companies--and about the competitive and economic landscape--into your decisionmaking. Similarly, financial market participants quickly assimilate publicly available information to help judge the market clearing price for securities that you issue.\n\nIndeed, this process is what makes the venue for today's discussion--the New York Stock Exchange, home of the world's deepest equities market--so appropriate. The NYSE provides a platform for real-time, information-rich assessments of leading global companies, incorporating both an evaluation of the overall economic outlook and firm-specific considerations. It is also fitting to be speaking today before members of the Securities Industry and Financial Markets Association, who trade in these markets daily.\n\nThe Federal Reserve, too, relies on multiple sources of data to help achieve our dual mandate: ensuring price stability and achieving maximum employment. Some of the data upon which we draw--statistical indicators of activity and prices in the real economy--tend to be backward-looking and subject to considerable revision. Other information we use is drawn from financial market prices; although subject to rapid change and \"noisy\" market signals, this information can be considerably more timely and forward looking.\n\nIn its role as a bank regulator and supervisor, the Federal Reserve also often looks to market prices to help assess the safety and soundness of financial institutions.\n\nToday, I will discuss the role of financial markets in effective monetary, regulatory, and supervisory policy making by the Federal Reserve. In particular, I will discuss the potential for markets to inform the Fed's policy judgments--even as our policies also affect markets. I will also describe the important role of markets in disciplining private entities. Of course, the views I will express are my own and not necessarily those of my colleagues on the Federal Open Market Committee (FOMC). 1\n\nMy remarks will cover three points. First, financial markets can inform and, in some cases, complement the actions of the Federal Reserve by providing timely information about the outlook for economic activity, inflation, and the health of individual financial institutions. Second, the Federal Reserve confronts many challenges when trying to extract relevant information from financial market prices--not least because these prices reflect the market's interpretation of our outlook as well as its independent assessments. Third, the market's disciplining of private entities is an important complement to the Federal Reserve's supervisory and regulatory functions, and the Fed can enhance market discipline by improving the flow of information from these regulated entities to the markets.\n\nI will begin with a discussion of how markets, in my judgment, inform the monetary policy process, and then turn to the role of markets in the supervisory and regulatory process.\n\nFinancial Markets and Monetary Policy\nMarkets affect monetary policy predominantly through the information provided by asset prices. The available menu of prices is extensive, including those of Treasury securities (nominal and real), corporate debt, equities, and derivatives. These prices embed investors' expectations of the future paths of economic growth, inflation, and financial conditions. At least as important, these prices also can provide some insight into the uncertainty surrounding likely outcomes. Monetary policy makers can use economic models and statistical techniques to extract the views of market participants about these key macroeconomic variables.\n\nLet me cite a few simple examples of how we interpret asset prices. Through open market operations, the FOMC sets the target federal funds rate, which is the overnight rate at which depositories lend to each other the balances they hold at the Federal Reserve. Interest rates for periods extending beyond that very short horizon, however, are established by market participants rather than the FOMC, although members of the Committee may be able to influence these longer-term rates somewhat through what is affectionately described as \"open mouth operations.\" In this way, market-based interest rates reflect primarily the path investors expect for monetary policy. That expected path is of keen interest to us as policymakers.\n\nThe market's view of very near term policy is reflected in futures contracts on federal funds. Futures on Eurodollars provide information on expectations for the period beyond the next six months or so. For longer time horizons, investors' views can be determined from yields on medium- and long-term Treasury securities. This determination is based on two estimates incorporated in the yield on a nominal Treasury security, such as the ten-year note. The first estimate is essentially a weighted average of the current one-year rate and a sequence of forward rates that contain information about the one-year spot rates expected to prevail over the next nine years. The second estimate is the term premium at each horizon, or the compensation investors require for holding securities an additional period. As might be expected, imprecision about our estimates of these pieces may well increase with the forecast horizon.\n\nTreasury inflation-protected securities (TIPS) are financial market assets that provide a judgment on forward-looking views about inflation. The gap between nominal Treasury yields and yields on TIPS of comparable maturities is called the breakeven inflation rate. The breakeven rate incorporates the market's expectation of inflation and the risk premium for uncertainty about these expectations. It also reflects liquidity differences between the two types of securities, which now are smaller than during the period immediately after TIPS were introduced in 1997. Today, breakeven rates implied by forward prices on TIPS indicate longer-run consumer price index inflation compensation of about 2-1/2 percent, in the middle of the range of the past several years. This is an example of information that may provide monetary policy makers with a reasonable source of market insight and may importantly complement an inflation outlook developed from economic models, survey responses, and other sources. Properly measuring inflation expectations is critically important to the Fed in its formulation of policy.\n\nMarkets for corporate equity and debt represent other important sources of information for the Fed. In addition to providing expected interest rates and inflation rates, equity prices incorporate investors' views about the growth of corporate earnings. Corporate bond prices embed expected default and recovery risks. Moreover, derivatives prices can provide other valuable information, and we can learn much by understanding the linkages between primary and derivatives markets.\n\nLet me underscore the role of market signals by discussing monetary policy in the current economic environment. Recent aggregate data indicate that overall economic activity slowed noticeably during the first nine months of the year. In spite of a series of shocks, the economy has proven to be remarkably resilient in recent years, and I expect it to remain so in the period ahead. A sharp pullback in the housing markets is likely to restrain aggregate activity as we move into next year. But as housing markets stabilize, I would expect overall economic performance to strengthen from the levels indicated by preliminary estimates of gross domestic product in the third quarter to a pace more consistent with the economy's long-term trend growth rate. Inflation, though down somewhat from its level earlier this year, remains uncomfortably elevated. Financial market prices imply that inflation will continue its gradual but persistent downward track during the forecast period. There remain, I believe, clear upside risks to that inflation outlook.\n\nPrices on federal funds futures and Eurodollar futures suggest that market participants expect the FOMC to cut the target federal funds rate about 50 basis points during 2007, a view consistent with expectations of a \"soft landing.\" At the same time, market-based options prices on these interest rate futures indicate that implied volatilities are quite low, suggesting a surprising degree of certainty regarding policy expectations. Taken at face value, market participants appear to be reasonably certain of a benign outcome for both economic growth and inflation. In contrast, my own judgmental forecast includes a wider range of possible outcomes than is implicit in these market-based measures.\n\nI am a strong advocate of incorporating forward-looking information from asset prices into the Fed's decision process, but we should not take market readings as determinative of policy. While we should look to financial markets for information, just as market participants look to the Fed for its policymaking views, distilling conclusions from markets is an imprecise exercise.\n\nWhy can't market prices be more assuredly relied upon? Asset prices contain term premiums, credit risk premiums, and liquidity premiums that vary over time and are themselves related to market expectations and uncertainty. Consequently, it can be difficult to determine whether movements in asset prices reflect a change in expectations, in uncertainty, or in some combination of premiums.\n\nAs an example, consider the changes in Treasury yields since the FOMC initiated the most recent tightening cycle. From mid-2004 to today, the period during which the FOMC raised the target federal funds rate from 1 percent to 5-1/4 percent, the ten-year rate has scarcely changed, on net, and now stands not much above 4-1/2 percent. Whether this configuration is a result of changes in expected rates or term premiums is an important issue for policymakers. Alternative explanations have markedly different implications for policy. If these changes reflect increased strength in underlying demand for longer-term Treasury securities, including from emerging economies, the decline should be reflected in a decline in term premiums. In such a case, all else equal, a tighter monetary policy might be preferred. On the other hand, if the decline reflects investors' views of a weaker path for the economy--the more typical interpretation of a flat or inverted yield curve--policymakers might prefer a more accommodative monetary policy.\n\nGiven the complexity of the signal-extraction problem, we should approach our task with considerable humility. We recognize that financial assets prices reflect the collective views of market participants. They may reflect not only changes in expected paths and uncertainty about those paths but also shifting relationships, changes in investor risk preferences, and developments in the structure of various securities markets. Thus, we use market prices alongside many other economic indicators, including statistical releases and large amounts of qualitative evidence.\n\nWe can enhance the role of markets by improving the availability of high-quality data for example, about corporate financial conditions and by working to improve our ability to extract signals from market data. The most significant challenge in this setting is, however, perhaps endemic to the task: Our own policies and actions affect market prices. As a result, when we look to financial markets for information, the information we seek may be shaped in part by our own views. The more that \"market information\" reflects our own actions, the less it is useful as a source of independent information to inform our policy judgments.\n\nWe need to be alert to this \"mirror problem,\" in which markets can cease to provide independent information on current and prospective financial and economic developments. In the extreme case, financial markets keenly follow the Federal Reserve, the Federal Reserve is equally attuned to the latest financial quotes, and fundamentals of the economy are obscured. Under such circumstances, asset prices might teach us only about our skills as communicators. Fortunately, the prospect for profits--the critical underpinning of all markets--mitigates this problem. Investors have strong financial incentives to analyze information about inflation and the macroeconomy to better predict the path of monetary policy. After all, Fed communications and forecasts are fallible. The anticipated dispersion of investors' views implies a distribution of returns with substantial rewards for those who get it right.\n\nMarket-based information is surely important in determining good monetary policy. This does not mean, however, that the Fed's goal is to align its views with those of the markets or that it wants the markets' views to match its own. Instead, policymakers benefit greatly by listening to views expressed in markets that are at least somewhat independent of FOMC communications. We can further enhance the role of markets by enriching our understanding of the interplay between communication policies of central banks and market prices. Good communication by the Fed should help members of the FOMC interpret market prices. Unnecessary market uncertainty or misinterpretation of our assessments will only muddy the waters.\n\nFinancial Markets and Financial Supervision and Regulation\nIn addition to making monetary policy decisions, the Federal Reserve maintains supervisory and regulatory authority over a wide range of financial institutions and activities. The Fed supervises and regulates banks and bank holding companies that together control about 96 percent of commercial banking assets in the United States.\n\nLet us consider the role of market discipline in financial supervision and regulation. First, market prices provide an independent assessment of the current and prospective financial condition of large financial firms. Second, markets can discipline the behavior of firms by adjusting the concomitant funding costs of firms as risks change.\n\nMarket discipline, however, may not always be fully effective in this context. The development of the federal safety net--deposit insurance, the discount window, and access to Fedwire and daylight overdrafts--has inevitably impeded the workings of market discipline in the regulatory arena. That is, the various elements of the safety net provide depository institutions and financial market participants with a level of safety, liquidity, and solvency that was far less prevalent before the advent of the Federal Reserve and the subsequent establishment of federal deposit insurance. By deterring liquidity panics, the safety net shields the overall economy from some of the worst effects of instability in the financial system. These benefits, however, are not without costs. The prospect of government intervention distorts market prices and may also engender excessive risk-taking.\n\nThe Federal Reserve works to reduce these distortions by enhancing market discipline and limiting expectations of government intervention. Market discipline can improve financial stability by aligning risks and rewards more closely. When risks are both known and measured, they are reflected in asset prices. To this end, bank regulators must continue to strive to develop risk-based capital measures that better reflect underlying risks. At least as important as getting capital levels right, however, are new capital frameworks to provide financial markets with better information on risk-taking by banks. In particular, by leading the development of new capital adequacy regimes, the Fed is actively working to improve the flow of information about financial institutions to market participants. 2 As a consequence of improved flows of information, market participants can better evaluate risks, price securities, and impose their own discipline on firms. These capital and disclosure reforms are aimed at improving the standardization of risk metrics and providing financial markets with meaningful disclosures for risk. Market forces can thus strengthen the incentives for banks to behave more as they would if there were no safety net at all.\n\nFor market discipline to work optimally, securities prices for the largest financial firms should reflect investor evaluations of financial risks--credit, market, and operational. Securities prices informed in this way should translate into higher funding costs when greater risks are undertaken, facilitate the appropriate level of monitoring for the effective management of counterparty risk, and help bank supervisors judge the financial condition of firms.\n\nAsset prices, however, will reflect risks only if uninsured creditors perceive that they are at risk of loss. Thus, investors should understand that the resolution procedure for bank failures does not require that all uninsured creditors be made whole. Rather, resolution requires only that uninsured creditors be made no worse off than they would have been if the bank had been liquidated in the marketplace. The ten largest U.S. banking organizations fund less than half their worldwide banking assets with deposits--insured, uninsured, and foreign. Thus, the role for market discipline is substantial: Uninsured creditors must do their own homework because protecting them is not the bank supervisor's job.\n\nPrices for financial firms are not \"pure plays\" on their expected financial conditions. Rather, the prices also incorporate the value of expected supervisory and regulatory actions should their financial condition deteriorate. These perceptions and levels of government guarantee vary substantially across firms. For example, the corrective actions used by bank supervisors to deal with undercapitalized banks are intended to encourage market discipline and to deter the expectation of regulatory forbearance. In addition, encouraging the issuance of financial market instruments, such as subordinated debt, can provide an important antidote to conjectures of government guarantees and to the misperception that some institutions are \"too big to fail.\" The threat of prohibited payments on the subordinated debt of an institution that becomes undercapitalized should be useful in ensuring vigilance by debt investors. As a result, capital adequacy becomes not the job solely of the regulator, but of market participants as well.\n\nThe Federal Reserve also works to enhance the role of market discipline in the broader financial system. For example, the Federal Reserve Bank of New York is working with dealers to improve the settlement and clearing practices of the credit derivatives industry. Reliable recordkeeping is crucial in times of stability; otherwise, it will not be available in times of distress. The Federal Reserve has also highlighted the systemic risks associated with the large portfolios of Fannie Mae and Freddie Mac. The inherent lack of counterparty discipline is a significant problem associated with the regulation of these government-sponsored enterprises (GSEs). Currently, this lack of market discipline, which is a consequence of conjectural federal government guarantees, is self-perpetuating: It has engendered a cost of capital for the GSEs that is nearly comparable to that of the federal government. It should be no surprise, then, that the GSE portfolios have grown dramatically since the early 1990s. Their growth rates have subsided more recently in light of recent accounting, regulatory, and governance problems, but without significant improvements in market discipline, it is likely that the rapid growth of GSE portfolios will resume.\n\nMarket information is not a panacea in the formulation of monetary policy and, likewise, it is not one in the context of supervision and regulation. First, market information is unavailable for many banks, often because they issue public debt only infrequently. Second, market discipline for banks is somewhat dependent on the Federal Reserve's policies and actions, and thus it has a \"mirror problem\" of its own. That is, through a \"certification effect,\" bank supervision can potentially create significant moral hazard in that investors may believe that governmental regulation supersedes their need to assess the firms' financial condition. Third, the objectives of financial markets and the Federal Reserve are not perfectly aligned. For example, equity holders of a failing institution may have an incentive to \"bet the bank\" and thereby maximize the value of the put option the institution believes it holds from the deposit insurer.\n\nThe onus continues to rest with the Federal Reserve and other financial regulators to harness the forces of market discipline as a necessary complement to more traditional modes of supervision and regulation.\n\nConclusion\nIn summary, markets inform and, in some cases, complement the monetary, supervisory, and regulatory actions of the Federal Reserve. As I hope that I have made clear, the interaction of market signals and policy is neither simple nor straightforward. You watch us and react to our actions, while simultaneously we monitor you and respond as best we can to the signals you provide about evolving economic and financial conditions. To do our part in preventing the signals from getting crossed, I believe that we at the Federal Reserve should continue our efforts to make our communications and intentions as clear as possible. That may be a tall order, but it is one worthy of our efforts.\n\nFootnotes\n\n1.  Nellie Liang, Wayne Passmore, Daniel Covitz, and Diana Hancock, of the Board's staff, contributed to these remarks. Return to text\n\n2.  The U.S. banking agencies recently asked for public comment on a notice of proposed rulemaking for implementing Basel II. Pillar three of Basel II is particularly intended to strengthen market discipline.  Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/warsh20061121a.htm",
        "title": "Financial Markets and the Federal Reserve",
        "date": "11/21/2006"
    },
    {
        "content": "November 16, 2006\n\nGovernor Randall S. Kroszner\n\nAt the Cato Institute Monetary Policy Conference, Washington, D.C.\n\nI am delighted to speak at the Cato Monetary Conference. I have participated in a number of these stimulating conferences since I was in graduate school in the 1980s and have learned a lot from them as my thinking on monetary economics and monetary policy has evolved. Rather than focus on my intellectual journey, however, I would like to focus on a fundamental change that has taken place globally since I first spoke here roughly twenty years ago. In the United States and in virtually every country around the world, inflation has declined, and in most countries dramatically so. In addition, the volatility of inflation and expectations of future inflation have also fallen significantly. I will call these changes experienced around the globe the conquest of worldwide inflation.\n\nI will begin by providing a few facts about the substantial improvement of inflation during roughly the past decade compared with the quarter century that preceded it. I will then try to understand why this remarkable decline in inflation has taken place. In particular, I argue that globalization, deregulation, and financial innovation, in part spurred by experiences of high inflation in the 1980s, have fostered currency competition that has led to improved central bank performance and, hence, the recent conquest of worldwide inflation. Friedrich Hayek had long ago advocated permitting greater competition among currencies, arguing that there would be a race to the top rather than a race to the bottom.1 Regardless of what one might think of Hayek's policy proposals, technological change in a globalized and competitive marketplace, I believe, has increased competition among currencies issued by central banks.\n\nThe increased competition among currencies has changed the ability and the incentives of governments and central banks to pursue high-inflation policies. As I will argue, such changes have allowed improvements in central bank independence, governance and credibility, thereby leading to better inflation outcomes. In addition, greater central bank credibility has allowed the development of long-term bond markets in many countries where such markets did not previously exist and flattened of yield curves around the globe as concerns about future inflation risks declined. Deeper bond markets with a wider range of available maturities encourage long-term planning and investment, and thus convey lasting gains, particularly in emerging markets. The important issue is whether the conquest of worldwide inflation will persist or be a temporary phenomenon.\n\nThe Worldwide Decline in Inflation\nFrom the 1950s until the late 1960s, inflation rates were relatively contained, and episodes of high inflation were rare. Following the collapse of the Bretton Woods fixed-exchange-rate system in the early 1970s, however, inflation became a worldwide phenomenon. Even in Germany, where prices had been the most stable of any country in the world as tracked by the International Monetary Fund (IMF), the purchasing power of the deutsche mark declined by more than half between 1972 and 1995.2 For the United States, purchasing power declined more than 70 percent over this period. For roughly half of all countries reporting to the IMF, the erosion of value of the currency was more than 90 percent.\n\nTo express this erosion in terms of cumulative inflation, $370 would be required today in the United Sates to purchase $100 worth of goods and services at 1972 prices. Brazil has by far had the worst experience of any country in the world during this period: The price level in Brazil is approximately 5 trillion times higher today than it was in 1972.\n\nSince the early 1990s, however, worldwide inflation has significantly declined. In the advanced economies, for instance, the median inflation rate has fallen from 7 percent in the 1980s to 2 percent in the current decade. In emerging markets, the median inflation rate has fallen from 9 percent to 4 percent over the same period. Indeed, the latest issue of the International Monetary Fund's World Economic Outlook shows that average inflation rates in both the advanced economies and the developing countries in recent years are at their lowest levels since at least the early 1970s.3 Indeed, the worst performers over the past five years have had inflation outcomes close to the average outcome in the 1970s.4 Thus, the worst inflation today is not nearly as bad as it once was.\n\nTo give some specific examples, just ten or twenty years ago, annual inflation rates in Brazil and Mexico exceeded 100 percent. But during the past five to ten years, Brazilian and Mexican inflation rates have remained low. In particular, inflation in Brazil failed to spike after Brazil experienced financial crises and sharp currency depreciations in the late 1990s. Given Brazil's history of hyperinflation, this stability is especially remarkable. Brazil did experience an uptick of inflation around its presidential election in 2002, but even this was minor by historical standards. This pattern of reduced inflation is seen across many countries, large and small, developed and developing.\n\nThe data thus clearly show that better inflation performance in the last decade is a worldwide phenomenon. But is this lower inflation regime likely to persist? One way to approach this issue is to investigate whether market participants appear to believe that this change is temporary or likely to last by examining measures of expected inflation. In the United States, where surveys go back furthest, long-term inflation expectations were as high as 8 percent in the early 1980s; they dropped to about 3 percent a decade ago and have since edged down a bit more, to about 2-1/2 percent, depending on your favorite measure of expectation.5 Over the past ten years, other advanced economies and a few emerging markets such as Korea have had a broadly similar experience.6\n\nIn Brazil and Mexico, for example, long-term inflation expectations have declined from a range of 7 percent to 10 percent a decade ago to a fairly steady 3 percent to 4 percent in recent years. Considering the high rates of inflation that these economies experienced not long ago, the low level and stability of long-term inflation expectations in recent years is remarkable. Indeed, few long-term inflation forecasts in any country currently exceed 5 percent, although Venezuela provides one counterexample, with a long-term inflation forecast now of 15 percent. Overall, these surveys suggest that market participants do expect low inflation to persist around the globe.\n\nIn addition, the risks of high inflation appear to have decreased as well. In particular, the volatility of inflation has declined notably almost everywhere. The standard deviation of inflation in the United States declined from as much as 4 percent in the early 1980s to around 1 percent a decade ago and has remained close to that level since then.7 Other advanced economies have had a similar experience. The decline in volatility is even sharper for the emerging markets. In the early to mid 1990s, the standard deviation of inflation in Brazil at times exceeded 100 percent and in Mexico it exceeded 30 percent. In the current decade, however, the standard deviation of Brazilian inflation has been relatively stable at around 5 percent, while the standard deviation of inflation in Mexico has declined steadily to around 2 percent, a level similar to that experienced in the advanced economies.\n\nFactors Behind the Conquest of Worldwide Inflation\nMy brief review of worldwide inflation performance suggests that something has changed in the past decade compared with the previous two decades: Inflation is substantially lower and less volatile and expectations of future inflation are also substantially lower. What has driven this change, which I call the conquest of worldwide inflation?8 In understanding the key factors behind this change, we can also shed further light on the likelihood of whether this low inflation regime will persist.\n\nIn a nutshell, I believe that the factors of globalization, deregulation, and financial innovation, arising partly in response to episodes of high inflation, have effectively eroded the central bank monopoly on the provision of monetary services and have enhanced global competition among currencies. These changes have, in turn, altered the incentives for central banks to behave badly and for finance ministries to use central banks as \"piggy banks\" to finance their fiscal policies. The resulting constraint on monetary policy, combined with increased public understanding of the costs of inflation, have led to institutional changes in central bank governance that bolster their credibility for maintaining price stability in the future. Improved central bank performance and credibility, thus, are the consequences of this combination of factors.\n\nTo develop this explanation in more detail, I will start by describing how globalization, deregulation, and innovation can alter the ability and incentive of a government to pursue a high-inflation policy.9 These factors are closely related and mutually reinforcing in many respects. Prompted in part by the collapse of central planning, many countries have turned increasingly to private markets and trade to deliver growth and progress. The resulting deregulation and greater openness has boosted innovation and has helped to increase global competition, or globalization, by shrinking the barriers of time and distance. Accordingly, trade and financial linkages between countries have soared in recent years to record levels.\n\nHow does this affect inflation? When governments resort to printing money to finance their spending, inflation rises and nominal assets lose their value. This loss of value is also known as the inflation tax. Globalization, deregulation, and innovation make it easier for citizens to move their wealth out of nominal assets in the local currency should their government resort to an inflation tax. The demand for local currency has become much more sensitive to the inflation rate, and this greater sensitivity reduces the amount of real resources that the government can obtain for a given level of inflation. Therefore, the government has a reduced incentive to resort to the inflation tax.\n\nThe specific channels by which globalization, deregulation, and financial innovation affect competition among currencies are many:\n\nGiven such developments that facilitate competition among currencies, a government that pressures a central bank to pursue an inflationary policy gets much less benefit from increased inflation because people can more rapidly and conveniently switch out of the local currency. Indeed, the web site of the central bank of Brazil explicitly acknowledges the role of inflation in driving financial innovations that enabled firms and households to economize on cash balances in that country. It states, \"Prior to the mid-1990s, [when inflation was stabilized] changes in the payment system in Brazil were motivated by the need to cope with high inflation rates. During that time, the system achieved significant technological progress, especially aimed at enhancing the speed of processing financial transactions.\"11\n\nIn addition to encouraging financial innovation, the painful experience of high inflation helped to educate the public and economists about the costs of inflation.12 Although the specific experiences differed across countries, almost everywhere public opinion eventually turned against allowing inflation to continue, and this public pressure reinforced the trend against inflationary policies. Economists and central bankers also devoted great attention to understanding the causes and consequences of inflation, providing the intellectual underpinning to policies oriented toward price stability.\n\nIn many countries, the factors I have discussed here led to changes in central banks that may make it more difficult to revert to high-inflation policies. The most notable change is the increased independence of many central banks and the correspondingly reduced control of the fiscal authorities over monetary policy. Central bank independence reduces the ability of a government to \"rob the piggy bank\" through a surprise inflation tax. In most cases, central bank independence can be reversed by a majority vote of the national legislature. But having to resort to such a vote is a greater obstacle to inflationary finance than that posed by previous arrangements, especially given the public's increased sensitivity and aversion to inflation.\n\nCentral bank independence has typically been granted in conjunction with an explicit mandate that makes the achievement of low and stable inflation one of the goals of monetary policy. Central bank independence, together with a mandate that includes price stability, increases the credibility of monetary policy. Policy is credible because the central bank's objectives are clear to the public and because the central bank can be held accountable for failing to achieve its objectives. When citizens are more aware of the costs of inflation, and when governments are less able to reap benefits from high inflation and thus have less incentive to use the central bank as a piggy bank, institutional reforms that make central banks more credible and independent are more likely to be adopted and sustained.13\n\nThe fundamental forces I have described today--globalization, deregulation, financial innovation, and public understanding about the costs of inflation--provided the impetus for fighting inflation and opened the political path to institutional reforms, such as central bank independence, that enhance central bank credibility. Once in place, these reforms made further progress against inflation easier and raised the costs of backsliding. As the benefits of stable prices accrue and as financial markets deepen and become more sophisticated, the benefits of sound economic policies will help to create support for institutional reforms that make returning to inflation harder--but not impossible--for future governments.\n\nImplications of the Conquest for Bond Markets\nWhat are the implications of the conquest of worldwide inflation for interest rates and yield curves? I believe that market confidence in continued low inflation has helped drive down the slope of the yield curve by reducing the premium demanded for holding long-term nominal assets. The conquest of inflation also has extended bond maturities and yield curves further into the future than ever before, most notably in many emerging markets.\n\nThe current low level of long-term yields in the United States and other advanced economies is widely acknowledged as somewhat of a puzzle, or, to use former Chairman Greenspan's term, a conundrum.14 Of course, flat and even inverted yield curves in advanced economies are nothing new. We know that the short end of the yield curve is dominated by monetary policy and cyclical factors.\n\nTo abstract from the potential effect of cyclical factors on the yield curve, consider the pattern of forward rates many years into the future, at which point the effects of current cyclical shocks would be expected to no longer be important. The yield on a ten-year bond, for instance, can be thought of as an average of a series of consecutive forward rates. If you could borrow and lend at the same rate as the U.S. Treasury, then you could lock in a three-month loan ten years from now by borrowing for ten years and three months and simultaneously lending the same principal for ten years. The difference between the interest you pay and the interest you earn on this transaction determines the implied forward rate ten years from today. The forward rate reflects not only the market expectation of the future short-term interest rate but also a \"term premium\" to compensate for the risk of a commitment to extend credit so far in the future, including the risk of future inflation.\n\nAt any point in time, then, we can calculate the short-term forward rate ten years ahead using the yield curve of U.S. Treasury coupon securities. This \"far forward\" rate has hovered around historically low levels of between 4-1/4 percent and 4-1/2 percent in the past year, more than 200 basis points (that is, two percent) below its average since 1990. Far-forward rates in other advanced economies have also declined over the past decade and are currently at or near historic lows.15\n\nTo some extent, low forward rates may reflect a persistent decline in expected future real rates of interest or in the real term premium. Chairman Bernanke has suggested that an excess of ex ante global saving relative to global investment has held down real interest rates around the world.16 Some of the factors behind this excess of saving over investment include the surge in revenues of oil and commodity exporters, a retreat in Asian investment demand from the boom that preceded the late 1990s, and a reduction in fiscal deficits in some Latin American countries. But these low bond yields also have a nominal aspect. The declines in inflation rates, in the volatility of inflation, and in surveys of long-term inflation expectations all point to a reduction in the compensation required by investors for the effects of future inflation on the returns to holding long-term bonds.\n\nThis development is particularly remarkable in many emerging markets, in which longer-dated fixed-coupon bonds issued in local currencies had ceased to exist during the inflationary 1970s and 1980s. The recent lengthening of maturities of domestic-currency-debt markets has, in many cases, not only extended the yield curve but--and this is one of the key results of the conquest of inflation--effectively created a local-currency yield curve that previously did not exist.\n\nSince 2000, ten-year nominal fixed-coupon bonds in the local currency have been introduced in Brazil, Chile, Colombia, Indonesia, Mexico, and Russia. Korea and Thailand introduced ten-year fixed coupon bonds in the 1990s. This year the government of Mexico issued a thirty-year fixed-coupon bond in pesos for the first time. The proportion of local-currency debt in Mexico maturing within one year was nearly 90 percent in 2002 and is now less than 75 percent.17 The Korean government continues to increase the proportion of its domestic currency debt in longer maturities, with the one-year-and-under segment falling from roughly one-half in 1999 to one-quarter by the end of last year. Moreover, maturity extension is not limited to emerging markets. France and the United Kingdom, for example, issued fifty-year bonds last year.\n\nIn addition to the development or extension of a yield curve worldwide, bond yields have tended to be relatively low and flat, at least in part because of the conquest of inflation. The flattening or slight inversion of yield curves in the major industrial economies, such as the United States, Japan, the euro area, the United Kingdom, and Canada is well known. For example, ten-year yields in the euro area are less than 50 basis points higher than three-month yields and the yield curve is currently downward-sloping in the United Kingdom.\n\nAt the same time that maturities have been extended, bond yields in local currencies of emerging-market countries have also declined. It is perhaps not surprising that, given their high rates of saving and generally high level of economic development, the governments of Hong Kong and Korea can borrow at levels close to those in the advanced economies. More notable, however, is that the Mexican government can borrow in pesos at a thirty-year maturity at only 8 percent. Although Mexico is perhaps the most striking example, it is not alone. Other middle-income emerging markets with single-digit yields on fixed-rate ten-year bonds in the local currency include Chile, Colombia, Malaysia, Russia, and Thailand, to name but a few. The computation of forward rates for most of these countries is difficult due to the relative sparseness of the maturity distribution, but for those countries in which five-year forward rates can be computed, they have been declining and have reached very low levels in the past year or so.\n\nOverall, the combination of lower and less volatile inflation around the world has led to a reduction in inflation expectations and lower perceived inflation risk, and hence to a lower premium in long rates for inflation uncertainty. I believe that these factors have been important contributors to the lower long-term yields and the flattening of yield curves, particularly in emerging markets. The establishment of markets for long-term nominal government and corporate debt in countries in which they did not exist a decade ago is powerful evidence of the faith that investors place in a future environment of price stability.\n\nBroader Economic Benefits\nThe economic benefits of price stability are too numerous and well known for me to cover here in detail. Long-run price stability certainly is essential for achieving maximum employment. However, I would like to underscore some benefits that are often overlooked, related to the development of markets for long-term bonds.\n\nPrice stability boosts growth by deepening financial markets. Given stable prices, savers and investors have more confidence about the ultimate value of their investments. Stable prices encourage the growth of financial intermediaries and financial markets. As I have discussed, many emerging markets have recently experienced a deepening of their local financial markets through a greater issuance of longer-dated paper. There is a strong positive link between banking and financial market development and economic growth, and a number of studies have concluded that banking and financial market development is a key driver of economic growth. Thus, greater central bank credibility, which permits more development of local financial markets, can have an economic benefit beyond the financial sector.18\n\nThe development of long-term local-currency bond markets may also help to enable governments and firms to plan long-term infrastructure and investment projects that boost economic development. Although such debt markets are only one of many factors that can help to lower the costs of long-term planning and enhance the ability to undertake long-term investments, such developments can improve decisionmaking. In particular, investment decisions are less likely to be constrained by having only short term financing available for longer-term projects, thereby allowing improved decision making, for governments, firms, and individuals.\n\nHigher and more stable growth combined with better ability to undertake long-term plans can help to improve the fiscal outlook for a country. A better fiscal outlook, in turn, increases confidence and financial market development and thus further boosts growth and, in a virtuous cycle, reinforces prospects for continued price stability. More-prudent fiscal policies, including lower deficits, longer debt maturities, and reduced foreign-currency debt can reduce the likelihood and potential severity of financial crises. These policies make the financial positions of emerging-market governments less vulnerable to movements in interest rates and exchange rates. A reduction in the perceived risk that a government may not be able to service its debts makes changes in investor sentiment and financial contagion less likely, thereby reducing financial market volatility.\n\nMaintaining Progress on the Conquest of Worldwide Inflation\nI have argued that globalization, deregulation, and financial innovation, in part spurred by recent experiences of high inflation, have fostered currency competition that has led to improved central bank performance and, hence, the conquest of worldwide inflation. The resulting enhancement of central bank governance and credibility has allowed the development of long-term bond markets in many countries and flattening of yield curves around the globe.\n\nI want to conclude by returning to the question of whether these phenomena are likely to persist. Globalization and innovation are genies that may prove difficult to put back in their bottles. Deregulation and global competition, however, may be more subject to change.\n\nAlthough I am an optimist, I would be remiss if I did not point out some risks to this otherwise cheerful narrative of conquest. The difficulty of reaching agreement in the Doha Round of trade negotiations highlights the risk of renewed protectionism and backtracking on deregulation. Trade barriers and regulations are anathema to globalization and competition. Barriers to the free flow of goods, services, capital, and technology would also diminish the force of innovation that has been so beneficial in the struggle against inflation. Transactions taxes and administrative barriers may hinder the development and liquidity of bond markets and there is still much progress that is required in many emerging markets on these fronts.\n\nHigh inflation can destroy an economy and result in enormous hardship for everyone involved. The benefits achieved through greater central bank credibility are substantial. Fortunately, economic forces have led to better central bank behavior around the world during the past decade. If citizens and politicians lose sight of these benefits and the forces that have led to enhanced currency competition are thwarted, these gains could prove fleeting. I believe that we must continue to work hard to lock in the gains achieved through anti-inflation credibility to ensure that the conquest will be long lasting. The lessons of the high inflation episodes are too important to forget.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20061116a.htm",
        "title": "The Conquest of Worldwide Inflation: Currency Competition and Its Implications for Interest Rates and the Yield Curve",
        "date": "11/16/2006"
    },
    {
        "content": "November 10, 2006\n\nChairman Ben S. Bernanke\n\nAt the Fourth ECB Central Banking Conference, Frankfurt, Germany\n\nMy topic today is the role of monetary aggregates in economic analysis and monetary policymaking at the Federal Reserve. I will take a historical perspective, which will set the stage for a brief discussion of recent practice.\n\nThe Federal Reserve’s responsibility for managing the money supply was established at its founding in 1913, as the first sentence of the Federal Reserve Act directed the nation’s new central bank \"to furnish an elastic currency.\"1 However, the Federal Reserve met this mandate principally by issuing currency as needed to damp seasonal fluctuations in interest rates, and during its early years the Federal Reserve did not monitor the money stock or even collect monetary data in a systematic way.2, 3\n\nThe Federal Reserve’s first fifteen years were a period of relative prosperity, but the crash of 1929 ushered in a decade of global financial instability and economic depression. Subsequent scholarship, notably the classic monetary history by Milton Friedman and Anna J. Schwartz (1963), argued that the Federal Reserve’s failure to stabilize the money supply was an important cause of the Great Depression. That view today commands considerable support among economists, although I note that the sources of the Federal Reserve’s policy errors during the Depression went much deeper than a failure to understand the role of money in the economy or the lack of reliable monetary statistics. Policymakers of the 1930s observed the correlates of the monetary contraction, such as deflation and bank failures. However, they questioned not only their own capacity to reverse those developments but also the desirability of doing so. Their hesitancy to act reflected the prevailing view that some purging of the excesses of the 1920s, painful though it might be, was both necessary and inevitable.\n\nIn any case, the Federal Reserve began to pay more attention to money in the latter part of the 1930s. Central to these efforts was the Harvard economist Lauchlin Currie, whose 1934 treatise, The Supply and Control of Money in the United States, was among the first to provide a practical empirical definition of money. His definition, which included currency and demand deposits, corresponded closely to what we now call M1. Currie argued that collection of monetary data was necessary for the Federal Reserve to control the money supply, which in turn would facilitate the stabilization of the price level and of the economy more generally.4 In 1934, Marriner Eccles asked Currie to join the Treasury Department, and later that year, when Eccles was appointed to head the Federal Reserve, he took Currie with him. Currie’s tenure at the Federal Reserve helped to spark new interest in monetary statistics. In 1939, the Federal Reserve began a project to bring together the available historical data on banking and money. This effort culminated in 1943 with the publication of Banking and Monetary Statistics, which included annual figures on demand and time deposits from 1892 and on currency from 1860.\n\nAcademic interest in monetary aggregates increased after World War II. Milton Friedman’s volume Studies in the Quantity Theory of Money, which contained Phillip Cagan’s work on money and hyperinflation, appeared in 1956, followed in 1960 by Friedman’s A Program for Monetary Stability, which advocated that monetary policy engineer a constant growth rate for the money stock. Measurement efforts also flourished. In 1960, William J. Abbott of the Federal Reserve Bank of St. Louis led a project that resulted in a revamping of the Fed’s money supply statistics, which were subsequently published semimonthly.5 Even in those early years, however, financial innovation posed problems for monetary measurement, as banks introduced new types of accounts that blurred the distinction between transaction deposits and other types of deposits. To accommodate these innovations, alternative definitions of money were created; by 1971, the Federal Reserve published data for five definitions of money, denoted M1 through M5.6\n\nDuring the early years of monetary measurement, policymakers groped for ways to use the new data.7 However, during the 1960s and 1970s, as researchers and policymakers struggled to understand the sharp increase in inflation, the view that nominal aggregates (including credit as well as monetary aggregates) are closely linked to spending growth and inflation gained ground. In 1966, the Federal Open Market Committee (FOMC) began to add a proviso to its policy directives that bank credit growth should not deviate significantly from projections; a similar proviso about money growth was added in 1970. In 1974, the FOMC began to specify \"ranges of tolerance\" for the growth of M1 and for the broader M2 monetary aggregate over the period that extended to the next meeting of the Committee.8\n\nIn response to House Concurrent Resolution 133 in 1975, the Federal Reserve began to report annual target growth ranges, 2 to 3 percentage points wide, for M1, M2, a still broader aggregate M3, and bank credit in semiannual testimony before the Congress. In an amendment to the Federal Reserve Act in 1977, the Congress formalized the Federal Reserve’s reporting of monetary targets by directing the Board to \"maintain long run growth of monetary and credit aggregates … so as to promote effectively the goals of maximum employment, stable prices, and moderate long-term interest rates.\"9 In practice, however, the adoption of targets for money and credit growth was evidently not effective in constraining policy or in reducing inflation, in part because the target was not routinely achieved.10\n\nThe closest the Federal Reserve came to a \"monetarist experiment\" began in October 1979, when the FOMC under Chairman Paul Volcker adopted an operating procedure based on the management of non-borrowed reserves.11 The intent was to focus policy on controlling the growth of M1 and M2 and thereby to reduce inflation, which had been running at double-digit rates. As you know, the disinflation effort was successful and ushered in the low-inflation regime that the United States has enjoyed since. However, the Federal Reserve discontinued the procedure based on non-borrowed reserves in 1982. It would be fair to say that monetary and credit aggregates have not played a central role in the formulation of U.S. monetary policy since that time, although policymakers continue to use monetary data as a source of information about the state of the economy.\n\nWhy have monetary aggregates not been more influential in U.S. monetary policymaking, despite the strong theoretical presumption that money growth should be linked to growth in nominal aggregates and to inflation? In practice, the difficulty has been that, in the United States, deregulation, financial innovation, and other factors have led to recurrent instability in the relationships between various monetary aggregates and other nominal variables. For example, in the mid-1970s, just when the FOMC began to specify money growth targets, econometric estimates of M1 money demand relationships began to break down, predicting faster money growth than was actually observed. This breakdown--dubbed \"the case of the missing money\" by Princeton economist Stephen Goldfeld (1976)--significantly complicated the selection of appropriate targets for money growth. Similar problems arose in the early 1980s--the period of the Volcker experiment--when the introduction of new types of bank accounts again made M1 money demand difficult to predict.12 Attempts to find stable relationships between M1 growth and growth in other nominal quantities were unsuccessful, and formal growth rate targets for M1 were discontinued in 1987.\n\nProblems with the narrow monetary aggregate M1 in the 1970s and 1980s led to increased interest at the Federal Reserve in the 1980s in broader aggregates such as M2. Econometric methods were also refined to improve estimation and to accommodate more-complex dynamics in money demand equations. For example, at a 1988 conference at the Federal Reserve Board, George Moore, Richard Porter, and David Small presented a new set of M2 money demand models based on an \"error-correction\" specification, which allowed for transitory deviations from stable long-run relationships (Moore, Porter, and Small, 1990). One of these models, known as the \"conference aggregate\" model, remains in use at the Board today. About the same time, Board staff developed the so-called P* (P-star) model, based on M2, which used the quantity theory of money and estimates of long-run potential output and velocity (the ratio of nominal income to money) to predict long-run inflation trends. The P* model received considerable attention both within and outside the System; indeed, a description of the model was featured in a front-page article in the New York Times. 13\n\nUnfortunately, over the years the stability of the economic relationships based on the M2 monetary aggregate has also come into question. One such episode occurred in the early 1990s, when M2 grew much more slowly than the models predicted. Indeed, the discrepancy between actual and predicted money growth was sufficiently large that the P* model, if not subjected to judgmental adjustments, would have predicted deflation for 1991 and 1992. Experiences like this one led the FOMC to discontinue setting target ranges for M2 and other aggregates after the statutory requirement for reporting such ranges lapsed in 2000.\n\nAs I have already suggested, the rapid pace of financial innovation in the United States has been an important reason for the instability of the relationships between monetary aggregates and other macroeconomic variables.14 In response to regulatory changes and technological progress, U.S. banks have created new kinds of accounts and added features to existing accounts. More broadly, payments technologies and practices have changed substantially over the past few decades, and innovations (such as Internet banking) continue. As a result, patterns of usage of different types of transactions accounts have at times shifted rapidly and unpredictably.\n\nVarious special factors have also contributed to the observed instability. For example, between one-half and two-thirds of U.S. currency is held abroad. As a consequence, cross-border currency flows, which can be estimated only imprecisely, may lead to sharp changes in currency outstanding and in the monetary base that are largely unrelated to domestic conditions.15, 16\n\nThe Board staff continues to devote considerable effort to modeling and forecasting velocity and money demand. The standard model of money demand, which relates money held to measures of income and opportunity cost, has been extended to include alternative measures of money and its determinants, to accommodate special factors and structural breaks, and to allow for complex dynamic behavior of the money stock.17 Forecasts of money growth are based on expert judgment with input from various estimated models and with knowledge of special factors that are expected to be relevant. Unfortunately, forecast errors for money growth are often significant, and the empirical relationship between money growth and variables such as inflation and nominal output growth has continued to be unstable at times.18\n\nDespite these difficulties, the Federal Reserve will continue to monitor and analyze the behavior of money. Although a heavy reliance on monetary aggregates as a guide to policy would seem to be unwise in the U.S. context, money growth may still contain important information about future economic developments. Attention to money growth is thus sensible as part of the eclectic modeling and forecasting framework used by the U.S. central bank.\n\nReferences\n\n\n\nBoard of Governors of the Federal Reserve System (1943). Banking and Monetary Statistics, 1914-1941. Washington: Board of Governors of the Federal Reserve System.\n\n---------- (1960). \"A New Measure of the Money Supply,\" Federal Reserve Bulletin, vol. 46 (October), pp.. 102-23.\n\n---------- (1976). Banking and Monetary Statistics, 1941-1970. Washington: Board of Governors of the Federal Reserve System.\n\n----- (1998). Federal Reserve Act and Other Statutory Provisions Affecting the Federal Reserve System. Washington: Board of Governors of the Federal Reserve System.\n\nBremner, Robert P. (2004). Chairman of the Fed: William McChesney Martin Jr. and the Creation of the American Financial System. New Haven: Yale University Press.\n\nCarpenter, Seth and Joe Lange (2003). \"Money Demand and Equity Markets.\" Federal Reserve Board Finance and Economics Discussion Series, 2003-3. Washington: Board of Governors of the Federal Reserve System, February.\n\nCurrie, Lauchlin (1935). The Supply and Control of Money in the United States, 2nd ed. Cambridge: Harvard University Press.\n\n-----------, ed. (1956). Studies in the Quantity Theory of Money. Chicago: University of Chicago Press.\n\nFriedman, Milton (1960). A Program for Monetary Stability. New York: Fordham University Press.\n\nFriedman, Milton and Anna J. Schwartz. (1963). A Monetary History of the United States, 1867-1960. Princeton: Princeton University Press.\n\nGoldfeld, Stephen M. (1976). \"The Case of the Missing Money.\" Brookings Papers on Economic Activity, 3:1976, pp. 683-739.\n\nHallman, Jeffrey J., Richard D. Porter and David H. Small (1991). \"Is the Price Level Tied to the M2 Monetary Aggregate in the Long Run?\" American Economic Review, 81(September), pp. 841-858.\n\nHumphrey, Thomas M. (1986). \"The Real Bills Doctrine (PDF 1.2 MB),\" in Thomas M. Humphrey, Essays on Inflation. Richmond: Federal Reserve Bank of Richmond.\n\nJudson, Ruth and Seth Carpenter (2006). \"Modeling Demand for M2: A Practical Approach,\" unpublished manuscript, Board of Governors of the Federal Reserve System, Division of Monetary Affairs, October.\n\nKilborn, Peter T. (1989). \"Can Inflation Be Predicted? Federal Reserve Sees a Way,\" New York Times, June 13.\n\nMankiw, N. Gregory and Jeffrey A. Miron (1986). \"The Changing Behavior of the Term Structure of Interest Rates,\" Quarterly Journal of Economics, 101(2), pp. 211-228.\n\nMeltzer, Allan H. (2003). A History of the Federal Reserve. Volume 1: 1913-1951. Chicago: University of Chicago Press.\n\nMoore, George R., Richard D. Porter, and David H. Small (1990). \"Modeling the Disaggregated Demands for M2 and M1: The U.S. Experience in the 1980s,\" in Peter Hooper et. al., eds., Financial Sectors in Open Economies: Empirical Analysis and Policy Issues. Washington: Board of Governors of the Federal Reserve System, pp. 21-105.\n\nO’Brien, Yueh-Yun C. (2005). \"The Effects of Mortgage Prepayments on M2.\" Federal Reserve Board Finance and Economics Discussion Series, 2005-43.\n\nU.S. Department of the Treasury (2006). The Use and Counterfeiting of United States Currency Abroad, Part 3 (PDF 601 KB). Washington: Department of the Treasury.\n\n\n\n\n\nFootnotes\n\n1.  Board of Governors of the Federal Reserve System (1998), 1-001. In his recent history of the Federal Reserve, Allan Meltzer (2003, p. 66) notes of some of the Act’s proponents that: \"[o]ne of their principal aims was to increase the seasonal response, or elasticity, of the note issue by eliminating the provisions of the National Banking Act that tied the amount of currency to the stock of government bonds.\" Return to text\n\n2.  See Mankiw and Miron (1986) for a discussion of the Fed’s seasonal interest-rate smoothing. The Federal Reserve did publish data on the issuance of Federal Reserve notes from its inception. Federal Reserve notes were only part of total currency in circulation, however, the remainder being made up of national bank notes, United States notes, Treasury notes, gold and silver certificates, and gold and silver coin. Beginning in 1915, the Federal Reserve Bulletin included data on currency that had been collected by the Treasury and data on total bank deposits that had been collected by the Office of the Comptroller of the Currency as a byproduct of its regulatory role, but publication was irregular. Return to text\n\n3.  Indeed, the Federal Reserve’s adherence to the real bills doctrine--which counseled against active monetary management in favor of supplying money only as required to meet \"the needs of trade\"--gave the new institution little reason to pay attention to changes in the money stock. See Humphrey (1986) for a history of the real bills doctrine. The constraints of the gold standard also restricted (without entirely precluding) active monetary management by the Federal Reserve. Return to text\n\n4.  In the second edition of his book, Currie (1935) wrote: \"The achievement of desirable objectives … rests entirely upon the effectiveness of control. The achievement, for example, of the objective of a price level varying inversely with the productive efficiency of society demands a highly energetic central banking policy and a high degree of effectiveness of monetary control… Even for the achievement of the more modest objective of lessening business fluctuations by monetary means, the degree of control of the central bank is of paramount importance.\" (pp. 3-4). Return to text\n\n5.  Board of Governors of the Federal Reserve System (1960). Return to text\n\n6.  In 1971, M1 was currency and demand deposits at commercial banks. M2 was M1 plus commercial bank savings and small time deposits, and M3 was M2 plus deposits at mutual savings banks, savings and loans, and credit unions; data from the latter type of institution were available only monthly. M4 was M2 plus large time deposits, and M5 was M3 plus large time deposits. Changes in definitions make it difficult to track the historical development of the various monetary aggregates. Approximately, the 2006 definition of M1 is equivalent to this older definition, the 2006 definition of M2 is equivalent to the older definition of M3, and the definition of M3 at its date of last publication was equivalent to the older definition of M5. M4 and M5 were dropped in a 1980 redefinition of the monetary aggregates. See Board of Governors of the Federal Reserve System (1976), pp. 10-11 and Anderson and Kavajecz (1994). Return to text\n\n7.  For instance, in late 1959 and early 1960, money growth declined as other economic indicators rose. The minutes of the December 1959 FOMC meeting report Chairman Martin as saying, \"I am unable to make heads or tails of the money supply,\" but those of the February 1960 meeting record his comment that \"the System ought to be looking at the growth of the money supply.\" For further discussion, see Bremner (2004), pp. 141-142. Return to text\n\n8.  M2 now includes currency and demand deposits (the components of M1) plus time deposits, savings deposits, and non-institutional money market funds. Return to text\n\n9.  Board of Governors of the Federal Reserve System (1998), 1-017 Return to text\n\n10.  Monetarists criticized the use of multiple targets, rather than a single objective. Another object of criticism was \"base drift,\" a set of practices that had the effect of re-setting the base from which money growth targets were calculated when the growth of one or more monetary aggregates exceeded the upper end of the Federal Reserve’s target range. Return to text\n\n11.  Whether the Federal Reserve’s policies under Chairman Volcker were \"truly\" monetarist was a much-debated question at the time. Return to text\n\n12.  The new accounts included negotiable-order-of-withdrawal (NOW) accounts and money market deposit accounts. Return to text\n\n13.  Hallman, Porter, and Small (1991) and Kilborn (1989). Return to text\n\n14.  Another possible explanation for this instability is the Goodhart-Lucas law, which says that any empirical relationship that is exploited for policy purposes will tend to break down. This law probably has less applicability in the United States than in some other countries, as the Federal Reserve has not systematically exploited the relationships of money to output or inflation, except perhaps to a degree in 1979-82. Return to text\n\n15.  For a recent summary, see U.S. Department of the Treasury (2006). Return to text\n\n16.  As another example, U.S. regulations require servicers of mortgage-backed securities to hold mortgage prepayments in deposits counted as part of M2 before disbursing the funds to investors. A wave of mortgage refinancing and the resulting prepayments can thus have significant effects on M2 growth that are only weakly related to overall economic activity. See O’Brien (2005) for more discussion. Return to text\n\n17.  See Judson and Carpenter (2006) for a summary. A special factor that helps to explain some episodes of variable money demand is stock market volatility (Carpenter and Lange, 2003). Return to text\n\n18.  A recent example of instability occurred in the fourth quarter of 2003, when M2 shrank at the most rapid rate since the beginning of modern data collection in 1959 without any evident effects on prices or nominal spending. Subsequent analysis has explained part of the decline in M2 (the transfer of liquid funds into a recovering stock market was one possible cause), and data revisions have eliminated an additional portion of the decline, but much of the drop remains unexplained even well after the fact. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20061110a.htm",
        "title": "Monetary Aggregates and Monetary Policy at the Federal Reserve: A Historical Perspective",
        "date": "11/10/2006"
    },
    {
        "content": "November 03, 2006\n\nVice Chairman Donald L. Kohn\n\nAt the American Bar Association, Banking Law Section, Washington, D.C.\n\nI am pleased to have the opportunity to speak to this group of lawyers who practice in the banking area. As banking lawyers are well aware, the Federal Reserve embodies a unique legal structure compared with other central banks around the world. Unlike the Bank of England, for example, the Federal Reserve is a not a single entity, but a decentralized system. The essential components are the Board of Governors, a federal government agency, and the twelve regional Federal Reserve Banks, which are structured essentially as private corporations.\n\nThis structure has served the nation exceptionally well over the years. In the past decade or so, however, the emergence of nationwide banking systems, significant changes in the nation's payments systems, technological advances, and other developments have prompted changes in the ways in which we meet our responsibilities, including a geographic consolidation of a number of the functions that historically had been performed by each of the individual Reserve Banks. Today, I will review these trends, and consider some of their practical and legal implications for the Reserve Banks and for the future role of the current decentralized network of Reserve Bank offices around the country.1\n\nThe Existing Structure of the Federal Reserve System\nThe current structure of the Federal Reserve--the combination of a centralized government agency and regional corporate Reserve Banks--is the product of a carefully crafted political compromise. In the early years of our nation, the First and Second Banks of the United States performed many basic central banking services. These were banks that, while chartered by Congress, were owned and managed by private, nongovernmental interests. Their charters, however, were allowed to expire, and their demise has been attributed to deep-seated opposition in some parts of the country to the centralization and concentration of economic power.\n\nAs the nation grew through the nineteenth and early twentieth centuries, it lacked any entity that was constituted to carry out the basic roles of a central bank. However, after a financial panic in 1907 forced a number of banks to close, disrupting the economy, a consensus emerged that the nation needed some form of central bank, and Congress created the National Monetary Commission. The commission, chaired by Rhode Island senator Nelson Aldrich, called for one central institution, with fifteen branches across the country, to issue currency and discount commercial paper. However, thirty-nine of the institution's forty-two-member board of directors would be bankers, which aroused the long-standing fear of some about the concentration of economic power in the hands of a few large banks. Agrarian and progressive interests, led by William Jennings Bryan, favored a central bank under public, rather than banker, control. But the vast majority of the nation's bankers, concerned about government intervention in the banking business, opposed a central bank structure directed by political appointees.\n\nThe legislation that Congress ultimately adopted in 1913 reflected a hard-fought battle to balance these two competing views and created the hybrid public-private, centralized-decentralized structure that we have today. A centralized governmental Federal Reserve Board, with members appointed by the President and confirmed by the Senate, exercises general oversight over the Federal Reserve System and works with the Reserve Banks to determine policies to fulfill the Federal Reserve's legislative mandates. The Reserve Banks were intended to be \"banker's banks\" and to carry out the operational functions of a central bank. Additional duties were assigned to them as a result of subsequent developments. Reserve Banks now have broad responsibilities, playing a central role in the nation's payment systems and also in monetary policy through their participation in open market operations and membership on the Federal Open Market Committee. In addition, the Reserve Banks supervise state-chartered banks that choose to be members of the Federal Reserve System and all bank holding companies, under the overall direction of the Board of Governors.\n\nEach of the Reserve Banks is structured as a separate corporation operating under a charter granted by the Comptroller of the Currency. The stock of each Bank is owned by the member commercial banks within its District; however, the holding of Reserve Bank stock is in the nature of an obligation that goes along with membership and does not carry with it the characteristics of control or financial interest normally attached to stock in a corporation. Each Reserve Bank is supervised by its own board of directors. This board of directors, subject to the approval of the Board of Governors, appoints a president, who serves as the Bank's chief executive officer and has general charge and control over the business of the Bank.\n\nHowever, in laying out the governance structure for the Reserve Banks, Congress adopted several significant departures from the standard business corporation model. Notably, three of each Bank's nine directors are selected by the Board of Governors, which designates one of its appointees as the Chairman. Three of the directors are elected by the stockholding banks and must represent the public. The remaining three directors are chosen by and represent the member banks. The Board of Governors is also given general oversight authority relating to the Reserve Banks and their activities. These characteristics reflect the fact that the Reserve Banks are structured to carry out public policy objectives set in the Federal Reserve Act, not to advance the interests of their shareholders.\n\nConsolidation of Certain Reserve Bank Functions\nIn recent years, in response to major developments in the financial industry and technology, as well as statutory and regulatory changes, a number of Reserve Bank functions have evolved from highly localized operations at individual Reserve Bank offices to more consolidated and centralized functions. This trend is particularly evident in the financial services the Reserve Banks offer to depository institutions.\n\nFrom the creation of the System, the Federal Reserve has not only been closely involved in overseeing the nation's payment systems but has also been an important operational component of that system. Historically, each Reserve Bank and Branch provided a full range of services to local institutions. In particular, the Federal Reserve Act gave the System the authority to establish a nationwide check-clearing system to minimize inefficiencies and disruptions.\n\nThe ongoing transformation of our retail payments system, resulting from shifts in consumer behavior and rapid industry innovation, has directly affected the operations of the Reserve Banks. The number of checks being written has been steadily falling as consumers increasingly take advantage of electronic payments mechanisms. In 2003, for the first time, the number of electronic payments in the United States, such as credit card, debit card, and automated clearinghouse (ACH) payments, exceeded the number of check payments. A range of data indicates that electronic payments have continued to increase and that check payments have continued to decline. Not only are more payments being made electronically, but more check payments are also being processed electronically, in part because of the enactment of the Check Clearing for the Twenty-first Century Act, commonly known as Check 21.2\n\nAs a result of these trends, the Reserve Banks' check-collection volume has declined. Since 1999, the number of checks collected through the Reserve Banks has fallen by about 30 percent. Consequently, the Reserve Banks have taken major steps to reduce check costs, including reducing the resources devoted to this service by transferring check-processing operations from some offices to more centralized locations. Today, twenty-two offices offer check processing, down from forty-five just three years ago.\n\nOther Reserve Bank services have been undergoing consolidation as well. At some Branches, cash processing by the Federal Reserve has been replaced by a cash depot operated under contract by armored carriers, who collect currency deposits from and distribute currency to depository institutions. The Federal Reserve System has also consolidated certain other services for banks and the U.S. Treasury, such as automated clearinghouse (ACH), offline Fedwire, and savings bond services. Similar trends are occurring in other areas of Reserve Bank operations. The Reserve Banks have found it more efficient to have a few central offices perform certain internal support and back-office services--such as managing information technology and payroll--rather than having each Bank conduct them individually. This trend toward consolidation of operations has precipitated significant structural changes at the Reserve Banks. Staff levels have been reduced throughout the System. Several Reserve Bank Branch offices now have, or soon will have, no remaining financial services operations. These Branches focus on matters such as community affairs, economic information gathering, economic and financial education, and director recruitment. Some Branches have sold their own buildings and operate in leased space.\n\nIn addition to affecting operations, the centralization of some Reserve Bank functions has raised issues related to the governance of the Reserve Banks in accordance with the provisions of the Federal Reserve Act. As mentioned earlier, Congress provided that each Reserve Bank is controlled by its own board of directors and president, subject to the general oversight and approval authority assigned to the Board of Governors by the Federal Reserve Act. Under the act, the directors are required to perform the duties \"usually appertaining to the office of directors of banking associations.\" When Reserve Bank functions are consolidated, operations that once had been performed by a Reserve Bank under the direction of that Bank's board and president are now carried out on its behalf by another Reserve Bank, which has its own governance reporting chain running to its president and board of directors. And some of the operations at each Reserve Bank are being performed for the other Banks. To address these complications, the Reserve Banks have put into place procedures designed to ensure accountability by those Banks that provide services to, or conduct functions for, other Banks on a centralized basis. More generally, as interdependencies have risen, the Reserve Banks have had increasingly to learn to manage processes and control risks across Reserve Banks. This learning process has been challenging at times as individual Reserve Banks have had to give up performing functions they had been providing for many years and as structures to oversee and manage across Reserve Banks were constructed and strengthened. But the challenges are being met.\n\nPayments and operations are not the only areas of responsibility affected by the changes in the structure of our financial system and in technology. In the supervision and regulation of financial institutions, it has become more and more common for Reserve Banks to cooperate, coordinate, and share resources and analysis as the Federal Reserve adapts to oversight of large complex banking organizations and prepares for the implementation of Basel II.\n\nThe continued consolidation and adaptation in at least some Reserve Bank functions is likely to continue. For example, check volumes are forecast to continue to decline as more payments are made electronically, and the clearing of paper checks will drop even faster as banks take advantage of Check 21. In response, the Reserve Banks will have to reduce even further the number of offices processing checks. In some other functions too we are likely to find opportunities to operate more efficiently and effectively from fewer locations.\n\nWe will be taking advantage of those opportunities. The Monetary Control Act, enacted in 1980, requires the Federal Reserve to set fees for providing certain payment services to depository institutions. These fees must, over the long run, recover all the direct and indirect costs of providing the services, including imputed costs that would have been incurred and imputed profits that would have been earned if the services had been provided by a private firm. And for these types of services, we will simply be following the dictates of a marketplace in which legislative and technological change is driving consolidation. More fundamentally, to protect our independence within the government--the arms-length relationship that provides monetary policy making with a degree of insulation from short-run political pressures--Congress has given us autonomy over our budget. It is incumbent on us to exercise stewardship of our small part of the public purse as responsibly as we can.\n\nThe Future of a Decentralized Federal Reserve System\nGiven these recent developments, what will the Federal Reserve System of the future look like? Will a decentralized system of twelve Reserve Banks located nationwide continue to make sense?\n\nI think that it will. But my reasons for thinking so have little to do with the delivery of services to depository institutions. Rather, they reflect the attributes of the Reserve Banks that flow from their status as independent, separate entities in close contact with their region's financial institutions, businesses, and communities.\n\nMost important, this status enables the Reserve Banks to bring informed, diverse views to our policy deliberations. Senior management at the Reserve Banks have their own staffs of economists, financial analysts, supervisors, payments system experts, and, yes, even attorneys, to help them formulate their positions in our policy councils. Local offices, where Federal Reserve officials can have direct and frequent contact with the financial institutions and businesses in the immediate region, can gather a wealth of very current information about economic conditions and the financial system within that region that may not be available from other sources. The diversity of perspectives, the analysis backing those views, and the information the Reserve Banks bring from their regional contacts help us reach better policy decisions. The Reserve Banks are also a source of continuity, experience, and institutional memory, especially when, as now, the Board of Governors has experienced rapid turnover.\n\nThe contributions of the Reserve Banks are perhaps most visible to the public in the sphere of monetary policy. All twelve presidents attend and participate actively in the meetings of the Federal Open Market Committee. What they tell the Committee about what they are hearing from their contacts helps us recognize shifting economic conditions before they are evident in the data. Their reports often illuminate the reasons for the data we are receiving, thereby helping us anticipate what will come next. Their policy perspectives are informed by the research and analysis of outstanding staffs.\n\nThe presidents and other senior Reserve Bank officials bring similar strengths to our discussions of other types of policies. The Banks' active role in payments helps us to design policies and procedures to encourage a safe and efficient payments system. Their daily contacts with depository institutions contributes to shaping our regulatory decisions and is a resource when we are managing crises. Their close relationship to their communities is important as we administer consumer-protection statutes and work to foster fair and open access to credit and greater public understanding of available financial services.\n\nI have stressed the information a Reserve Bank brings from its community to Federal Reserve policymaking, but Reserve Banks also play an increasingly important role in reaching out to convey information to the community. Through extensive education efforts, speeches by their presidents and other officers, and discussions with their boards of directors and other groups, Reserve Banks foster understanding about the System, its policies, and its objectives, as well as about matters of general economic interest.\n\nIn addition, because of their many contacts with their communities, local offices of the Reserve Banks play a major role in the Federal Reserve's efforts to improve financial education and financial literacy. In recent years, financial innovations have widened the range of households that have access to opportunities to borrow and lend and have added to the variety and complexity of instruments they can use. The Federal Reserve has responded by putting added emphasis on promoting improved financial literacy among the general public. Greater financial sophistication among consumers will produce a number of economic benefits. Informed consumers place market pressure on financial institutions to offer better products at better prices. Increased financial literacy is also likely to result in better management of personal finances.\n\nIn sum, strong, independent Reserve Banks grounded in their regions have been critical to the success of the Federal Reserve. The private-public, centralized-decentralized character of the Federal Reserve System has contributed to policymaking and has been a part of the fabric of our independence within the government. The Reserve Banks will need to continue to adapt to changing circumstances, but their essential characteristics will be preserved and will continue to enable them to help the Federal Reserve pursue the very important public policy objectives we have been entrusted with.\n\n\n\n\n\nFootnotes:\n\n1.  My remarks today reflect my own thoughts and not necessarily those of the other members of the Federal Reserve Board. Rich Ashton and Yvonne Mizusawa, of the Board's Legal Division, contributed to these remarks. Return to text\n\n2. Before Check 21, a bank had to present the original paper check to the paying bank unless the paying bank had agreed to accept electronic presentment of the check. Although Check 21 did not mandate the electronic processing or presentment of checks, it did authorize a new negotiable instrument, called a substitute check, which is the legal equivalent of the original check. By permitting banks to use substitute checks in the check-collection process when the recipient could not or would not accept electronic payment, Check 21 has facilitated the expanded use of electronic technologies in check processing, enabling the banking industry and the Federal Reserve to improve the efficiency and cost-effectiveness of its check-processing operations. Return to text",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20061103a.htm",
        "title": "The Evolving Role of the Federal Reserve Banks",
        "date": "11/3/2006"
    },
    {
        "content": "November 02, 2006\n\nGovernor Susan S. Bies\n\nAt the Drake-FEI Lecture, Des Moines, Iowa\n\nThank you for inviting me to speak with you today.  I am pleased to have the opportunity to address both leading financial professionals--the members of Financial Executives International--as well as future leaders--the students here at Drake University.  In my remarks today, I will discuss the near-term outlook for the U.S. economy and some of the longer-run issues that economic policy makers should consider. I want to emphasize that these views are my own and not necessarily those of my colleagues on the Federal Open Market Committee (FOMC).\n\nEconomic activity slowed in the middle part of this year.  Real gross domestic product increased at a 2.6 percent annual rate in the second quarter of this year, and last week the Commerce Department announced that output rose at only a 1.6 percent rate in the third quarter.  These figures are down notably from the nearly 3-1/2 percent average pace of the preceding two years.  Despite the recent slowing in output, however, resource utilization remains relatively high by historical standards and thus continues to be a potential source of upward pressure on inflation.\n\nIn the aftermath of the 2001 recession, the FOMC eased monetary policy substantially.   However, the degree of easing in place in 2003 and 2004 was clearly unsustainable and risked overheating the economy.  Since mid-2004, the FOMC has gradually moved monetary policy from an accommodative stance to a more neutral position.  As a consequence, the elements now appear to be in place for some easing of resource utilization rates over the next year or so and a reduction in inflationary pressures.  However, substantial uncertainty surrounds the near-term outlook.  In determining the future path of interest rates, the FOMC will be guided by the incoming data on both output and prices, so let’s begin by reviewing recent developments.\n\nEconomic Activity\nThe slowdown in the growth of real GDP since the spring largely reflects a cooling of the housing market:  The number of single-family and multifamily housing starts has fallen nearly 25 percent since the beginning of the year; sales of both new and existing homes have dropped sharply since their peak of last summer, and the inventory of unsold homes has soared.  At the same time, homes are appreciating more slowly and in some markets prices are even declining.\n\nWhile much of the downshift in the housing market appears to have occurred already, some further softening may yet lie ahead.  Nonetheless a variety of factors should help limit any remaining contraction in housing demand.  For example, despite the 4-1/4 percentage point increase in short-term interest rates over the past two years, the interest rate on a thirty-year fixed-rate mortgage has increased only about 1/2 percentage point, and borrowing costs continue to be relatively low.  The ongoing growth in real incomes and the recent increase in the stock market wealth of households should also support the demand for housing.\n\nIt is encouraging also that the recent weakness in residential construction does not appear to have spilled over to other sectors.  For instance, employment has been growing smartly in nonresidential construction, even as it has shrunk in the residential sector.  In addition, consumer confidence currently stands a bit above its long-run average and consumption is still being fueled by past house-price gains, which raised household wealth.  This contrasts with previous slowdowns in the housing market, which have typically coincided with widespread economic weakness.\n\nAlthough the slowdown in the housing market has so far done little to reduce consumer outlays, other factors do appear to have had a damping effect.   In particular, consumption likely was restrained earlier this year by the rise in energy prices, which took a large bite out of household budgets.  The rise in energy prices over the past few years has also affected the auto sector--reducing the demand for sport utility vehicles and other gas-guzzling automobiles.  As a result, inventories of these vehicles have risen, and domestic automakers have been cutting production in response.\n\nIn the business sector, spending on nonresidential construction has been particularly robust.  In the third quarter, nonresidential investment grew at an annual rate of 14 percent, down from the sizzling 20 percent pace in the previous quarter but still very substantial.  Expenditures on drilling and mining structures have increased particularly rapidly in response to high prices for natural gas and crude oil.  Investment in other types of structures, such as offices and commercial buildings, has also been strong over the past year or so.\n\nSpending on equipment and software, which grew quite rapidly from mid-2004 to early 2006, has advanced at a more moderate pace lately.  The recent slowdown in the growth of business sales would be expected, all else equal, to have a damping influence on capital spending, and in fact business confidence has moved down since the start of the year.  However, order books for capital goods such as industrial machinery and other types of heavy equipment appear to be full and should support near-term investment gains.  Moreover, the demand for information technology equipment is also likely to be well maintained, in part because of the recent introduction of a new generation of microprocessing chips and more-efficient large servers.\n\nCurrent financial conditions also are supportive of business spending.  Corporate balance sheets are strong and flush with cash, and broad stock price indexes are up more than 10 percent so far this year.  At the same time, yield spreads on corporate bonds across the ratings spectrum have been low, supported by the strong balance sheets and robust profit growth.\n\nInflation\nThe picture painted here is one of an economy that has been growing solidly, albeit at a rate below its potential.  What are the implications of this picture for inflation prospects?  Consumer prices excluding food and energy have accelerated over the past year, and this clearly is a concern.  The core inflation rate rose 2.4 percent over the most recent four quarters, up from 2.0 percent for the same period a year ago.  In thinking about the macroeconomic consequences of inflation, it makes sense to abstract from the prices of energy and food when the focus is on the short run.  Temporary shocks to food and energy prices typically don’t translate into changes in inflationary pressure.  However, if these shocks persist, they may have an effect on core inflation and, more generally, on the economic behavior of households and businesses.  Core inflation can be affected when the price changes are propagated along the production chain--say from oil prices to the prices of chemicals and ultimately to the prices of goods made with those chemicals.  In addition, the shocks to food and energy prices may affect inflation expectations.  Thus, we also pay attention to broader measures of inflation.\n\nNonetheless, the scene appears to be set for a deceleration in prices over time.  One contributing factor is likely to be the slowing in activity I already discussed, which should ease the overall pressure on resources.  Another important factor affecting the inflation outlook is household and business expectations for inflation.  As best we can judge, inflation expectations appear to be well contained:  Measures of longer-term inflation expectations, based on surveys and on a comparison of yields on nominal and inflation-indexed government debt, have remained within the ranges in which they have fluctuated in recent years.   Finally, the recent decline in energy prices, if it is sustained, should reduce cost pressures along the production chain.\n\nOne upside risk to the inflation outlook comes from the labor market.  The unemployment rate declined steadily between the second half of 2003 and the beginning of 2006 and has stood at a relatively low 4.7 percent for the past six months.  With labor markets comparatively tight by historical standards, unit labor costs have begun to accelerate, especially since the end of last year, and firms may pass on some of these higher costs to consumers.  However, the large markup of prices over costs--the margin is currently well above its historical average--could act as a shock absorber if cost strains were to intensify. Thus, in my judgment, inflation appears poised to decelerate in coming months as energy prices stabilize and resource pressures ease.  But the risks to that outlook seem tilted toward the upside.\n\nAggregate Supply\nIn considering the appropriate setting for monetary policy, the level of the economy’s underlying productive capacity--its potential output--is the benchmark against which we assess actual output.  Accordingly, whether the recent slowdown in economic activity eases resource constraints enough to reduce inflationary pressure depends importantly on how fast potential output is growing.  If the key determinants of potential output--the workforce, economic efficiency, and the capital stock--grow quickly, as they did in the second half of the 1990s, then GDP can also rise quickly without increasing the pressure on the economy’s resources.  Conversely, a reduced rate of growth of potential output would require slower growth of actual GDP to keep resource pressures from increasing.\n\nI’d like to spend a little time examining in greater depth the outlook for some of the factors that determine potential output, starting with the labor force.  The size of the labor force depends on a combination of two factors: the size of the working-age population and the likelihood that members of this population join the labor force--a likelihood that economists refer to as the labor force participation rate.\n\nThe labor force participation rate tends to vary over the business cycle as potential workers become more or less encouraged about job prospects.  However, the influence of labor force participation on potential output does not depend on short-run conditions in the labor market but rather on long-run changes due to demographic and social factors.  For instance, in the 1950s and 1960s the labor force participation rate stood at just under 60 percent.  In subsequent years, women entered the labor force in large numbers and thus dramatically pushed up the participation rate.  Indeed, by some estimates, the increase in the labor force participation of women aged sixteen years and older added a little more than 1/2 percentage point per year to the growth rate of potential output between the late 1960s and the early 1990s.\n\nNow, the United States is facing another change in the trend of labor force participation.  The baby boomers, the large population born between 1946 and 1964, are getting older, and the oldest are turning sixty this year.  Older individuals tend to have relatively low participation rates, with many people starting to retire in their fifties and more still when they reach sixty and then sixty-five.  Thus, with the aging of the boomers, a large share of the population is entering the low-participation years, which will tend to pull down the aggregate labor force participation rate.\n\nRecent work by economists at the Federal Reserve Board has explored how changes in the age distribution of the population affect the participation rate.  For instance, between 1995 and 2005 the participation rate declined on net from 66.4 percent to 66.0 percent.  The study suggests that changes in the age distribution of the population--the movement of a large portion of the population from their high-participation-rate years to their later, low-participation-rate years--can explain the bulk of the decline.1  The changing age distribution--primarily the aging of the baby boomers--is expected to lower the participation rate by about 0.2 percentage point next year and continue to lower it over the next several years.\n\nHowever, this decomposition assumes that the participation rate for each age group is constant at its average between 1995 and 2005.  But the propensity of individuals of a given age to participate in the labor force changes over time.  Already, individuals aged fifty-five and older are working more than they did ten years ago, perhaps because of better health; higher levels of education; and a reduction, over time, in the share of workers employed in physically strenuous occupations.  Unfortunately, there is still much we do not understand about the increase in the participation rates of older workers, so it is difficult to predict how much their participation will rise in the future.  However, given the magnitude of the predicted age-related decline, it is unlikely that changes in behavior could completely offset it.\n\nAs I noted earlier, the reduction in the growth of the labor force and, thus, of potential output has important implications for how we interpret incoming economic data.  For example, to the extent that the aging of the baby boomers reduces the growth in labor force participation and hence potential output, the benchmark we use for assessing the macroeconomic implications of actual GDP growth will need to be lower.  Similarly, changes in the expected growth rate of the labor force affect our interpretation of the monthly employment data.  If the labor force participation rate remains at its current level, then what might be thought of as the “equilibrium” growth rate of payroll employment--that is, the increase consistent with a stable unemployment rate--would be about 140,000 per month.  However, if the labor force participation rate instead declines 0.2 percentage point over the next year, as suggested by the Fed’s staff research, then the comparable equilibrium payroll employment growth would be closer to 110,000 per month.\n\nWhile reductions in the labor force participation rate will apparently damp the growth rate of potential output in coming years, productivity growth, another important factor in determining the capacity of the economy, likely will remain supportive. Although productivity growth has stepped down from the scorching pace seen early in the recovery, factors remain in place for continued solid growth over the next few years. One element is capital deepening, that is, the rate at which the stock of equipment, software, and so forth is expanding relative to the number of workers, or--to put it even more simply, how fast workers are getting more of the tools they need. As I mentioned earlier, business investment spending has been strong in recent years and seems likely to remain at a high level for some time. Another element is improvements in the efficiency of how businesses do business. Here it appears that the flexibility of business processes and product, financial, and labor markets in the United States will continue to allow for the quick adoption of new technologies and the efficient reallocation of resources.\n\nOn balance, despite the outlook for continued solid longer-run productivity growth, the slowing in trend labor force growth will likely yield a modest deceleration the growth of potential output.  However, the considerable uncertainty that, as I noted earlier, surrounds the prospect for all of these elements makes it extremely difficult in real time to discern changes in potential output.  Ferreting out the changing trends in these elements is an important part of making monetary policy.  For example, the early identification of the resurgence of productivity growth, and hence of potential output growth, that began in the mid-1990s allowed the Federal Reserve to put in place a monetary policy that accommodated both strong economic growth and low inflation during the second half of that decade.2  Similarly, it is important now to try to understand the new forces determining potential output growth so that monetary policy can respond accordingly.\n\n\n\n\n\nFootnotes:\n\n1.  Stephanie Aaronson, Bruce Fallick, Andrew Figura, Jonathan Pingle, and William Wascher (2006),  “The Recent Decline in the Labor Force Participation Rate and Its Implications for Potential Labor Supply,” Brookings Papers on Economic Activity,  1:2006, pp. 69-154. Return to text\n\n2.  One of the papers used by many observers inside and outside the Federal Reserve to suggest the possibility of a mid-1990s inflection point in productivity growth was Carol Corrado and Lawrence Slifman (1999), \"Decomposition of Productivity and Unit Costs,\" American Economic Review, vol. 89 (May), pp. 328-32. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20061102a.htm",
        "title": "The Economic Outlook",
        "date": "11/2/2006"
    },
    {
        "content": "November 01, 2006\n\nChairman Ben S. Bernanke\n\nAt the Opportunity Finance Network’s Annual Conference, Washington, D.C.\n\nGood afternoon and thank you for inviting me to speak to your annual conference. Since I assumed my new responsibilities earlier this year, as well as during my earlier stint at the Federal Reserve as a member of the Board of Governors, I have met numerous times with community development leaders to discuss both their achievements and the challenges they face. In visiting some underserved communities, I have seen first-hand the effects of various development initiatives. I have also regularly attended the meetings of the Board’s Consumer Advisory Council, which brings together community representatives and lenders to discuss a range of consumer and community development issues. These experiences have helped me appreciate the many ways that community development financial institutions (CDFIs) work to strengthen communities and improve the lives of lower-income people.\n\nEnsuring that every American has the chance to improve his or her economic circumstances through hard work, saving, entrepreneurship, and other productive activities is essential for building healthy communities and achieving sustainable economic growth. The Federal Reserve and the CDFI community share a common interest in increasing economic opportunity for all Americans. The Federal Reserve supports local economic development, for example, through its active engagement in financial literacy programs, through its community outreach efforts, through aspects of its role in regulating banking and financial markets, and by its research in regional economics. But the Fed’s central mission--to help maintain a financial and macroeconomic environment that fosters price stability and maximum sustainable employment--is of necessity focused on the economic performance of the nation as a whole. Monetary policy is a blunt tool that cannot target industries, population groups, or regions. In contrast, as you know, CDFIs operate primarily at the microeconomic level, community by community. Using techniques such as financial counseling, local market research, and specialized lending, CDFIs work with partners in both the public and the private sectors to help unlock the economic potential of lower-income and underserved communities.\n\nThe theme of my remarks today is our shared goal of increasing economic opportunity. I will first discuss some of the progress that has been made in recent years in the economic situations of lower-income households and communities as well as some of the important challenges that remain. I will also offer my perspective on how CDFIs and their partners can help to meet those challenges.\n\nImprovements in Economic Opportunity and Some Challenges\nIn the past decade or so, U.S. households overall have experienced notable gains in terms of some key indicators of economic opportunity. Three such indicators that I will briefly discuss are access to credit, rates of homeownership, and small business development. Moreover, as measured by these indicators, recent improvements in traditionally underserved markets appear to have been as great as or greater than those in middle- and upper-income households and communities. At the same time, however, the gaps between lower-income households and other households with respect to these measures of opportunity remain wide.\n\nAccess to Credit\nAccess to credit is an important element of economic opportunity and community economic development: It supports homeownership and small-business creation and provides greater financial flexibility for households. In recent years, advances in information and communication technologies, improved methods of risk measurement and risk assessment, the availability of more-comprehensive information about individuals’ credit histories, and an increased ability of retail lenders to obtain funds from capital markets have led to what has been called the “democratization” of credit. As the pricing of credit risk has become more sophisticated and more consistent, as scale economies have reduced costs, and as funding sources have increased, lenders have been able to extend credit to households and businesses that might previously have been considered uncreditworthy.\n\nThe growth of subprime mortgage lending is one indication of the extent to which access to credit has increased for all households, including those with lower incomes. In 1994, fewer than 5 percent of mortgage originations were in the subprime market, but by 2005 about 20 percent of new mortgage loans were subprime.1 Indeed, the expansion of subprime lending has contributed importantly to the substantial increase in the overall use of mortgage credit. From 1995 to 2004, the share of households with mortgage debt increased 17 percent, and in the lowest income quintile, the share of households with mortgage debt rose 53 percent.2\n\nAlthough the emergence of risk-based pricing has increased access to credit for all households, it has also raised some concerns and questions, which are magnified in the case of lower-income borrowers. For example, although subprime lending has grown substantially, are prime credit products sufficiently available and do lenders effectively compete in all communities, including historically underserved communities? How well are lower-income borrowers matched with credit products and loan terms that fit their circumstances? Are borrowers aware of the terms and conditions of their loans, and more generally, are consumers sufficiently well informed to be wary of potentially misleading marketing tactics and to shop effectively among lenders? Some evidence, including recent Federal Reserve research on consumers holding adjustable-rate mortgages, suggests that awareness could be improved, particularly among borrowers with lower incomes and education levels.3 This research suggests the need for greater financial literacy and increased access to financial counseling, a point to which I will return.\n\nThe release this year and last of mortgage price data gathered under the Home Mortgage Disclosure Act (HMDA) has highlighted a different, but potentially related, concern about access to credit on equal terms. The data show that blacks and Hispanics are considerably more likely than other borrowers to receive higher-priced loans.4 This finding has several possible--and not mutually exclusive--explanations, ranging from illegal discrimination to the effects of legitimate pricing factors not captured in the HMDA data, such as loan-to-value ratios and borrower credit history. Of course, as an agency committed to the rigorous enforcement of the fair lending laws, our job is to distinguish legitimate from illegitimate sources of pricing differentials among the banking institutions we supervise. In our enforcement efforts, we analyze the HMDA price disparities in conjunction with other information, such as the adequacy of the lender’s fair lending controls and the presence of business practices that may put lenders at risk for pricing discrimination. For example, a lender might offer its loan officers financial incentives that have the effect of inducing them to charge some applicants higher interest rates or to “steer” them to higher-priced loan products. Lenders at risk for pricing discrimination receive targeted reviews of their pricing to ensure that they are complying with fair lending laws.\n\nLoan price disparities, however, are not just a legal and supervisory issue. They also raise important social and policy concerns. The questions I raised earlier about access to prime products, lender competition, and borrower awareness and financial literacy may well be relevant to understanding the price disparities we observe. Further research to explore these questions and their possible connection to disparities in lending to members of minority groups would be highly worthwhile. In fact, the Federal Reserve’s upcoming Community Affairs Research Conference will feature several papers that explore these issues.5\n\nHomeownership\nThe important issue of loan pricing aside, expanded access to mortgage credit has helped fuel substantial growth in homeownership. The national rate of homeownership increased from 1995 through mid-2006, reaching nearly 69 percent of all households this year.6 All major racial and ethnic groups have made gains in homeownership, but in percentage terms the largest increases have been made by minority households. In particular, since 1995 the homeownership rate has increased 7 percent among white households, 11 percent among black households, and 19 percent among Hispanic households. However, despite the relatively more rapid growth in minority homeownership, significant differences persist: For example, the homeownership rate for blacks and Hispanics remains about two-thirds the rate for non-Hispanic whites.\n\nAs for homeownership in lower-income areas, the Federal Reserve’s Survey of Consumer Finances indicates that, from 1995 to 2004, census tracts in all income groupings experienced gains in homeownership, with rates in lower-income areas growing somewhat faster than those in higher-income areas.7 But, again, important gaps remain. For example, in 2004, the rate of homeownership in lower-income areas was roughly 47 percent, compared to 72 percent in middle-income areas.\n\nSmall Businesses\nAnother area in which progress has been made both generally and in lower-income communities is small business development. Small businesses are essential to the economic well-being and vibrancy of local communities and of the U.S. economy as a whole. The U.S. Small Business Administration estimates that small businesses account for about half of private-sector output and employ more than half of private-sector workers.8 Moreover, because small businesses sometimes become big ones, small-business ownership can be a significant stepping stone for economic advancement, particularly in traditionally underserved populations. Between 1997 and 2002, the number of businesses owned by Asians, blacks, Hispanics, and women grew more than 20 percent for each group--more than twice the national rate of increase for all businesses.9 Nevertheless, small businesses face continual challenges. Each year, about half a million firms close, in some cases because of difficulties obtaining credit.10 Interestingly, the data do not indicate that experiences in obtaining credit differ greatly across neighborhoods of different income levels. According to the Federal Reserve’s Survey of Small Business Finances, for example, the proportions of businesses that were either denied credit or did not apply for fear of being turned down were similar across neighborhood income groups.11\n\nCDFIs as a Solution to Market Failures\nMany factors have contributed to the economic gains that I have cited, including broad macroeconomic forces and advances in the delivery of financial services. CDFIs have also played a valuable role by analyzing the economic potential of lower-income markets and developing strategies and marshaling resources to tap that potential.\n\nAs CDFI leaders, you are keenly aware of the economic challenges that you work to overcome each day. Economists find it useful to think about these challenges in the context of the economics of market failure. Standard economic analysis tells us that when competitive conditions prevail in a market, the resulting prices induce firms and individuals to allocate resources in a manner that tends to maximize social welfare.12 However, economists also recognize that various deviations from idealized market conditions, termed market failures, can inhibit the efficient allocation of resources.13 In one type of market failure, called a neighborhood externality, the actions of one person affect the well-being or economic welfare of others in the local area, but the individual taking the action neither bears the full costs of nor reaps the full benefits from those actions. Because the individual does not bear the full consequences of the actions taken, he or she may act in a way that is not in the best economic interest of the neighborhood as a whole. For example, the failure of some owners to maintain their properties can lower the value of well-maintained properties in the same neighborhood. Ultimately, such spillover effects from neglected properties can lead to underinvestment in the whole community, potentially harming all neighborhood residents and businesses.\n\nA related type of market failure studied by economists is known as an information externality. An information externality may arise when information about economic opportunities in an area has the potential to benefit many investors but is costly to gather. As a result, no single potential investor may find obtaining the data to be profitable. For example, on average, lower-income areas have fewer owner-occupied homes and record fewer home-purchase loans than higher-income areas do.14 Lower transaction activity makes accurately gauging property values and evaluating credit risks in those areas more difficult, which may inhibit the extension of credit.15 Alternatively, lower-income people may have shorter and more-irregular credit histories, making an evaluation of their individual creditworthiness more difficult and costly. Because a potential investor who bears the costs of obtaining data about underserved neighborhoods may be able to obtain only a portion of the full economic benefits, these data may remain uncollected.16\n\nOne purpose of CDFIs is to help overcome these and other market failures that inhibit local economic development. For example, by facilitating larger-scale property development projects, coordinating public and private investment efforts, and working to improve amenities and services in a local area, CDFIs may help to solve collective action problems and reduce neighborhood externalities. CDFIs can counter information externalities by assuming the cost of learning about their local communities and developing specialized financial products and services that better fit local needs. In general, CDFIs provide coordinated development activities and community-specific information that the market may not supply on its own.\n\nAmong other benefits, the familiarity with each community that CDFIs develop can help to gauge and control risk.17 For example, the use by CDFIs of appraisers who specialize in evaluating properties in a particular community produces more-reliable estimates of the value of the loan collateral. Likewise, CDFIs structure loans and use public and private credit enhancements both to increase borrowers’ ability to qualify for loans and to spread the associated credit risk among a mix of private creditors and other providers of funds.\n\nAlthough these specialized techniques can reduce credit risk, they are labor-intensive and, consequently, expensive. Most private lending institutions reduce costs by adopting processes that are highly standardized and automated. Such systems are not necessarily compatible with lending to borrowers who require substantial screening, counseling, and monitoring or with acquiring specialized information about community development lending. Part of the explicit mission of CDFIs is to assume the costs of conducting such research and analyses in underserved communities. CDFIs have also developed techniques and strategies--such as flexible underwriting criteria, specialized loan products, and intensive financial education programs--to meet the financial circumstances of their communities. Moreover, in recent years, CDFIs have been working to expand their role as information brokers beyond the local communities they serve. Through national initiatives to collect industry-wide data and to securitize community development loan portfolios, CDFIs are working to expand access to credit and capital in lower-income markets. In short, knowledge and expertise--together with the ability to build new relationships--are the principal contributions that CDFIs bring to the marketplace and to underserved communities.\n\nIs Community Development Lending Profitable?\nCan private-market participants profit from community development lending? Data based on Community Reinvestment Act (CRA) examinations tell us much about the volume of such loans but less about their performance and profitability. However, a Federal Reserve survey found that nearly all banks reported that their community development activities were profitable, at least to some degree.18 About two-thirds of the banks also reported receiving some benefit from their lending unrelated to loan profitability, such as an improved image in the community.\n\nSince the Federal Reserve report, studies undertaken by the CDFI Data Project show that, for 2004, charge-off rates for CDFI portfolios were similar to those for the banking industry as a whole.19 These studies and market data suggest that banks and other private organizations may become an increasingly significant source of competition for CDFIs. That is good news, not bad news. Indeed, the surest sign of a CDFI’s success is that private investors see viable investment opportunities in the neighborhoods in which the CDFI has been operating.\n\nThe Continuing Relevance of CDFIs\nAlthough in some sense the mission of CDFIs is to make themselves unnecessary, I expect that the knowledge and good will that they have accumulated in local communities will continue to make them relevant. For example, I mentioned earlier the loan pricing disadvantages faced by members of minority groups that have appeared in the HMDA data. CDFIs may be able to help reduce those discrepancies by using their local knowledge and financial expertise to offer alternatives to conventional subprime lending. The Opportunity Finance Network, for instance, will be competing with subprime lenders via a mortgage-credit platform that centralizes some CDFI lending processes and directly links counseling and lending services. At the same time, CDFIs continue to expand their ability to attract private investment funds, for example, through increasing transparency and developing the means of providing objective evaluations of their financial and mission-related effectiveness.\n\nThese efforts demonstrate the ability of CDFIs to adapt their business strategies to evolving markets, as indeed they have done throughout their thirty-year history. I expect that the local knowledge and specialized financial expertise that CDFIs provide will continue to add significant economic value and complement market forces in the support of community economic development. Thus, CDFIs are likely to contribute to our shared goal of expanding economic opportunity for many years to come.\n\nFootnotes\n\n1.  Estimates are based on information from Inside Mortgage Finance Publications (2005 and earlier years), Mortgage Market Statistical Annual (Bethesda, Md.: IMFP), www.imfpubs.com  Return to text\n\n2.  Calculations by Federal Reserve Board staff from the Federal Reserve Board’s Survey of Consumer Finances, 1995 and 2004. Further information on the Survey of Consumer Finances is in Arthur B. Kennickell, Martha Starr-McCluer, and Annika E. Sunden (1997), “Family Finances in the U.S.: Recent Evidence from the Survey of Consumer Finances,” Federal Reserve Bulletin, vol. 83 (January), pp. 1-24; and Brian K. Bucks, Arthur B. Kennickell, and Kevin B. Moore (2006), “Recent Changes in U.S. Family Finances: Evidence from the 2001 and 2004 Survey of Consumer Finances,” (444 KB PDF) Federal Reserve Bulletin, vol. 91 (Winter), pp. A1-A38, www.federalreserve.gov/pubs/bulletin/2006/financesurvey.pdf.  Return to text\n\n3.  Brian Bucks and Karen Pence (2006), “Do Homeowners Know Their House Values and Mortgage Terms,” Finance and Economics Discussion Series 2006-03 (Washington: Board of Governors of the Federal Reserve System, March).  Return to text\n\n4. Robert B. Avery, Kenneth P. Brevoort, and Glenn B. Canner (2006), “Higher-Priced Home Lending and the 2005 HMDA Data,” Federal Reserve Bulletin, vol. 92 (September), www.federalreserve.gov/pubs/bulletin/2006/hmda/bull06hmda.pdf (580 KB PDF).  Return to text\n\n5. Federal Reserve System Community Affairs Research Conference, “Financing Community Development: Learning from the Past, Looking to the Future”, March 29-30, 2007, http://www.philadelphiafed.org/econ/conf/financingcd/callforpapers-ca-research2007.pdf.  Return to text\n\n6. www.huduser.org/periodicals/ushmc.html  Return to text\n\n7. Calculations by Federal Reserve Board staff from the Federal Reserve Board’s Survey of Consumer Finances, 1995 and 2004, www.federalreserve.gov/pubs/oss/oss2/scfindex.html.  Return to text\n\n8. U.S. Small Business Administration, Office of Advocacy, www.sba.gov/advo/stats/sbfaq.pdf (91 KB PDF).  Return to text\n\n9. U.S. Census Bureau, Survey of Business Owners, www.census.gov/csd/sbo/.  Return to text\n\n10. U.S. Small Business Administration, Office of Advocacy, www.sba.gov/advo/stats/sbfaq.pdf (91 KB PDF).  Return to text\n\n11. Calculations by Federal Reserve staff from the Federal Reserve Board’s 2003 Survey of Small Business Finances. For further information, see www.federalreserve.gov/pubs/bulletin/2006/smallbusiness/smallbusiness.pdf (178 KB PDF).  Return to text\n\n12. Kenneth J. Arrow (1951), “An Extension of the Basic Theorems of Classical Welfare Economics,” in J. Neyman (ed.), Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability (Berkeley and Los Angeles: University of California Press), pp. 507-32.  Return to text\n\n13. Paul Milgrom and John Roberts (1992), Economics, Organization, and Management (Englewood Cliffs, N.J.: Prentice Hall).  Return to text\n\n14. The decennial census and annual HMDA data indicate, for example, that the average number of owner-occupied properties and home purchase loans in lower-income areas is less than half the average number in higher-income areas.  Return to text\n\n15. This argument is developed in detail in William W. Lang and Leonard I. Nakamura (1993), “A Model of Redlining,” Journal of Urban Economics, vol. 33 (March), pp. 223-34.  Return to text\n\n16. A detailed model of this phenomenon is in William C. Gruben, Jonathan A. Neuberger, and Ronald H. Schmidt (1990), “Imperfect Information and the Community Reinvestment Act,” (2.7 MB PDF) Federal Reserve Bank of San Francisco, Economic Review, vol. 3 (Summer), pp. 27-46.  Return to text\n\n17. Board of Governors of the Federal Reserve System (1993), Report to the Congress on Community Development Lending by Depository Institutions (Washington: Board of Governors, October).  Return to text\n\n18. Board of Governors of the Federal Reserve System (2000), The Performance and Profitability of CRA-Related Lending, Report to the Congress submitted pursuant to section 713 of the Gramm-Leach Bliley Act of 1999 (Washington: Board of Governors, July), www.federalreserve.gov/boarddocs/surveys/craloansurvey/default.htm.  Return to text\n\n19. CDFI Coalition, CDFI Data Project, 2004, www.cdfi.org/cdfiproj.asp#fy; and Elizabeth C. Klee and Gretchen C. Weinbach (2006), “Profit and Balance Sheet Developments at U.S. Commercial Banks in 2005,” Federal Reserve Bulletin, vol. 92 (June) www.federalreserve.gov/pubs/bulletin/2006/bankprofits/0606bankprofit.pdf (221 KB PDF).  Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20061101a.htm",
        "title": "Community Development Financial Institutions: Promoting Economic Growth and Opportunity",
        "date": "11/1/2006"
    },
    {
        "content": "October 17, 2006\n\nGovernor Susan Schmidt Bies\n\nAt the American Bankers Association Annual Convention, Phoenix, Arizona\n\nGood morning. It is always an honor to address the American Bankers Association. Having been a banker, I find it particularly interesting to address this group in my current role as supervisor and central banker. I hope my past private sector experience helps provide a useful perspective on our current regulatory and supervisory policies.\n\nToday I would like to focus on the topic of enterprise risk management. I am quite pleased to see more and more sessions at conferences devoted to risk management, analyzing its different facets and exploring ways to tailor it to specific institutions and situations. Indeed, there is a growing understanding that good risk management should be an integral part of running any type of business. A key theme I would like to highlight today is that all banking institutions should seek ways to improve risk management, but that the methods to improve risk management should depend on the size and sophistication of the institution.\n\nIn my remarks I will look at some recent cases in which we believe bankers and supervisors have learned some key lessons about enterprise risk management, or ERM. These lessons demonstrate how good risk management increases business efficiency and profitability. But before I start discussing particular examples, I want to take a step back and give you my thoughts on ERM generally.\n\nGeneral Thoughts on Enterprise Risk Management\nThe financial services industry continues to evolve to meet the challenges posed by emerging technologies and business processes, new financial instruments, the growing scale and scope of financial institutions, and changing regulatory frameworks. A successful ERM process can help an organization meet many of these challenges by providing a framework for managers to explicitly consider how risk exposures are changing, determine the amount of risk they are willing to accept, and ensure they have the appropriate risk mitigants and controls in place to limit risk to targeted levels.\n\nOf course, ERM is a fairly broad topic that can mean different things to different people. For our purposes here today, I will define ERM as a process that enables management to effectively deal with uncertainty and associated risk and opportunity, enhancing the capacity to build stakeholder value. Borrowing from ERM literature, I would say that ERM includes\n\n\n\nSome of you are probably familiar with the ERM framework published over a year ago by the Committee of Sponsoring Organizations of the Treadway Commission, or COSO. The COSO framework provides a useful way to look at ERM and helps generate further discussion. In the COSO framework, ERM consists of eight interrelated components derived from the way management runs an enterprise and integrated with the management process: (1) internal environment, (2) objective setting, (3) event identification, (4) risk assessment, (5) risk response, (6) control activities, (7) information and communication, and (8) monitoring. Each of these is described in more detail in the COSO literature.\n\nNotably, the COSO framework states explicitly that, while its components will not function identically within every entity, its principles should apply to all sizes of institutions. Small and mid-size entities, for example, may choose to apply the framework in a less formal and less structured way and scale it to their own needs--as long as quality is maintained. This underscores the message from bank supervisors that good risk management is expected of every institution, regardless of size or sophistication. Naturally, there will still be some tension between what supervisors expect and what bankers do, but we hope that supervisory expectations for risk management are becoming more and more aligned with the way that bankers run their businesses.\n\nAnd as most of you know, running a smaller or less complex bank presents different types of challenges and requires a risk management framework appropriately tailored to the institution. For example, smaller organizations often face a challenge of ensuring independent review of processes and decisions since officers and staff members often have multiple responsibilities that can present conflicts of interest.\n\nHaving made some general points, I would now like to discuss a few recent examples from banking that highlight the importance of ERM. With the benefit of hindsight, the financial regulators and the industry have been trying to distill the lessons learned from these recently identified weaknesses in risk management and internal control in the financial services sector.\n\nCompliance Risk\nOne area in which ERM provides tangible value is compliance risk. This type of risk may arise when an organization fails to comply with the laws, regulations, or codes of conduct that are applicable to its business activities and functions. The Federal Reserve expects banking organizations to have in place an infrastructure that can identify, monitor, and effectively control the compliance risks that they face. Needless to say, the infrastructure should be commensurate with the nature of the organization's compliance risk. For a large complex banking organization, dealing with compliance risk can be particularly challenging unless it has a well-developed risk management program. On the other hand, smaller organizations with limited staffs face a challenge in keeping up to date with changing regulations.\n\nTo create appropriate compliance-risk controls, organizations should first understand compliance risk across the entire entity. Understandably, this can be a daunting task, but I think most would agree that an effective risk assessment is critical. Managers should be expected to evaluate the risks and controls within their scope of authority at least annually. An enterprise-wide compliance-risk management program should be dynamic and proactive. It should assess evolving risks when new business lines or activities are added, when existing activities and processes are altered, or when there are regulatory changes. The process should include an assessment of how those changes may affect the level and nature of risk exposures, and whether mitigating controls are effective in limiting exposures to targeted levels. To avoid having a program that operates on autopilot, an organization must continuously reassess its risks and controls and communicate with all employees who are part of the compliance process. If compliance is seen as a one-off project, an organization faces the risk that its compliance program will not keep up with the changes in its services or customer mix. The board of directors needs to ensure the organization has a top-to-bottom compliance culture that is well communicated by senior management so that all staff members understand their compliance responsibilities. Clear lines of communication and authority help to avoid conflicts of interest.\n\nCompliance-risk management can be more difficult for management to integrate into an organization's regular business processes because it often reflects mandates set out by legislation or regulation that the organization itself does not view as key to its success. For example, bankers understand how vital credit-risk management and interest-rate risk management are to their organizations, because they reduce the volatility of earnings and limit losses. However, regulations enacted for broader societal purposes can be viewed as an expensive mandate. For example, the Patriot Act requires significant reporting of transactions to the government, and many in the banking industry have expressed frustration about the burden associated with such reporting. I can assure you, we recognize banking organizations' investment in and commitment to compliance with regulatory requirements, including those imposed by anti-money-laundering and counter-terrorism regulations. The Federal Reserve will continue to work with our counterparts in the federal government to encourage feedback to the industry on how reporting is contributing to our common fight against money laundering and terrorism.\n\nOperational Risk\nOver the past few years, the Federal Reserve has been increasing its focus on operational risk. For many nonfinancial organizations, the largest share of enterprise risk is likely to be operational risk, as opposed to credit and interest-rate risk. Banks have learned much from the practices that nonfinancial firms have developed over the years. Operational risk has more relevance today for bankers largely because they are able to shed much of their interest-rate and credit risk through sales of loans, use of financial derivatives and sound models to manage the risks that are retained. Further, the fastest-growing revenue streams are increasingly related to transaction processing, servicing accounts, and selling sophisticated financial products. To be successful, organizations must have complex systems to execute these activities. Banks are also utilizing advanced models to estimate and manage credit risk and market risk exposures. Growing use of sophisticated models requires stronger risk management practices since weaknesses in the models' operational design and data integrity can lead to significant losses. Thus, effective risk management requires financial institutions to have more-knowledgeable employees to identify system requirements, monitor their effectiveness, and interpret model results appropriately.\n\nWe have learned quite a bit about operational risk from our examinations of banking organizations. For example, during routine examinations we look at the adequacy of banks' procedures, processes, and internal controls. Such reviews include transaction testing of control routines in higher-risk activities. For example, a bank's wire transfer activities and loan administration functions are often targeted for review, and our experiences have identified some common weaknesses in operational control that are worthy of attention.\n\nWith wire transfers and similar transactions, a banking organization could suffer a significant financial loss from unauthorized transfers and incur considerable damage to its reputation if operational risks are not properly mitigated. A few recurring recommendations from our reviews are to (1) establish reasonable approval and authorization requirements for wire transactions to ensure that an appropriate level of management is aware of the transaction and to establish better accountability; (2) establish call-back procedures, passwords, funds transfer agreements, and other authentication controls related to customers' wire transfer requests; and (3) pay increased attention to authentication controls, since this area may also be particularly susceptible to external fraud.\n\nLoan administration is another area where banking organizations could suffer significant financial losses from inappropriate segregation of duties or lack of dual controls. An institution could also incur considerable damage to its reputation if operational risk factors are not properly mitigated. A few recurring recommendations from these types of reviews that may be applied to corporations more generally are to (1) ensure that loan officers do not have the ability to book and maintain their own loans; (2) confine employee access to only those loan system computer applications that are consistent with their responsibilities; and (3) provide line staff with consistent guidance, in the form of policies and procedures, on how to identify and handle unusual transactions.\n\nMortgage Lending\nEffectively managing the risk of a mortgage portfolio involves much more than prudent underwriting. Experienced risk managers should understand the need to temper their enthusiasm during boom times by considering carefully the accompanying risks. These risks include the possibility that expectations for future income growth for marginal borrowers may be optimistic. In addition, there could be an accumulation of outsized portfolio concentrations that leave the institution exposed to a downturn in that sector. And the need to consider these risks is most pronounced when competition among lenders for market share is most intense.\n\nDuring the recent housing boom, faced with soaring home prices and rising interest rates, many borrowers have sought to lower their debt service obligations by turning to mortgages with nonstandard payment and amortization schedules. Much of the new loans extended in the past two years have been nontraditional mortgages, including adjustable-rate mortgages with teaser rates and negative amortization features. At the same time, some banks have weakened proof of income and appraisal standards, and did not fully assess borrowers' ability to pay when interest rates rise and full amortization begins. In addition, a fair share of the recent lending with nontraditional products has been in the subprime market.\n\nNet housing wealth (as a multiple of income) also jumped over the same period. To some extent this increase is a source of comfort, providing larger collateral cushions to lenders. And a solid base of household housing wealth has been important to household confidence and influenced their appetite for consumption spending. But we know from experience the risks of extending credit with too much emphasis on collateral values. A borrower's equity in his or her home matters most when a property is foreclosed, something that both lenders and borrowers would prefer to avoid. Having equity in a home can provide an added incentive for borrowers to stay current on their loans, of course, especially for second homes and investment properties. Most important, borrowers want their home mortgage payments to remain current, and that requires cash flow that is adequate to comfortably service the loan.\n\nAt present, mortgage delinquencies remain low, although delinquencies on subprime mortgages have risen in recent months. The recent rise in subprime mortgage delinquencies has been concentrated among adjustable-rate subprime loans, which is probably related to interest rates resetting--as the first reset tends to occur much earlier for subprime ARMs than prime ARMs. The outlook for mortgage credit quality remains favorable, but modestly cautionary signs are on the horizon. We have had clear initial signals in recent months that housing prices are no longer rising as they had been and are declining modestly in some key markets. Growth in housing wealth may slow or stagnate while the debt service obligation continues to rise, as teaser rates expire and fully-indexed loan rates eventually catch up with increases in market rates. While we continue to expect that mortgage delinquencies will remain manageable, lenders should closely monitor future developments.\n\nInformation Security\nIssues involving information security and identity theft have received quite a bit of attention from the federal government over the past several years. Not too long ago, President Bush signed an executive order that created an Identity Theft Task Force for the purpose of strengthening federal efforts to protect against identity theft. The heads of the federal bank regulatory agencies are designated members of this task force; and as supervisors of financial institutions, I believe we can offer a valuable perspective on this issue.\n\nAs you have probably noticed, cyber attacks and security breaches involving nonpublic customer information appear in the headlines almost every week. These events have cost the financial services industry millions of dollars in direct losses and have done considerable reputational damage. The cost of identity theft to affected consumers is also significant. Banking organizations' increased use of the Internet as a communication and delivery channel have resulted in the need for and use of more-sophisticated control mechanisms, such as enterprise-wide firewall protections, multifactor authentication schemes, and virtual private-network connections.\n\nWhile many of the widely publicized information security breaches have involved parties outside the affected banking organization accessing the organization's customer information, organizations also remain at risk for breaches or misuses of information by an insider. During our examination activities, we have seen operating losses that were traced back to weak controls over insiders' access to information technology systems interfacing with electronic funds transfer networks. Further investigation into these situations suggests that the duration and magnitude of the fraud and resulting losses is a direct function of the internal party's access to accounting and related systems.\n\nSeveral lessons have emerged. First, institutions should tightly control access to funds transfer systems and ensure that access settings enforce separation of duties, dual controls, and management sign-offs. Second, an institution's senior management should be restricted from regular access to business-line functional systems, especially funds transfer systems. When such restriction is impractical, additional controls must be in place and functioning effectively. Finally, effective management of information security risk, even when focused on a specific function, requires an enterprise-wide approach to yield a true and complete evaluation of the associated risks.\n\nPortfolio Credit Risk\nPortfolio credit risk also should be recognized and managed across the entire organization. In some cases, firms may be practicing good credit risk management on an exposure-by-exposure basis, but they may not be paying close enough attention to aggregation of exposures across the entire organization.\n\nPracticing good portfolio credit risk management is not easy. Institutions often encounter challenges in aggregating exposures and identifying and measuring credit concentrations within the entire portfolio. Naturally, supervisors from time to time have concerns about growing credit risk concentrations at banks and bankers' ability to manage them. A current example is commercial real estate (CRE). Recently, the U.S. banking agencies issued proposed supervisory guidance on managing CRE concentrations.\n\nWhile banks' underwriting standards are generally stronger than they were in the 1980s, the agencies are proposing the CRE guidance now to reinforce sound portfolio management principles that a bank should have in place when pursuing a CRE lending strategy. A bank should be monitoring performance both on an individual loan basis as well as on a collective basis for loans collateralized by similar property types or in the same markets. In addition, while lending to different geographic areas can provide diversification, bankers should be mindful of potential problems when they begin to lend outside their market or \"footprint,\" where they normally have better market intelligence. In recent years, supervisors have observed banks lending outside their established footprint--to maintain a customer relationship--into real estate markets with which they have less experience.\n\nOne misconception about our draft CRE guidance relates to the proposed explicit thresholds. Contrary to what many think, these thresholds are not intended as hard limits. Rather, the thresholds should be viewed as supervisory screens that examiners should use to identify banks with potential CRE concentration risk. Examiners would expect organizations to strengthen their portfolio risk management as CRE concentrations grow. This would include effective monitoring of emerging conditions in the real estate market segments where a bank is lending. Institutions are expected to conduct their own analyses of CRE concentration risk and establish their own concentration limits. Institutions, after all, are in the best position to identify and understand their concentration risk and it is the job of supervisors to confirm that institutions are indeed doing so.\n\nConclusion\nAt the Federal Reserve, we believe that all banking organizations need good risk management. An enterprise-wide approach is appropriate for setting objectives across the organization, instilling an enterprise-wide culture, and ensuring that key activities and risks are being monitored regularly. Clearly, there is always an opportunity to improve upon ERM strategies and maintain the proper discipline to implement them effectively. In addition, bankers should be mindful that problems can sometimes quickly arise in a business line or unit that has presented no past difficulties. Accordingly, it is always helpful to evaluate the \"what if\" scenarios even for the most pristine of business units.\n\nBut the manner in which risk management challenges are addressed can--and should--vary across institutions, based on their size, complexity, and individual risk profile. In many cases, it simply does not make sense for small organizations to adopt the most sophisticated risk management practices--but that does not absolve such smaller institutions of their responsibility to improve risk management. Additionally, as supervisors, we want to ensure that institutions are not only identifying, measuring, and managing their risks but also developing and maintaining appropriate corporate governance structures to keep up with their business activities and risk taking. Our hope is that the guidance we offer to bankers on these various topics is becoming more consistent with their own risk management practices.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20061017a.htm",
        "title": "A Supervisory Perspective on Enterprise Risk Management",
        "date": "10/17/2006"
    },
    {
        "content": "October 16, 2006\n\nChairman Ben S. Bernanke\n\nBefore the Annual Convention of the American Bankers Association, Phoenix, Arizona, and the Annual Convention of America’s Community Bankers, San Diego, California(via satellite)\n\nToday I will discuss bank regulation and supervision from a cost-benefit perspective, focusing on how the Federal Reserve and our fellow bank regulators take benefits and costs into account when we develop rules and supervisory policies. As you know, the Federal Reserve's regulatory powers and responsibilities derive ultimately from statutes passed by the Congress and signed by the President. Historically, the goals of banking regulation have included the safety and soundness of bank operations, the stability of the broader financial system, the promotion of competition and efficiency in banking, assistance to law enforcement, consumer protection, and broader social objectives. Whatever the motivation, once the Congress decides that a particular issue must be addressed, it typically (though not always) gives the federal banking agencies significant discretion to devise the regulations and supervisory policies that implement the statute. Often, the agencies collaborate in developing rules, and we rely heavily on input from the public received both through formal requests for comment and through other channels, such as consultations with industry or consumer groups.\n\nIn setting regulatory and supervisory policy, we are first concerned with ensuring that the rules reflect the intent of the Congress. We also seek to implement the will of Congress in a manner that provides the greatest benefit at the lowest cost to society as a whole. Perhaps I should emphasize the phrase \"society as a whole.\" We are ever mindful that banks and their customers bear a large share of the costs of regulation. Minimizing the regulatory burden on banks is very important. But other parts of society, besides enjoying some of the benefits of regulation, also share some of the costs, both direct and indirect. Making good regulatory policy requires that we take a broad view of the way our rules affect our economy and our society, while maintaining a suitable degree of humility about our ability to accurately quantify the relevant benefits and costs.\n\nIn the rest of my remarks, I will briefly illustrate how the Federal Reserve, along with the other federal banking agencies, applies these principles in three major areas of bank regulation and supervision: the Bank Secrecy Act, bank capital standards, and the Community Reinvestment Act. Finally, I will highlight some elements of the Regulatory Relief Act, which the Congress passed this year. Periodically the Congress reviews the federal banking laws to determine if the costs imposed by some laws are no longer justified by the associated benefits, and this act is its latest effort in this area. The Federal Reserve supported and actively contributed to the development of this legislation.\n\nThe Bank Secrecy Act\nThe prevention and detection of the criminal misuse of the financial system, including threats to national security such as the financing of terrorist activities, are among the highest of public policy priorities. The primary goal of the Bank Secrecy Act (BSA), passed by the Congress in 1970, is to help deter, detect, and investigate money laundering and other financial crimes, including terrorist financing. As you know, this act gives U.S. banking institutions the responsibility to obtain sufficient customer information to detect and report suspicious activity.\n\nThe potential benefits of the information obtained through the BSA are large, but implementation of the act should not ignore other public policy considerations, including the need to maintain a reasonable expectation of financial privacy for legitimate bank customers and to ensure that reporting requirements do not unduly impede the efficient operation of the payments system. The principal concern about the BSA that we hear from the banking industry, of course, is the cost of compliance. The Federal Reserve recognizes that the provisions of the BSA require considerable effort by banks to obtain, document, and provide the required information. Deterring and identifying misuse of the financial system, as important as that is, should not be so onerous that it stifles innovation, interferes with the critical economic functions of financial intermediaries, places undue burden on bank shareholders and customers, or reduces the international competitiveness of U.S. banks. To address these important concerns, the Federal Reserve has worked and will continue to work closely with the other federal banking agencies and the Treasury Department's Financial Crimes Enforcement Network (FinCEN), the administrator of the BSA, to look for ways to streamline the reporting processes created by the BSA without diminishing the value to law enforcement of the information produced.\n\nThe regulatory burden of the BSA is also affected by supervisory policy. From a supervisory perspective, I see at least three areas in which progress could be made in reducing the burden on banks. First, the industry should have the opportunity to receive feedback about the usefulness of reporting suspicious activity as well as guidance about how better to identify the most significant risks. Some useful steps have been taken. For example, FinCEN's publication, The SAR Activity Review, includes aggregate information and case studies about suspicious activity report (SAR) filings and use, and law enforcement representatives have undertaken outreach efforts to communicate to the financial services industry the importance of BSA reports in investigations and prosecutions. Efforts to further increase feedback would help banks allocate their compliance resources more efficiently while complying with the act and preventing misuse of the financial system.\n\nSecond, the banking industry should have effective channels for voicing concerns about burden or about lack of clarity regarding regulatory standards and supervisory expectations. One such channel is the Bank Secrecy Act Advisory Group, created by the Congress, which includes representatives from government and the financial services industry. In particular, the examination subcommittee of the advisory group can serve as a conduit for the industry to raise issues of supervisory concern. The Federal Reserve will continue to seek industry input through a variety of channels, including meetings with banking groups and as part of the supervisory process itself.\n\nThird, supervisors should continue to work to improve the consistency of their approach to compliance and to ensure that adequate guidance is provided to assist banks in the assessment and management of risks. The release of the Bank Secrecy Act/Anti-Money Laundering Examination Manual in June 2005 was, I believe, an important step in that direction. The five federal banking agencies in collaboration with FinCEN developed the manual, with input from state banking agencies and the Office of Foreign Assets Control. The manual, which was revised this year, emphasizes that supervision of compliance efforts should be risk-based; that is, supervisors should focus on banks' policies and procedures, not on isolated incidents, with particular attention to the areas in which the most serious problems might arise. In a difficult area like this one, it is also particularly important that supervisors be flexible, using good judgment and a collaborative approach to help banks achieve the objectives of the act. Your feedback on the manual and on the related supervisory procedures is welcome.\n\nBank Capital Standards\nBank capital standards provide a second illustration of our efforts to balance the benefits and costs of regulation and supervision. Capital regulation is the cornerstone of bank regulators' efforts to maintain a safe and sound banking system, a critical element of overall financial stability. For example, supervisory policies regarding prompt corrective action are linked to a bank's leverage and risk-based capital ratios. Moreover, a strong capital base significantly reduces the moral hazard risks associated with the extension of the federal safety net.\n\nThe banking regulators broadly agree that the current risk-based capital regime, known as Basel I, is inadequate for the largest and most complex banking organizations. For this reason, in 2004, all the U.S. banking agencies joined other members of the Basel Committee on Banking Supervision in supporting a new international capital adequacy framework, called Basel II. Basel II capital requirements will be much more risk-sensitive than those in Basel I and will provide stronger incentives for institutions to improve the measurement and management of risk. Basel II will also give supervisors a better framework for evaluating the adequacy of a bank's capital buffer above the regulatory minimums and should improve market discipline by providing financial markets with better information on banks' risk-taking.\n\nThe U.S. banking agencies recently asked for public comment on a Notice of Proposed Rulemaking (NPR) for implementing the Basel II advanced approaches in the United States. In developing this proposal, our paramount concern has been ensuring the safety and soundness of the U.S. banking system. This concern can be seen, for example, in the proposal's transitional safeguards, which go beyond those in the 2004 Basel Committee text by providing greater protection against unintended declines in minimum capital requirements during the initial years of Basel II implementation.\n\nAt the same time, we have tried to reduce regulatory burdens in several ways. In particular, the advanced elements of Basel II are intended to apply only to the very largest and most internationally active U.S. banking organizations, not to the great majority of U.S. banks. For banks not adopting Basel II, the agencies have been developing a modernized but easier-to-implement capital framework, known as Basel IA. Under Basel IA, the capital treatments for certain activities will be more risk-sensitive than those under Basel I, thus better aligning the treatments with those in Basel II. The NPR for Basel IA should be issued soon. Some bankers have suggested that Basel I may still be suitable for many small banks and that, consequently, they should have the option of whether to move to Basel IA. We will consider this possibility carefully.\n\nWe have also been working to promote a level playing field internationally for U.S. banking organizations that adopt Basel II. Indeed, maintaining competitive equity was one of our key motivations for developing Basel II jointly with foreign supervisors through the Basel Committee. More recently, we have been working through the Basel Committee's Accord Implementation Group to mitigate home-host conflicts while promoting consistent implementation of Basel II internationally.\n\nDespite these efforts, some significant differences do exist between the United States and other countries in the proposed implementation of Basel II's advanced approaches, beyond the transitional safeguards. Early comments on the Basel II NPR suggest that, whatever the merits of these international differences in rules, they are likely to add to implementation costs and home-host issues, particularly for globally active banks operating in multiple jurisdictions. Before we issue a final rule, we intend to review all international differences to assess whether the benefits of rules specific to the United States outweigh the costs. In particular, we will look carefully at differences in the implementation of Basel II that may adversely affect the international competitiveness of U.S. banks.\n\nMany other opportunities may exist to reduce the burden of the new capital regulations. Public comments will be critical in shaping the final rules, and we will look to banking organizations for help in identifying aspects of the NPR that would impose competitive inequities or undue costs. I am confident that, working together, we can do more to level the competitive landscape and reduce burden without compromising our primary objective of maintaining the stability of the U.S. banking and financial system.\n\nCommunity Reinvestment Act\nI now want to shift from the role of bank regulation and supervision in the national and global context and discuss one aspect of its place in local markets. Clearly, banks strengthen their local communities by providing a range of services and facilitating the flow of credit necessary to support economic development. However, economic development in some communities, particularly lower-income communities, may be hampered by what economists call \"market failures.\" For example, if information about economic opportunities is particularly costly to obtain in lower-income neighborhoods, then potentially profitable loans and investments may not get made. Another form of market failure may arise because of so-called neighborhood effects: Because the values of homes and businesses are affected by the overall economic vitality of the neighborhood in which they are located, the returns to an individual bank's investments in a given area may depend on whether other banks are investing in that area as well. But if no bank is willing to go first, so to speak, the neighborhood may be underserved and potentially profitable opportunities may be missed.\n\nTo address these possible market failures, to ensure that depository institutions help to meet the credit needs of their communities, and to achieve broader social goals such as expanding home ownership, the Congress in 1977 passed the Community Reinvestment Act (CRA). A key goal of the CRA is to induce banking institutions to invest in acquiring the knowledge and expertise needed to find profitable lending opportunities in lower-income neighborhoods, thereby removing an important barrier to the extension of credit in those neighborhoods. Likewise, to the extent that the CRA leads a number of banks to provide credit and services to an underserved area, the returns to each bank's investments in that neighborhood should improve, reducing the \"first mover\" problem. Indeed, many banks have found that lending and investment in lower-income neighborhoods can be profitable, which has led them to expand their activities in those areas.\n\nAs you know, the CRA requires that each banking institution's record of serving lower-income areas be regularly evaluated and that these ratings be made public. The Congress has given the banking agencies substantial discretion to determine the methods by which they assign CRA ratings. As experience with the CRA has accumulated and as the economic environment has changed, the agencies, with the benefit of public input, have exercised that discretion with an eye toward both increasing the effectiveness of the act and reducing its costs. For example, the early CRA rules emphasized process over performance, and major changes were made to the regulations in 1995 to make the CRA evaluations more oriented toward performance. These changes increased reporting burdens for some institutions, as the new rules required them to collect and submit data concerning their lending performance. In the judgment of the agencies, the broader social benefits of a more-quantitative, performance-based method of assigning CRA ratings justified the increase in regulatory burden. However, aware of that burden, the agencies exempted the smallest community banks and thrifts from the data-reporting requirement and allowed them to undergo streamlined evaluations of their retail lending and services.\n\nBeginning in 2001, the agencies revisited the issue, undertaking a careful review of the benefits and costs of the data-reporting requirements applied to non-exempt community banks and thrifts, as well as the associated rules that determined eligibility for streamlined CRA evaluations. As part of this effort, members of the Federal Reserve Board staff published a study comparing the retail lending and services of community banks and thrifts eligible for streamlined evaluations and exempt from data reporting with the activities of comparable institutions without those exemptions (Avery and others, 2005). The analysis suggested that exempting a larger number of relatively small institutions from the more onerous requirements would not adversely affect the provision of retail lending or services in lower-income communities. Consistent with that analysis, in 2005 the agencies substantially increased the number of community banks and thrifts eligible for streamlined evaluation and exempt from the data-reporting requirement. At the same time, the agencies also addressed several concerns about the effectiveness of CRA regulations in encouraging these institutions to invest in community development. Among the concerns expressed was that the method for evaluating the community development records of larger community banks and thrifts was inflexible and produced unintended costs. After reviewing public input and the available evidence on the costs and benefits of the current process, the agencies adopted a new evaluation method that considers all of the community development activities of such institutions under one test. Although we recognize that this change is very recent, we welcome feedback on how well it is working for financial institutions and communities alike.\n\nRegulatory Relief\nMy focus thus far has been on how the Federal Reserve and the other banking agencies develop regulatory and supervisory approaches to implement the applicable laws. But regulatory agencies may also play a role in the legislative process itself, for example by raising issues that may require legislative remedy, commenting on proposed legislation, and providing technical assistance in the drafting of bills. Our extensive practical experience in implementing legislation makes us particularly well placed to advise the Congress when legislation is not achieving its intent or is imposing costs on banks or on society that exceed its benefits.\n\nIn this regard, I am pleased that Congress recently passed, and the President signed, the Financial Services Regulatory Relief Act of 2006. Certainly, the act does not address every concern that banks and regulators have raised about regulatory burden, and I hope that the Congress will continue to revisit these issues. But the legislation does include a number of provisions that, when implemented, should provide substantial relief to banking organizations and increase efficiency in the banking system while enhancing the Federal Reserve's tools for conducting monetary policy.\n\nAmong the act's most important provisions are two that relate to reserve requirements. Federal law currently obliges the Board to establish reserve requirements on transaction accounts and prohibits the Board from setting these reserve requirements below 8 percent for amounts above the so-called low-reserve tranche. Because the Federal Reserve is not permitted to pay interest on the balances held at Reserve Banks to meet reserve requirements, depositories have an incentive to reduce their required reserve balances to a minimum. Institutions use various techniques to minimize required reserves, such as sweep programs that move funds between deposit accounts subject to reserve requirements and money market accounts not subject to those requirements. From the perspective of society as a whole, sweep programs have little or no economic value to justify their cost of implementation.\n\nThe Regulatory Relief Act will allow the Federal Reserve to pay depository institutions interest on the balances held to meet reserve requirements; it also gives the Board the discretion to lower the ratio of required reserves to transaction accounts. The Board has long sought these amendments, which were also supported by the American Bankers Association and America's Community Bankers. Unfortunately, for reasons related to congressional budget scoring, these amendments will not become effective until October 2011. Nevertheless, when the Federal Reserve is able to begin paying interest on required reserve balances, much of the regulatory incentive for depositories to engage in resource-wasting efforts to minimize reserve balances will be eliminated, to the economic benefit of banks, their depositors, and their borrowers.\n\nThe act will also allow the Federal Reserve to pay interest on contractual clearing balances and excess reserve balances, two types of balances that depository institutions hold voluntarily at Reserve Banks. By helping to stabilize the demand for voluntary reserve balances, this authority may allow the Federal Reserve to implement monetary policy without the need for required reserve balances. In these circumstances, the Board--as authorized by the act--could consider reducing or even eliminating reserve requirements, thereby reducing a regulatory burden for all depository institutions.\n\nOther important provisions of the act will provide banking organizations immediate regulatory relief. For example, the act immediately raises to $500 million, from $250 million, the asset threshold below which a well-capitalized and well-managed insured depository institution may qualify for an extended eighteen-month cycle for safety and soundness examinations. We estimate that this change will allow about 1,200 additional federally-insured institutions to qualify for an extended examination cycle without compromising safety and soundness.\n\nThe act also requires that the Board and the Securities and Exchange Commission (SEC) jointly issue a new, single set of rules to implement the \"broker\" exceptions for banks that were adopted as part of the Gramm-Leach-Bliley Act. The act requires that our agencies jointly issue new proposed rules within 180 days of enactment, but Chairman Cox of the SEC has actively engaged with the banking regulators on this issue and has already indicated that he would like to have proposed rules ready for public comment by the end of this year. We look forward to continuing to work with Chairman Cox, the SEC's other commissioners and staff, and our fellow federal banking agencies in developing workable rules that do not disrupt the traditional activities of banks.\n\nConclusion\nI have covered a lot of ground today. My central theme has been that good regulatory and supervisory policies should implement congressional intent in ways that maximize social benefits and minimize social costs. The regulatory burden on banks is not the only element of social cost, but it is an important component. Accordingly, in developing regulatory and supervisory policies, the Federal Reserve and the other banking agencies will continue to pay close attention to the implications of those policies for regulatory burden, competitiveness, and efficiency in banking. In practice, our ability to accurately assess those costs and benefits depends greatly on close collaboration with and feedback from the banking industry. We look forward to working with you on these issues.\n\nReference\n\nAvery, Robert B., Glenn B. Canner, Shannon C. Mok, and Dan S. Sokolov (2005). \"Community Banks and Rural Development: Research Relating to Proposals to Revise the Regulations That Implement the Community Reinvestment Act,\" Federal Reserve Bulletin, vol. 91 (Spring), pp. 202-35.",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20061016a.htm",
        "title": "Bank Regulation and Supervision: Balancing Benefits and Costs",
        "date": "10/16/2006"
    },
    {
        "content": "October 12, 2006\n\nGovernor Frederic S. Mishkin\n\nWeissman Center Distinguished Lecture Series, Baruch College, New York, New York\n\nIt is a great pleasure, for very personal reasons, to be here at Baruch College to deliver my first speech as a Federal Reserve Governor. My now-deceased father, Sidney Mishkin, whom I still miss every day, proudly graduated more than seventy years ago from Baruch, then called City College. Indeed, you might have noticed the Sidney Mishkin Gallery when you came into the building. The gallery was part of a gift that my father made to this institution upon his death fifteen years ago. The education he got at Baruch during the depths of the Great Depression--attending class at night because he had to work to support not only himself but also his parents--helped give him the opportunity to become a successful businessman. I hope that the students here will likewise take advantage of the opportunities bestowed by their education and make important contributions to our economy.\n\n\n\nNow, let me turn to the topic at hand: Can more globalization--in particular, financial globalization--be a force for good?\n\nThe globalization of trade and information over the past half century has lifted vast numbers of the world's people out of extreme poverty. Despite the doom and gloom that you often hear, world economic growth since the Second World War has been at the highest pace ever recorded. What we are seeing in countries that are export oriented, and thus able to take advantage of the present age of globalization, is a reduction in poverty and a convergence of income per capita toward industrial-country levels. In India and China, for example, globalization in recent years has lifted the incomes of more than 1 billion people above the levels of extreme poverty.\n\nAlthough economic globalization has come a long way, in one particular dimension--finance--it is very far from complete. As documented in the superb book by Maurice Obstfeld and Alan Taylor, Global Capital Markets, financial globalization has made its greatest strides in rich countries. Gross international capital flows, which have risen enormously in recent years, move primarily among rich countries. The exchange of assets in these flows is undertaken to a large extent to enable individuals and businesses to diversify their portfolios, putting some of their eggs in the baskets of other rich countries. International capital is generally not flowing to poor countries and is thus not enhancing their development.\n\nThe gross amount of capital flowing each year to emerging-market economies increased dramatically in the 1990s and is now more than $600 billion. That amount may sound like a lot, but it is only one-seventh of total international capital flows excluding official reserves. The situation is even more remarkable when one adds into the picture the capital flows out of developing countries, especially the governmental acquisition of international reserves; then we see that emerging-market economies have, on net, actually been sending capital to rich countries. The United States is currently running very large trade and current account deficits--more than $800 billion--because Americans are buying more goods and services from abroad than they are selling overseas. These deficits are being financed by the foreign acquisition of U.S. assets, especially bonds, with emerging-market economies providing the United States with about $300 billion per year. The Chinese government, for example, has accumulated nearly $1 trillion of foreign assets; much of this is invested in the United States, and China is now one of the largest holders of U.S. Treasury securities in the world.\n\nAlso remarkable is that capital flows from rich countries to developing countries relative to total capital are far smaller than they were in the first age of globalization, during the late nineteenth and early twentieth centuries. By 1914, about half of the stock of capital in Argentina was supplied by rich foreign countries, particularly the United Kingdom. Today, less than 6 percent of Argentine capital has been supplied by foreigners. And this change in the pattern of capital flows has not been confined to Argentina.\n\nAs Nobel laureate Robert Lucas has pointed out, this feature of international capital flows is a paradox: Why doesn't capital flow from rich to poor countries? We know that labor is cheap in poor countries, and so we might think that capital would be especially productive there. Just think of how hugely profitable a factory might be in a country where wages are one-tenth of those in the United States. Capital should, therefore, have extremely high returns in such countries, and we should expect massive flows of capital from rich countries (where the returns on capital should be relatively low) to poor countries (where they should be far higher). In fact, there has been a big increase in the amount of capital moving to emerging-market economies in recent years, but capital primarily still flows from one rich country to another, where the returns on capital are similar.\n\nThus, financial globalization is far from complete, and that fact raises a set of questions. Should financial systems in developing countries become more integrated with the rest of the world? If so, what should be done to accomplish that integration?\n\nLet me pause here to address an underlying issue that obscures much of the debate about financial globalization. The important role of the financial system in the economy is not well understood by the average person, and even many economists are shocked by the high salaries paid to investment bankers and other financial professionals. \"After all,\" many wonder, \"what do these financial professionals produce? Nothing concrete comes from their highly paid work.\"\n\nThis view, although common, betrays a fatal misunderstanding. Getting the financial system to work well is critical to the success of an economy. To understand why, think of the financial system as the brain of the economy: That is, it acts as a coordinating mechanism that allocates capital, the lifeblood of economic activity, to its most productive uses by businesses and households. If capital goes to the wrong uses or does not flow at all, the economy will operate inefficiently, and ultimately economic growth will be low. No work ethic can compensate for a misallocation of capital and the resulting failure to invest in the most profitable ventures. Hard work will not be productive unless it is accompanied by the right amount and kinds of capital. Indeed, workers in poor countries often work longer hours than their counterparts in rich countries, and yet they remain poor. When they emigrate to countries with a developed financial system and the resulting superior endowment of capital, they often become rich.\n\nFinancial globalization--opening a country's financial markets to foreign capital and financial institutions--will confer several important benefits on developing countries. First, by bringing in new capital, financial globalization will lower the cost of capital, thereby encouraging investment, which in turn promotes growth. Second, when foreign capital and financial institutions are allowed to enter a country, they improve the allocation of capital. Third--the most important benefit and one not usually emphasized--globalization of a country's financial system, if it is designed to promote competition in domestic financial markets, helps promote the development of better property rights and institutions. Better property rights and institutions make the domestic financial sector work better. They facilitate the movement of capital to productive uses and prepare the domestic financial sector to better handle the increased capital flows that would come with the opening of the country's financial sector.\n\nThe benefits of globalization of trade in goods and services are not controversial among economists. Polls of economists indicate that one of few things on which they agree is that the globalization of international trade, in which markets are opened to flows of foreign goods and services, is desirable. But financial globalization, the opening up to flows of foreign capital, is highly controversial, even among economists, despite benefits of the sort I just mentioned.\n\nFor example, in his best-selling book Globalization and its Discontents, Nobel laureate Joseph Stiglitz is very critical of globalization because he sees the opening up of financial markets in emerging-market economies to foreign capital as leading to economic collapse. Even Jagdish Bhagwati, one of the leading economists defending globalization of trade (after all, his book is titled In Defense of Globalization), is highly skeptical of financial globalization, stating that \"the claims of enormous benefits from free capital mobility are not persuasive.\"1 George Soros, the prominent financier, opens his book On Globalization with a chapter entitled \"The Deficiencies of Global Capitalism.\"\n\nOne reason for the controversy is that opening up the financial system to foreign capital flows has led to some disastrous financial crises causing great pain, suffering, and even violence. These crises can arise when bad policies encourage excessive risk taking by financial institutions, policies that rich elites in the developing countries often advance for their own profit. There are those (including Stiglitz and Bhagwati) who put the primary blame for the failures of financial globalization in emerging-market economies on outsiders, specifically on the International Monetary Fund, or what they refer to as the Wall Street-Treasury complex. The evidence has brought me to the conclusion that institutions like the IMF or the U.S. Treasury are not primarily to blame, although neither are they blameless--public and private financial institutions active in the international capital markets have often aided and abetted poorly designed financial globalization, although that was not their intention.\n\nAnother objection to focusing on financial development and globalization as key factors in economic growth is that it is far from clear that emerging-market economies are finance constrained: In other words, they often do not have trouble getting money for investments. But throwing money at investments does not work. Indeed, as the experience of recent years indicates, too much money flowing into these countries often resulted in bad loans and investments, which led to financial crises. The argument for the importance of developing a good financial infrastructure in these countries is not so much that it increases investment but that it promotes the allocation of investment to the uses that will do the most good for the economy.\n\nThat result--improving the allocation of investments--is something that foreign aid has generally not been able to accomplish. Although many people lament the paltry amount of aid that rich countries provide to poor countries, and although aid tightly focused on technical assistance or the financing of local projects has often had important successes, large aid projects have generally not worked well in promoting development because typically they have not provided the right incentives. In his book The Elusive Quest for Growth, William Easterly cites the extraordinary example of Zambia: If the $2 billion of aid Zambia received from the advanced countries and international aid organizations since its independence had gone into productive investments, Zambia would now have an income of more than $20,000 per capita, putting it in the club of rich nations. Instead, Zambia has a per capita income of $600, one-third lower than its per capita income at independence.\n\nI noted that globalization designed to promote competition in domestic financial markets helps promote the development of better property rights and institutions. How do better property rights and institutions improve investment outcomes? Well, consider the opposite: If you live in a country where it is easy for others to take your property away, either by force or through government corruption, you would be crazy to invest there. Without these investments, workers in your country will be unable earn high wages because they won't have sufficient capital--buildings, machines, and other infrastructure--to make them highly productive. Poverty will be severe. Hence, the most basic set of growth-promoting institutions are those that promote property rights--a strong judiciary enforcing the rule of law and a government free of corruption. Beyond these basic arrangements are others that specifically promote an efficient financial system through regulation and oversight of financial institutions. With the protection of property rights, honest government, and financial oversight and enforcement, would-be investors with the best projects will be the ones who actually get external funds to invest--and this is the crucial role of the financial system.\n\nWe have seen that the repression of the financial system is a great obstacle to economic growth and the reduction of poverty in poorer countries. Yet, if financial development offers such tremendous benefits, why doesn't every country jump on the path to growth and prosperity by imitating the institutions of the advanced economies? Part of the answer is that good institutions need to be home-grown; institutional frameworks that have been developed in the rich countries frequently do not translate well to poorer countries. This is a lesson that many in the advanced economies of the world have yet to learn. The development of good institutions in the advanced countries took hundreds of years; as they grew, they adapted to local conditions. Poor countries must develop their own institutions, and the citizens of these nations must feel they have ownership of the institutions or the institutions will be ineffective and short lived.\n\nIf it was difficult and time consuming for the advanced economies to develop a good financial infrastructure, it will be even harder in many developing countries because to do so they must overcome what is often a far more dysfunctional political environment. The benefits that accrue from financial development are dispersed over a wide range of people--to those who could then buy houses with the help of a mortgage, obtain an automobile loan to buy a car, get capital to start a new business, and finance new investments in existing businesses. These potential beneficiaries have little power to demand these benefits. In contrast, the costs would be focused on rich elites and special interests, who often have a lot of political clout and have much to lose from institutional development that encourages an efficient financial system and promotes competition.\n\nFor an example of dysfunctional institutions that obstruct economic growth while benefiting certain narrow interests, consider the importance of collateralized loans. The use of collateral is a crucial tool that helps the financial system make loans because it reduces losses when loans go sour. A person who would pledge land or capital for a loan must, however, legally own the collateral. Unfortunately, as Hernando De Soto has documented in his book The Mystery of Capital: Why Capitalism Triumphs in the West and Fails Everywhere Else, legalizing the ownership of capital is extremely expensive and time consuming for the poor in developing countries. To give just one of De Soto's many astonishing examples, obtaining legal title to a dwelling on urban land in the Philippines can require taking 168 bureaucratic steps through 53 public and private agencies over a period of 13 to 25 years.\n\nThe high cost of setting up a legal business or legally purchasing land is another barrier to establishing clear property rights in many developing countries. Businesses that are not legally established cannot get legally enforceable loans. Setting up a simple business in the United States generally requires only filling out a form and paying a nominal licensing fee. In contrast, De Soto's researchers found that legally registering a small garment workshop in Peru required 289 days, at 6 hours per day; the cost was about $1,200, which was about 30 times the monthly minimum wage. The lack of property rights for all but the very rich, as documented by De Soto, is a serious impediment to financial development.\n\nGovernment is often the primary source of financial repression in developing countries. Strong property rights, a crucial element in financial development, severely constrain a government's ability to expropriate land, factories, or ideas whenever it wants to profit from them. Rapacious governments whose rulers treat their countries as personal fiefdoms are not uncommon: We have seen these governments in Saddam Hussein's Iraq, Robert Mugabe's Zimbabwe, and Ferdinand Marcos's Philippines. Even officials in less tyrannical governments have been known to use the power of the state to get rich. Not surprisingly, then, many governments pay lip service to property rights but do not encourage a rule of law to protect them.\n\nSo how can emerging-market economies harness their financial systems to make financial globalization work for them and help them get rich? The short answer is, Develop good institutions that allocate capital efficiently. The next question is, How?\n\nWe know something that developing countries have done and can do to successfully promote development: Pursue an external orientation and create a successful export sector. That strategy not only forces the economy to become more efficient but also creates a demand to improve institutions that encourage financial development. It can do so by weakening the profits and power of the rich elites and special interests who oppose institutional development, and it can even encourage them to support institutional reforms to restore their profits. Globalization can therefore help generate the political will for institutional reform. We have seen this happen in emerging-market economies that have experienced rapid growth, such as China, India, Singapore, South Korea, Taiwan, and Chile.\n\nWhat can the rich countries do? Besides providing technical assistance and incentives for institutional development, advanced countries can help by opening up their own markets to exports from poorer countries--much needs to be done in that regard, particularly in agricultural products. Opening up rich-country markets to goods and services from developing countries is far more important than financial aid in alleviating world poverty, and such openness also promotes financial development and stability in poorer countries. Those who are against opening up markets in the advanced economies are in effect against reducing poverty abroad and even at home, although they often don't realize it. True, closing off markets in rich countries may help some workers in the short run (although in the long run it will make the average worker worse off because it will lower productivity growth), but this help comes at the expense of the far-poorer worker in the developing world. Those in advanced economies who lose their jobs from this opening of markets certainly deserve our sympathy and our support, but that support should come in ways other than trade restrictions.\n\nI will conclude by saying that those who oppose any and all globalization have it completely backward: Protectionism, not globalization, is the enemy. It is true that, by itself, globalization in both finance and trade is not enough to ensure economic development and that economies must position themselves to handle foreign capital flows. But as I said, to be against globalization as such is most assuredly to be against poor people, and this is presumably not the position antiglobalizers want to take. Developing countries cannot get rich unless they globalize in both trade and finance. Making financial flows truly worldwide and creating robust, efficient financial markets in developing countries is not optional: It needs to be the focus of the next great globalization. In sum, I want to challenge those who oppose globalization to rethink their objections. As Kofi Annan, the Secretary General of the United Nations, has put it, \"The main losers in today's very unequal world are not those who are too much exposed to globalization. They are those who have been left out.\"2 Rather than opposing or limiting globalization, we in the rich countries and those in the developing countries must, as a moral imperative, work together to make globalization work for the general good of people all over the world.\n\nFootnotes\n\n1.  Jagdish Bhagwati (2004), \"The Capital Myth: The Difference between Trade in Widgets and Dollars,\" Foreign Affairs 77, no. 3 p 7.  Return to text\n\n2.  From remarks at an UNCTAD conference in February 2000, in Johan Norberg (2003), In Defense of Global Capitalism (Washington: Cato Institute), p. 155.  Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/mishkin20061012a.htm",
        "title": "Globalization: A Force for Good?",
        "date": "10/12/2006"
    },
    {
        "content": "October 11, 2006\n\nGovernor Susan Schmidt Bies\n\nAt the British Bankers' Association 10th Annual Supervision Conference, London, England\n\nGood morning. I would like to thank the British Bankers' Association for the invitation to speak at this important conference. Today, I want to focus on some current supervisory issues to illustrate how financial stability, portfolio concentration, regulatory capital, enterprise compliance, and clear consumer disclosures all reflect varying types of risk exposures and risk mitigation. When banks determine their business strategies and the inherent risks they create, supervisors expect banks to develop appropriate risk management practices to ensure that mitigating controls are in place to limit risks to the desired levels.\n\nAssessing Recent Conditions\nAs a central banker, I realize that a strong, stable financial system is necessary for the health of the broader economy. Excessive volatility in financial markets can significantly raise the cost of capital for business investment and adversely affect real economic expansion. Moreover, a weak financial sector can significantly impede the monetary transmission mechanism, potentially limiting the ability of the central bank to stimulate the economy. Since commercial banks are crucial players in the financial system, efforts to improve their risk management can help mitigate the impact of shocks on the financial system and the real economy. Further, in order to foster the stability of the financial system, we need to support the resiliency of our financial infrastructure and promote sound supervisory oversight.\n\nDespite a number of notable shocks, financial markets have generally remained stable in recent years. Investors currently seem optimistic about the economic and financial outlook, with risk spreads relatively narrow and implied volatilities fairly low. Businesses have been reporting strong profits and solid balance sheets. Financial market functioning has remained good, with most measures of financial market liquidity remaining within typical ranges. At this point, investors do not appear to believe that financial institutions are unduly exposed to any particular risk type or to risks in the aggregate.\n\nThe natural follow-on from any central banker or supervisor to such a positive description of financial conditions is predictable: We should not necessarily expect such relative calm to continue indefinitely. Clearly, the evolution of financial institutions and markets has not removed the underlying risks and uncertainties associated with financial transactions, and, in fact may create new forms of risk that may not be clearly understood. Financial institutions and other market participants must still make decisions and take actions with incomplete knowledge about the condition of their counterparties.\n\nCRE Concentration Risk\nWhile credit quality has been very good in recent years in the U.S., regulators are concerned about some emerging practices. One example is commercial real estate (CRE) in the United States, an area in which some banks have become increasingly concentrated. In fact, the aggregate CRE concentrations for small-to-medium-sized U.S. banks, relative to capital, are now twice the exposures before the substantial real estate downturn in the late 1980s. As supervisors, we want to ensure that risk management practices and capital levels are appropriate given the level of concentrations, and that institutions have thought through the possible consequences of a market downturn in CRE.\n\nU.S. regulators recently issued for comment proposed new guidance for examiners on CRE. Historically, CRE has been a highly volatile asset class. Borrowers and bankers with properties in distress can disrupt their local real estate market by cutting rents or offering leasehold improvements and other incentives to attract or keep tenants in an effort to generate cash flow. This can negatively affect the local real estate market as a whole, and adversely affect even good projects. CRE is a highly volatile asset class in that credit losses in most years are relatively low compared with many other types of bank loans. But in times of stress, the loss rate on CRE can jump considerably higher relative to the good years, compared with the behavior of other types of loans. As banks' concentration of CRE grows, they must upgrade their portfolio risk management practices, especially monitoring proposed projects and conditions in the sector of the CRE market in which they are lending. Since CRE losses tend to cluster in times of stress, bankers must focus more intently on their risk appetite for losses as their concentration grows. This means considering how much capital can be placed at risk if the portfolio of CRE loans hits a stress period and comparing that loss exposure to the relative returns in CRE lending. As banks increase their concentration in CRE, supervisors will expect to see bankers pay greater attention to strengthening their portfolio risk management practices.\n\nImproving Bank Capital Measures\nOf course, banking is, and should be, a business in which banks take and manage risks. Bankers implicitly accept risk as a consequence of providing services to customers and also take explicit risk positions that offer profitable returns relative to their risk appetites. The job of bank supervisors is to ensure that bank capital represents an adequate cushion against losses, especially during times of financial instability or stress. Supervisors should continue to support approaches that minimize the negative consequences of risk taking by financial institutions, particularly those institutions that could contribute to financial instability. One such approach is the Basel II framework.\n\nBy now most of you are aware that last month the U.S. banking agencies released their latest proposals with respect to Basel II and are now seeking comment on those proposals. I imagine that many of you have already read these extensive documents. The U.S. banking agencies are eagerly awaiting comments on the proposals and now expect to engage in a continuing dialogue with all interested parties as to whether the proposals meet our stated objectives and how they can be improved. We hope that those reviewing the documents understand that Basel II is intended to promote the stability of the U.S. financial system by ensuring the safety and soundness of the largest U.S. banks. Thus, the ability of Basel II to promote safety and soundness is the first criterion on which the proposed Basel II framework should be judged.\n\nIndeed, the Federal Reserve's main reason for pursuing the advanced approaches of Basel II is the growing inadequacy of current Basel I regulatory capital rules for the large, internationally active banks that are offering ever more complex and sophisticated products and services. We need a more risk-sensitive capital framework for these particular banks, and we believe that Basel II is such a framework. In addition, Basel II promotes risk-management enhancements and improves market discipline, as well as providing supervisors with a more conceptually consistent and more transparent framework for evaluating systemic risk, particularly through credit cycles. Basel II should be able to establish a more coherent relationship between regulatory measures of capital adequacy and day-to-day supervision of banks, enabling examiners to better evaluate whether banks are holding prudent levels of capital given their risk profiles.\n\nThe reasons for pursuing Basel II also provide justification for the recent Basel revisions to the 1996 Market Risk Amendment (MRA). Since adoption of the MRA, banks' trading activities have become more sophisticated and have given rise to a wider range of risks that are not easily captured in their existing value-at-risk (VaR) models. For example, more products related to credit risk, such as credit default swaps and tranches of collateralized debt obligations, are now included in the trading book. These products can give rise to default risks that are not captured well in methodologies required by the current rule specifying a ten-day holding period and a 99 percent confidence interval, thereby creating potential arbitrage opportunities between the banking book and the trading book. The U.S. agencies issued their proposals to revise Market Risk capital requirements at the same time as the Basel II proposals, and we seek comment on the market risk proposals as well. Notably, in the United States we would continue to have banks with significant trading book activity hold additional capital for the risks inherent in that line of business, whether they remain Basel I banks or move to Basel II.\n\nGiven the international composition of this audience, I want to offer a few thoughts about the cross-border issues related to Basel II implementation. As you know, the U.S. agencies participate with other national supervisors in the Accord Implementation Group and other groups to identify differences in implementation and discuss possible ways to harmonize rules and thereby reduce burden on cross-border banking organizations. We recognize that the adoption of differing approaches to Basel II by various countries may create challenges for banking organizations that operate in multiple jurisdictions, but it is good to remember that cross-border banking has always raised specific challenges that supervisors from various countries have worked hard to address. Let me assure all bankers here that supervisors are aware that the process of adopting national versions of Basel II has heightened concerns about home-host issues. The Federal Reserve and other U.S. agencies have, for many years, worked with international counterparts to limit the difficulty and burden that have arisen as foreign banks have entered U.S. markets and as U.S. banks have established operations in other jurisdictions. We encourage bankers that have questions and concerns about home-host issues to promptly communicate with their regulators in all jurisdictions so that the issues can be addressed.\n\nEnterprise Compliance Risk Management and BSA/AML\nI would now like to turn to another area of the financial sector that regulators are focused on: compliance-risk management. This type of risk may result when an organization fails to comply with the laws, regulations, or standards or codes of conduct that are applicable to its business activities and functions. The Federal Reserve expects each banking organization to have a compliance infrastructure and culture in place across the entire institution that can identify and effectively control the compliance risks it faces.\n\nTo create appropriate compliance-risk controls, organizations must first understand risks across the entire entity. Managers should be expected to evaluate the risks and controls within their scope of authority at least annually. I also emphasize the need for the board of directors and senior management to ensure that staff members throughout their organizations understand the compliance objectives and each member's role in implementing the compliance program. The compliance function should be independent of management whose activities it reviews and monitors. An enterprise-wide compliance-risk management program should be dynamic and proactive, meaning it constantly assesses evolving risks when new business lines or activities are added or when existing services or processes are altered. To avoid having a program that operates on \"autopilot,\" an organization must continuously reassess its risks and controls and train employees to effectively implement those controls.\n\nAn integrated approach to compliance-risk management can be particularly effective for U.S. Bank Secrecy Act and anti-money-laundering (BSA/AML) compliance. Often, the identification of a BSA/AML risk or deficiency in one business activity or subsidiary can indicate potential problems or concerns in other activities across the organization. A primary concern for international banking organizations continues to be controlling BSA/AML risk effectively across their various operating units in the United States.\n\nWe recognize the commitment that organizations have made to compliance with BSA/AML requirements, and, in return, we continue to work to ensure that obligations in this area are clearly communicated to banking organizations and examiners alike. The Federal Reserve strives to provide clear and comprehensive guidance that directly communicates our expectations to the institutions we supervise. This year, the U.S. banking agencies published an updated Federal Financial Institutions Examination Council (FFIEC) BSA/AML Examination Manual to cover new regulatory issuances and respond to industry requests for further guidance. We expect to issue annual updates to the manual to cover developments in this evolving area of risk.\n\nThe FFIEC BSA/AML Examination Manual reflects a common view of the federal banking agencies and the U.S. Treasury Department's Financial Crimes Enforcement Network (FinCEN) with regard to BSA/AML compliance expectations. The agencies universally stress that the purpose of a BSA/AML examination is to assess the overall adequacy of a banking organization's BSA/AML controls, in view of that particular organization's lines of business and customer mix. Focusing on the BSA/AML process across the enterprise is critical to ensuring that resulting controls are risk-based, so that resources are directed appropriately.\n\nTogether with our U.S. Treasury and law enforcement counterparts, we are working to disseminate information about perceived money-laundering or terrorist-financing threats. By identifying emerging vulnerabilities, we can better collaborate with banking organizations to develop systems and procedures to combat criminals' abuse of the financial sector. For example, the interagency Money Laundering Threat Assessment (4.1 MB PDF) is one step we have taken--with fifteen other U.S. government bureaus, offices and agencies, including law enforcement--to identify significant concerns and communicate them to banking organizations.\n\nWe are also working on an international basis to advance sound principles for compliance risk management. Last year, the Basel Committee published supervisory guidance on the compliance function in banks. That guidance lays out several key principles around which a successful compliance risk program should be organized. They include responsibilities of the board of directors and senior management, the need for proper independence and adequate resources in the compliance risk function, the specific duties of the compliance function, and the relationship with internal audit. Despite strong efforts by many bankers, there are still some indications that more work needs to be done. Indeed, some of the recently publicized compliance incidents may have been avoided if the institutions involved had more closely adhered to the Basel Committee's guidance.\n\nConsumer Protection\nIn addition to overall compliance risk management, the Federal Reserve is also concerned with consumer protection. Of course, consumer protection laws vary from country to country and bankers must ensure they measure up to each national standard. However, we think it is also simply good business sense for bankers to make sure that their customers clearly understand the price, features, and risks of products and services being offered to them.\n\nRecent advances in risk management and financial instruments have allowed financial institutions to offer a variety of new products to a wider range of customers. This is particularly prominent in mortgage products. Traditionally, in the United States, the majority of borrowers had mortgages with fixed rates and equal monthly payments that were sufficient to cover the accrued interest and pay down the principal. While nontraditional mortgages, including \"interest only\" mortgages and \"payment option\" adjustable-rate mortgages, have been available for many years, they were largely designed for higher-income borrowers who needed payment flexibility. The initial lower monthly payments make these mortgages attractive to borrowers who expect their income to increase. Payment flexibility also provides benefits to borrowers with seasonal or irregular income.\n\nIn recent years, however, these products have been offered to a broader array of consumers, including some for whom they may not be well suited. Borrowers may not fully recognize the risks of nontraditional mortgages, particularly the \"payment shock\" if the loan's interest rate increases, or when the consumer is required to make fully amortizing payments. It is important for consumers to have the information necessary to understand the features and risks of these types of mortgages. As bankers create more complex products, they should pay particular attention to improving the quality of their disclosures and sales practices, so that consumers can clearly understand the features of those products.\n\nThe Federal Reserve is committed to improving the information consumers receive about these products, including improving the disclosures required under the U.S. Truth in Lending Act. As we work to improve the understanding of disclosure information to consumers, the Federal Reserve also engages in outreach activities and conducts research to help us better understand consumer behavior and inform our judgment with regard to the best approaches to achieve consumer understanding. To this end, we sponsor consumer surveys, hold public hearings, discuss issues with our Consumer Advisory Council, and conduct consumer focus groups and other types of consumer testing.\n\nConclusion\nThe topics I have focused on this morning, portfolio concentration risk in CRE, improved risk sensitivity of Basel II, enterprise wide compliance of BSA/AML and clear communication to consumers about complex products, emphasize that risk management needs to be integrated into daily operations of banks. As an organization chooses to accept more risk exposure to implement its strategies, it is imperative that the organization strengthen its risk management practices appropriately. A bank should clearly understand the implications of the risks it chooses to accept, ensure that the risk mitigants it has chosen work effectively, and that capital is strong enough to support the bank throughout an extended period of unexpected losses.\n\nI would also like to note that the Federal Reserve has an excellent working relationship with the Financial Services Authority and the Bank of England. We have worked together effectively on bank-specific issues and broader questions of regulatory policy and financial stability. While there is a wide range of issues with which supervisors must grapple, including those I have discussed today, I believe that supervisory cooperation can indeed create a safe and sound global marketplace while allowing financial institutions to remain prosperous.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20061011a.htm",
        "title": "A U.S. Supervisor's Perspective on Current Banking Issues",
        "date": "10/11/2006"
    },
    {
        "content": "October 04, 2006\n\nChairman Ben S. Bernanke\n\nBefore  The Washington Economic Club, Washington, D.C.\n\nIn coming decades, many forces will shape our economy and our society, but in all likelihood no single factor will have as pervasive an effect as the aging of our population. In 2008, as the first members of the baby-boom generation reach the minimum age for receiving Social Security benefits, there will be about five working-age people (between the ages of twenty and sixty-four) in the United States for each person aged sixty-five and older, and those sixty-five and older will make up about 12 percent of the U.S. population. Those statistics are set to change rapidly, at least relative to the speed with which one thinks of demographic changes as usually taking place. For example, according to the intermediate projections of the Social Security Trustees, by 2030--by which time most of the baby boomers will have retired--the ratio of those of working age to those sixty-five and older will have fallen from five to about three. By that time, older Americans will constitute about 19 percent of the U.S. population, a greater share than of the population of Florida today.\n\nThis coming demographic transition is the result both of the reduction in fertility that followed the post-World War II baby boom and of ongoing increases in life expectancy. Although demographers expect U.S. fertility rates to remain close to current levels for the foreseeable future, life expectancy is projected to continue rising. As a consequence, the anticipated increase in the share of the population aged sixty-five or older is not simply the result of the retirement of the baby boomers; the \"pig in a python\" image often used to describe the effects of that generation on U.S. demographics is misleading. Instead, over the next few decades the U.S. population is expected to become progressively older and remain so, even as the baby-boom generation passes from the scene. As you may know, population aging is also occurring in many other countries. Indeed, many of these countries are further along than the United States in this process and have already begun to experience more fully some of its social and economic implications.\n\nEven a practitioner of the dismal science like me would find it difficult to describe increasing life expectancy as bad news. Longer, healthier lives will provide many benefits for individuals, families, and society as a whole. However, an aging population also creates some important economic challenges. For example, many observers have noted the difficult choices that aging will create for fiscal policy makers in the years to come, and I will briefly note some of those budgetary issues today. But the implications of demographic change can also be viewed from a broader economic perspective. As I will discuss, the broader perspective shows clearly that adequate preparation for the coming demographic transition may well involve significant adjustments in our patterns of consumption, work effort, and saving. Ultimately, the extent of these adjustments depends on how we choose--either explicitly or implicitly--to distribute the economic burdens of the aging of our population across generations. Inherent in that choice are questions of intergenerational equity and economic efficiency, questions that are difficult to answer definitively but are nevertheless among the most critical that we face as a nation.\n\nDemographic Change and the Federal Budget\nAs I have already mentioned, the coming demographic transition will have a major impact on the federal budget, beginning not so very far in the future and continuing for many decades. Although demographic change will affect many aspects of the government’s budget, the most dramatic effects will be seen in the Social Security and Medicare programs, which provide income support and medical care for retirees and which have until now been funded largely on a pay-as-you-go basis. Under current law, spending on these two programs alone will increase from about 7 percent of the U.S. gross domestic product (GDP) today to almost 13 percent of GDP by 2030 and to more than 15 percent of the nation’s output by 2050. The outlook for Medicare is particularly sobering because it reflects not only an increasing number of retirees but also the expectation that Medicare expenditures per beneficiary will continue to rise faster than per capita GDP. For example, the Medicare trustees’ intermediate projections have Medicare spending growing from about 3 percent of GDP today to about 9 percent in 2050--a larger share of national output than is currently devoted to Social Security and Medicare together.\n\nThe fiscal consequences of these trends are large and unavoidable. As the population ages, the nation will have to choose among higher taxes, less non-entitlement spending, a reduction in outlays for entitlement programs, a sharply higher budget deficit, or some combination thereof. To get a sense of the magnitudes involved, suppose that we tried to finance projected entitlement spending entirely by revenue increases. In that case, the taxes collected by the federal government would have to rise from about 18 percent of GDP today to about 24 percent of GDP in 2030, an increase of one-third in the tax burden over the next twenty-five years, with more increases to follow. (This calculation ignores the possible effects of higher tax rates on economic activity, an issue to which I will return later.) Alternatively, financing the projected increase in entitlement spending entirely by reducing outlays in other areas would require that spending for programs other than Medicare and Social Security be cut by about half, relative to GDP, from its current value of 12 percent of GDP today to about 6 percent of GDP by 2030. In today’s terms, this action would be equivalent to a budget cut of approximately $700 billion in non-entitlement spending.\n\nBesides tax increases, spending cuts, or reform of the major entitlement programs, the fourth possible fiscal response to population aging is to accommodate a portion of rising entitlement obligations through increases in the federal budget deficit. The economic costs and risks posed by large deficits have been frequently discussed and I will not repeat those points today. Instead, I will only observe that, among the possible effects, increases in the deficit (and, as a result, in the national debt) would shift the burden of paying for government spending from the present to the future. Consequently, the choices that fiscal policy makers make with respect to these programs will be a crucial determinant of the way the economic burden of an aging population is distributed between the current generation and the generations that will follow.\n\nA Broader Economic and Generational Perspective\nIndeed, framing the issue in generational terms highlights the fact that the economic implications of the coming demographic transition go well beyond standard considerations of fiscal policy and government finance, important as those are. For reasons that I will explain in a moment, the aging of the population is likely to lead to lower average living standards than those that would have been experienced without this demographic change. How that burden of lower living standards is divided between the present and the future has important implications for both intergenerational fairness and economic efficiency.\n\nWhy will the coming demographic transition carry a cost in terms of long-run living standards? Assuming it unfolds as expected, the projected aging of the population implies a decline over time in the share of the overall population that is of working age and thus, presumably, in the share of the population that is employed. For any given level of output per worker that might be attained at some future date, this decline in the share of people working implies that the level of output per person must be lower than it otherwise would have been. In a sense, each worker’s output will have to be shared among more people. Thus, all else being the same, the expected decline in labor force participation will reduce per capita real GDP and thus per capita consumption relative to what they would have been without population aging. These reductions in output and consumption per person represent an economic burden created by the demographic transition.\n\nAlthough some adverse effect of population aging on future per capita output and consumption is probably inevitable, actions that we take today, in both the public and the private spheres, have the potential to mitigate those effects. One such action would be to find ways to increase our national saving rate. If the extra savings were used to increase the nation’s capital stock--the quantity of plant and equipment available for use by workers--then future workers would be more productive, ameliorating the anticipated effects on per capita output and consumption. Alternatively, using extra saving to acquire financial assets abroad (or to reduce foreign obligations) would also increase the resources available in the future.\n\nBy saving more today, we can reduce the future burden of demographic change. However, as any economist will tell you, there is no such thing as a free lunch. Saving more requires that we consume less (to free up the needed resources) or work more (to increase the amount of output available to dedicate to such activities). Either case entails some sacrifice on the part of the current generation. Consequently, a tradeoff exists: We can mitigate the adverse effect of the aging population on future generations but only by foregoing consumption or leisure today. This analysis is simple, but it shows why the coming demographic transition has economic implications that go well beyond the effect of aging on the federal budget.\n\nIn recent work, economists at the Board of Governors have used a stylized model to get a rough estimate of the magnitudes of the intergenerational tradeoffs that we face.1 Their analysis takes as a starting point a baseline scenario in which U.S. demographics remain (hypothetically) the same in the future as they are today. In this counterfactual scenario, the ratio of workers to the overall population is assumed to remain at its current level over time and per capita consumption grows with productivity. Now in reality, as I have noted, an aging population will reduce labor force participation, so the likely future trajectory of per capita consumption over time lies below that implied by the baseline scenario that assumes away the demographic change. The shape of the actual consumption trajectory depends, however, on the saving behavior of the current generation. If today’s saving rate is low, then the current generation can enjoy consumption close to what it would have been if the aging issue did not exist. However, in this case, the burden on future generations will be relatively great. Alternatively, the current generation could consume less and save more, which would allow the consumption of future generations to be closer to what it would have been in the absence of population aging.\n\nHow big are these effects? To assess magnitudes, the Board economists first examined the case in which the nation saves at its current rate for the next twenty years, thereby largely insulating the baby-boom generation from the effects of the coming demographic transition. After that, they assumed, consumption falls and saving rates rise, with all future generations experiencing the same percentage reduction in consumption relative to the baseline in which no population aging occurs. Their rough calculations suggest that, in this case, the per capita consumption of future generations would be about 14 percent less than what it would have been in the absence of demographic change.\n\nFor comparison, they next considered the case in which the burden of demographic change is shared more equally among current and future generations. They considered a case in which the national saving rate, instead of staying at its current level for the next twenty years, rises immediately. Further, they asked by how much today’s saving rate would have to increase to lead to equal burden-sharing among current and future generations. (\"Equal burden-sharing\" is interpreted to mean that the current generation and all future generations experience the same percentage reduction in per capita consumption relative to the baseline scenario without population aging.) They found that equal burden-sharing across generations could be achieved by an immediate reduction in per capita consumption on the order of 4 percent (or, since consumption is about two-thirds of output, by an increase in national saving of about 3 percentage points.) This case obviously involves greater sacrifice by the current generation, but the payoff is that all future generations enjoy per capita consumption that is only 4 percent, rather than 14 percent, below what it would have been in the absence of population aging. The large improvement in the estimated living standards of future generations arises because of the extra capital bequeathed to them by virtue of the current generation’s assumed higher rate of saving.\n\nThese numbers shouldn’t be taken literally but the basic lesson is surely right--that the decisions that we make over the next few decades will matter greatly for the living standards of our children and grandchildren. If we don’t begin soon to provide for the coming demographic transition, the relative burden on future generations may be significantly greater than it otherwise could have been.2\n\nAt the heart of the choices our elected representatives will have to make regarding the distribution of these costs across generations will be an issue of fairness: What responsibility do we, who are alive today, have to future generations? What will constitute ethical and fair treatment of those generations, who are not present today to speak for themselves? If current trends continue, the typical U.S. worker will be considerably more productive several decades from now. Thus, one might argue that letting future generations bear the burden of population aging is appropriate, as they will likely be richer than we are even taking that burden into account. On the other hand, I suspect that many people would agree that a fair outcome should involve the current generation shouldering at least some of that burden, especially in light of the sacrifices that previous generations made to give us the prosperity we enjoy today.\n\nThe choice of which generations should bear the burden of population aging has consequences for economic efficiency as well as for intergenerational equity. If we decide to pass the burden on to future generations--that is, if we neither increase saving now nor reduce the benefits to be paid in the future by Social Security and Medicare--then the children and grandchildren of the baby boomers are likely to face much higher tax rates. A large increase in tax rates would surely have adverse effects on a wide range of economic incentives, including the incentives to work and save, which would hamper economic performance. Alternatively, to avoid large tax increases, the government could decide to sharply reduce non-entitlement spending in the future. However, such actions might also have important social costs that need to be taken into consideration.\n\nSharing the Burden of Population Aging\nIf, as a nation, we were to accept the premise that the baby-boom generation should share at least some of the burden of population aging, what policy steps might be implied? As I have already noted, from a broad economic perspective, the most useful actions are likely to be those that promote national saving. Perhaps the most straightforward way to raise national saving--although not a politically easy one--is to reduce the government’s current and projected budget deficits. To the extent that reduced government borrowing allows more private saving to be used for capital formation or to acquire foreign assets, future U.S. output and income will be enhanced and the future burdens associated with demographic change will be smaller.\n\nIncreasing private saving, which is the saving of both the corporate sector and the household sector, is likewise desirable. Corporate saving, in the form of retained earnings, is currently at relatively high levels, but household saving rates are exceptionally low.3 A broad-based increase in household saving would benefit both the economy and the millions of American families who currently hold very little wealth.\n\nUnfortunately, many years of concentrated attention on this issue by policymakers and economists have failed to uncover a silver bullet for increasing household saving. One promising area that deserves more attention is financial education. The Federal Reserve has actively supported such efforts, which may be useful in helping people understand the importance of saving and to learn about alternative saving vehicles. Psychologists have also studied how the framing of alternatives affects people’s saving decisions. For example, studies suggest that employees are much more likely to participate in 401(k) retirement plans at work if they are enrolled automatically--with a choice to opt out-- rather than being required to actively choose to join. The pension bill recently passed by Congress and signed by the President included provisions to increase employers’ incentives to adopt such opt-out rules; it will be interesting to see whether such rules are adopted and, if so, how effective they are in promoting employee saving.\n\nOther steps can also help increase the future productive capacity of the economy and thereby reduce the adverse effects of demographic change. For example, devoting resources to improving our K-12 education system, expanding access to community colleges, increasing on-the-job training, and stimulating basic research could augment the nation’s capital in the broadest sense of the term and might have desirable distributional effects as well.\n\nAnother response to population aging is to adopt measures that encourage participation in the labor force, particularly among older workers. In the near term, increases in labor force participation would raise income; some of this income would be saved and would thus be available to augment the capital stock. In the long run, higher rates of labor force participation, particularly by those who would otherwise be in retirement, could help to offset the negative effect of population aging on the share of the population that is working.\n\nTo some extent, increased labor force participation by older workers may happen naturally. Increased longevity and health will encourage greater numbers of older people to remain longer in the workforce. And slower growth in the labor force will motivate employers to retain or attract older workers--for example through higher wages, more flexibility in work schedules, increased training directed toward older workers, and changes in the retirement incentives provided by pension plans.\n\nReform of our unsustainable entitlement programs should also be a priority. The nature and timing of those reforms will be determined, of course, by our elected representatives. However, the intergenerational perspective does provide a few insights that might be helpful to policymakers as they undertake the needed reforms. First, restructuring the finances of our entitlement programs to minimize their reliance on deficit spending will enhance national saving and reduce the burden on future generations. Second, changes in the structure of entitlement programs should preserve or enhance the incentives to work and to save; for example, we should take care that benefits rules do not penalize those who may wish to work part-time after retirement. Finally, the imperative to undertake reform earlier rather than later is great. As illustrated by the simulation I discussed earlier, the longer the delay in putting our entitlement programs on a sound fiscal footing, the heavier the burden that will be passed on to future generations. Moreover, the sooner any restructuring of entitlement programs takes place, the easier it will be for people now in their working years to prepare, for example, by saving more today. However, if reform is delayed and fiscal exigencies ultimately force changes in these programs with little notice to potential retirees, their ability to adjust their behavior appropriately could be much reduced.\n\nConclusion\nOver the next few decades, the U.S. population will grow significantly older, a development that will affect our society and our economy in many ways. In particular, the coming demographic transition will create severe fiscal challenges, as the cost of entitlement programs rises sharply. I hope to have persuaded you today, however, that the economic implications of this transition go well beyond fiscal policy. From a broader economic perspective, the question is how the burden of an aging population is to be shared between our generation and the generations that will follow us. A failure on our part to prepare for demographic change will have substantial adverse effects on the economic welfare of our children and grandchildren and on the long-run productive potential of the U.S. economy.\n\nReferences\n\nAuerbach, Alan J., Jagadeesh Gokhale, and Laurence J. Kotlikoff (1991). \"Generational Accounts: A Meaningful Alternative to Deficit Accounting,\" in David Bradford, ed., Tax Policy and the Economy, vol. 5, National Bureau of Economic Research. Cambridge, Mass.: MIT Press, pp. 55-110.\n\nElmendorf, Douglas, and Louise Sheiner (2000). \"Should America Save for Its Old Age? Fiscal Policy, Population Aging, and National Saving\" Journal of Economic Perspectives, vol. 14 (Summer), pp. 57-74.\n\nGokhale, Jagadeesh, and Laurence J. Kotlikoff (2001). \"Is War Between Generations Inevitable?\" Report no. 246, National Center for Policy Analysis. Dallas, November.\n\nSheiner, Louise, Daniel Sichel, and Lawrence Slifman (2006). \"A Primer on the Macroeconomic Consequences of Population Aging,\" unpublished working paper, Board of Governors of the Federal Reserve System, Division of Research and Statistics, September.\n\nFootnotes\n\n1.  See Sheiner, Sichel, and Slifman (2006) and Elmendorf and Sheiner (2000) for discussions of the basic approach. Return to text\n\n2.  Another approach for gauging the potential impact of demographic change on future generations is the generational accounting framework developed by Auerbach, Gokhale, and Kotlikoff (1992). This framework begins with the assumption that, for people living today, tax rates will not be increased and benefits will not be cut. On that assumption, one can calculate the taxes (net of transfers received) that future generations will have to pay to achieve long-term balance in the government budget. According to recent estimates using this approach, to achieve long-term budget balance the net tax rate on future generations will have to be about double the tax rate on current taxpayers (Gokhale and Kotlikoff, 2001). This approach looks at the intergenerational issue through the prism of fiscal policy rather than taking the broader economic perspective I have emphasized today, and its underlying assumptions are somewhat different. However, the basic message--that failure by the current generation to address the economic implications of aging will impose significant costs on future generations--is the same.  Return to text\n\n3.  It is worth noting that a household’s saving need not equal its change in wealth, since the standard definition of saving excludes capital gains. One plausible explanation of the recent low level of household saving rates is that capital gains in stocks and in residential real estate, by increasing wealth, have reduced the motivation of households to save out of current income. If that explanation is correct, then the recent slowdown in the appreciation of house prices should lead ultimately to some increase in household saving rates, all else equal. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20061004a.htm",
        "title": "The Coming Demographic Transition: Will We Treat Future Generations Fairly?",
        "date": "10/4/2006"
    },
    {
        "content": "October 04, 2006\n\nVice Chairman Donald L. Kohn\n\nAt the Money Marketeers of New York University, New York, New York\n\nI am pleased to be with you tonight to discuss my views on current economic conditions and the economic outlook. These have been challenging times for economic forecasters and policymakers. Since the summer of 2005, the economy has absorbed a wide variety of shocks--major hurricanes, ongoing geopolitical tensions, and substantial increases in energy prices--and has adapted to a rise in short-term interest rates to more normal levels. Yet real gross domestic product (GDP) increased a respectable 3-1/2 percent from the second quarter of 2005 to the second quarter of 2006, and the unemployment rate fell to 4-3/4 percent. At the same time, however, headline consumer price inflation has been quite high, and an upward movement in core inflation has raised concerns about the persistence in price pressures, a very worrisome development from the point of view of a monetary policy maker.\n\nThe economic outlook for the next few years will be importantly shaped by ongoing responses to these developments. Reflecting those responses, economic activity has slowed noticeably over the course of the year, and inflation, though down from its level earlier this year, remains uncomfortably elevated. However, I expect that the continuing adjustment will be relatively benign overall: The economy will grow at a moderate pace for a while, somewhat below the rate of increase of its potential, and then growth will begin to strengthen. In addition, as the cost pressures from the run-up in energy and materials prices begin to play out, or perhaps even partly reverse, and as pressures on resources ease slightly, I think we will likely see much lower headline inflation and a gradual diminution of core consumer price inflation.\n\nI know that to some this story of a soft landing seems too good to be true--the triumph of hope over experience. One question is whether it is even feasible. Can inflation pressures decrease with only a modest shortfall of economic growth from potential? And is it possible that a modest decline in resource utilization will not cumulate into something more serious, as it has tended to do in the past--at least without a major adjustment of policy? My view is that this economy is capable of generating the type of favorable outcome that I have just sketched, but, as in any period of transition, policymakers must be aware of heightened risks on all sides of the forecast. I must emphasize that these views are my own and not necessarily those of my colleagues on the Federal Open Market Committee (FOMC).1\n\n\n\nEconomic Activity\nBased on the data we now have, the growth of real GDP in the third quarter appears to have remained as subdued as it was in the second quarter and may well have slowed further. As we enter the fourth quarter, little in the way of hard economic data or anecdotal information suggests any sharp shift in the pace of economic activity. If that is so, the economy could be in the process of registering several consecutive quarters of growth below its potential rate, the first time it has done so since early 2003.\n\nAfter three years of growth above potential, some slowing was inevitable and desirable. Trees do not grow to the sky, and neither do the stocks of houses and durable goods held by households and businesses. In addition, the run-up in energy prices sapped consumers' purchasing power and cut into firms' profit margins. These hits to real incomes have restrained the growth of household and business spending. The effects of the high relative price of energy that we have experienced for much of the past year do appear to be reducing the demand for energy-intensive products. In particular, the major domestic automakers are cutting production to eliminate unwanted stocks of gas-guzzlers, and these cuts are exerting a further drag on the growth of real GDP in the second half of this year.\n\nSpending is also being restrained by the removal of monetary policy accommodation over the past two years. Without these policy actions, the developing pressures of demand on potential supply would have added to inflationary pressures. As anticipated, higher interest rates have been felt most clearly in the market for residential real estate. The adjustment in these markets has proven to have been more rapid and deeper than many economists had predicted, and we have yet to see signs that indicate just how the process will work itself out. Given the importance of housing markets in the evolution of the economy, I will spend a bit more time discussing the performance of this sector over the past five years and the factors that are likely to shape the adjustment process that is now under way.\n\nFrom a trough of fewer than 1.5 million units at an annual rate during the recession of 2000, starts of new single-family and multifamily homes rose to a post-World War II high of 2.2 million units last year. Sales of new and existing homes followed the same broad pattern, and the boom in residential real estate markets was a powerful force driving the post-2000 economic expansion. Monetary policy played an important role in these developments: Responding to the weakness in other sectors of the economy, the FOMC held short-term interest rates at unusually low levels over much of this period. With inflation expectations well contained, with investment weak relative to saving in other countries, and with investors requiring much less extra compensation for holding longer-term obligations, long-term mortgage rates also dropped to historically low levels. Housing affordability increased substantially, and the homeownership rate hit new highs. In addition, a speculative element may have emerged in this market as investors projected rapid price increases into the future.\n\nAnd those price increases were considerable. Between the beginning of 2001 and the end of 2005, the constant-quality price index for new homes rose 30 percent and the purchase-only price index of existing homes published by the Office of Federal Housing Enterprise Oversight (OFHEO) increased 50 percent. These increases boosted the net worth of the household sector, which further fueled the growth of consumer spending directly through the traditional \"wealth effect\" and possibly through the increased availability of relatively inexpensive credit secured by the capital gains on homes. By the end of last year, however, the high price of houses and rising interest rates had begun to take a meaningful toll on demand for homes.\n\nDetermining the exact timing of the recent peak in the housing market is difficult given the effects of last year's hurricanes, the volatility in the data, and timing differences in the evolutions of home sales and housing starts. That said, the fourth quarter of last year seems to provide a reasonable reference point: Since that time, housing starts have fallen about 20 percent, and home sales are down 10 percent. Home-price appreciation has also slowed dramatically since late last year, and some local markets have experienced outright price declines. Homebuilders report that cancellations have increased sharply, especially for second homes. Realtors note that existing houses are staying on the market longer, and sellers must increasingly make concessions to buyers.\n\nHow much longer will the correction in housing last, and how much deeper will it go? I do not have a definitive answer but would venture four observations. First, the reported declines in new home prices in a number of areas should help to facilitate the rebalancing of supply and demand in those markets--though it may accentuate the adverse spillover of the housing market correction to other sectors. Deeper price cuts would allow builders to clear out their inventories of unsold homes sooner, helping to stabilize the pace of residential construction activity faster, but the near-term hit to household wealth presumably would also be greater. Second, calculations about the sustainable level of housing starts based on demographic factors, such as population growth and household formations, suggest that starts may be closer to their trough than to their peak. Although such calculations are, in general, not particularly useful for near-term forecasting, they do suggest that any overbuilding in 2004 and 2005 was small enough to be worked off over coming quarters at close to the current level of housing starts. Third, the Federal Reserve has returned short-term interest rates only to more-normal levels and long-term rates are unusually low relative to those short-term rates. This situation stands in sharp contrast to some past downturns in the housing market that followed actions by the Federal Reserve to tighten credit conditions significantly. And fourth, continuing growth in real incomes should underpin the demand for housing and, as home prices stop rising, help to erode affordability constraints.\n\nTo date there is little evidence that this correction in the housing market has had any significant adverse spillover effects on other parts of the economy. The production of construction supplies has decelerated, but in general, resources freed up in the residential market appear to have been largely absorbed in nonresidential building or elsewhere. Indeed, after languishing for many years, the market for nonresidential structures seemed to revive around the time that the residential market was starting to show signs of slowing. This shifting of resources can likely continue for a while longer given the declines seen in office and commercial vacancy rates and the higher rates of capacity utilization in manufacturing.\n\nStill, adverse spillovers will occur, and, as I indicated, their extent depends in part on the changing mix of prices and quantities as the housing market adjusts. In the past, outright declines in the nominal prices of houses have been relatively rare and localized. If something like this pattern prevails again, the decline in real housing wealth relative to incomes will be modest, and household saving rates should trend gradually higher. Such a rise in personal saving would not be an adverse outcome for an economy that generates relatively little saving domestically.\n\nOne reason I expect the economic expansion to continue despite the retrenchment in housing markets is the recent declines in energy prices. Oil prices have fallen around $15 a barrel from their recent highs this summer, and because of abundant supplies and cooperative weather, the spot price of natural gas is down significantly as well. If sustained, these lower prices are likely to boost consumers' purchasing power and help to offset to some extent the adverse spillover effects from weakness in the housing market.\n\nIn addition, financial conditions remain quite supportive of borrowing and spending. Market interest rates are not high in nominal or real terms; credit spreads are narrow and equity prices continue to rise, conditions that keep the cost of business finance down and suggest investor confidence in the future course of the economy.\n\nAs the inventory overhangs in residential housing and automobiles are worked off, economic growth should pick up again to a rate closer to the growth rate of its potential. One potential pitfall in this argument is that, in the past, a noticeable and sustained shortfall of growth from its potential and an accompanying decrease in resource utilization have often cumulated into a full-fledged recession. Several features of the current financial situation, however, support my contention that \"this time will be different.\" These recessions have often been triggered by a highly restrictive stance of policy and a generalized tightening of credit conditions through high long-term rates, wide risk spreads, and a pull-back of bank lending. Obviously, these conditions are not present today. Although one cannot rule out the possibility that a withdrawal from risk-taking could impinge on credit supplies and intensify downward pressure on activity, the preconditions for such a response do not seem to be in place. Business balance sheets are in very good shape and financial institutions are quite well capitalized.\n\nTo be sure, the risks to these expectations of a limited shortfall of growth from potential seem to me to be weighted toward a weaker outcome. The housing market is not yet clearing, prices are still elevated relative to rents, the overhang could be larger than I perceive, and the attendant readjustment could be more abrupt and destabilizing--and could possibly even overshoot on the downside. And spillovers from the housing market could extend well beyond wealth effects if households had been relying on easy access to rising housing equity to finance a substantial portion of their consumption spending. Consumer confidence could erode as job growth and income gains slow, thereby sparking a steeper rise in saving. But, given current information, including on consumer confidence and spending, I judge my more benign scenario the more likely outcome.\n\nInflation\nWould such an outcome for economic activity be consistent with an abatement of inflation pressures? As you know from our announcements, minutes, and public utterances, the members of the FOMC are very concerned about the rise in core consumer price inflation over the past year. From a pace of 2 percent in the twelve months ending in August 2005, the rate of core personal consumption expenditures (PCE) inflation has risen to 2-1/2 percent.\n\nOne of the key issues in the analysis of core inflation is the role of the pass-through of energy cost increases into the prices of other goods and services. The pass-through turns out to be harder to find either econometrically or in the price data themselves than any savvy consumer might think. Turning first to the data, a detailed breakdown of the consumer price index shows that the prices of the most energy-intensive services, such as air travel or refuse collection, have picked up considerably, a result likely attributable, at least in part, to the run-up in fuel prices over the past few years. But these items represent a relatively small part of the core index; the small acceleration in many other nonshelter portions of the index, while consistent with a small pass-through of energy costs, could also be attributable to non-energy factors.\n\nWhen we try to model energy pass-through econometrically, the results indicate that a break occurred in pricing patterns in the early 1980s: Pass-through is clearly evident before 1980 but it is difficult to find thereafter. I suspect this pattern has something to do with the monetary policy reaction to those shocks and its effect on inflation expectations. In the 1970s, monetary policy not only accommodated the initial shocks but also allowed second-round effects to become embedded in more persistent increases in inflation. Since the early 1980s, the pass-through to core prices has been limited or non-existent, at least in part because households and firms have expected the Federal Reserve to counter any lasting inflationary impulse that they might produce. This result reinforces the need today to keep inflation expectations well anchored. In addition, movements in relative oil prices were more persistent before 1980 and less persistent after--until recently. After 1980, households and firms probably expected deviations of energy prices from long-run averages to be largely reversed and saw less reason to try to adjust wages and prices in response to what they viewed as transitory changes in energy costs.\n\nIn the final analysis, I think we probably saw some pass-through of higher energy costs into core inflation once price and wage setters came to believe that the rise in energy prices would not soon be reversed. But the magnitude of the effect has been small--perhaps on the order of a cumulative 1/2 percentage point or less since the end of 2003. If crude oil prices hold at close to current levels over the next few years, the resulting absence or even partial reversal of these energy cost shocks should, all else equal, put some modest downward pressure on core inflation.\n\nConsumer energy prices have already flattened out according to the August data, and we will probably see a big decline in September's report. This decrease will not erase the increases of the past few years, but I believe that it will contribute to a lessening of consumers' fears that continued energy-price increases will lead to a ratcheting up of inflation in the long run. Indeed, the most recent readings on inflation expectations from the University of Michigan Survey Research Center showed a noticeable decline in September, especially in the inflation rate expected twelve months ahead. In financial markets, the spread of nominal over indexed yields has also retreated substantially at the near end of the yield curve. To a monetary policy maker focused on the evolution of inflation expectations, these developments are indeed steps (albeit small) in the right direction.\n\nAnother major force driving up core consumer price inflation over the past year has been shelter costs, especially tenants' rent and owners' equivalent rent. Together, these two components account for a substantial part of the core price indexes--38 percent for the consumer price index (CPI) and 17 percent of PCE prices--and as a result, small shifts in price trends in these areas can have a noticeable effect on core inflation. For example, after running at about a 2-1/2 percent pace for several years, increases in owners' equivalent rent stepped up to an annual rate of 5 percent in the six months ending in August. As you know, these prices are imputed from the rental housing market, and quite possibly this acceleration resulted from a shift in demand toward rental housing as higher interest rates and home prices, along with reduced expectations of capital gains, made the owner-occupied market increasingly less attractive. In response to greater demand, the supply of rental housing should increase over time, in part by drawing from the overhang of owner-occupied units; hence, I do not expect rents to be a major influence on core inflation a year or two from now, the horizon that is the focus of monetary policymaking. Clearly, however, the band of uncertainty about such a forecast is rather wide.\n\nNot only should the contribution of energy and shelter costs to underlying inflation be diminishing over coming quarters, but the generalized pressure of demand on supply should also decrease if, as I am anticipating, economic growth falls short of potential for a time. I would not expect modest changes in the output gap to exert more than a marginal influence on inflation. But the anticipated slower pace of growth will result in an environment in which firms will be less able to pass on increases in costs.\n\nOne potential source of higher costs comes from the labor market. I would not be surprised to see a gradual rise in labor costs as workers capture a greater share of the productivity gains of recent years. However, compensation per hour, a measure derived from unemployment insurance tax records, indicates that labor costs accelerated sharply, to a pace of 7-3/4 percent from the second quarter of 2005 to the second quarter of 2006. In contrast, readings from the employer cost index (ECI), which is derived from a probability sample of firms, shows labor costs decelerating. Some of the divergence appears to be the result of an increased volume of stock option exercises in early 2006--an occurrence captured by the compensation per hour measure but not by the ECI--and these option exercises should not represent costs that firms actually internalize when calculating their marginal cost of production. Thus, in my own thinking, I have tended to discount, though not dismiss, the latest readings on labor costs. However, I acknowledge that rising labor costs are an upside risk to my inflation outlook, especially if they occur under product-market conditions in which firms can readily pass costs through.\n\nIn sum, I think that the odds favor a gradual reduction in core inflation over the next year or so, but the risks around this outlook do not seem symmetric to me: Important upside risks to the outlook for inflation warrant continued vigilance on the part of the central bank. I say that not only because of the questions about underlying labor costs and about the future direction of energy and shelter prices but also because our understanding of the inflation process is limited, and I cannot rule out the possibility that the upward movement earlier this year reflected a more persistent impulse that I cannot now identify. Although I believe I have offered plausible explanations for the acceleration of inflation this spring and summer and reasonable rationales for expecting inflation to moderate, I would feel much more confident about where we are heading if I had a more accurate bearing on the direction from whence we have come. In addition, in my view, if inflation failed to abate it could impose considerable costs on economic performance over time, a concern that brings me to the topic of monetary policy.\n\nMonetary Policy\nEven if my relatively favorable forecast comes true, the level of short-term interest rates that will produce this forecast remains uncertain. Obviously, as my FOMC voting record indicates, I believe that, for now, the current level of short-term rates has the best chance of fostering this outcome. Looking ahead, policy adjustments will depend on the implications of incoming data for the projected paths of economic activity and inflation. I must admit I am surprised at how little market participants seem to share my sense that the uncertainties around these paths and their implications for the stance of policy are fairly sizable at this point, judging by the very low level of implied volatilities in the interest rate markets.\n\nAs I have outlined tonight, I think that the risks to my outlook for economic activity may be skewed a bit to the downside, while those to my forecast of gradually declining inflation are tilted to the upside. In my view, in the current circumstances, the upside risks to inflation are of greater concern. Although to date inflation expectations have remained contained, failure to check and then reverse the greater inflation pressures of earlier this year would risk embedding those higher inflation rates in the decisions of households and businesses, an outcome that would be costly to reverse and would impinge on the economy's long-term performance.\n\nFootnotes\n\n1.  Charles Struckmeyer, of the Board's staff, contributed to these remarks. Return to text",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20061004a.htm",
        "title": "Economic Outlook",
        "date": "10/4/2006"
    },
    {
        "content": "September 27, 2006\n\nGovernor Randall S. Kroszner\n\nAt The Forecasters Club of New York, New York, New York\n\nI am delighted to be able to speak before the Forecasters Club of New York. One of the things I’ve enjoyed most during the past five years in both public service and in academia has been the opportunity to engage actively in economic forecasting. I chaired the so-called Troika-2 process when I was a member of the Council of Economic Advisers in 2001 to 2003 through which the economic forecast that is the basis for Administration’s budget is formulated. When I returned to the University of Chicago, I presented the annual economic forecast for the Graduate School of Business in both Chicago and New York, and I believe I see some people here who had attended those events. Since becoming a Governor in March, I have had the privilege of working on the forecasting with the superb staff at the Federal Reserve. In a sense, forecasting is where the \"rubber\" of economic theory meets the \"road\" of the real world. As such, it is intellectually exciting and challenging.\n\nOne of the foremost challenges has been forecasting productivity developments and their macroeconomic implications. As you know, productivity growth is the key source of higher living standards in the long run. But, of course, it also is an important influence shaping shorter-run economic developments as well as monetary policy decisions. Today I will talk about some of the forces that drive productivity growth, the macroeconomic implications of changes in the longer-run trend of productivity, and the prospects for productivity growth. My views on these topics are my own and do not necessarily reflect the views of my colleagues on the Federal Reserve Board or the Federal Open Market Committee.\n\nThe revolution in information technology (IT) is commonly taken as the initiating force behind the acceleration in productivity seen since 1995. Although I believe that IT is a necessary ingredient, I don’t believe it is sufficient. In particular, the interaction of IT advances with the flexible markets in the United States continues to be a crucial ingredient. The IT revolution has not simply allowed a worker to turn the crank faster on an improved machine (the traditional way we think of technological innovation) but opened the possibility of fundamentally altering the way production (or provision of a service) takes place; hence, the crucial role for flexible labor, product, and financial markets. As I will describe in more detail, this interaction effect of IT with a flexible economy can help to explain why the IT revolution has produced higher productivity growth in the United States but not in many other industrialized economies.\n\nWhile it is important for policymakers to understand the sources of the productivity resurgence, it is also important for us to understand the macroeconomic implications. An often overlooked implication is that, all else equal, an increase in the growth rate of productivity will tend to put upward pressure on real interest rates. But in fact we have not seen the predicted rise in real rates. Of course, we do not live in the world of simple economic models so all other things are not equal. In particular, I believe one reason is that sound economic policies have created a more stable economic environment, and with that has come low and stable inflation and an ongoing desire by foreigners to invest in the United States to reap higher returns associated with higher productivity growth than may be available in their economies.\n\nAt bottom, I expect that the flexibility of U.S. markets will continue to provide a nourishing environment for technological and process advances, and that very flexibility, along with sound monetary policy, will also allow the U.S. economy to enjoy the benefits of the evolving macroeconomic dynamics that accelerating productivity sets in motion.\n\nA Framework for Analyzing the Growth of Labor Productivity\nA great success story for the American economy has been the resurgence of productivity growth that began around 1995.1 From 1973 to 1995, labor productivity in the nonfarm business sector increased at an annual rate of 1-1/2 percent. (Labor productivity is defined in terms of output per hour of work in the economy.) In contrast, from 1995 to 2000, productivity accelerated to a 2-1/2 percent rate. Perhaps even more remarkably, despite a recession, the fall of the dot-com market, a broad stock market correction, terrorism, and corporate governance scandals, productivity has accelerated even further since 2000. Despite some slowing in the past few quarters, productivity in the nonfarm business sector has risen at an average annual rate of about 3 percent over the past 5-1/2 years.\n\nMany economists, including myself, use growth accounting as a framework for analyzing productivity developments. In its simplest terms, growth accounting decomposes the growth rate of labor productivity into two major components. One is the contribution to productivity growth that comes from giving workers more capital to work with, such as equipment or software; the standard term for this component is \"capital deepening.\" The other major contribution comes from the growth of multifactor productivity--that is, the efficiency with which labor and capital are combined to create output.2 Multifactor productivity growth reflects such things as business process innovations--for example, enhanced supply-chain management techniques or more-effective retail store layouts; advancements in technology, such as the development of new-generation computer chips; or most any other type of improvement in the efficiency of a firm’s operations. This aggregate growth-accounting framework forms the economic underpinning of key comprehensive productivity statistics produced by the Bureau of Labor Statistics.\n\nWork done over the past decade takes a somewhat more disaggregated approach to growth accounting in order to get inside the aggregate numbers and try to get a better handle on the sources of our remarkable productivity performance since 1995.3 But all of this is just accounting--albeit, in some research, very elaborate and painstakingly constructed accounting. The deeper analytical questions are, What are the forces driving capital deepening and the growth of multifactor productivity, especially since 1995, and Why does the United States seem to have experienced increased productivity growth, since the mid-1990s, that has not been shared by many other industrial economies?\n\nThe Forces Driving the Growth of Labor Productivity\nIn broad terms, the story for the post-1995 productivity resurgence that comes out of the various studies that take a disaggregated approach, as well as case studies such as those conducted by McKinsey (2002), is well known. Technological advances in the IT-producing sector--that is, multifactor productivity--and associated investments in more and better production equipment (capital deepening) started things off. These developments were bolstered by investments in IT equipment and software by firms outside of the IT-producing sector, improvements in the knowledge and skills needed to use effectively the equipment and software, and innovations in business processes.\n\nAn unexplained puzzle, however, remains in that story. If productivity growth were simply a matter of installing ever-more-powerful computers or reading ever-more-advanced technical manuals, then all countries with access to the breakthroughs in information technology from the past two decades should have enjoyed the same productivity revival as the United States. But that hasn’t been the case. Since 1995, productivity in the United States has grown substantially faster than in other advanced industrial countries. For example, a recent study by van Ark and Inklaar (2005) indicates that while productivity in the United States accelerated after 1995, average productivity in Europe actually decelerated--indeed, they estimate that the trend in the fifteen countries that made up the European Union before 2004 has been decelerating since the mid-1980s. According to van Ark and Inklaar, some of the differential reflects faster capital deepening in the United States, but much of it reflects a surge in multifactor productivity growth in the United States outside of the IT sector since 2000.4\n\nInnovations in information technology, however, cannot be the whole story--flexibility at the firm level and in labor markets, and competitive pressure throughout the economy, also play their roles.5 Businesses must be flexible enough to adopt new technologies and then to transform themselves in ways that allow technology-intensive investment to have the highest possible effect on productivity growth. Similarly, labor markets must be flexible enough to allow for the prompt re-allocation of resources in response to changes in demand. The economy also must be competitive enough to allow useful innovations at some firms to be transmitted throughout the industry by market pressure.\n\nAt the firm level, an important characteristic of the American economy is that we have a business culture that rewards nimbleness, innovation, and entrepreneurship. As an example, consider the case of U.S. retail trade. To be sure, firms in this industry invested heavily in information technology in the 1990s. Yet they did not become more productive simply by buying faster computers and returning to business as usual. As discussed in a recent in-depth study by McKinsey & Co., IT investments were combined with a host of changes in business practices to raise productivity. Perhaps the best example is the use of IT to improve the links in the supply chain from vendor to retailer, to create a so-called glass pipeline through which retailers’ orders can be monitored as they progress (McKinsey 2002).\n\nFlexibility also has been evident in other industries. Consider, for example, a study done of a medical products company that made a large investment in computer-integrated manufacturing (Brynjolfsson and Hitt 2000). The flexibility gained by this investment necessitated a host of other changes in business practice, such as the elimination of piece rates, the encouragement of workers to stop the production line if it is not running at full speed, and a reduction in management layers. Eventually, productivity rose so much that the firm painted the windows of this site black so that competitors could not see how the new system worked!\n\nThere is an important historical parallel in the United States to the interaction of technological innovation and flexibility in producing higher productivity growth. In the early twentieth century, the electrification of production operations and the electric motor did not substantially raise manufacturing productivity until firms realized that electricity allowed them to rethink the layout of their factories. Rather than build a many-storied factory around a centralized power source, a firm could disperse electric motors around a single-story plant and thereby create the modern assembly line. Workers then could use specialized tools to undertake new activities. This redesign allowed the firm to optimize material handling, change production lines more easily, and perform maintenance on individual sections of the plant without idling production throughout the facility (David, 1990).\n\nMore broadly, the increasing competitiveness of the American economy over the past quarter century or so has brought with it a market imperative for creativity, innovation, and efficiency. In 1977, Fred Kahn, the Cornell University economist, came to Washington as the chairman of the now-defunct Civil Aeronautics Board and as an adviser to President Carter on deregulation. With Professor Kahn as the prime mover, the Airline Deregulation Act was passed in 1978. This act started the ball rolling, and in fairly short order the Congress passed legislation that deregulated the rail, trucking, and interstate bus industries. Deregulation removed barriers to entry and made it possible for a multitude of new firms to enter the transportation industry.\n\nFreely allowing the entry of new firms generates competitive pressures that have a ripple effect throughout the economy. For example, it’s difficult to imagine that online retailers could have become so successful without access to inexpensive, interstate (and international) package delivery services. But the proliferation of these delivery services would not have been possible without the deregulation of the transportation industry in the late 1970s and early 1980s.\n\nThe converse also may be true. Regulatory barriers to entry in the retail sector in Japan and Europe--for example, restrictions on land use and shopping hours--appear to have impeded the development of more-innovative, and presumably more-productive, types of retailing (Gordon, 2004). More generally, research conducted at the Federal Reserve suggests that regulatory environments in a number of industrial countries have impeded the adoption of information technologies and slowed productivity growth (Gust and Marquez, 2004).\n\nIncreased trade liberalization, which lowers barriers to the international flow of goods, financial capital, and direct investment, also spurs innovation and creativity. An interesting illustration of the connection is the productivity of multinational corporations. Research conducted by Federal Reserve System economists and others has found that, in the United States, multinational firms are more productive than domestically oriented firms, and the difference holds regardless of whether the parent firm is headquartered in the United States or abroad.6 Perhaps even more remarkable is a finding that, in the United Kingdom, multinational firms owned by U.S. parents are more productive than multinational firms owned by British parents (Bloom, Sadun, and Van Reenen, 2006).\n\nI draw two conclusions from this work. First, trade liberalization appears to have made it possible for multinational firms to institute highly efficient cross-border supply chains within their firms that seem to have allowed them to boost significantly the efficiency of their worldwide operations. Second, U.S. firms, on average, have more flexible and innovative business practices, sometimes called organizational capital, that a liberalized trade regime apparently allows them to transfer to their foreign operations.\n\nTaking Account of Productivity in a Macro Forecast\nLet me switch gears now from the sources of our remarkable productivity performance to some of its macroeconomic implications. A good deal of research, including the Board’s large-scale econometric model of the U.S. economy (FRB/US), suggests that what is called Say’s law still holds.7\n\nThat is, in the model, an increase in the level of productivity (reflecting, for example, some technological advance) causes businesses and financial markets to revise upward their views about the level of expected profits, and it causes households to revise upward their views about the level of permanent income. The higher level of expected profits and returns to capital, in turn, lead to a rise in business investment. Similarly, personal consumption expenditures are boosted in response to the rise in permanent income. The initial increases in spending are then followed by multiplier effects. A dynamic feedback also occurs on the supply side as the higher level of investment spending increases the capital stock (relative to the supply of worker hours), which gives a small fillip to productivity and potential output. Ultimately, the increases in aggregate supply are matched by an equivalent increase in aggregate demand.8 This is, of course, Say’s law.\n\nWhat I’ve just described is a sketch of what happens after a one-time rise in the level of productivity. In the case of an ongoing rise in the growth rate of productivity, the dynamics and macro consequences are more complicated. In particular, all else equal, a positive shock to the growth rate of productivity will tend to put upward pressure on real interest rates.9 The upward pressure on real interest occurs, in part, because investment must rise to keep the growth of the capital stock in line with the faster growth of gross domestic product. In addition, a shock to the growth rate of productivity boosts household’s assessments of the growth rate of their permanent income, while increases in the expected growth rate of profits and dividends raise asset values, including the value of equities, relative to current income. The combination of faster expected growth of permanent income and higher stock market wealth tends to raise consumption relative to income and, concomitantly, lower personal saving. Thus, all else equal, the increase in demand for financing relative to domestic saving will tend to boost real interest rates.10\n\nThe dynamics of this process, and how long they take to play out, depend on several factors. One factor is how quickly the productivity change is incorporated into household and business expectations. The change in expectations can be drawn out if households and firms are slow to recognize an inflection point in the productivity growth trend or are highly uncertain about how long any observed change in the data might last. This seems to have been the case in the mid-1990s, when it took some time for that recognition to begin to sink in to the mind-set of most households and businesses. To his credit, Chairman Greenspan was one of the first to call the sea change in our productivity performance to public attention.11\n\nAnother factor influencing the dynamics of the process the degree to which businesses, financial markets and consumers are forward-looking in their economic behavior. If they are myopic in their behavior or tend to discount the future very heavily, then the dynamic response of the economy to a change in the growth rate of productivity will be drawn out.\n\nAt this point, you might be saying to yourself, \"Hold on; if an increase in the productivity growth trend is supposed to boost real interest rates, why have real rates been falling since around mid-2004 and are low by historical standards?\" The answer involves that favorite safety net of economists, the \"all else equal\" caveat. In fact, all else has not been equal. Importantly, the term premium embedded in interest rates has been falling. The term premium reflects the extent of uncertainty about future prospects for inflation and for real economic activity. The reduction in the term premia appears to be associated, in part, with the greater economic stability we have been enjoying. Real activity has become less volatile; moreover, inflation is lower and, as long as we at the Federal Reserve do our job, more predictable.12\n\nThe huge inflow of foreign saving into the United States, undoubtedly, also has been important. As then-Governor Bernanke observed in 2005, differential demographic trends and rates of return on investment between the United States and many of the world’s other rich countries is part of the explanation for that inflow (Bernanke, 2005). Rich countries, with populations that are aging faster than ours, have a strong motive to save to provide for an impending sharp increase in the number of retirees relative to the number of workers. Moreover, many advanced economies outside the United States also have a paucity of domestic investment opportunities relative to the United States. As a consequence of high desired saving and low prospective returns to domestic investment, the mature industrial economies outside of the United States, as a group, seek to lend abroad. The higher prospective returns in the United States may be due in part to the higher productivity growth that the United States has been experiencing relative to many other industrialized countries resulting from the interaction effect of IT innovations and the flexibility of the United States economy relative to other countries.\n\nWhat about the effects of a productivity shock on inflation? Ultimately inflation is determined by the policy actions of the central bank. In the short run, however, a change in the trend growth rate of productivity can influence inflation dynamics. A one-time change in the level of productivity, or transitory volatility in productivity growth rates, are unlikely to have lasting effects on business pricing behavior. Economic theory and econometric evidence suggest that only a persistent shock to the rate of change of productivity has persistent consequences for rate of change of prices--that is, inflation.\n\nIf we lived in a world with no impediments to competition in labor and product markets, with prices and wages that freely and quickly moved up and down in response to shifts in economic conditions, then a change in productivity growth would be promptly matched by a corresponding change in nominal compensation per hour. As a consequence, unit labor costs would be unchanged, and all else equal, so would inflation.\n\nBut, we don’t live in such a perfectly competitive, frictionless world. Nominal compensation per hour initially seems to respond sluggishly to changes in the economy, including productivity shocks. As a result, an increase in productivity growth, for example, initially slows the growth of unit labor costs, which firms--under competitive pressure--then pass on to their customers, thereby slowing price inflation. As price inflation slows and as, with a lag, nominal compensation per hour accelerates, the growth rate of real compensation per hour increases so that over time workers share in the benefits of faster productivity growth. Indeed, in the past, any rise in the level of productivity has eventually been fully translated into a rise in the level of real compensation per hour. How quickly the re-equilibration takes place depends in part on the extent of competition in product markets and the nature of the wage-bargaining process. Up until now, the process in our economy has taken at least a few years, but it has always occurred.\n\nProductivity and Real Compensation Per Hour\nWhat I have dubbed the re-equilibration of productivity and real compensation per hour is just another manifestation of one of the great stylized facts of macroeconomics: In the past, deviations in the labor share of income from its mean value of roughly two-thirds have eventually been reversed. But the two-thirds share is an empirical observation about the U.S. economy; it is not an immutable number derived from the first principles of economic theory. As it turns out--I’ll leave the proof to you as a homework assignment--mean reversion in the labor share is equivalent to the observation that over time labor productivity and real compensation per hour have moved together; in the jargon of econometrics, they are co-integrated. (See chart.)\n\nAs I just mentioned, when the labor share deviates from its long-run average or, equivalently, a gap opens between productivity and real compensation per hour, the reversion to the mean (that is, the closing of the gap) can take quite a while. In recent years, the labor share has moved down as increases in real compensation per hour have, for the most part, lagged behind productivity growth, but the timing and extent of the change in the labor share depends in part on the particular statistical measure chosen.13\n\nA challenge for forecasters is deciphering whether this latest drop in the labor share is transitory, as such drops have been in the past, or whether some structural aspect of the economy, such as the wage-bargaining process, has changed to make the drop in the labor share permanent. More likely, the adjustment process is taking a long time to play out, as it did in the 1990s, and some recent evidence may suggest that the gap is beginning to close. Assuming that the drop is transitory, another challenge for forecasters is predicting whether the adjustment to real compensation per hour will be driven by a pickup in the growth of nominal compensation per hour or by a reduction in inflation.\n\nProspects for Productivity\nLet me close with some comments on the outlook for productivity. Recent estimates by a number of economists suggest that the underlying trend in productivity in the nonfarm business sector is about 2-1/2 percent per year, close to the rate of productivity growth achieved during the period from 1995 to 2000.14 I think a good case can be made for the view that the strong productivity growth of the post-1995 era will persist for some time. The rate of technology growth appears to be proceeding apace, and further diffusion of already existing technologies and applications to more firms and industries should continue to boost productivity.\n\nConclusion\nAn important lesson of the U.S. productivity resurgence is that an open economy with flexible labor, capital, and product markets is critical for a nation to enjoy the full benefits of recent IT innovation and, thus, to enhance a nation’s productivity performance going forward. In my opinion, the productivity developments that we are likely to see in coming years will be fostered by a U.S. economy that remains very flexible, highly competitive, and open--if anything, it is becoming even more flexible, competitive, and open. If this assessment is reasonably close to the mark, the prospects for future improvements in our nation’s longer-run living standards should be quite favorable, and this underscores the importance of maintaining an open, flexible, and stable economy. Even small increases in productivity growth have tremendous cumulative effects over time on production and income. Let me close by quoting the Nobel laureate Robert Lucas who once said that when one contemplates the effect that sustained economic growth has on human welfare, it is hard to think about anything else.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20060927a.htm",
        "title": "What Drives Productivity Growth? Implications for the Economy and Prospects for the Future",
        "date": "9/27/2006"
    },
    {
        "content": "September 11, 2006\n\nVice Chairman Donald L. Kohn\n\nAt the Western Payments Alliance 2006 Payments Symposium, Las Vegas, Nevada\n\nI am pleased to have the opportunity to speak at this symposium examining the continuing convergence of paper and electronic payments. Today marks the fifth anniversary of the September 11th terrorist attacks on the United States. On that day, as our nation came to grips with the terrible events it had just witnessed, the Federal Reserve remained open and operating, in part to help ensure that the nation’s payments system continued to function. In the aftermath of the attacks, the financial industry has taken many steps to strengthen the resilience of our nation’s critical payments infrastructures. For the retail payments system, the September 11th attacks highlighted the banking industry's extensive reliance on air transportation as planes came to a standstill and the collection of checks slowed dramatically. This prompted a heightened focus on how electronic processing technologies could be applied to the check-collection system to reduce the reliance on air transportation and improve check-processing efficiency more generally.\n\nMy remarks today, which reflect my own thoughts and not necessarily those of the other members of the Federal Reserve Board, will focus on the future of the check-collection system and the future role of the Federal Reserve in retail payments services.1\n\nA Period of Transition in the Retail Payments System\nToday, shifts in consumer behavior and rapid industry innovation, along with legal and regulatory change, are dramatically reshaping our retail payments system. Because of the increasing availability and declining cost of convenient electronic payment alternatives, many payments that were until recently being made in paper form are today being made electronically. In addition, new electronic technologies are now being harnessed to improve the processing of checks.\n\nThe 2003 Federal Reserve study on the use of retail payment instruments revealed dramatic changes in consumer behavior. It found that, for the first time ever, the number of electronic payments in the United States--such as credit card, debit card, and automated clearinghouse (ACH) payments--exceeded check payments. A range of data indicates that electronic payments have continued to increase and that check payments have continued to decline. Debit cards, primarily used by consumers for everyday purchases, are the fastest growing segment of the retail payments system. Consumers seem to view debit cards as a natural progression from cash and checks because they are a convenient electronic means of making payments without incurring the additional debt often associated with credit card use. In fact, on at least one major network, debit card payments are reported to have surpassed credit card payments.\n\nThis shift in payment behavior can be attributed in part to changes in the rules and regulations governing the ACH network, which have facilitated the use of this network for one-time, nonrecurring payments. As you know, in the past the ACH was used mainly for recurring payments, such as payroll and mortgage payments. Today, consumer purchases at stores, over the telephone, and on the Internet can be completed using the ACH. Regulatory and rule changes have also facilitated the use of the ACH to convert checks that consumers mail to businesses or provide at the point of sale into electronic payments. These new uses of the ACH for one-time payments have driven the continuing double-digit growth rates of ACH transaction volume. Given these dynamic changes in the payments system, the Federal Reserve is planning to repeat its triennial survey of retail payments use next year to take another snapshot of the nation’s retail payments system.\n\nThe Federal Reserve’s Experience with Check 21\nNot only are more payments being made electronically, but more check payments are also being processed electronically, in part because of the Check Clearing for the 21st Century Act, or Check 21.2 Clearly, Check 21 has begun to diminish the importance of geography and physical transportation in check processing, and banks have started to reengineer their backroom processes to accommodate end-to-end electronic check clearing.\n\nSince October 28, 2004, the date Check 21 took effect, the Reserve Banks’ Check 21 volume has grown rapidly, reflecting a trend that will lead to the widespread electronic processing of checks in the not-too-distant future. Private-sector service providers that offer Check 21 services are also experiencing rapid growth in volume as the banking industry becomes more interested in and capable of using Check 21 authority to clear checks. Today, about 17 percent of the checks deposited with the Federal Reserve Banks, or slightly over 6 million checks a day valued at about $20 billion, are deposited using the Reserve Banks’ Check 21 product suite. As expected, depository institutions have been somewhat slower in agreeing to accept their check presentments electronically because of the complexity of integrating such presentments into back-office processing and risk-management systems. As a result, the use of substitute checks is widespread. Nevertheless, in July 2006 almost 4 percent of the Reserve Banks’ daily volume, or about one and a half million checks, was presented to paying banks in electronic Check 21 files, and this volume is growing rapidly.\n\nThus, banks are starting to realize many of the benefits of the end-to-end electronic check processing that were envisioned when Check 21 was enacted, including efficiency gains and cost savings. In addition, they are beginning to offer their customers new and better services. For example, some banks are offering their business customers the ability to truncate checks and deposit them electronically. Also, banks are now able to set a later-in-the-day cutoff hour for check deposits because they can transmit checks electronically from their branches to their central processing facilities for collection. As a result, banks should be able to provide customers with improved funds availability, more efficient cash management services, and better access to services for their geographically remote customers.\n\nThe Evolution of Electronic Check Collection\nHow will electronic check collection evolve in the years to come? The industry is discussing several possible models. In the first, check images are transmitted to the paying bank along with the MICR-line payment information.3 In the second, the MICR information is transmitted to the paying bank while the check images are stored in remote archives that can be accessed on demand. Both models are already being used to some extent and each has its advantages and disadvantages. I suspect that, as the industry gains greater experience with electronic check collection, the superiority of one of the models may become clearer.\n\nThose who favor the first model believe that it allows paying banks to better manage their risks and customer relationships. Paying banks would not have to rely on multiple image archive providers, with whom they may have no direct contractual relationship, to obtain check images for customer online banking services and backroom operations. Others believe that a small number of centralized check-image archives, as envisioned in the second model, would be more cost-effective and would not appreciably increase risk or degrade customer service. In this model, the MICR information on a check could be transmitted over a dedicated network or, as recently suggested by some bankers, the ACH. The ACH is seen as a potentially attractive option because it is an electronic system that reaches all depository institutions and could eliminate the need to print substitute checks.\n\nThe flow of check information over the ACH system raises the legal question of whether the payments should be characterized as checks or electronic fund transfers. If the payments were characterized as checks, under current check law banks can choose whether to receive presentments electronically or in paper form. A key principle underlying Check 21 was to maintain this choice by permitting banks to participate in electronic check processing when their business case justified doing so. Requiring banks to accept electronic check presentment simply because they participate in the ACH system and have agreed to abide by NACHA rules might be viewed as contrary to this principle. Alternatively, if the payments were characterized as electronic funds transfers, then how would banks obtain the authorizations required under Regulation E to convert the transactions into ACH payments? When evaluating the relative benefits of using the ACH network for these payments, we must consider these threshold legal questions.\n\nA Perspective on the Future U.S. Check System\nThe checkless society has been predicted for decades; however, as we near 2007, we know that even though checks are used less frequently, they are still used widely. Industry experts are understandably wary of predicting what lies ahead for the U.S. check system. However, as we engage in strategic discussions on the future of checks and contemplate investments in check-processing infrastructure, we need to consider how the check system might evolve. Let me take a first step and offer one perspective.\n\nThe decline in check use has already caused the Reserve Banks to reduce by half the number of offices at which they process paper checks. In 2003, the Reserve Banks had forty-five check-processing offices nationwide, versus the twenty-two they have today and the eighteen that they will have by early 2008. Further consolidations are likely as check volumes continue to decline and as checks are increasingly processed electronically. Ultimately, perhaps sometime late in the next decade, the Reserve Banks might process checks at only a single office nationwide. These changes in the Federal Reserve’s check-processing infrastructure will benefit bank customers by entitling them to earlier funds availability on their check deposits, because all checks will eventually become local checks, which are generally subject to a maximum permissible hold period of two business days. This will present risk-management challenges for banks because a bank seldom learns that a local check is unpaid before it must make the funds available to the customer for withdrawal.\n\nMoreover, while some checks are being collected faster as a result of electronic processing, in the future other checks might be collected more slowly. We have already seen some banks decide to process large- and small-dollar checks differently. Specifically, many banks are expediting the collection of larger-dollar checks by taking advantage of the Check 21 authority. On the other hand, banks are continuing to collect smaller-dollar checks in paper form, using physical transportation networks, because the value of collecting these smaller-dollar checks faster does not justify the cost of doing so.\n\nAs larger-dollar checks are increasingly cleared electronically, the use of relatively expensive dedicated check-transportation networks to clear the aggregate value of the remaining checks will be harder and harder to justify economically. As a result, it is quite possible that it will take longer to collect smaller-dollar checks as dedicated, high-cost check-transportation networks are scaled back or eliminated and these checks are transported using slower, less costly means.\n\nThe Federal Reserve is now studying whether overall improvements in the check-collection system would be sufficient in the near term to justify the Board’s use of its authority under the Expedited Funds Availability Act to reduce maximum hold periods.4 The likely slowing of the collection of smaller-value checks could make any change problematic in the near term. Nonetheless, as I have already noted, recent and future consolidations of Federal Reserve check-processing regions have benefited, and will continue to benefit, bank customers as many nonlocal checks become local and therefore subject to shorter maximum holds.\n\nIf the payments system evolves as I have just outlined, then it is possible that in the next ten to twenty years the Reserve Banks will accept only checks that are deposited electronically and that can be presented electronically; any remaining paper checks may have to be cleared through other channels. In this scenario, the current paper-check infrastructure of the Federal Reserve that is heavily reliant on physical transportation will be replaced by an electronic-processing infrastructure with a production data center and a few backup sites, not unlike today’s ACH network.\n\nThe Role of the Reserve Banks in the Provision of Retail Payments Services\nThe changes I have just described could very well have implications for the role of the Federal Reserve in the payments system. From its inception in 1913, the Federal Reserve has not only been closely involved in overseeing the nation’s payments system but has also been an important operational component of that system. This latter role has involved competing with the private sector to provide certain retail payments. Congress originally wanted the Federal Reserve to play this operational role to reduce inefficiencies in the payments arena. This role has changed considerably over the past century, with some changes resulting from the enactment of statutes such as the Monetary Control Act, or the MCA.\n\nThe MCA went a long way toward establishing a relatively level playing field on which the Reserve Banks and private-sector payments providers have competed over the past quarter century. However, because of the inherent differences between the central bank and private-sector service providers, a truly level playing field will never be entirely possible. The Reserve Banks enjoy certain advantages, such as an unsurpassed credit rating, that makes them an attractive service provider in times of financial stress. On the other hand, the Reserve Banks do not have the flexibility enjoyed by their competitors to negotiate fees and other service terms with individual customers. Because of these differences, it is incumbent on us from time to time to reexamine our operational role in the payments system.\n\nThese assessments have focused in the past, and should continue to focus in the future, on the Federal Reserve’s role as a provider of retail payment services. Large-value payments systems, such as Fedwire, are typically viewed as core, systemically important services, and they are commonly provided by other central banks around the globe. A rationale for the participation of a government-related entity, such as the Federal Reserve, in the retail payments system is harder to formulate. The most recent assessment of this operational role was conducted in the late 1990s, by the Committee on the Federal Reserve in the Payments Mechanism, better known as the Rivlin Committee, and it focused on the Federal Reserve’s role in the check and ACH systems. Ultimately, the committee concluded that the Federal Reserve should remain a provider of both check-collection and ACH services and should play a more active role in helping the banking industry become more efficient. The committee determined that the Federal Reserve played an important role in providing both check and ACH services to small and remote institutions and that, if the Federal Reserve stopped providing these services, it could disrupt the market in the short run, resulting in higher prices and more regulation, with little promise of substantial benefits over the long run. In recent years, we have actively pursued a strategy of engaging the industry in discussions on payments system issues and offering services designed to encourage the increased use of electronics.\n\nAs we move into a more steady-state electronic check environment, the Federal Reserve may find it appropriate once again to review its longer-term operational role in the retail payments system. Clearly, at that time, the Federal Reserve’s national reach will no longer be a compelling reason for its operational role. As the Federal Reserve assesses its role in providing retail payment services, it will be important to consider how the circumstances that have provided the rationale for the Federal Reserve’s continuing involvement in retail payments services may have changed. The review would have to address, among other things, the following questions: If the Reserve Banks were to withdraw from check and ACH services, would these services continue to be provided competitively and cost-effectively? Would depository institutions continue to have equitable access to these services? In the event of an unanticipated shock, would the payments system be sufficiently resilient?\n\nConclusion\nIn conclusion, the retail payments system will continue to become increasingly electronic even though the exact nature of that system is not yet clear. What is clear, however, is that the Federal Reserve will continue to foster a safe and efficient payments system. This shift away from paper and toward the electronic processing of payments has significant operational and legal implications that all of us need to ponder. I believe that an ongoing dialogue among payments system participants will help us address, in a balanced and thoughtful manner, these important issues that affect the long-term strategic direction of the U.S. financial system. This symposium is a welcome and constructive part of that dialogue.\n\nFootnotes\n\n1.  Helena L. Tenenholtz and Jeffrey S. H. Yeganeh, of the Board’s staff, contributed to this speech.  Return to text\n\n2.  Check 21 removed legal barriers that were preventing electronic technologies from being applied to long-established check-collection processes. Before Check 21, a bank had to present the original paper check to the paying bank unless the paying bank had agreed to accept presentment of the check electronically. While Check 21 did not mandate the electronic processing or presentment of checks, it did authorize a new negotiable instrument, called a substitute check, which is the legal equivalent of the original check. By permitting banks to use substitute checks in the check-collection process when the recipient could not or would not accept electronic presentment, Check 21 has facilitated the expanded use of electronic technologies in check processing, enabling the banking industry to improve the efficiency and cost-effectiveness of its check-processing operations over the long run.  Return to text\n\n3.  The magnetic ink character recognition (MICR) line is the line of numbers printed near the bottom of the check, which generally includes the paying bank’s routing number, the customer’s account number, the check number, and the amount of the check.  Return to text\n\n4.  The Expedited Funds Availability Act requires that the Board reduce the maximum hold periods to the period of time necessary for the depositary bank to reasonably expect to learn of the nonpayment of most checks in a given category. Because roughly half of all checks are for amounts of less than $100, the improvements in the check system due to Check 21 would likely not result in faster collection and return of most local or nonlocal checks in the near term.  Return to text",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20060911a.htm",
        "title": "Evolution of Retail Payments and the Role of the Federal Reserve",
        "date": "9/11/2006"
    },
    {
        "content": "September 01, 2006\n\nChairman Ben S. Bernanke\n\nAt the presentation of the Order of the Palmetto, Dillon, South Carolina\n\nI am very pleased to have the opportunity to come back to Dillon and very grateful for the honor you have done me today. Thank you all very much for your thoughtfulness and your efforts to prepare for this event. It is good to see so many people that I remember, as well as their children and maybe even a few grandchildren.\n\nMy family’s connection with Dillon goes back about sixty years. My grandfather Jonas Bernanke and his wife, Lina, moved here in the 1940s when Jonas purchased a local drugstore. Jonas named the store with his own initials--hence Jay Bee Drugs. Jonas’s sons--my dad and my uncle--bought the store from their father and ran it as partners for many years. The original store was on Main Street, but when the chain stores entered the local market, my dad and uncle built a new, larger, and more modern store a block off Main. They competed by offering personal, first-name service, by delivering prescriptions without charge, and by coming down to the store at any time, nights or Sundays, to fill an emergency prescription. Dillon had very few doctors at the time, and lots of people came to my dad and uncle for advice on basic health matters. My mother, who gave up a job teaching elementary school when I was born, often helped out in the store. As a child, I was supposed to help out as well, but I usually ended up in the store’s comic book section.\n\nOur family moved to Dillon when I was very small, and all my childhood memories are here. I went to East Elementary, J.V. Martin Junior High, and Dillon High School. Quite a few friends were classmates for all twelve grades. I made many other friends playing saxophone in the high-school marching band. My family and I attended Dillon’s small synagogue, Ohav Shalom, which is unfortunately no longer in existence.\n\nAs a teenager, like many other teenagers, I itched to get away from the small town in which I was raised to see the bright lights of the big city. I got my wish, leaving when I was seventeen to attend college and graduate school in Boston. I met my wife in Boston, and we have lived in Palo Alto, California; Princeton, New Jersey; and now Washington. We have two children, a son and a daughter, now in graduate school and college, respectively. My wife is a teacher and is meeting her classes today in a public charter high school in Washington.\n\nAlthough the idea of leaving Dillon to attend college was exciting for me, I realize now that I learned a lot from living here for seventeen years. I learned, among many other lessons, a few things about work. I saw the long hours and persistent effort my parents put in to make their independent small business successful. After I graduated from high school I spent the summer as a construction worker helping to build Dillon’s Saint Eugene hospital, and during the summers of my college years, I waited tables six days a week at the South of the Border. I took two lessons from those experiences: First, in small towns like Dillon and in communities all across the United States, people work very hard every day to support themselves and their families. I remember that, on the first day I came home from the construction site that summer, I was too tired to eat and I fell asleep in my chair.\n\nThe second thing I learned in Dillon is that Americans are economically ambitious; they seek opportunity and advancement. I remember the fellow construction worker who wanted to become foreman someday and a waitress who was saving to go to college. I was impressed by these experiences, and I think they were an important reason I went into economics, which a great economist once called the study of people in the ordinary business of life.\n\nNow I am an economic policy maker, and I sit in a nice office in Washington looking at reports and tables of data and following the fluctuations of the financial markets. However, I try not to forget what underlies all those data: millions of Americans working hard, trying to better themselves economically, struggling to manage their family finances, and worrying about the price of gas and college tuition. I take my work extremely seriously because I know that, if my colleagues at the Federal Reserve and I do our jobs right, we will help our economy prosper and give more people the economic opportunities they seek.\n\nLet me thank you all again for inviting me back to Dillon and for sharing this morning with me.",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060901a.htm",
        "title": "Remarks",
        "date": "9/1/2006"
    },
    {
        "content": "August 31, 2006\n\nChairman Ben S. Bernanke\n\nBefore Leadership South Carolina, Greenville, South Carolina\n\nOne of the most important economic developments in the United States in the past decade or so has been a sustained increase in the growth rate of labor productivity, or output per hour of work. From the early 1970s until about 1995, productivity growth in the U.S. nonfarm business sector averaged about 1-1/2 percent per year--a disappointingly low figure relative both to U.S. historical experience and to the performance of other industrial economies over the same period.1 Between 1995 and 2000, however, the rate of productivity growth picked up significantly, to about 2-1/2 percent per year--a figure that contributed to the view, held by many at the time, that the United States might be entering a new economic era.\n\nTalk of the \"new economy\" faded with the sharp declines in the stock valuations of high-tech firms at the turn of the millennium. Yet, remarkably, productivity accelerated further in the early part of this decade. From the end of 2000 to the end of 2003, productivity rose at a 3-1/2 percent annual rate and, even after recent downward revisions to the data, it is estimated to have increased at an average annual rate of 2-1/4 percent since the end of 2003. These advances were achieved despite adverse developments that included the 2001 recession, the terrorist attacks of September 11, corporate governance scandals, and in the past few years, devastating hurricanes and very substantial increases in the cost of energy.\n\nWhy is the rate of productivity growth so important? Economists agree that, in the long run, productivity growth is the principal source of improvements in living standards. The logic is simple: In the long run, what we can consume as a nation is closely tied to how much we can produce. The link between the growth of productivity and the standard of living of the average person is somewhat looser in the short-to-medium run, because variation in factors such as the share of the population that is employed, the division of income between capital and labor, and the distribution of each type of income across households also matters. Nevertheless, the rate of productivity growth influences the economy in important ways even in the short run, affecting key variables such as the growth rate of output, employment gains, and the rate of inflation.\n\nToday I will discuss the acceleration of productivity that has occurred over the past decade and our current understanding of its causes. In the discussion I will comment on two puzzles raised by this improved performance and then conclude by briefly addressing the longer-term prospects for productivity growth in the United States.\n\nThe U.S. Productivity Resurgence and Its Causes\nWhat underlies the resurgence in U.S. productivity growth? Explanations of the rise in productivity growth since about 1995 have evolved somewhat over time. By 2000 or so, an emerging consensus held that the pickup in productivity growth was, for the most part, the product of both rapid technological progress and increased investment in new information and communication technologies (IT) during the 1990s (see Jorgenson and Stiroh, 2000, and Oliner and Sichel, 2000). According to this view, developments in IT promoted U.S. productivity growth in two ways. First, technological advances allowed the IT-producing sectors themselves to exhibit rapid productivity growth. For example, the development of more-reliable semiconductor manufacturing equipment and faster wafer-inspection technologies increased the rate at which companies such as Intel were able to produce microprocessors. Intel was also able to shorten its product cycle and increase the frequency of new chip releases, shifting its product mix toward more-powerful and, consequently, higher-value chips. Both the more-rapid pace of production and the higher average quality of output raised productivity at Intel as well as at competing firms that were forced to keep pace.\n\nSecond, advances in information technology also promoted productivity growth outside the IT-producing sector, as firms in a wide range of industries expanded their investments in high-tech equipment and software and used the new technologies to reduce costs and increase quality. Some large retailers, for example, developed IT-based tools to improve the management of their supply chains and to increase their responsiveness to changes in the level and mix of customer demands. Securities brokers and dealers achieved substantial productivity gains by automating their trading processes and their back-office operations. In the durable goods sector, automobile producers developed programmable tooling systems to increase the flexibility of their manufacturing processes--for example, to permit vehicles based on different platforms to be produced on the same assembly line. One study (Stiroh, 2002) found that a majority of U.S. industries experienced an acceleration of productivity in the latter part of the 1990s. Significantly, the study also found the gains to be the greatest in industries that use IT capital most intensively.\n\nUndoubtedly, the IT revolution and the resurgence of productivity in the United States after 1995 were closely connected. However, the technology-based explanation of increased productivity growth does raise a couple of puzzles (see, for example, McKinsey and Company, 2001 and 2005, and Basu and others, 2003). First, the United States was not the only country to have access to the new technologies or to have experienced a rapid expansion in IT investment; other industrial countries also invested heavily in these technologies in the 1980s and 1990s. Yet, with a few exceptions, the available data show that productivity growth in other advanced countries has not increased to the extent seen in the United States. Second, as I have noted, productivity growth increased very rapidly earlier this decade and has continued to rise at a solid pace, even though IT investment declined sharply after the stock prices of high-tech firms plummeted in 2000. More generally, as a historical matter, increases in IT investment have not always been followed in short order by increases in productivity growth. This observation raises the question of why, in some cases, the putative productivity benefits of investments in new technologies do not occur until years after those investments are made.\n\nIn regard to the first puzzle--the fact that the United States has enjoyed greater productivity growth in recent years than other advanced countries--the comparison with Europe is particularly interesting. Throughout most of the post-World War II period, productivity growth in Europe exceeded that in the United States, at first because of the rapid gains during the postwar reconstruction and then later because of a gradual convergence of European technology and business practices to American standards. By one estimate, European productivity increased from 44 percent of the U.S. level in 1950 to near-equality with the United States by 1995 (Gordon, 2004 and 2006). However, the available data suggest that, since about 1995, productivity growth in European nations has slowed, on average, in contrast to the pickup experienced in the United States. These trends have led to an increasing divergence in productivity levels in the United States and Europe (see van Ark and Inklaar, 2005).\n\nResearchers have made the important point that differences in productivity growth between the United States and Europe appear not to have been particularly large in the IT-producing sectors, where U.S. strengths in the development of computers and semiconductors have been partly offset by European leadership in communications. Rather, the U.S. advantage has been most evident in the IT-using sectors, which have performed better in the United States than elsewhere. What accounts for this apparent U.S. advantage?\n\nDifferences in economic policies and systems likely have accounted for some of the differences in the performance of productivity. One leading explanation for the strong U.S. productivity growth is that labor markets in the United States tend to be more flexible and competitive, market characteristics that have allowed the United States to realize greater economic benefits from new technologies. For example, taking full advantage of new information and communication technologies may require extensive reorganization of work practices, the reassignment and retraining of workers, and ultimately some reallocation of labor among firms and industries. Regulations that raise the costs of hiring and firing workers and that reduce employers’ ability to change work assignments--like those that exist in a number of European countries--may make such changes more difficult to achieve.\n\nLikewise, in product markets, a high degree of competition and low barriers to the entry of new firms in most industries in the United States provide strong incentives for firms to find ways to cut costs and to improve their products. In some other countries, in contrast, the prominence of government-owned firms with a degree of monopoly power, together with a regulatory environment that protects incumbent firms and makes the entry of new firms difficult, reduces the competitive pressure for innovation and the application of new ideas. For example, some economists have argued that restrictions on land use and on shopping hours in Europe have impeded the development of \"big box\" retail outlets, reducing competition and denying European firms the economies of scale that have been important for productivity growth in the retail sector in the United States (Gordon, 2004). More generally, recent empirical research has typically found that economies with highly regulated labor and product markets are indeed less able to make productive use of new technologies (Gust and Marquez, 2004). Also, although it is not a feature unique to the United States, the increasing degree of openness of our economy to trade and foreign investment and the consequent exposure of U.S. companies to the rigors of international as well as domestic competition, may have promoted productivity growth.2 As a leading example, productivity gains in U.S. manufacturing--which is particularly subject to international competition--have been especially impressive in recent years, averaging, by one measure, about 6 percent per year over the past decade.3\n\nA number of other explanations have been advanced for the relatively stronger performance of productivity in the United States in recent years, including international differences in management practices, the depth and sophistication of U.S. capital markets, more favorable attitudes toward competition and entrepreneurship in the United States, and the role of U.S. research universities in fostering innovation.4 Further study of national productivity differentials clearly is warranted.\n\nThe second productivity puzzle relates to the further acceleration in productivity that occurred earlier in this decade despite the decline in IT investment after 2000 and the rather modest recovery in recent years.5 Again, a number of explanations have been proposed, including business restructuring and an even more rapid pace of technical change and of the diffusion of technological advances. It is interesting, however, that the recent episode is not the first time that we have seen productivity improvements lagging well behind investments in new technology. Notably, computers were first commercialized in the 1950s, and personal computers began to come into widespread use in the early 1980s; but until the mid-1990s, these developments had little evident effect on measures of productivity. Indeed, an oft-quoted quip by economist Robert Solow held that, as of the late 1980s, \"computers are everywhere except in the productivity statistics.\"\n\nIn attempting to explain the tendency of productivity growth to lag behind investments in new technologies, economists have emphasized that much more than the purchase of new high-tech equipment is needed to achieve significant gains in productivity. In particular, to be successful, managers must have a carefully thought-out plan for using new technologies before they acquire them. Case studies of individual industries show that the planning for technological modernization has not always been adequate, with the result that some purchases of high-tech equipment and software have not added much to productivity or profits. The idea that managers can buy the hardware first and then decide what to do with it does not square with the evidence.\n\nSome observers have characterized the new information and communication technologies as general-purpose technologies, which means that--like earlier major innovations such as electrification and the internal combustion engine--they have the potential to revolutionize production and make many new goods and services available to consumers (see Bresnahan and Trajtenberg, 1995). To make effective use of such a technology within a specific firm or industry, however, managers must supplement their purchases of new equipment with investments in firm- or industry-specific research and development, worker training, and organizational redesign--all examples of what economists call intangible capital. Although investments in intangible capital are, for the most part, not counted as capital investment in the national income and product accounts, they appear to be quantitatively important.6 One recent study estimated that, by the late 1990s, investments in intangible capital by U.S. businesses were as large as investments in traditional tangible capital such as buildings and machines (Corrado, Hulten, and Sichel, 2006).\n\nRecognizing the importance of intangible capital has several interesting implications. First, because investment in intangible capital is typically treated as a current expense rather than as an investment, aggregate saving and investment may be significantly understated in the U.S. official statistics. Second, firms’ need to invest in intangible capital--and thus to divert resources from the production of market goods or services--helps to explain why measured output and productivity may decline or grow slowly during the period after firms adopt new technologies. Finally, the concept of intangible capital may shed light on the puzzle of why productivity growth has remained strong despite the deceleration in IT investment. Because investments in high-tech capital typically require complementary investments in intangible capital for productivity gains to be realized, the benefits of high-tech investment may become visible only after an extended period during which firms are making the necessary investments in intangibles.\n\nLonger-Term Prospects for Productivity Growth\nHistorical analyses of the sources of fluctuations in productivity growth are challenging, but not nearly so challenging as trying to predict how productivity will evolve in the future. However, because the rate of productivity growth is a primary determinant of economic performance, policymakers have few options other than to try to forecast future gains in productivity. For example, estimates of long-term productivity growth are needed to determine the rate of output growth that the economy can sustain in the long run without generating inflationary pressures.\n\nThe task of trying to predict the behavior of productivity in the medium-to-long run is complicated by the fact that productivity growth generally varies with the business cycle, tending to be below its longer-term trend when the economy is contracting and above that trend when the economy is in the early stages of an expansion (see Basu and Fernald, 2001, for a discussion). (This well-documented pattern makes the strong growth of productivity during the early part of this decade, a period that featured a recession and generally slow growth, all the more remarkable.) Economists use statistical methods to try to abstract from cyclical influences to determine the longer-term trend in productivity. What do they find?\n\nAs of a couple of years ago, the consensus among leading researchers was that productivity in the nonfarm business sector was likely to grow at about 2-1/2 percent per year in the longer term, close to the rate of productivity growth achieved during the 1995 to 2000 period (see, for example, Baily, 2003; Gordon, 2003; and Jorgenson, Ho, and Stiroh, 2004). On the one hand, recent data revisions to the national income and product accounts have shown that productivity growth over the past few years was slightly weaker than we thought, leading some analysts to revise down their estimates of trend productivity growth about 1/4 percentage point or so per year. On the other hand, the fact that productivity growth has remained solid in recent years increases confidence that a larger fraction of those productivity gains reflects longer-term developments and a smaller fraction reflects cyclical factors. On net, the recent experience does not appear to require a significant rethinking of long-term productivity trends. Indeed, recent estimates by leading economists continue to peg the expected longer-term rate of productivity growth at roughly 2-1/2 percent per year.7\n\nOf course, as the saying goes, past returns do not guarantee future results, and not all the evidence supports this optimistic view of productivity trends. For example, although spending on high-tech equipment and software has recovered noticeably from its recent lows, growth in IT spending remains well below the rates observed before the 2001 recession. Some industry participants have suggested that less-rapid growth in IT spending may reflect the absence of major new business applications for IT--\"killer apps,\" as they are called. Moreover, until we have a more complete understanding of the factors behind productivity growth in the past five years, we should be cautious in drawing any strong conclusions about the future.\n\nThese caveats notwithstanding, a case can be made that the strong productivity growth of the post-1995 era is likely to continue for some time. Notably, the price of computing power continues to fall sharply, having declined by nearly half in the five years between 2000 and 2005. Increased computing power has in turn contributed to advances in other fields, such as biotechnology, and has helped to increase the range of goods and services available to businesses and consumers. Moreover, whatever the pace of future technological progress, further diffusion of already-existing technologies and applications to more firms and industries should continue to increase aggregate productivity for a time.\n\nI have focused today on how technological change and investment, both tangible and intangible, promote productivity growth. I will close by noting that, from the perspective of society as a whole, a particularly important form of intangible investment in future years will be investment in the skills of the U.S. labor force. As we know from everyday experience, few jobs or occupations have not been affected in some way by the technological changes of recent years, a trend that will certainly continue. Not only scientists and engineers but also nurses, auto mechanics, and factory workers now use advanced technologies every day. But new technologies will translate into higher productivity only to the extent that workers have the skills needed to apply them effectively. Moreover, because technology is always changing, the acquisition of those skills has become a lifelong challenge, one that continues well after formal education is completed. If the recent gains in productivity growth are to be sustained, ensuring that we have a workforce that is comfortable with and adaptable to new technologies will be essential.\n\nReferences\n\nBaily, Martin (2003). \"The U.S. Economic Outlook: Investment, Productivity, Deflation (396 KB PDF).\" Washington: Institute for International Economics (April)\n\nBasu, Susanto, and John Fernald (2001). \"Why is Productivity Procyclical? Why Do We Care?\" in C. Hulten, E. Dean, and M. Harper, eds., New Developments in Productivity Analysis, National Bureau of Economic Research, Studies in Business Cycles. Chicago: University of Chicago Press, pp. 225-296.\n\nBasu, Susanto, John Fernald, Nicholas Oulton, and Sylaja Srinivasan (2003). \"The Case of the Missing Productivity Growth, or Does Information Technology Explain Why Productivity Accelerated in the United States but not the United Kingdom?\" NBER Macroeconomics Annual, vol. 18, pp. 9-71.\n\nBloom, Nick, Raffaella Sadun, and John Van Reenen (2006). \"It Ain’t What You Do, It’s the Way You Do I.T.: Investigating the Productivity Miracle Using the Overseas Activities of U.S. Multinationals,\" working paper. London: Centre for Economic Performance, London School of Economics (May).\n\nBosworth, Barry, and Jack Triplett (2006). \"Is the 21st Century Productivity Expansion Still in Services? And What Should Be Done About It? (126 KB PDF)\" paper presented at the 2006 Summer Institute sponsored by the National Bureau of Economic Research and the Conference on Research in Income and Wealth, held in Cambridge, Mass., July 17.\n\nBresnahan, Timothy, and Manuel Trajtenberg (1995). \"General Purpose Technologies: ‘Engines of Growth’?\" Journal of Econometrics, vol. 65 (January), pp. 83-108.\n\nCorrado, Carol, Paul Lengermann, Eric Bartelsman, and J. Joseph Beaulieu (2006). \"Modeling Aggregate Productivity at a Disaggregate Level: New Results for U.S. Sectors and Industries (502 KB PDF),\" paper presented at the 2006 Summer Institute sponsored by the National Bureau of Economic Research and the Conference on Research in Income and Wealth, held in Cambridge, Mass., July 17.\n\nCorrado, Carol, Charles Hulten, and Daniel Sichel (2006). \"Intangible Capital and Economic Growth,\" Finance and Economics Discussion Series 2006-24. Washington: Board of Governors of the Federal Reserve System, April.\n\nGordon, Robert J. (2003). \"Exploding Productivity Growth: Context, Causes, and Implications (349 KB PDF),\" Brookings Papers on Economic Activity, 2:2003, pp. 207-98.\n\nGordon, Robert J. (2004). \"Why Was Europe Left at the Station When America’s Productivity Locomotive Departed?\" Working Paper Series 10661. Cambridge, Mass.: National Bureau of Economic Research, August.\n\nGordon, Robert J. (2006). \"Issues in the Composition of Welfare Between Europe and the United States,\" unpublished paper, Northwestern University (July).\n\nGust, Christopher, and Jaime Marquez (2004). \"International Comparisons of Productivity Growth: The Role of Information Technology and Regulatory Practices,\" Labour Economics, vol. 11 (February), pp. 33-58.\n\nJorgenson, Dale, and Kevin Stiroh (2000). \"Raising the Speed Limit: U.S. Economic Growth in the Information Age (458 KB PDF),\" Brookings Papers on Economic Activity, 1:2000, pp. 125-235.\n\nJorgenson, Dale, Mun Ho, and Kevin Stiroh (2004). \"Will the U.S. Productivity Resurgence Continue?\" Current Issues in Economics and Finance, vol. 10 (December), pp. 1-7.\n\nJorgenson, Dale, Mun Ho, and Kevin Stiroh (2006). \"The Sources of the Second Surge of U.S. Productivity and Implications for the Future,\" unpublished paper, Federal Reserve Bank of New York, March.\n\nLewis, William W. (2004). The Power of Productivity: Wealth, Poverty, and the Threat to Global Stability. Chicago: The University of Chicago Press.\n\nMcKinsey and Company (2001). U.S. Productivity Growth 1995-2000: Understanding the Contribution of Information Technology Relative to Other Factors. Washington: McKinsey Global Institute.\n\nMcKinsey and Company (2005). \"U.S. Productivity after the Dot-Com Bust,\" perspective paper. Washington: McKinsey Global Institute.\n\nOliner, Stephen, and Daniel Sichel (2000). \"The Resurgence of Growth in the Late 1990s: Is Information Technology the Story?\" Journal of Economic Perspectives, vol. 14 (Autumn), pp. 3-22.\n\nStiroh, Kevin (2002). \"Information Technology and the U.S. Productivity Revival: What Do the Industry Data Say?\" American Economic Review, vol. 92 (December), pp. 1559-76.\n\nStiroh, Kevin (2006). \"The Industry Origins of the Second Surge of U.S. Productivity Growth,\" unpublished paper, Federal Reserve Bank of New York, July.\n\nvan Ark, Bart, and Robert Inklaar (2005). \"Catching up or Getting Stuck? Europe’s Trouble to Exploit ICT’s Productivity Potential,\" Research Memorandum GD-79. Groningen, The Netherlands: Groningen Growth and Development Centre, September.\n\nFootnotes\n\n1.  I will use \"labor productivity\" and \"productivity\" interchangeably in my remarks today. An alternative productivity concept, multifactor productivity, measures the quantity of output that can be produced by a given combination of capital and labor. Changes in labor productivity reflect changes in both multifactor productivity and the amount of capital per worker. Return to text\n\n2.  Lewis (2004) discusses the link between competition and productivity. Return to text\n\n3.  The figure for growth in output per hour in the text uses the Federal Reserve’s industrial production index for the manufacturing sector as the measure of output. Return to text\n\n4.  Regarding differences in management practices, Bloom, Sadun, and Van Reenen (2006) found that business establishments in the United Kingdom that are owned by U.S. multinationals get higher productivity from information technology than do other establishments in that country. Their study tied the differential to the management and organizational practices employed by U.S. firms. Return to text\n\n5.  Indeed, productivity accelerated in a wide range of industries that had not experienced much improvement in productivity growth in the 1990s. For discussions of more recent developments at the industry level, see Corrado, Lengermann, Bartelsman, and Beaulieu (2006), Bosworth and Triplett (2006), and Stiroh (2006). Return to text\n\n6.  Software is one intangible investment that is treated as part of business fixed investment in the U.S. national accounts. Return to text\n\n7.  Martin Baily puts the trend for the nonfarm business sector a little above 2-1/2 percent (conversation with Board staff in August 2006). Robert Gordon reports a current trend of 2.6 percent but predicts that it will move lower in the next couple of years (conversation with Board staff in August 2006). Jorgenson, Ho, and Stiroh (2006) put the trend at 2.6 percent for the private economy (a sector quite close to nonfarm business), but that figure was generated before the recent NIPA revisions. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060831a.htm",
        "title": "Productivity",
        "date": "8/31/2006"
    },
    {
        "content": "August 25, 2006\n\nChairman Ben S. Bernanke\n\nAt the Federal Reserve Bank of Kansas City's Thirtieth Annual Economic Symposium, Jackson Hole, Wyoming\n\nWhen geographers study the earth and its features, distance is one of the basic measures they use to describe the patterns they observe. Distance is an elastic concept, however. The physical distance along a great circle from Wausau, Wisconsin to Wuhan, China is fixed at 7,020 miles. But to an economist, the distance from Wausau to Wuhan can also be expressed in other metrics, such as the cost of shipping goods between the two cities, the time it takes for a message to travel those 7,020 miles, and the cost of sending and receiving the message. Economically relevant distances between Wausau and Wuhan may also depend on what trade economists refer to as the \"width of the border,\" which reflects the extra costs of economic exchange imposed by factors such as tariff and nontariff barriers, as well as costs arising from differences in language, culture, legal traditions, and political systems.\n\nOne of the defining characteristics of the world in which we now live is that, by most economically relevant measures, distances are shrinking rapidly. The shrinking globe has been a major source of the powerful wave of worldwide economic integration and increased economic interdependence that we are currently experiencing. The causes and implications of declining economic distances and increased economic integration are, of course, the subject of this conference.\n\nThe pace of global economic change in recent decades has been breathtaking indeed, and the full implications of these developments for all aspects of our lives will not be known for many years. History may provide some guidance, however. The process of global economic integration has been going on for thousands of years, and the sources and consequences of this integration have often borne at least a qualitative resemblance to those associated with the current episode. In my remarks today I will briefly review some past episodes of global economic integration, identify some common themes, and then put forward some ways in which I see the current episode as similar to and different from the past. In doing so, I hope to provide some background and context for the important discussions that we will be having over the next few days.\n\nA Short History of Global Economic Integration\nAs I just noted, the economic integration of widely separated regions is hardly a new phenomenon. Two thousand years ago, the Romans unified their far-flung empire through an extensive transportation network and a common language, legal system, and currency. One historian recently observed that \"a citizen of the empire traveling from Britain to the Euphrates in the mid-second century CE would have found in virtually every town along the journey foods, goods, landscapes, buildings, institutions, laws, entertainment, and sacred elements not dissimilar to those in his own community.\" (Hitchner, 2003, p. 398). This unification promoted trade and economic development.\n\nA millennium and a half later, at the end of the fifteenth century, the voyages of Columbus, Vasco da Gama, and other explorers initiated a period of trade over even vaster distances. These voyages of discovery were made possible by advances in European ship technology and navigation, including improvements in the compass, in the rudder, and in sail design. The sea lanes opened by these voyages facilitated a thriving intercontinental trade--although the high costs of and the risks associated with long voyages tended to limit trade to a relatively small set of commodities of high value relative to their weight and bulk, such as sugar, tobacco, spices, tea, silk, and precious metals. Much of this trade ultimately came under the control of the trading companies created by the English and the Dutch. These state-sanctioned monopolies enjoyed--and aggressively protected--high markups and profits. Influenced by the prevailing mercantilist view of trade as a zero-sum game, European nation-states competed to dominate lucrative markets, a competition that sometimes spilled over into military conflict.\n\nThe expansion of international trade in the sixteenth century faced some domestic opposition. For example, in an interesting combination of mercantilist thought and social commentary, the reformer Martin Luther wrote in 1524:\n\n\n\nThe structure of trade during the post-Napoleonic period followed a \"core-periphery\" pattern. Capital-rich Western European countries, particularly Britain, were the center, or core, of the trading system and the international monetary system. Countries in which natural resources and land were relatively abundant formed the periphery. Manufactured goods, financial capital, and labor tended to flow from the core to the periphery, with natural resources and agricultural products flowing from the periphery to the core. The composition of the core and the periphery remained fairly stable, with one important exception being the United States, which, over the course of the nineteenth century, made the transition from the periphery to the core. The share of manufactured goods in U.S. exports rose from less than 30 percent in 1840 to 60 percent in 1913, and the United States became a net exporter of financial capital beginning in the late 1890s.1\n\nFor the most part, government policies during this era fostered openness to trade, capital mobility, and migration. Britain unilaterally repealed its tariffs on grains (the so-called corn laws) in 1846, and a series of bilateral treaties subsequently dismantled many barriers to trade in Europe. A growing appreciation for the principle of comparative advantage, as forcefully articulated by Adam Smith and David Ricardo, may have made governments more receptive to the view that international trade is not a zero-sum game but can be beneficial to all participants.\n\nThat said, domestic opposition to free trade eventually intensified, as cheap grain from the periphery put downward pressure on the incomes of landowners in the core. Beginning in the late 1870s, many European countries raised tariffs, with Britain being a prominent exception. Britain did respond to protectionist pressures by passing legislation that required that goods be stamped with their country of origin. This step provided additional grist for trade protesters, however, as the author of one British anti-free-trade pamphlet in the 1890s lamented that even the pencil he used to write his protest was marked \"made in Germany\" (James, 2001, p. 15). In the United States, tariffs on manufactures were raised in the 1860s to relatively high levels, where they remained until well into the twentieth century. Despite these increased barriers to the importation of goods, the United States was remarkably open to immigration throughout this period.\n\nUnfortunately, the international economic integration achieved during the nineteenth century was largely unraveled in the twentieth by two world wars and the Great Depression. After World War II, the major powers undertook the difficult tasks of rebuilding both the physical infrastructure and the international trade and monetary systems. The industrial core--now including an emergent Japan as well as the United States and Western Europe--ultimately succeeded in restoring a substantial degree of economic integration, though decades passed before trade as a share of global output reached pre-World War I levels.\n\nOne manifestation of this re-integration was the rise of so-called intra-industry trade. Researchers in the late-1960s and the 1970s noted that an increasing share of global trade was taking place between countries with similar resource endowments, trading similar types of goods--mainly manufactured products traded among industrial countries.2 Unlike international trade in the nineteenth century, these flows could not be readily explained by the perspectives of Ricardo or of the Swedish economists Eli Heckscher and Bertil Ohlin that emphasized national differences in endowments of natural resources or factors of production. In influential work, Paul Krugman and others have since argued that intra-industry trade can be attributed to firms' efforts to exploit economies of scale, coupled with a taste for variety by purchasers.\n\nPostwar economic re-integration was supported by several factors, both technological and political. Technological advances further reduced the costs of transportation and communication, as the air freight fleet was converted from propeller to jet and intermodal shipping techniques (including containerization) became common. Telephone communication expanded, and digital electronic computing came into use. Taken together, these advances allowed an ever-broadening set of products to be traded internationally. In the policy sphere, tariff barriers--which had been dramatically increased during the Great Depression--were lowered, with many of these reductions negotiated within the multilateral framework provided by the General Agreement on Tariffs and Trade. Globalization was, to some extent, also supported by geopolitical considerations, as economic integration among the Western market economies became viewed as part of the strategy for waging the Cold War. However, although trade expanded significantly in the early post-World War II period, many countries--recalling the exchange-rate and financial crises of the 1930s--adopted regulations aimed at limiting the mobility of financial capital across national borders.\n\nSeveral conclusions emerge from this brief historical review. Perhaps the clearest conclusion is that new technologies that reduce the costs of transportation and communication have been a major factor supporting global economic integration. Of course, technological advance is itself affected by the economic incentives for inventive activity; these incentives increase with the size of the market, creating something of a virtuous circle. For example, in the nineteenth century, the high potential return to improving communications between Europe and the United States prompted intensive work to better understand electricity and to improve telegraph technology--efforts that together helped make the trans-Atlantic cable possible.\n\nA second conclusion from history is that national policy choices may be critical determinants of the extent of international economic integration. Britain's embrace of free trade and free capital flows helped to catalyze international integration in the nineteenth century. Fifteenth-century China provides an opposing example. In the early decades of that century, the Chinese sailed great fleets to the ports of Asia and East Africa, including ships much larger than those that the Europeans were to use later in the voyages of discovery. These expeditions apparently had only limited economic impact, however. Ultimately, internal political struggles led to a curtailment of further Chinese exploration (Findlay, 1992). Evidently, in this case, different choices by political leaders might have led to very different historical outcomes.\n\nA third observation is that social dislocation, and consequently often social resistance, may result when economies become more open. An important source of dislocation is that--as the principle of comparative advantage suggests--the expansion of trade opportunities tends to change the mix of goods that each country produces and the relative returns to capital and labor. The resulting shifts in the structure of production impose costs on workers and business owners in some industries and thus create a constituency that opposes the process of economic integration. More broadly, increased economic interdependence may also engender opposition by stimulating social or cultural change, or by being perceived as benefiting some groups much more than others.\n\nThe Current Episode of Global Economic Integration\nHow does the current wave of global economic integration compare with previous episodes? In a number of ways, the remarkable economic changes that we observe today are being driven by the same basic forces and are having similar effects as in the past. Perhaps most important, technological advances continue to play an important role in facilitating global integration. For example, dramatic improvements in supply-chain management, made possible by advances in communication and computer technologies, have significantly reduced the costs of coordinating production among globally distributed suppliers.\n\nAnother common feature of the contemporary economic landscape and the experience of the past is the continued broadening of the range of products that are viewed as tradable. In part, this broadening simply reflects the wider range of goods available today--high-tech consumer goods, for example--as well as ongoing declines in transportation costs. Particularly striking, however, is the extent to which information and communication technologies now facilitate active international trade in a wide range of services, from call center operations to sophisticated financial, legal, medical, and engineering services.\n\nThe critical role of government policy in supporting, or at least permitting, global economic integration, is a third similarity between the past and the present. Progress in trade liberalization has continued in recent decades--though not always at a steady pace, as the recent Doha Round negotiations demonstrate. Moreover, the institutional framework supporting global trade, most importantly the World Trade Organization, has expanded and strengthened over time. Regional frameworks and agreements, such as the North American Free Trade Agreement and the European Union's \"single market,\" have also promoted trade. Government restrictions on international capital flows have generally declined, and the \"soft infrastructure\" supporting those flows--for example, legal frameworks and accounting rules--have improved, in part through international cooperation.\n\nIn yet another parallel with the past, however, social and political opposition to rapid economic integration has also emerged. As in the past, much of this opposition is driven by the distributional impact of changes in the pattern of production, but other concerns have been expressed as well--for example, about the effects of global economic integration on the environment or on the poorest countries.\n\nWhat, then, is new about the current episode? Each observer will have his or her own perspective, but, to me, four differences between the current wave of global economic integration and past episodes seem most important. First, the scale and pace of the current episode is unprecedented. For example, in recent years, global merchandise exports have been above 20 percent of world gross domestic product, compared with about 8 percent in 1913 and less than 15 percent as recently as 1990; and international financial flows have expanded even more quickly.3 But these data understate the magnitude of the change that we are now experiencing. The emergence of China, India, and the former communist-bloc countries implies that the greater part of the earth's population is now engaged, at least potentially, in the global economy. There are no historical antecedents for this development. Columbus's voyage to the New World ultimately led to enormous economic change, of course, but the full integration of the New and the Old Worlds took centuries. In contrast, the economic opening of China, which began in earnest less than three decades ago, is proceeding rapidly and, if anything, seems to be accelerating.\n\nSecond, the traditional distinction between the core and the periphery is becoming increasingly less relevant, as the mature industrial economies and the emerging-market economies become more integrated and interdependent. Notably, the nineteenth-century pattern, in which the core exported manufactures to the periphery in exchange for commodities, no longer holds, as an increasing share of world manufacturing capacity is now found in emerging markets. An even more striking aspect of the breakdown of the core-periphery paradigm is the direction of capital flows: In the nineteenth century, the country at the center of the world's economy, Great Britain, ran current account surpluses and exported financial capital to the periphery. Today, the world's largest economy, that of the United States, runs a current-account deficit, financed to a substantial extent by capital exports from emerging-market nations.\n\nThird, production processes are becoming geographically fragmented to an unprecedented degree.4 Rather than producing goods in a single process in a single location, firms are increasingly breaking the production process into discrete steps and performing each step in whatever location allows them to minimize costs. For example, the U.S. chip producer AMD locates most of its research and development in California; produces in Texas, Germany, and Japan; does final processing and testing in Thailand, Singapore, Malaysia, and China; and then sells to markets around the globe. To be sure, international production chains are not entirely new: In 1911, Henry Ford opened his company's first overseas factory in Manchester, England, to be closer to a growing source of demand. The factory produced bodies for the Model A automobile, but imported the chassis and mechanical parts from the United States for assembly in Manchester. Although examples like this one illustrate the historical continuity of the process of economic integration, today the geographical extension of production processes is far more advanced and pervasive than ever before. As an aside, some interesting economic questions are raised by the fact that in some cases international production chains are managed almost entirely within a single multinational corporation (roughly 40 percent of U.S. merchandise trade is classified as intra-firm) and in others they are built through arm's-length transactions among unrelated firms. But the empirical evidence in both cases suggests that substantial productivity gains can often be achieved through the development of global supply chains.5\n\nThe final item on my list of what is new about the current episode is that international capital markets have become substantially more mature. Although the net capital flows of a century ago, measured relative to global output, are comparable to those of the present, gross flows today are much larger. Moreover, capital flows now take many more forms than in the past: In the nineteenth century, international portfolio investments were concentrated in the finance of infrastructure projects (such as the American railroads) and in the purchase of government debt. Today, international investors hold an array of debt instruments, equities, and derivatives, including claims on a broad range of sectors. Flows of foreign direct investment are also much larger relative to output than they were fifty or a hundred years ago.6 As I noted earlier, the increase in capital flows owes much to capital-market liberalization and factors such as the greater standardization of accounting practices as well as to technological advances.\n\nConclusion\nBy almost any economically relevant metric, distances have shrunk considerably in recent decades. As a consequence, economically speaking, Wausau and Wuhan are today closer and more interdependent than ever before. Economic and technological changes are likely to shrink effective distances still further in coming years, creating the potential for continued improvements in productivity and living standards and for a reduction in global poverty.\n\nFurther progress in global economic integration should not be taken for granted, however. Geopolitical concerns, including international tensions and the risks of terrorism, already constrain the pace of worldwide economic integration and may do so even more in the future. And, as in the past, the social and political opposition to openness can be strong. Although this opposition has many sources, I have suggested that much of it arises because changes in the patterns of production are likely to threaten the livelihoods of some workers and the profits of some firms, even when these changes lead to greater productivity and output overall. The natural reaction of those so affected is to resist change, for example, by seeking the passage of protectionist measures. The challenge for policymakers is to ensure that the benefits of global economic integration are sufficiently widely shared--for example, by helping displaced workers get the necessary training to take advantage of new opportunities--that a consensus for welfare-enhancing change can be obtained. Building such a consensus may be far from easy, at both the national and the global levels. However, the effort is well worth making, as the potential benefits of increased global economic integration are large indeed.\n\nReferences\n\nBloom, Nick, Raffaella Sadun, and John Van Reenen (2006). \"It Ain't What You Do It's the Way That You Do I.T.--Investigating the Productivity Miracle Using the Overseas Activities of U.S. Multinationals,\" unpublished paper, Centre for Economic Performance, March.\n\nBordo, Michael, Barry Eichengreen, and Douglas Irwin (1999). \"Is Globalization Today Really Different than Globalization a Hundred Years Ago?\" NBER Working Paper No. 7195, June.\n\nCorrado, Carol, Paul Lengermann, and Larry Slifman (2005). \"The Contribution of MNCs to U.S. Productivity Growth, 1977-2000,\" unpublished paper, Board of Governors of the Federal Reserve System, July.\n\nCriscuolo, Chiara, and Ralf Martin (2005). \"Multinationals and U.S. Productivity Leadership: Evidence from Great Britain,\" Centre for Economic Performance, Discussion Paper No. 672, January.\n\nDoms, Mark E. and J. Bradford Jensen (1998). \"Comparing Wages, Skills, and Productivity between Domestically and Foreign-Owned Manufacturing Establishments in the United States,\" in R.E. Baldwin, R.E. Lipsey, and J. David Richardson, eds., Geography and Ownership as Bases for Economic Accounting, NBER Studies in Income and Wealth, vol. 59, Chicago, Ill.: University of Chicago Press, pp. 235-58.\n\nFindlay, Ronald (1992). \"The Roots of Divergence: Western Economic History in Comparative Perspective,\" AEA Papers and Proceedings, vol. 82:2, May, pp. 158-61.\n\nFindlay, Ronald, and Kevin O'Rourke (2002). \"Commodity Market Integration 1500-2000,\" Centre for Economic Policy Research, Discussion Paper No. 3125, January.\n\nGrubel, Herbert, and P.J. Lloyd (1975). Intra-Industry Trade, New York, New York: John Wiley & Sons.\n\nHanson, Gordon, Raymond Mataloni, and Matthew Slaughter (2005). \"Vertical Production Networks in Multinational Firms,\" Review of Economics and Statistics, vol. 87:4, November.\n\nHistorical Statistics of the United States: Earliest Times to Present (Millennial Edition) (2006). New York, New York: Cambridge University Press.\n\nHitchner, Bruce (2003). \"Roman Empire,\" in Joel Mokyr ed., The Oxford Encyclopedia of Economic History, Oxford, England: Oxford University Press, vol. 4, pp. 397-400.\n\nJames, Harold (2001) The End of Globalization: Lessons from the Great Depression, Cambridge, Massachusetts: Harvard University Press.\n\nKurz, Christopher (2006). \"Outstanding Outsourcers: A Firm- and Plant-Level Analysis of Production Sharing,\" Finance and Economics Discussion Series 2006-04, Federal Reserve Board, March.\n\nMaddison, Angus (2001). The World Economy: A Millenial Perspective, Paris, France: OECD Development Centre.\n\nStandage, Tom (1998). The Victorian Internet, New York, New York: Walker Publishing Company.\n\nFootnotes\n\n1.  Data are from Historical Statistics of the United States (2006). Return to text\n\n2.  See, for example, Grubel and Lloyd (1975). Return to text\n\n3. Maddison (2001) and International Monetary Fund data. Return to text\n\n4. See, for example, Hanson, Mataloni, and Slaughter (2005). Return to text\n\n5.  Some of the key empirical papers in this literature are Doms and Jensen (1998); Criscuolo and Martin (2005); Corrado, Lengermann, and Slifman (2005); Bloom, Sadun, and Van Reenen (2006), and Kurz (2006). Return to text\n\n6.  See, for example, Bordo, Eichengreen, and Irwin (1999). Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060825a.htm",
        "title": "Global Economic Integration: What's New and What's Not?",
        "date": "8/25/2006"
    },
    {
        "content": "July 18, 2006\n\nGovernor Kevin Warsh\n\nAt the American Enterprise Institute, Washington, D.C.\n\nThank you for the opportunity to speak today on financial conditions and economic activity in the corporate sector. A striking feature of this economic expansion has been the historically high holdings of cash and short-term securities that the corporate sector has accumulated since 2001. But business attitudes seem to be in the process of change--cash has started to decline and borrowing has picked up--so this topic is especially timely.\n\nToday, I will first provide a historical perspective on cash balances at corporations, and discuss some factors that may have encouraged firms to hold such large cash balances during this recovery. I will also evaluate whether these factors are likely to persist. Then, I will discuss the more recent indicators that suggest that the buildup in corporate cash balances may be abating, and offer some reasons for that change. In particular, some of the unwinding of cash balances appear, in part, to be due to a renewed focus on capital spending and other business expansion efforts by executives. This renewal has been spurred, notably, by growing external pressures to boost shareholder value. To be sure, such developments may cause the liquidity and credit quality of some corporations to recede a bit from their very high levels but will not, in my view, put at risk their strong balance sheets or impede the solid expansion of business spending.\n\nI would like to stress that I will not be addressing the immediate economic outlook or the recent conduct of monetary policy. Rather, my comments reflect observations about longer-term trends for business sector activities.1\n\nCorporate Financial Conditions and Cash Balances\nIn the past several years, we have witnessed a dramatic improvement in the financial health of the nonfinancial corporate sector. This improvement is clearly evident in the low default rate on corporate bonds and the relatively few delinquencies that have occurred on business loans at commercial banks: Both have dropped markedly since 2002 and have remained close to their historic lows during the past couple of years. Likewise, risk spreads on corporate bonds--the gap between yields on corporate bonds and comparable-maturity Treasuries--narrowed from their peaks in late 2002 to low levels in early 2004. Since then, they have hovered around fairly low rates, suggesting that investors continue to have a favorable outlook for corporate credit quality. Despite some recent turbulence, owing in part to geopolitical events, stock prices have logged robust gains over the past 3-1/2 years, and broad equity indices have now retraced most of the ground lost between 2000 and 2002.\n\nSome of the improvement in the financial condition of businesses is due to substantial efficiency gains by firms. Moreover, the resulting robust gains in labor productivity have been well ahead of compensation growth and have dramatically boosted corporate profits. Indeed, profits as a share of sector product--in essence profit margins--jumped to more than 14 percent in the first quarter, the highest level in decades. The improvement in the financial conditions also reflects fundamental changes in firms’ balance sheets. In particular, firms have boosted their liquidity by extending debt maturities, replacing shorter-term debt with longer-term debt; at the same time, they have reduced total leverage. Many firms retired debt using proceeds from equity offerings, asset sales, or mounting profits. As a result, from 2002 to 2004, nonfinancial corporate debt grew at an annual rate of about 2 percent, its slowest pace since the early 1990s.\n\nOne of the most striking financial developments since 2001 has been the rapid increase in corporate holdings of cash and short-term securities. Since the end of 2001, the ratio of cash holdings to assets has risen sharply (see exhibit).2 This increase is even more pronounced when (on a consolidated basis) cash is measured relative to investment, defined as the sum of capital spending and research and development (R&D) spending during the preceding twelve months. The ratio of cash to investment has averaged about 60 percent during the past few decades. Generally, it is higher in recession periods, consistent with a strong precautionary savings motive by firms that face costly external financing (Almeida, Campello, and Weisbach, 2004). And, in 2001, the ratio of cash to investment was already somewhat elevated. But the ratio then soared to more than 150 percent by year-end 2004, as investment fell far short of cash flow from operations and net financing. This juxtaposition is unusual when the economy is expanding. Indeed, former Federal Reserve Board Chairman Alan Greenspan pointed to this unusual configuration a year and a half ago. \"Although capital investment has been advancing at a reasonably good pace,\" he said, \"it has nonetheless lagged the exceptional rise in profits and internal cash flow.\"3\n\nSubsequently, aggregate cash holdings remained elevated through much of last year, even as the economy continued to advance smartly. Only in the very recent quarters has this trend seemed to abate somewhat. We are left with an overall picture of a highly liquid corporate sector--perhaps more like that which we would expect of corporate balance sheets in the early stages of an expansion. As policymakers and forecasters, we might ask why corporations have built up such a large stock of liquid assets. Is the buildup of cash indicative of a \"new equilibrium\"? Is there something about the current economic or regulatory environment that requires the maintenance of high liquidity levels? And, does the current level of liquidity portend a more robust surge in corporate spending?\n\nPartial Explanations for Large Holdings of Cash\nAlmost by definition, the rise and fall in cash holdings can be linked to fluctuations of operating cash flows and capital expenditures. A simple regression of changes in cash, on changes in operating cash flows, and on changes in capital expenditures--using annual data from 1957 to 2001--accounts reasonably well for historical fluctuations in cash at U.S. nonfinancial corporations, with a regression R-squared of over 50 percent. However, in this framework, the current period appears to be an outlier.4 That is, even after factoring in the current-cycle dynamics of strong profit growth and relatively modest investment, a good part of the rise in liquid assets remains unaccounted for--as much as one half of the total rise in cash-to-assets--according to Federal Reserve staff estimates. A number of explanations have been advanced for this unusual rise, and they have a bearing on judging whether this is a temporary shift born of the current environment or a result of a persistent change in behavior.\n\nForeign Operations\nThe first explanation relates to the growing significance of foreign operations of U.S. multinationals in countries with lower corporate tax rates and the recent accumulation of earnings retained by the foreign subsidiaries. When U.S. multinationals receive dividends from their foreign subsidiaries and the host country levies a lower tax rate than the U.S. corporate tax rate, any repatriated dividends are subject to U.S. corporate taxes. Thus, many companies have an incentive to leave those funds with their foreign subsidiaries, even if the funds far exceed plans for foreign capital expenditures. In such circumstances, those funds are often parked in cash or short-term investments overseas and are, thus, less accessible than one might infer from a consolidated balance sheet.\n\nIt is difficult to determine whether a corporation’s cash is held at home or abroad, but we know that significant holdings of cash are concentrated at large multinational firms. In particular, the Board staff’s analysis (of Standard & Poor’s Compustat data) indicates that the ratio of cash to total assets at domestic-only companies rose slightly less than 20 percent between year-end 2001 and 2004. At the same time, the cash intensity of balance sheets at multinational companies increased more than 50 percent. Moreover, recent research has demonstrated a strong statistical link between the accumulation of cash and the estimated tax burden from repatriating foreign earnings (Hartzell, Titman, and Twite, 2006).\n\nAs a means of unlocking those offshore cash holdings, the Congress and the President provided U.S. companies a one-time opportunity--through the American Jobs Creation Act of 2004--to repatriate foreign profits at a much reduced statutory tax rate. Indeed, Wall Street estimates indicate that many companies have capitalized on this opportunity: An extra $250 billion may have been repatriated during the past four quarters. That estimate appears consistent with the recent pattern of distributions from foreign income reported in the Commerce Department’s international transactions data.\n\nWith access to such large holdings of cash, we would expect corporations to disburse an unusually large portion of current cash earnings either through de-leveraging, accelerated dividends, or increased share repurchases. To the extent that any of these firms face financing constraints, the repatriated cash could provide a boost to new fixed investment, R&D projects, or acquisitions. Indeed, during the past few quarters, companies have been raising shareholder payouts and making new investments.\n\nThe buildup of cash at foreign subsidiaries could account for a good part of the unexplained buildup in corporate cash--that is, the rise in cash that is not explained by the dynamics of profits and investment. Because we expect that foreign subsidiaries will continue to grow, and because of the ongoing tax liability associated with repatriation, we should not be surprised if overall corporate cash balances remained somewhat higher compared to previous decades.\n\nFocus on Liquidity\nA second factor that may contribute to an elevated new equilibrium of cash balances is the renewed emphasis that investors may be placing on balance sheet liquidity, particularly in the aftermath of the commercial paper (CP) defaults that occurred at the end of the previous expansion. At that time, even financially solvent firms faced the prospect that they might have difficulty rolling over their maturing CP. This led investors and rating agencies to scrutinize the ability of firms to manage short-term liquidity events. Consequently, many firms raised cash holdings to mitigate these concerns.\n\nBusiness Caution\nAs I discussed earlier, regression analysis suggests that the robust growth in profits and the comparatively modest recovery in capital expenditures leaves a good part of the extraordinary cash buildup unexplained. But that analysis takes capital expenditures as given. In doing so, it sidesteps some of the important questions with which economists and policymakers have wrestled: Why did business fixed investment contract so severely in the most recent recession? And, why wasn’t the rebound from the recession more vigorous, particularly in light of such high profitability? Although this expansion is more than four years old, the ratio of business fixed investment to gross domestic product (in current dollars) is still well below its forty-year average.\n\nTwo hypotheses for the behavior of investment have received a great deal of attention. The first hypothesis is that a capital overhang, caused by excessive investment during the previous boom, held back the recovery in capital spending. While it is difficult to measure excess capital, the Federal Reserve Board staff estimates that any broad-based capital overhang was probably eliminated early in the recovery. Of course, more persistent capital overhangs may well have been experienced in some sectors, most notably in telecommunications equipment, though that sector accounts for less than 10 percent of business fixed investment.\n\nAnother leading hypothesis is that businesses were more risk-averse than warranted by the underlying economic fundamentals, especially early in the expansion. One obvious source of caution was the degree of conviction about the strength and sustainability of the recovery. Concerns about terrorism and other geopolitical uncertainties were likely at play as well. Although many periods of recovery are accompanied by concerns of economic growth and political turmoil, surveys in 2002 and 2003 suggested that business leaders were experiencing a more prolonged sense of gloom, with measures of sentiment dropping to low levels for as long two years beyond the trough in the business cycle. Since the spring of 2003, however, the economy has been expanding briskly and thus the durability of the recovery should be a less significant constraint on capital expenditures now. Indeed, more recently, some surveys of business confidence and capital spending plans have reached or exceeded the levels of the late 1990s. Nonetheless, concerns about global political uncertainties often reemerge in executive suites and board rooms as important factors for business spending plans.\n\nRegulatory Environment\nStill, one corollary of the corporate-caution explanation may have persisted. Many have argued that the conditions created by the corporate governance scandals--and the regulatory response to those events--have contributed to a more cautious attitude toward risk taking. More concretely, the scandals themselves, and the markets’ reaction to them, are said to have caused firms to restrain capital spending. Clearly, Sarbanes-Oxley compliance costs have been substantial, diverting funds and, probably even more importantly, some of the attention of chief executive officers (CEOs) and boards of directors from capital spending and R&D plans. Every meeting that board members and executives spend focused predominantly on compliance issues is, by definition, meeting time generally not being spent on big strategic questions. However, proving the connection between executives’ \"mind-share\" and capital expenditure rates is difficult. Moreover, firms are benefiting to some degree from reforms arising from Sarbanes-Oxley. Thus, research on the net effects of Sarbanes-Oxley being undertaken by AEI and other institutions should be exceptionally useful as the economic, political, and legal environment continues to evolve. Based on the evidence available to date, I believe that the uncertainty resulting from the regulatory and legal environment has had meaningful economic implications for business investment and cash holdings.\n\nCatalysts for Change\nEach of these factors--increased foreign operations, greater investor focus on liquidity, business caution due to geopolitical uncertainty, concerns about the sustainability of the recovery, and a more process-intensive regulatory and legal environment--likely contributed to the buildup in cash through this expansion. But we have recently seen signs that the cash hoarding trend may have abated or reversed. During the past several quarters, the ratios of cash to assets and cash to investment have slipped, and in the first quarter of this year, the ratio of debt to assets edged up. These reversals can be attributed, in part, to the resurgence of share repurchases, the growth in dividend payouts, cash–financed merger activity, and a pickup in capital investment.\n\nThe pace of share repurchases accelerated strongly in 2005, especially in the fourth quarter, when they exceeded $400 billion at an annual rate. These accelerated shareholder payouts were to be expected given that repatriated profits from firms’ foreign operations had become available to finance investment, acquisitions, and retire debt.\n\nIn addition, signs of changing business attitudes towards expansion have emerged. Merger and acquisition activity picked up markedly in the second half of 2005 and continued to accelerate in the first half of 2006, also contributing to the drawdown of cash. Cash-financed mergers in the past four quarters resulted in a retirement of public equity of more than $250 billion, four to five times the average rate since the peak of the previous expansion. Investment spending is also expanding at a solid pace. Data from the national accounts through the first quarter indicate that domestic real outlays for new equipment and software have been growing at an annual rate of almost 10 percent since the beginning of 2005. Spending on high-tech equipment has been rising at an annual rate of more than 15 percent, several percentage points faster than in 2003 and 2004, with spending especially strong for telecommunications equipment. Outlays for nonresidential construction firmed in 2005 and turned up notably in the first quarter of this year. Not surprisingly, outlays on structures used in energy production strengthened in response to higher energy prices, but the rise in the first quarter also reflected an increase for office, retail, and industrial structures.\n\nAt the same time that the growth in cash balances has started to reverse, the pace of debt financing has also picked up. In the first half of this year, the net amount that firms raised from bonds, commercial paper, and bank loans reached its highest level since the current economic expansion began. Unlike the earlier period of this expansion, more of the funds raised of late are being deployed for merger and acquisition activity and other corporate spending. Considerably less is being used to refinance higher-cost debt or to lengthen debt maturities.\n\nThe recent signs that cash hoards are being trimmed and leverage is increasing suggest a move towards more normal conditions. These developments may be amplified by increased pressure from shareholder groups in conjunction with a more active market for corporate control. A prominent feature of the current environment is the notable amount of leveraged buyout (LBO) activity. An LBO or the threat of an LBO acts to discipline management to raise shareholder value (Jensen, 1986; Kaplan, 1989). LBOs, and other forms of private equity investment, may force management to implement strategic changes rapidly to restore the firm’s return on equity and boost share prices, in part by increasing financial leverage. LBOs increased sharply in late 2005, and they continued apace in the first half of this year. The value of public equity retired by LBOs has accounted for almost one-third of the total retired from total mergers and is at its highest level since the boom in the late 1980s.\n\nMany signs point to a continuation of this high pace of equity retirements. Valuations for many firms appear attractive to private equity sponsors. (Although, as I noted earlier, stock prices have risen quite a bit since 2002, their gains have been far outstripped by the boom in earnings.) Lower valuations for firms in industries that are in the throes of downsizing also provide opportunities for LBOs. Perhaps at least as important, private equity funds are flush with funds, having raised more than $100 billion in 2005, a pace that continued in the first quarter of 2006, and general partners of these funds are actively scouting for opportunities to deploy this capital. This robust equity funding, combined with the currently accommodative debt markets, has made many more firms potential targets. Moreover, with the rise of so-called club deals, in which private equity funds pool their capital, even very large corporations--replete with large excess-cash positions--are potential targets. The bottom line is greater pressure on business executives to maximize shareholder value.\n\nAs a complement to traditional private equity funds, hedge funds are also more active as they seek alternative investments to enhance returns. Many hedge funds have contributed to the deal activity by providing debt financing, either by directly investing in mezzanine debt or by participating in loan syndications. And in a departure from typical past practices, hedge funds are also making investments, sometimes alongside more traditional LBO funds. More often in these types of transactions, hedge funds will purchase large, non-controlling stakes in potential targets to exert greater influence. They will then agitate for changes to boost shareholder value. They may push for asset sales, higher dividends, share repurchases, or other changes that make the firm an attractive target to potential acquirers.\n\nIn my view, internal and external forces have come together to reduce cash levels from their recent highs, although they remain above historical norms. Private equity investors are increasingly pressuring CEOs to make more fundamental changes in their financial and strategic positions, and CEOs themselves appear to have already shifted their focus somewhat from compliance issues to building their businesses more aggressively through increased capital expenditures and increased cash-financed acquisitions. Debt ratios may rise from their current low levels, and a modest slippage in corporate credit quality may result. Indeed, most forecasts call for a rise in default rates from their near-record lows. And while risk spreads for lower-rated bonds are up a bit this year, they do not suggest that debt investors are anticipating a significant deterioration over the near- to medium-term horizon. Nonetheless, should some emerging trends, such as lower required interest coverage ratios, gain traction, they might induce a more pronounced deterioration in credit quality than is currently expected. Moreover, as recent events highlight, geopolitical developments will continue to be an important component of the risk profile that businesses face.\n\nConclusion\nIn sum, it appears that firms are likely to continue to draw down their cash balances from the elevated levels witnessed during the past few years. Increasingly, firms appear to be accelerating shareholder buybacks, raising dividends, increasing business spending, and becoming more acquisitive. Notwithstanding these very recent developments, I would expect firms to hold more cash than has been the norm over the previous few decades due to changes in the economic and geopolitical environment, and a more rigorous legal and regulatory setting. Clearly, corporations in the United States have shown a remarkable ability to adapt and thrive in recent years despite these changes, and I expect them to continue to do so in the period ahead.\n\nReferences\n\nAlmeida, Heitor, Murillo Campello, and Michael Weisbach (2004). \"The Cash Flow Sensitivity of Cash,\" Journal of Finance, vol. LIX (August), pp. 1777-1804.\n\nHartzell, Jay, Sheridan Titman, and Gary Twite (2006), \"Why Do Firms Hold So Much Cash: A Tax-based Explanation,\" unpublished paper (March).\n\nJensen, Michael (1986). \"Agency Costs of Free Cash Flow, Corporate Finance and Takeovers,\" American Economic Review: Papers and Proceedings, vol. 76 (May), pp. 323-29.\n\nKaplan, Steven (1989). \"The Effects of Management Buyouts on Operating Performance and Value,\" Journal of Financial Economics, vol. 24 (Feb.), pp. 215-46.\n\nFootnotes\n\n1.  I should note that I am expressing my own opinions, which are not necessarily those of my colleagues on the Board of Governors of the Federal Reserve or on the Federal Open Market Committee. Nellie Liang and Steve Sharpe, of the Board’s staff, contributed to these remarks. Return to text\n\n2.  Data are for U.S. corporations from Standard and Poor’s Compustat, reported on a consolidated basis. Return to text\n\n3.  Testimony of Chairman Alan Greenspan, February 16, 2005, Federal Reserve Board’s semiannual Monetary Policy Report to the Congress, before the Committee on Banking, Housing, Urban Affairs, U.S. Senate. Return to text\n\n4.  Corporate cash holdings also rose in a number of other G-7 countries, but I do not address whether the buildup in those countries might, as in the United States, be unusual, or whether it is adequately explained by its historical relationship with fluctuations in cash flow and investment spending. Return to text\n\n\n\nReturn to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/warsh20060718a.htm",
        "title": "Corporate Cash Balances and Economic Activity",
        "date": "7/18/2006"
    },
    {
        "content": "July 06, 2006\n\nVice Chairman Donald L. Kohn\n\nAt the European Economics and Financial Centre Seminar, House of Commons, London, England\n\nI am pleased to have the opportunity to return to the Houses of Parliament. I say \"return\" because I appeared here once previously, before the Treasury Select Committee to testify on my report on monetary policy processes at the Bank of England. Two memories stand out from that visit: One is of the mouse that ran across the floor as we were eating lunch--I am sure it could not have been a rat in these precincts; the other is of the thoughtful, informed character of the give and take with the members of the committee, who, perhaps, were just being relatively nice to a visitor from a central bank with only ninety years of history at the time.\n\nIn the past few years, the global economy has enjoyed low inflation and robust growth. This is an experience to which policymakers of all varieties have contributed. But today I am not going to address the prospects for the global economy to extend that progress in the near term. Chairman Bernanke, in his congressional testimony later this month, will address the immediate outlook for activity and prices in the United States and the global economy more generally, and the recent conduct of monetary policy. Instead, I thought I would step back and think about the implications of some longer-term trends for economic performance and for policies, monetary and other. In particular I want to concentrate on several aspects of the reduction of barriers to trade, capital flows, and labor migration--often encapsulated in the word \"globalization.\" First, I will talk about the extent to which globalization has itself contributed to the low-inflation environment in the United States. But the freer flows around the globe have probably also contributed to the size and persistence of global imbalances--the current account deficit of the United States and the surplus of the rest of the world. I will next spend a few minutes on the causes of these imbalances, how they might unwind, and the policies that should be put in place to raise the odds on orderly adjustment.1\n\nGlobalization and Inflation\nAlthough inflation is ultimately a monetary phenomenon, it is important to stress the word \"ultimately\" in that formulation. The long run in which monetary policy exerts its influence on nominal magnitudes is the summation of individual short runs in which pressures in labor and product markets help to shape price dynamics. In the world in which we live, it seems natural to expect, as others have argued, that the greater integration of product and financial markets would have exerted some downward pressure on inflation. I cannot look back at the experience in the United States over the past decade without discerning the imprint of such forces. The opening up of China and India, in particular, represents a potentially huge increase in the global supply of mainly lower-skilled workers. And it is clear that the low cost of production in these and other emerging economies has led to a geographic shift in production toward them; from a U.S. perspective, the ratio of imported goods to domestically produced goods has risen noticeably in recent years.\n\nHowever, the extent of the disinflationary forces let loose by this shift in the pace of globalization is less obvious. The United States is more open, but it is also large in size and scope. Many U.S. goods and most services are still produced domestically with little competition from abroad. In addition, the significant expansion of production in China and elsewhere has put substantial upward pressure on the prices of oil and other commodities, many of which are imported for use as inputs to production in the United States. While we can point to types of goods for which prices are restrained by forces from abroad, the net effects of globalization on domestic inflation of all goods and services need not even be negative, especially in today’s environment of strong global growth.\n\nOne challenge in assessing the effect of increased globalization is the paucity of empirical research on this issue, which is understandable given the shortness of the span over which these forces have been particularly acute. Nevertheless, the existing research does highlight several channels through which globalization might have helped to hold down domestic inflation in recent years. These channels include the direct and indirect effects on domestic inflation of lower import prices, a heightened sensitivity of domestic inflation to foreign demand conditions and perhaps less sensitivity to domestic demand conditions, downward pressure on domestic wage growth, and upward pressure on domestic productivity growth.\n\nLet me summarize the empirical evidence from work on U.S. inflation my colleagues and I have done at the Federal Reserve Board, as well as from our readings of other studies. In the United States, the increase in core import prices since the mid-1990s has averaged about 1-1/2 percentage points less per year than the increase in core consumer prices. According to our model, the direct and indirect effects of this decline in the relative price of imports held down core inflation by between 1/2 and 1 percentage point per year over this period, an estimated effect that is substantially larger than it would have been in earlier decades. However, much of the decline in import prices during this period was probably driven by the appreciation of the dollar in the late 1990s and the effects of technological change on goods prices rather than by the growing integration of world markets. In addition, import prices have risen at least as rapidly as core consumer prices over the past several years and thus no longer appear to be acting as a significant restraint on inflation in the United States.\n\nA second aspect of the hypothesis is that as economies become more integrated, their domestic inflation will be less sensitive to domestic demand pressures and more sensitive to foreign demand conditions than it was earlier. While this seems eminently plausible, recognize that this is a partial-equilibrium effect, identifying one, among many, determinants of inflation, and consequently difficult to verify empirically.\n\nMost researchers, in fact, agree that inflation in the United States is less sensitive to domestic demand conditions today than it was twenty years ago. But numerous researchers have attributed this persistently low inflation to the improved credibility of monetary policy. In that regard, most of the decline in the sensitivity of U.S. inflation to the domestic unemployment gap occurred in the 1980s--too early to be associated with the more recent acceleration in the pace of globalization and more coincident with the sea change in the attitude toward inflation worldwide.\n\nThis aspect of the globalization hypothesis, however, would be bolstered if the decline in the sensitivity of inflation to domestic demand was accompanied by an increased sensitivity to foreign demand. Efforts to find such a link have met with mixed results, with some researchers having found large effects and others having found no effect. Our own analysis of this issue indicates that these results are sensitive to how the foreign output gap is defined and to how the inflation model is specified, suggesting that any effect may not be especially robust. That said, the difficulties with measuring slack in the U.S. economy are compounded when describing the global economy, making settlement of this issue especially difficult.\n\nSimilarly, the evidence that globalization has helped to restrain unit labor costs in recent years is not definitive. One hypothesis is that the increase in the supply of low-skilled workers associated with the emergence of China and other East Asian countries as low-cost centers of production has damped the growth of nominal wages in the United States. But a stable statistical relationship between labor compensation and various measures of globalization has eluded researchers. However, many of the changes are relatively recent, giving empiricists few observations. And, in that regard, the recent behavior of some, though not all, measures of aggregate compensation seem to have been somewhat lower than models would have predicted. Of course, several purely domestic factors could help to account for any shortfall, such as the aftereffects of the unusually sluggish recovery in job growth early in this expansion or a possible downward drift in the natural rate of unemployment. But it also is a pattern that would be consistent with downward pressures from an expansion in global labor supply. In support of this link, some studies have found a relationship between industry wage growth and import penetration, and between the relative decline in wages of low-skilled workers and trade, but the effects are generally small.\n\nA second possibility is that globalization has restrained unit labor costs by raising productivity. Increasing volumes of trade should bolster productivity as economies concentrate their resources in those sectors in which they are relatively more efficient. But I have seen little direct evidence on the extent to which globalization may have boosted aggregate productivity growth in the United States in recent years. Nevertheless, research at the Board finds that multinational corporations, which may have greater opportunities to realize efficiencies by shifting production locations, accounted for a disproportionate share of aggregate productivity growth in the late 1990s. And some microeconomic studies have found a relationship between global engagement and productivity at the firm level. Thus, it seems possible that the persistently high growth rates of multifactor productivity in recent years in the United States may partly be due to the productivity-enhancing effects of globalization. However, these effects should not be unique to the United States. The fact that many other advanced economies facing similar competitive pressures are not experiencing the same outcome gives me pause.\n\nThis evidence suggests that the old line of Churchill--that two economists give three different answers to any question--still holds. My own assessment is that, quite naturally, the greater integration of the U.S. economy into a rapidly evolving world economy has affected the dynamics of inflation determination. Unfortunately, huge gaps and puzzles remain in our analysis and empirical testing of various hypotheses related to these effects. But, for the most part, the evidence seems to suggest that to date the effects have been gradual and limited: a greater role for the direct and indirect effects of import prices; possibly some damping of unit labor costs, though less so for prices from this channel judging from high profit margins; and potentially a smaller effect of the domestic output gap and a greater effect of foreign output gaps, but here too the evidence is far from conclusive. In particular, the entry of China, India, and others into the global trading system probably has exerted a modest disinflationary force on prices in the United States in recent years.\n\nMoreover, we should recognize that these could be one-off effects to the extent that they reflect the global imbalances that I will speak about next, rather than just the integration of emerging-market economies into the global-trading system. If so, any disinflationary effects could dissipate or even be reversed in coming years. For example, the fact that China and some other emerging-market economies have resisted upward pressure on their exchange rates and are running trade surpluses has undoubtedly contributed to their disinflationary effects on the rest of the world. The prices of their exports are lower than they would be if market forces were given greater scope in foreign exchange markets, and they are supplying more goods and services to the rest of the world than they themselves are demanding. These imbalances are not likely to be sustained indefinitely. The elevated rates of national saving in these economies--and, in some, relatively restrained rates of investment--are not likely to persist in the face of ongoing improvements in the functioning of their financial markets, increases in the depth of their product markets, and fuller development of economic safety nets. As individuals in these countries are increasingly drawn to investing at home and consuming more of their wealth and as their real wages catch up to past productivity gains, the upward pressures on these countries’ currencies will intensify, their demand will come into better alignment with their capacity to produce, cost advantages will decline, and these economies will exert less, if any, downward pressure on inflation in the United States or other advanced economies.\n\nGlobal Imbalances\nThe first thing to keep in mind about global imbalances is their scale. The U.S. current account deficit is enormous--on the order of $800 billion or 6-1/2 percent of gross domestic product--and it is not likely to shrink substantially in the immediate future, given the current configuration of economic activity and prices around the world. Obviously, the U.S. deficit has as a counterpart an equal current account surplus in the rest of the world combined, after allowance for gaps in the statistical reporting system.\n\nThe size and persistence of these imbalances reflects two interrelated forces. First is the gap between spending and production in the United States and a similar gap of opposite sign in the rest of the world. The United States as a whole is spending much more than it is producing. Saving rates are especially low in our household and federal government sectors--both of which are spending more than their current income. This configuration is not so unusual for the government, but it is for households, where low interest rates (until recently) and the rising value and easier accessibility of housing wealth apparently have boosted spending relative to income.\n\nOutside the United States, the shortfall of domestic demand relative to production capacity has importantly reflected weak business investment along with high saving rates, a mechanism identified by my colleague, Chairman Bernanke. Low levels of investment relative to profits, sales, and the cost of capital are global, including in the United States. Indeed, it is one reason interest rates have been so low through much of the recent global expansion. In the United States, the shortfall in business demand has been made up for largely by the household sector, as I just noted; for a variety of reasons, that has been less the case in many other countries, especially in Asia, and these countries have, in effect, relied on exports to fill the gap between demand and potential production.\n\nThe second force affecting global imbalances has been the continuing strong demand for dollar assets, without which exchange rates and other prices already would have adjusted to limit the growth of the imbalances. Arguably, it was this strong demand, in response to the step-up in productivity growth in the United States and the Asian financial crisis, that appreciated the dollar in the late 1990s and initiated the string of large U.S. current account deficits. But the demand has continued this decade, albeit with some fluctuations, financing the growing U.S. current account deficit.\n\nPrivate investors apparently perceive opportunities for relatively high returns on dollar assets in light of the more rapid growth of productivity in the United States than in many other industrialized economies. The attraction of dollar assets likely also is enhanced by the liquid nature of the markets in which they trade and because as collateral these assets are protected by the rule of law and have been a safe haven in times of stress. The globalization of financial markets and the increased willingness of investors to look beyond their own borders for opportunities may well have facilitated the transfer of savings needed to sustain the U.S. current account deficit. In that regard, both the pull of global demands for our assets as well as the push of our needs to finance our trade imbalance explains the current conjuncture.\n\nForeign official holdings of dollar assets also have risen substantially, especially in Asia. Governments there apparently read one lesson of the financial crisis of the 1990s as the need for a large war chest of reserves. In addition, against the backdrop of very high private saving rates, they may be concerned about their ability to generate sufficient domestic demand to provide employment opportunities, in some cases for the growing numbers of people who want to shift from agriculture to higher productivity jobs often in urban areas.\n\nAlthough private and government demands for dollar assets have allowed the U.S. current account deficit and foreign surpluses to persist, these imbalances are not sustainable indefinitely. In the United States, both public and private saving will need to rise to meet the oncoming needs of an aging population. At some point, risk-adjusted returns on investments in the rest of the world will begin to look favorable relative to holding dollar assets. Dollar assets are becoming an increasing proportion of non-U.S. portfolios; this can continue for a time, but not forever. At some point, the United States is going to need to finance its imports with the proceeds of its exports, not with foreign saving.\n\nExperience with current account adjustments by industrialized economies--for example, by the United States in the 1980s--suggests that the transition to a more sustainable configuration is not likely to be disruptive. But we cannot be sure, particularly because the U.S. experience is unique given the dollar’s role as a reserve currency and Americans’ relatively favorable returns on assets held abroad. The world economy is in uncharted territory with regard to the size of the imbalances. Various asset markets have experienced rather sharp fluctuations in prices in recent decades, some of which have threatened disruption in the United States and have contributed to sluggish growth elsewhere, as in Japan following the real estate boom and bust; we certainly cannot rule out the possibility of further sharp asset price movements as product prices and spending adjust. Recent research reinforces the common-sense conclusion that no single policy or private action will be sufficient to effect the necessary changes. Adjustment will need to proceed along several dimensions at the same time, including changes in relative prices and in domestic demand around the globe.\n\nThe Role of Policy\nThat observation brings me to my final topic--the role of public policy in addressing these imbalances. Sound public policies will enhance the chances that any transition will be smooth. They can contribute by facilitating needed adjustments in spending, production, and relative prices and by taking steps to foster strong, flexible product and financial markets that are resilient to more abrupt changes in asset prices and spending patterns, cushioning the effect of any such fluctuations on output and product prices.\n\nA permanent correction to the spending imbalances in the United States must involve further progress on fiscal discipline and a long-run solution to the financing problems of entitlement programs--Social Security, Medicare, and Medicaid. Without a resolution of these fiscal problems, it would be all the more difficult to bring aggregate production and spending into balance and the resultant intensified pressures on interest rates, as the flow of foreign saving into the United States levels out or declines, would exacerbate adjustment difficulties in other sectors.\n\nSmooth adjustment of global current account imbalances cannot be brought about by actions of the United States alone. Our trading partners also need to take steps. Indeed, were U.S. domestic demand to moderate and provide less stimulus abroad in the form of reduced demand for exports from our trading partners, central banks in those countries would need to adjust the stance of monetary policy to maintain full utilization of resources. Moreover, in many cases, the root cause of deficient domestic demand seems to be more structural than cyclical in nature, calling for more micro-oriented measures to promote flexible and efficient labor and product markets. Such initiatives should yield higher productivity growth and more vigorous spending, boosting rates of return on capital investment outside the United States. These changes in turn would boost the demand for U.S. exports and would likely shift portfolio preferences away from dollar-denominated assets.\n\nOther public policies, here and abroad, can have an important influence on the transition process by working to facilitate market flexibility. For example, increased exchange rate flexibility in key Asian currencies will be essential to enable the monetary authorities to contain inflation through market-oriented policies rather than inefficient direct controls. Greater flexibility also will enhance the ability of all the world’s economies to adapt to the huge increases in the effective supply of labor and its productivity and in demand that has resulted from these economies becoming part of the global trading system. In addition, the United States and its trading partners should vigorously protect the current degree of the openness of their labor and product markets and should continue to pursue the difficult goal of reducing trade barriers further.\n\nThese and other types of market flexibility help facilitate needed shifts in spending and prices; without them, rigidities might impede such stabilizing changes, causing adjustments to break out forcefully in other, more disruptive ways. Increased market flexibility would also ease the macroeconomic stabilization burden placed on fiscal and monetary policy--an important consideration, given that policymakers cannot anticipate the nature and incidence of all the elements of the adjustment process.\n\nIn this regard, prudential regulation is also important because it increases the ability of policymakers to focus on stabilizing aggregate output and inflation. By ensuring that financial institutions are adequately capitalized and are managing risks well, and are in general well prepared to deal with major changes in asset prices, they are in a better position to weather any necessary changes in policy settings. Prudential regulation also decreases the risk that the actions of impaired financial institutions could disrupt the flow of credit and thereby intensify what might already be difficult adjustments. A surge in financial market innovations and shift in trading participants has paralleled the rise in global imbalances in recent years. The Federal Reserve, under the leadership of the Federal Reserve Bank of New York, has been working with other regulators in the United States and elsewhere, including the Financial Services Authority in London, along with the private sector to strengthen the infrastructures and risk management around these new markets and participants.\n\nFinally, monetary policy plays a role in reacting to these imbalances and their inevitable unwinding. I start my thinking on this topic from the premise that monetary policy--in the United States or elsewhere--has not been a major factor behind the increases in global imbalances. As I argued a little while ago, the imbalances reflect saving and investment behavior along with demands for assets in various economies. To be sure, spending and production respond to changes in interest rates, but how the balance between the two is affected by policy is not clear. Policies to affect demand might have offsetting influences on relative prices. For example, a tighter monetary policy in the United States might damp demand but could also appreciate the exchange rate, with ambiguous effects on the current account. As a consequence, monetary policies are not well suited to initiate current account adjustments.\n\nThese imbalances certainly affect the forces of supply and demand and have consequences for price stability. At the Federal Reserve and at other central banks, we have been reacting to the changes in spending and prices that have accompanied the buildup of these imbalances in ways intended to keep inflation low and stable and our economies producing near their maximum sustainable potential. The imbalances are important to us in so far as they affect the macroeconomy, and in this regard they are just a few of the factors that the Federal Reserve considers in assessing the prospects for price and output stability. Similarly, we will need to take account of any influences on the macroeconomy of the unwinding of the imbalances when that occurs.\n\nContinued strong demand for dollar assets will be critical to keeping that unwinding smooth and not disruptive. The Federal Reserve can contribute by being sure the public remains confident that the purchasing power of their dollar assets will not erode unexpectedly. As long as inflation expectations remain contained, relatively faster growth of the prices of imported goods for a time would be associated with only a temporary bulge in inflation and would result in a needed change in relative prices. The lesson from the 1970s, however, is that an unchecked or permanent increase in inflation would only feed back adversely on demand for dollars. Such an unmooring of the anchor of price stability could only elevate the odds on abrupt changes in interest rates and asset prices, instability in the U.S. economy, and disorder in global adjustments.\n\nFootnotes\n\n1.  These views are my own and not necessarily those of other members of the Board of Governors and the Federal Open Market Committee.  Return to text",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20060706a.htm",
        "title": "Reflections on Globalization and Policies",
        "date": "7/6/2006"
    },
    {
        "content": "July 04, 2006\n\nGovernor Susan Schmidt Bies\n\nAt the Risk Capital 2006 Forum, Paris, France\n\nGood morning. And if you will indulge me on this U.S. national holiday, I would like to wish you a happy Fourth of July. I am honored to be addressing such an esteemed collection of professionals in the field of risk management. As you know, the Federal Reserve is strongly committed to the continuing evolution of risk measurement and management at U.S. banking organizations. Thus, we want you to be successful at your craft, and meetings such as this will facilitate the communication of emerging best practice ideas across financial institutions.\n\nToday I will provide an update on progress with respect to Basel II implementation in the United States, and describe areas where further advances in risk management practices are most needed to ensure the success of this new risk-based capital standard. Naturally, given the international composition of this audience, I will also offer some thoughts on cross-border implementation issues associated with Basel II, including so-called home-host issues.\n\nMoving to Basel II\nBy now most of you are aware that on March 30 of this year the Federal Reserve Board approved a draft of the U.S. notice of proposed rulemaking (NPR) on the Basel II capital framework. I imagine that many of you have already read the draft’s 400-plus pages. Once all of the U.S. banking agencies have completed their individual review and approval processes, the NPR will be issued in the Federal Register, meaning that it will be officially out for comment. It appears that those approval processes are going well and there have not been any unanticipated issues so far. Indeed, the recent completion of the Office of Management and Budget’s (OMB) review of the draft NPR brings us one step closer to issuing the NPR for comment.\n\nSince the release of the draft NPR by the Federal Reserve, the U.S. banking agencies have met with a number of industry groups, comprising both U.S. and foreign banks, to discuss the document. From our perspective, these discussions have been informative and helpful, and we believe that some of the clarifications we have provided have been useful to bankers. I would like to make one further clarification, on a procedural matter that may be a bit confusing: Although we welcome comments on the draft NPR at this time, bankers should not expect that those comments will be incorporated in the NPR that is released in the Federal Register. That stage will come later in our rulemaking process. A proposed 120-day comment period will be provided once the final NPR has been released, and all comments given now and during that period will be considered as the final rule is written.\n\nBefore discussing some of the specifics in the draft NPR, I would like to review the Federal Reserve’s reasons for pursuing Basel II.\n\nRationale for Moving to Basel II\nThe current Basel I capital framework, adopted nearly twenty years ago, has served us well. But it has become increasingly inadequate for large, internationally active banks that are offering ever more complex and sophisticated products and services. We need a better capital framework for these particular banks, and we believe that Basel II is such a framework.\n\nOne of the major improvements in Basel II is the closer linkage between capital requirements and the way banks manage their actual risk. The current Basel I measures have very limited risk-sensitivity and do not provide bankers, supervisors, or the marketplace with meaningful measures of risk at large complex organizations. Under Basel I, a bank’s capital requirement does not adequately reflect gradations in asset quality and does not change over time to reflect deterioration in asset quality. Further, there is no explicit capital requirement to account for the operational risk embedded in many of the services from which the largest institutions generate a good portion of their revenues.\n\nIn addition to strengthening the linkage between “minimum regulatory capital,” as calculated in Pillar 1, and the way banks manage their “actual” capital, Basel II should make the financial system safer by encouraging continuing improvement in risk-measurement and risk-management practices at the largest banks. Pillar 1 of Basel II is based on many of the economic capital practices of the most sophisticated banks and therefore brings minimum regulatory capital requirements closer to the capital generated by banks’ internal models. By providing a consistent framework within which the largest banks calculate minimum regulatory capital requirements, supervisors will more readily be able to identify those portfolios and banks whose capital is not commensurate with their inherent risk levels. Working within this consistent framework and engaging in ongoing and regular dialogue with supervisors will in turn help inform management about how its proprietary risk measurement and management models compare with the range of current practices and where enhancements are needed. We have already seen some progress in risk measurement and management at many institutions in the United States and around the globe as a result of preparations for Basel II. Admittedly, banks have told us that some of the costs for Basel II would have been incurred anyway. But if anything, Basel II has accelerated the pace of this change.\n\nBasel II can also provide supervisors with a more conceptually consistent and more transparent framework for evaluating systemic risk in the banking system, particularly through credit cycles. Thus it improves on Basel I, which requires banks to hold the same level of capital for a given portfolio, no matter how the portfolio’s inherent risk may change over time. Further, as bankers gain experience with the advanced approaches under Basel II, they will have better information on how their risk taking may vary through various cycles. Therefore, Basel II establishes a more coherent relationship between how supervisors assess regulatory capital and how they supervise banks, enabling examiners to better evaluate whether banks are holding prudent levels of capital, given their risk profiles.\n\nThe reasons I’ve just given for pursuing Basel II also provide justification for the recent Basel revisions to the 1996 Market Risk Amendment (MRA). Since adoption of the MRA, banks’ trading activities have become more sophisticated and have given rise to a wider range of risks that are not easily captured in their existing value-at-risk (VaR) models. For example, more products related to credit risk, such as credit default swaps and tranches of collateralized debt obligations, are now included in the trading book. These products can give rise to default risks that are not captured well in methodologies required by the current rule specifying a ten-day holding period and a 99 percent confidence interval, which can create potential arbitrage opportunities between the banking book and the trading book. The U.S. agencies are in the final stages of drafting the NPR to revise Market Risk capital requirements, which will be issued for comments. In the United States we would continue to have banks with significant trading book activity hold additional capital for the risks inherent in that line of business, whether they remain Basel I banks or move to Basel II.\n\nBridging the Gap Between Regulatory Capital Requirements and Internal Bank Practice\nWith Basel II, U.S. supervisors are attempting to use the internal risk-measurement and risk-management information produced by large complex institutions to manage their own risks in such a way as to augment the risk sensitivity and overall meaningfulness of minimum regulatory capital measures. Basel II, by tying Pillar 1 minimum regulatory capital calculations to bank-generated inputs, offers greater transparency about the practices that stand behind the inputs provided by banks and the way those inputs are calculated. In fact, through the Basel II framework, supervisors will be able to compare results across banks and provide better information to bankers about how their models and methodologies agree with sound practice and where they may be an outlier.\n\nOf course, we understand that the extent to which banks’ internal inputs can be used in minimum regulatory capital requirements is limited, for a variety of reasons. Today’s banks have highly customized models for running their businesses, which of course is entirely appropriate. But as supervisors, we need to ensure adequacy and enforceability of our minimum regulatory capital requirements while maintaining some consistency across banks. In a broader sense, we are working to protect the safety and soundness of our financial system. Naturally, as we seek to develop a common framework that will work for large complex banks globally, we recognize an inherent tension between our regulatory requirements and internal bank practice. We are working to strike the right balance to achieve our goals without making Basel II purely a compliance exercise and creating undue burden.\n\nNeed for Strong Capital\nBasel II is intended to improve regulatory capital requirements, especially for large complex organizations, through greater risk sensitivity of regulatory capital and improved linkage to banks’ actual capital risk management. That is why the U.S. agencies have chosen to adopt only the most advanced options for credit risk and operational risk minimum regulatory capital calculations in the United States, and to limit the requirement of Basel II to only a small number of banking institutions that fit the definition of large, complex, and internationally active.\n\nIt is important to recognize that Basel II is a complete capital framework consisting of three pillars. While much of the focus to date has been on Pillar 1 and the calculation of minimum regulatory capital, the importance of Pillar 2, which provides for supervisory review and oversight of an institution’s overall capital adequacy, should not be overlooked. Pillar 2 is intended to ensure that banks have adequate capital to support all the risks to their business, and to further encourage them to develop and use better techniques to monitor and manage the risks. It addresses some kinds of risk that are not captured in Pillar 1, such as credit concentration, interest rate, and liquidity risks. And it provides supervisors with the opportunity to assess compliance with the minimum standards and disclosure requirements of the advanced approaches in Pillar 1.\n\nSome key principles of the supervisory review process are discussed in the framework document. Time does not permit a full discussion of each, but I do want to at least mention them today. First, banks should have a process for assessing their overall capital adequacy in relation to their risk profile and a strategy for maintaining their capital levels. Second, supervisors should review and evaluate banks’ internal capital adequacy assessments and strategies, as well as their ability to monitor and ensure their compliance with regulatory capital ratios--and should take appropriate supervisory action if they are not satisfied with the results of this process. Third, supervisors should expect banks to operate above the minimum regulatory capital ratios and should have the ability to require banks to hold capital in excess of the minimum. Finally, supervisors should seek to intervene at an early stage to prevent capital from falling below the minimum levels indicated by the risk characteristics of a particular bank--and should require rapid remedial action if capital is not maintained or restored.\n\nMany of the actions implied by the Pillar 2 principles are already part of the supervisory process in the United States. The agencies plan to provide some information about our expectations for Pillar 2, beyond what is included in the existing U.S. supervisory process, in forthcoming supervisory guidance.\n\nLet me assure you that we at the Federal Reserve would not be pursuing Basel II if we thought that it would in any way undermine the strong capital base that U.S. institutions now enjoy. As a central bank and a supervisor of banks, bank holding companies, and financial holding companies, the Federal Reserve is committed to ensuring that the Basel II framework delivers a strong and risk-sensitive base of capital for our largest and most complex banking institutions. That is why we supported moving ahead with the draft NPR, which was modified to address concerns identified in the fourth quantitative impact study, known as QIS4, and includes additional safeguards to ensure strong capital levels during the transition to Basel II. We will remain vigilant in monitoring and assessing Basel II’s impact on individual and aggregate minimum regulatory capital levels on an ongoing basis. As an extra degree of precaution, the U.S. banking agencies also decided to delay for a year the start of the parallel-run period.\n\nStarting with the parallel run, and both during and after the transition period, the U.S. agencies will rely on ongoing, detailed analyses to evaluate the results of Basel II, so as to ensure prudent levels of capital. Importantly, Basel II represents a new way of thinking about regulatory capital; it is complex, reflecting the complexity of risk measurement and management for the largest, most complex banking institutions. Banking institutions and their supervisors will need to have ongoing dialogue and work diligently to make sure Basel II is working as we expect it to. But we believe it is a powerful approach to making regulatory capital more risk sensitive. To be quite clear, the Federal Reserve believes that strong capital is critical to the health of our banking system, and we believe that Basel II will help us continue to ensure that U.S. banks maintain capital levels that serve as an appropriate cushion against their risk taking.\n\nSome Aspects of the U.S. Proposals\nAs you know, the draft U.S. Basel II NPR is based on the 2004 framework issued by the Basel Committee and adheres to the main elements of that framework. But the U.S. agencies have exercised national discretion and have tailored the Basel II framework to fit the U.S. banking system and U.S. financial environment--just as their counterparts in other countries have tailored the framework to their situations. For example, as I have just mentioned, the U.S. agencies continue to propose that we implement only the advanced approaches of Basel II, namely the advanced internal-ratings-based approach (AIRB) for credit risk and the advanced measurement approaches (AMA) for operational risk.\n\nAlso, the draft NPR proposes a more gradual implementation timetable and a more rigorous set of transition safeguards than those set forth in the 2004 Basel II framework. For instance, the U.S. agencies are proposing three years of transition floors below which a bank’s minimum required capital under Basel II would not be permitted to fall. The first transition period would have a floor of 95 percent relative to the general risk-based capital rules, the second period 90 percent, and the third period 85 percent. Implementation of a more gradual transition timetable is justified in part by a recognition that banks need more time to prepare--and that we as supervisors need more time to analyze transition information and ensure that there are no unintended consequences.\n\nAs you are aware, the QIS4 exercise identified some areas requiring further clarification by regulators and additional work by bankers on risk models and databases. One of the key areas in the NPR influenced by these results pertains to banks’ estimates of loss given default (LGD). Many QIS4 participants reported difficulty computing LGDs (which must reflect downturn conditions), in part because their data histories were not long enough to capture weaker parts of the economic cycle. To address this problem, the agencies have proposed a supervisory mapping function that can be used in the interim by those institutions unable to estimate appropriate downturn LGDs. The mapping function allows an institution to “stress” its expected LGDs, generating an input to the capital calculation that conforms to the Basel II requirements and hence produces a more appropriate capital requirement. The Federal Reserve supported the introduction of this supervisory mapping function, as an important component of U.S. Basel II implementation, in order to address a specific challenge articulated by the industry. Banks would be able to shift from using the mapping function to using their own internal estimates of LGDs when their own estimates become reliable.\n\nAbout a month ago, the Basel Committee released the results of QIS5, which was conducted by a number of countries but not the United States, since we had already conducted QIS4. As you know, the QIS4 results from U.S. institutions are not completely comparable with the QIS5 results from institutions in other countries. Nonetheless, some similarities are worth noting. First, the aggregate declines in minimum regulatory capital of banks using the advanced approaches were similar in the two exercises, particularly considering that QIS4 did not include the 1.06 capital multiplier and QIS5 did. For both exercises the aggregate declines reflected relatively good economic times--which, as we know all too well, do not last forever. Second, both studies pointed to dispersion of changes in minimum required capital. The dispersion among banks in QIS5 was attributed largely to a combination of differences in portfolio characteristics and differences and uncertainties in estimation methodologies. The U.S. agencies highlighted similar issues in their public release on QIS4 and have stated their intention to monitor issues related to dispersion very closely in the future, because we want to ensure that the Basel II framework does indeed accurately produce similar capital for similar risk. The Basel Committee’s release on QIS5 also stated that methodologies and systems for LGD calculation are still being developed and that, as a result, some of the effects of downturns may have been underestimated. The U.S. agencies' release on QIS4 stated that U.S. banks also faced some challenges in estimating downturn LGDs.\n\nBasel I Modifications\nAt this point I would like to say just a few words about ongoing efforts to revise the existing Basel I regulatory capital rules for non-Basel II institutions. We expect only one or two dozen banks to move to the U.S. version of Basel II in the near term, meaning that the vast majority of U.S. banks will continue to operate under Basel I, which will be amended through a separate rulemaking process. The Basel I framework has already been amended more than twenty-five times in response to changes in banking products and the banking environment and as a result of a better understanding of the risks of individual products and services. The U.S. agencies believe that now is another appropriate time to propose modifications to Basel I rules. The agencies have issued an advance notice of proposed rulemaking discussing possible changes to increase the risk sensitivity of U.S. Basel I rules and to mitigate any competitive distortions that might be created by introducing Basel II. We are now in the process of reviewing comments on the advance notice and working on a notice of proposed rulemaking. We are mindful that within the current structure of the Basel I rules, amendments to those rules should not be too complex or too burdensome for the large number of banks to which the revised rules will apply.\n\nWith regard to both the Basel II proposals and the proposed Basel I amendments, we understand the need for full transparency. For that reason, we expect to have overlapping comment periods for the Basel II NPR and the NPR for the proposed Basel I amendments. In fact, we want all interested parties to compare, contrast, and comment on the two proposals in overlapping timeframes. Accordingly, our proposals could change as a result of comments received or new information gathered.\n\nCross-Border Implementation of Basel II\nAs I noted earlier, each country must implement Basel II as appropriate for its particular jurisdiction. To that end, the U.S. banking agencies are acting to ensure that the Basel II framework is implemented in the United States in a prudential manner. We recognize that the adoption of differing approaches to Basel II by various countries may create challenges for banking organizations that operate in multiple jurisdictions. It is good to remember that cross-border banking has always raised specific challenges that supervisors from various countries have worked hard to address. Let me assure all bankers here that supervisors are aware that the process of changing to new national versions of Basel II has heightened concerns about home-host issues. The Federal Reserve and other U.S. agencies have, for many years, worked with international counterparts to limit the difficulty and burden that have arisen as foreign banks have entered U.S. markets and as U.S. banks have established operations in other jurisdictions.\n\nThe United States is working to complete its national-standard-setting process, as we recognize that the lack of a final rule creates uncertainty, for both banks and foreign supervisors, about exactly what will be required. At the same time, we have been working with our colleagues on the Basel Accord Implementation Group (AIG) for the past few years to identify issues arising from differences in national standards of the Basel II framework. All of the supervisory bodies participating in the AIG effort are committed to making the transition to Basel II successful.\n\nWe have heard from some bankers that they are concerned about home-host issues. Many of the issues are institution specific, and all the U.S. banking agencies encourage regular meetings between bankers and supervisors to identify specific concerns. These meetings are also an opportunity for bankers to make supervisors aware of their individual bank’s implementation plans and progress, and for supervisors to make bankers aware of current supervisory expectations with respect to those banks. Indeed, the meetings we have had to date to discuss the draft NPR have provided us with useful information about how our proposals are being interpreted and perceived.\n\nWe continue to ask for details about concerns bankers may have about cross-border implementation; it is a great help, as we work on our proposals, to hear your specific issues. We were pleased that at a few recent meetings, bankers did, in fact, provide some very specific information about how possible differences in implementation could generate extra burden and affect the way in which the bankers conduct their business. Suggestions for changes to our proposals that are specific, detailed, and supported by facts will allow us, in drafting the final rule, to more clearly identify alternative solutions to the issues raised.\n\nOf course, all Basel-member countries have their own rollout timelines and their own ways of addressing matters that are left to national discretion under the Accord, which is entirely appropriate. So while the United States appears to be a bit of an outlier with respect to its implementation timetable, other countries are grappling with their own challenges, too. As you know, a number of other countries plan to implement the full set of Basel II approaches for credit risk (standardized, foundation IRB, and advanced IRB) and operational risk (basic indicator, standardized, and the advanced measurement approaches), with all but the advanced approaches expected to be implemented next year in many Basel countries. This underscores the importance of regular dialogues between home and host supervisors and banks. I hope that bankers pay special attention to the Basel Committee’s June 2006 paper “Home-Host Information Sharing for Effective Basel II Implementation,” as it provides guidance on how cooperative efforts can successfully realize the objectives of Basel II.\n\nConclusion\nThe Federal Reserve, as I have indicated, believes that Basel II is a worthwhile endeavor despite the challenges facing us. The current regulatory capital framework, based on Basel I, is not adequate for the largest, most complex U.S. banking organizations. We applaud the risk-management improvements made by institutions so far, but additional work remains to be done before banks can meet our expectations--not just for Pillar 1, but also for Pillars 2 and 3.\n\nImplementing Basel II is a serious undertaking, with many moving parts both domestically and internationally. The U.S. banking agencies are cognizant of the challenges associated with Basel II, including the challenge of ensuring that its effects are those that we intend. We are also aware of the difficulties that could arise in implementing Basel II on a cross-border basis. So far, we have been pleased with our talks with the industry about our implementation plans and how those plans interact with the plans of other countries. We understand that there are questions to be answered and issues to be addressed, and we hope that bankers--and other interested parties--will provide detailed feedback on the U.S. NPR so that we can assess the entire spectrum of comments. While some observers may be critical of the slower pace of the U.S. agencies, we believe that our deliberate pace is necessary to ensure that the effects of Basel II are indeed understood and that all comments are taken into consideration.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20060704a.htm",
        "title": "Addressing Challenges Raised by Basel II Implementation",
        "date": "7/4/2006"
    },
    {
        "content": "June 16, 2006\n\nGovernor Donald L. Kohn\n\nAt the Federal Reserve Bank of Boston's 51st Economic Conference, Chatham, Massachusetts\n\nThank you for the opportunity to participate in this conference on global imbalances, a topic of growing importance. Although I will touch on global imbalances, I would like to focus on globalization’s potential influence on inflation and the associated implications for monetary policy. It seems a natural focus for a policymaker at a central bank, and, indeed, several of my colleagues on the Federal Open Market Committee (FOMC) have also addressed this issue in recent months.1 As you would see from reading their remarks, no consensus has yet emerged about how globalization has been influencing recent inflation developments, and part of my intention today is to illustrate some of the considerable challenges that are involved in attempting to identify the extent to which the recent pickup in the pace of global economic integration has influenced inflation dynamics in the United States.2\n\nOf course, the trend toward greater international integration of product and financial markets has been established for quite a while; the share of U.S. economic activity involved in international trade (measured by nominal exports plus imports as a share of nominal gross domestic product) has been rising since the early 1970s. However, this trend has accelerated markedly over the past fifteen years or so. In particular, the economies of eastern Europe became more integrated into the global economy, and China, India, and some other East Asian market economies have emerged as important players in the global trading system.\n\nAlthough inflation is ultimately a monetary phenomenon, it seems natural to expect, as others have argued, that these developments would have exerted some downward pressure on inflation in the United States. The opening up of China and India, in particular, represents a potentially huge increase in the global supply of mainly lower-skilled workers. And it is clear that the low cost of production in these and other emerging economies has led to a geographic shift in production toward them--not just from the United States but also from other formerly low-cost producers such as Mexico, Korea, Singapore, and Taiwan.3 Trade surpluses in China and in other East Asian countries have increased sharply over the past decade, and from a U.S. perspective, the ratio of imported goods to domestically produced goods has accelerated noticeably in recent years.\n\nHowever, the extent of the disinflationary effect of this shift in the pace of globalization is less obvious. Many U.S. goods and most services are still produced domestically with little competition from abroad. In addition, the significant expansion of production in China and elsewhere has put substantial upward pressure on the prices of oil and other commodities, many of which are imported for use as inputs to production in the United States. Indeed, the effects of globalization on domestic inflation need not even be negative, especially in today’s environment of strong global growth.\n\nOne challenge in assessing the effect of increased globalization is the lack of research on this issue. At a research conference on modeling inflation held at the Federal Reserve Board last fall, none of the papers even touched on issues related to globalization. And, although some new and interesting research is emerging from places like the International Monetary Fund and the Bank for International Settlements, much of this work is still quite preliminary.4 Nevertheless, the existing research does highlight several channels through which globalization might have helped to hold down domestic inflation in recent years. These channels include the direct and indirect effects on domestic inflation of lower import prices, a heightened sensitivity of domestic inflation to foreign demand conditions (and perhaps less sensitivity to domestic demand conditions), downward pressure on domestic wage growth, and upward pressure on domestic productivity growth.5\n\nIn trying to clarify my own thinking about the likely magnitude of these effects, I find that a useful starting point is a simple reduced-form equation that attempts to explain movements in inflation and then to ask whether and how the statistical relationships embedded in this equation have been affected by globalization. The equation is a standard one in use at the Board and elsewhere. It relates core consumer price inflation--using, say, the index for core personal consumption expenditures (PCE) or the core consumer price index (CPI)--to resource utilization, lagged inflation, changes in relative prices of food and energy, and changes in relative import prices. Using this framework, we can look for the effect of globalization in several ways. First, we can look for influences that are directly controlled for in the model--notably the influence on domestic inflation of changes in import prices. Second, we can look for evidence of globalization-related structural change in the model by examining the stability of the parameter estimates. Third, we can see whether we have omitted from the standard model any variables that might be interpreted as representing changes in globalization. And, finally, we can look for evidence of model errors that would be consistent with the hypothesis that globalization has been restraining inflation. I will focus in particular on the past five years or so, which, judging from the data on U.S. trade shares, is when the pace of globalization appears to have picked up.\n\nI will start with the import price channel--the hypothesis that increased globalization has depressed import prices and thus domestic inflation. Importantly, the estimated strength of this channel should capture not only the direct effects of import prices on the cost of living in the United States but, also, at least a portion of the indirect effects of actual and potential import competition on the prices of goods produced domestically. In the reduced-form model that I’ve just described, the effects of import prices on inflation show up quite clearly; furthermore, the estimated effects appear to have increased over time, with the increase apparently stemming primarily from the upward trend in the share of imported consumer goods in household spending.6\n\nWe can use the model to get a rough idea of how relative changes in import prices have influenced domestic inflation by simulating how core consumer prices would have behaved if relative import prices had instead remained constant. In particular, the increase in core import prices since the mid-1990s has averaged about 1-1/2 percentage points less per year than the increase in core consumer prices. According to the model simulation, which also builds in the associated reduction in inflation expectations, the direct and indirect effects of this decline in the relative price of imports held down core inflation by between 1/2 and 1 percentage point per year over this period, an estimated effect that is substantially larger than it would have been in earlier decades. However, much of the decline in import prices during this period was probably driven by movements in exchange rates and the effects of technological change on goods prices rather than by the growing integration of world markets.7\n\nIn addition, import prices have risen at about the same average pace as core consumer prices over the past several years and thus no longer appear to be acting as a significant restraint on inflation in the United States. This step-up in the rate of change of import prices obviously reflects, to some extent, recent movements in the dollar, especially its depreciation in 2004. However, it also reflects large increases in the prices of a number of imported commodities, which have been attributed in part to the rapid expansion of activity in China and other Asian countries.\n\nA second hypothesis is that increases in global capacity have held down U.S. inflation in recent years by limiting the ability of U.S. producers to raise prices in response to increases in the domestic costs of production. At a basic level, the elevated profit margins of U.S. producers over the past few years seem inconsistent with this hypothesis. But it does raise a broader issue about the determinants of inflation--that is, whether U.S. inflation is now less sensitive to domestic demand pressures and more sensitive to foreign demand conditions than it was earlier. In the context of the inflation model, we can examine this issue in two ways. First, we can look for evidence that the coefficients on the domestic output or unemployment gaps have fallen over time. Second, we can add a measure of foreign excess demand to the model to see whether it helps to explain domestic inflation in recent years.\n\nWith regard to the first test, we do find evidence that the coefficient on the unemployment gap has fallen in the United States. In particular, the coefficient from a model estimated over the past twenty years appears to be about one-third lower than when the model is run over a forty-year period. Of course, globalization is not the only potential explanation for this result, and numerous other researchers have cited persistently low inflation and the improved credibility of monetary policy as having played a more important role. In fact, in rolling regressions, the timing of the decline in the sensitivity of inflation to the unemployment gap appears to be too early to be associated with the more recent acceleration in the pace of globalization.\n\nThis aspect of the globalization hypothesis would be bolstered if the decline in the sensitivity of inflation to domestic demand was accompanied by an increased sensitivity to foreign demand. Efforts to find such a link have met with mixed results, with some researchers having found large effects and others having found no effect.8 Our own analysis of this issue indicates that these results are sensitive to how the foreign output gap is defined and to how the inflation model is specified, suggesting that any effect may not be especially strong.\n\nSimilarly, the evidence that globalization has helped to restrain unit labor costs in recent years is mixed. One hypothesis is that the increase in the supply of low-skilled workers associated with the emergence of China and other East Asian countries as low-cost centers of production has put downward pressure on the growth of nominal wages in the United States. However, a model of changes in aggregate labor compensation that is similar in structure to the price-inflation model that I described earlier does not detect a stable relationship between measures of globalization (for example, import price changes or the BIS estimates of the foreign output gap) and aggregate wage dynamics in the United States. That said, the recent changes in some, though not all, measures of aggregate compensation seem to have been somewhat lower than such models would have predicted. Of course, several purely domestic factors could help to account for any shortfall, such as the aftereffects of the unusually sluggish recovery in job growth early in this expansion or a possible downward drift in the nonaccelerating-inflation rate of unemployment. But it also is a pattern that would be consistent with downward pressures from an expansion in global labor supply. In support of this link, some cross-section studies have found a relationship between industry wage growth and import penetration, while the research on wage inequality tends to relate some of the relative decline in wages of low-skilled workers to trade, although in both types of studies the effects are generally relatively small.9 Similarly, research from the Federal Reserve Bank of New York shows a modest relationship between exchange rate fluctuations and wage growth, with larger effects evident for the wages of lower-skilled workers.10\n\nA second possibility is that globalization has restrained unit labor costs by raising productivity. Increasing volumes of trade should bolster productivity as economies concentrate their resources in those sectors in which they are relatively more efficient. But I have seen little direct evidence on the extent to which globalization may have boosted aggregate productivity growth in the United States in recent years. Nevertheless, research at the Board finds that multinational corporations, which may have the greater opportunities to realize efficiencies by shifting production locations, accounted for a disproportionate share of aggregate productivity growth in the late 1990s.11 And some microeconomic studies have found a relationship between global engagement and productivity at the firm level.12 Thus, it seems possible that the persistently high growth rates of multifactor productivity in recent years may partly be due to the productivity-enhancing effects of globalization.\n\nIn this regard, I would note that a potential shortcoming of my approach to assessing the effects of globalization on inflation is that these effects may be too recent to be captured adequately by the data. That is, it may be too soon for globalization to have generated statistically observable changes in the parameter estimates or structure of the standard inflation model. Nonetheless, if the influence of globalization on inflation is as substantial as many claim, we might have expected the standard model to have had difficulty in predicting recent inflation trends. For example, if recent increases in world labor supply are restraining domestic unit labor costs to a significant degree or if there are other important influences on inflation that are related to globalization but difficult to quantify in the context of the standard model, we would expect to have seen sizable model errors over the past several years.\n\nAgain, the evidence points to some limited influence of globalization on U.S. inflation. If we use out-of-sample dynamic simulations of a model for core PCE price inflation estimated from 1985 through the end of 2001, we find that, although the model overpredicts inflation over the past several years, the errors average only 0.1 to 0.2 percentage point per year, considerably less than one might have expected given the anecdotes in the popular press. In contrast, the forecast errors from a model of core CPI inflation are larger (averaging roughly 1/2 to 1 percentage point per year since mid-2001), perhaps suggestive of some influence from globalization.\n\nWhat do I conclude from all of this evidence? My own assessment is that, quite naturally, the greater integration of the U.S. economy into a rapidly evolving world economy has affected the dynamics of inflation determination. Unfortunately, huge gaps and puzzles remain in our analysis and empirical testing of various hypotheses related to these effects. But, for the most part, the evidence seems to suggest that to date the effects have been gradual and limited: a greater role for the direct and indirect effects of import prices; possibly some damping of unit labor costs, though less so for prices from this channel judging from high profit margins; and potentially a smaller effect of the domestic output gap and a greater effect of foreign output gaps, but here too the evidence is far from conclusive. In particular, the entry of China, India, and others into the global trading system probably has exerted a modest disinflationary force on prices in the United States in recent years.\n\nMoreover, we should recognize that these disinflationary effects could dissipate or even be reversed in coming years. They reflect, at least in part, the global imbalances that are the subject of this conference, rather than just the integration of emerging-market economies into the global trading system. For example, the fact that China and some other emerging-market economies have resisted upward pressure on their exchange rates and are running trade surpluses has undoubtedly contributed to their disinflationary effects on the rest of the world. The prices of their exports are lower than they would be if market forces were given greater scope in foreign exchange markets, and they are supplying more goods and services to the rest of the world than they themselves are demanding. These imbalances are not likely to be sustained indefinitely. The elevated rates of national saving in these economies--and, in some, relatively restrained rates of investment--are not likely to persist in the face of ongoing improvements in the functioning of their financial markets, increases in the depth of their product markets, and fuller development of economic safety nets. As individuals in these countries are increasingly drawn to investing at home and consuming more of their wealth and as their real wages catch up to past productivity gains, the upward pressures on their currencies will intensify, their demand will come into better alignment with their capacity to produce, cost advantages will decline, and these economies will exert less, if any, downward pressure on inflation in the United States.\n\nThis observation brings me to my final point, which is about monetary policy. Clearly, the greater integration of the world’s economies does leave the United States more open to influences from abroad. In one sense, a more open economy may be more forgiving as shortfalls or excesses in demand are partly absorbed by other countries through adjustments of our imports and exports. And, to the extent that the United States can draw upon world capacity, the inflationary effect of an increase in aggregate demand might be damped for a time. But we are also subject to inflationary forces from abroad, including those that might accompany a shift to a more sustainable pattern of global spending and production, or those that might emanate from rising cost and price pressures. Moreover, a smaller response of inflation to domestic demand also implies that reducing inflation once it rose could be difficult and costly. And, from another perspective, integrated financial markets can exert powerful feedback, which may be less forgiving of any perceived policy error. For example, if financial market participants thought that the FOMC was not dedicated to maintaining long-run price stability--a notion that I can assure you is not correct--they would be less willing to hold dollar-denominated assets, and the resulting decline in the dollar would tend to add to inflationary pressures. Clearly, policymakers need to factor into their decisions the implications of globalization for the dynamics of the determination of inflation and output.\n\nIn the end, however, policymakers here and abroad cannot lose sight of a fundamental truth: In a world of separate currencies that can fluctuate against each other over time, each country’s central bank determines its inflation rate. If the FOMC were to allow the U.S. economy to run beyond its sustainable potential for some time, inflation would eventually rise. And, this pickup would become self-perpetuating if it became embedded in inflation expectations. Thus, while a better understanding of the implications of globalization will aid in our understanding of inflation dynamics, it is also clear that such developments do not relieve central banks of their responsibility for maintaining price and economic stability.\n\nFootnotes\n\n1.  For example, Richard W. Fisher (2005), \"Globalization and Monetary Policy,\" Warren and Anita Marshall Lecture in American Foreign Policy, Harvard University, November 3; and Janet L. Yellen (2006), \"Monetary Policy in a Global Environment,\" speech delivered at The Euro and the Dollar in a Globalized Economy Conference, University of California at Santa Cruz, May 27. Return to text\n\n2.  Deb Lindner and William Wascher, of the Board’s staff, contributed to these remarks. The views expressed are my own and are not necessarily shared by my colleagues on the Board or the FOMC.  Return to text\n\n3.  See, for example, International Monetary Fund (2005), \"Mexico: Staff Report for the 2005 Article IV Consultation,\" October 2005; and Alan G. Ahearne, John G. Fernald, Prakash Loungani, and John W. Schindler (2003), \"China and Emerging Asia: Comrades or Competitors?\" International Finance Discussion Paper 2003-789 (Washington: Board of Governors of the Federal Reserve System, December). Return to text\n\n4.  Thomas Helbling, Florence Jaumotte, and Martin Sommer (2006), \"How Has Globalization Affected Inflation?\" IMF World Economic Outlook (Washington: IMF, April), chapter 3; Claudio Borio and Andrew Filardo (2006), \"Globalization and Inflation: New Cross-Country Evidence on the Global Determinants of Domestic Inflation,\" unpublished paper, Bank for International Settlements, March. Return to text\n\n5.  Ken Rogoff also argues that globalization has increased the incentives for central banks to keep inflation low (Kenneth S. Rogoff, 2003, \"Globalization and Global Disinflation,\" in Monetary Policy and Uncertainty: Adapting to a Changing Economy,\" a symposium sponsored by the Federal Reserve Bank of Kansas City, pp. 77-112.) Return to text\n\n6.  As is standard in such models, we use a price measure for \"core\" imports, defined as imports of goods excluding energy, computers, and semiconductors. When the change in relative import prices is weighted by the import share, the coefficient in the model is fairly stable. Return to text\n\n7.  Research at the Board examined the direct effects of Chinese exports on global import prices from the mid-1990s to 2002 and found only a modest effect of U.S. import prices. Of course, it is possible that China’s influence on import prices has grown in recent years as its trade share has expanded. Refer to Steven B. Kamin, Mario Marazzi, and John W. Schindler (2004), \"Is China ‘Exporting Deflation’?\" International Finance Discussion Paper 2004-791 (Washington: Board of Governors of the Federal Reserve System, January). Return to text\n\n8.  Borio and Filardo (2006) and Gamber and Hung (2001) found that foreign resource utilization had sizable effects on U.S. inflation, while Tootell (1998) found little to no effect. Refer to Borio and Filardo, \"Globalization and Inflation\"; Edward N. Gamber and Juann H. Hung (2001), \"Has the Rise in Globalization Reduced U.S. Inflation in the 1990s?\" Economic Inquiry, vol. 39 (January), pp. 58-73; and Geoffrey M. B. Tootell (1998), \"Globalization and U.S. Inflation,\" Federal Reserve Bank of Boston, New England Economic Review (July/August), pp. 21-33. Return to text\n\n9.  See, for example, Helbling, Jaumotte, and Sommer, \"How Has Globalization Affected Inflation?\"; and William R. Cline (1997), Trade and Income Distribution (Washington: Institute for International Economics). Return to text\n\n10.  Linda Goldberg and Joseph Tracy (2003), \"Exchange Rates and Wages,\" unpublished paper, Federal Reserve Bank of New York. Return to text\n\n11.  Carol Corrado, Paul Lengermann, and Larry Slifman (2005), \"The Contribution of MNCs to U.S. Productivity Growth, 1977-2000,\" unpublished paper, Board of Governors of the Federal Reserve System. Return to text\n\n12.  For example, Mark E. Doms and J. Bradford Jensen (1998), \"Productivity, Skill, and Wage Effects of Multinational Corporations in the United States,\" in D. Woodward and D. Nigh, eds., Foreign Ownership and the Consequences of Direct Investment in the United States: Beyond Us and Them (Westport, Conn.: Quorum Books), pp. 49-68. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20060616a.htm",
        "title": "The Effects of Globalization on Inflation and Their Implications for Monetary Policy",
        "date": "6/16/2006"
    },
    {
        "content": "June 16, 2006\n\nGovernor Randall S. Kroszner\n\nAt the Institute of International Bankers, New York, New York\n\nGovernor Randall S. Kroszner presented identical remarks at the Bankers' Association for Finance and Trade, New York, New York, on June 15, 2006\n\nToday I want to talk about some new and exciting developments in bond markets around the world. The motivation for my discussion is the current puzzling situation of a relatively flat yield curve combined with relatively low real and nominal long-term interest rates, which has occurred both in developed economies and in emerging markets. I will explore some possible explanations for the pattern and will focus on changes in the prospects for and risks to the long-term inflation outlook, particularly in emerging-market economies. In particular, I will highlight how financial innovations and international competitive pressures, combined with a better public understanding of the costs of inflation and changes in the institutions of central banking, have helped improve the credibility of central banks and inflation outcomes in many emerging markets.\n\nUntil recently, many emerging-market countries simply did not have a yield curve because there was effectively no market for debt issued in domestic currency beyond a very short horizon. The credibility of central banks has been crucial to this deepening of the domestic capital market, which is typically associated with higher economic growth.\n\nI am optimistic that these developments will continue, as I believe that the move toward low inflation rates reflects important technological and institutional factors that are likely to persist. Nevertheless, there are still risks, which underscore the importance of continuing to reap the benefits of improved central bank behavior and credibility in emerging markets and around the world.\n\nGlobal Developments in the Bond Market\nIn February 2005, former Federal Reserve Board Chairman Alan Greenspan noted a puzzle in the U.S. economy related to the slope of the yield curve and the level of the long-term interest rate.1 Long-term interest rates had remained low and stable despite a solid economic recovery and a sustained period of monetary policy tightening during which the target federal funds rate went from 1 percent to 2-1/2 percent. When he first publicly noted this \"conundrum,\" as he called it, the ten-year Treasury yield was just over 4 percent. Today, despite an additional 250 basis points of increase in the target federal funds rate, the nominal ten-year Treasury yield is roughly 5 percent, still very low by historical standards, compared to an average of more than 7-1/2 percent since 1980.\n\nThe combination of a rising short rate and a relatively stable long rate has led to a very flat yield curve. During the last quarter-century, for example, the difference between the yield on the ten-year Treasury note and the yield on the three-month Treasury bill has been roughly 1-3/4 percent (or, to be exact, 179 basis points from 1980 to the present). During the last year, that difference has been less than 50 basis points and is currently less than half of that. Thus, this essentially flat slope is atypical in U.S. experience.\n\nI am sure that you are all familiar with the simple relationship between short-term and long-term interest rates. The yield on a ten-year bond, for instance, can be thought of as a series of consecutive forward rates. If you could borrow and lend at the same rate as the U.S. Treasury, then you could lock in a three-month loan ten years from now by borrowing for ten years and three months and simultaneously lending the same principal for ten years. The difference between the interest you pay and the interest you earn on this transaction determines the implied forward rate ten years from today.2 The forward rate reflects not only the market expectation of the future short-term interest rate but also a \"term premium\" to compensate for the risk in committing to extend credit so far in the future, including the risk of future inflation.3\n\nAt any point in time, then, we can calculate the short-term forward rate ten years ahead based on the yield curve of U.S. Treasury coupon securities.4 This \"far forward\" rate makes the conundrum even more puzzling because it reached historically low levels of almost 4-1/4 percent last year, more than 200 basis points below its average since 1990, and has rebounded only somewhat this year. In real terms, the far forward rate calculated from inflation-indexed securities is similarly below its long run average.\n\nThe U.S. bond market conundrum has occurred in parallel with similar developments in foreign bond markets. In major industrial countries, bond yields have trended down, in some cases reaching historical lows recently. Yields are also low in real terms, as measured by inflation-indexed bonds. Far-forward short rates in recent years have also reached unusually low levels in many industrial countries.\n\nThe most interesting and, I believe, perhaps least studied recent developments in the bond markets concern the changes in emerging markets. While it is well known that the yield spreads on dollar-denominated bonds of emerging-market governments included in the EMBI+ index are near all time lows (even taking into account the recent rise), two phenomena in emerging markets have received less attention.\n\nOne is the development of markets for longer-dated fixed-coupon bonds issued in local currencies. This phenomenon is, from my perspective, quite remarkable and belies the assertion that the \"original sins\" of bad policy from the past have doomed the development of domestic currency bond markets in many emerging markets. The recent lengthening of maturities of domestic-currency debt markets has, in many cases, not only extended a yield curve but effectively created a local currency yield curve that simply did not exist earlier.\n\nSince 2000, ten-year nominal fixed-coupon bonds in local currency have been introduced in Brazil, Colombia, Indonesia, Mexico, and Russia, while Korea issued a ten-year fixed coupon bond in 1995. To illustrate in more detail, the governments of Mexico and Korea have been able extend the average maturity of their local-currency debt significantly in just the past few years. The Mexican government issued ten-year maturities in 2001 and then 20-year maturities in 2003. The proportion of local-currency debt in Mexico maturing within one year was nearly 90 percent in 2002 and is now below 75 percent. (I have included floating rate debt in the one-year maturity category.) The Korean government continues to increase the proportion of its domestic currency debt in longer maturities, with the one-year-and-under segment falling from roughly one-half in 1999 to one-quarter by the end of last year.\n\nTwo, bond yields in local currencies of emerging-market countries have also declined. It is perhaps not surprising that, given their high rates of saving and generally high level of economic development, the governments of Hong Kong and Korea can borrow at close to industrial-country levels. More notable, however, is that the Mexican government can borrow in pesos at a ten-year maturity at rates that have averaged roughly 9 percent. And Mexico is not unique in this regard. Other middle-income emerging markets with ten-year local-currency fixed rate bond yields in the single digits include Chile, Malaysia, Russia, and Thailand, to name but a few. For countries with longer maturities, implied short-term interest rates five years ahead also have been declining and have reached very low levels, although there have been some increases in the past few months.\n\nWhat is driving these changes? There are a number of complementary, not alternative, explanations.\n\nExplanations for the Low Real Bond Yields\nChairman Bernanke has suggested that an excess of ex ante global savings relative to global investment, sometimes referred to as a global savings glut, has held down real interest rates around the world and encouraged capital inflows to the United States.5 Some of the factors behind this savings glut include the surge in revenues of oil and commodity exporters, a reduction in fiscal deficits in some Latin American countries, and a retreat in Asian investment demand from the boom that preceded the late 1990s financial crises while saving rates stayed high in Asia. The savings glut story helps to explain the real component of low bond yields as well as the pattern of global capital flows, which was Chairman Bernanke’s focus. Another factor behind declining real yields in some emerging markets is that their improved fiscal situation not only increases national saving but also calms fears about the ability of governments to service their debt.\n\nHowever, there is also a nominal aspect of low global bond yields. In the rest of my talk, I would like to emphasize the worldwide decline of inflation and perceived inflation risk as a key contributor to low nominal bond yields.\n\nExplanations for the Low Nominal Bond Yields \nInflation rates in major industrial and developing regions have trended down over the past twenty-five years. Compared with the period 1980 to 1999, median inflation rates from 2000 to 2004 fell from 5 percent to 2 percent in industrialized countries and from 14 percent to 4-1/2 percent in emerging markets, according to the most recent statistics from the International Monetary Fund. Not long ago, annual inflation rates in Brazil and Mexico at times exceeded 100 percent. But during the past decade, Brazilian and Mexican inflation rates have remained low. In particular, inflation in Brazil did not spike up after its financial crises and sharp currency depreciations in the late 1990s. Given Brazil’s history of hyperinflation, this stability is especially remarkable. Brazil did experience a small spike of inflation around its presidential election in 2002, but even this was minor by historical standards. The pattern of low inflation is seen across many countries, large and small.\n\nA few years ago I did some research that showed how inflation rates around the world have fallen significantly since the 1970s and 1980s, both in terms of averages and medians.6 Indeed, the IMF’s April 2006 World Economic Outlook notes that average inflation rates in both the industrial countries and the developing countries in recent years are at their lowest levels since at least the early 1970s. More important, I found that the worst inflation performers (specifically the 10 percent of the countries of the world experiencing the highest inflation) had much lower inflation rates than the worst performers from the 1970s, 1980s, and 1990s. Thus, the worst behavior is not as bad as it once was.\n\nDo markets expect low inflation to persist in the long run? To answer this question, we can look at measures of expected inflation. Consensus Economics surveys hundreds of professional forecasters in numerous countries each April. The surveys allow us to examine forecasts of inflation around the world six to ten years ahead beginning in 1996. The latest observation, in April 2006, for example, is the forecast of a given country’s average consumer price inflation rate from 2012 through 2016. For both a representative sample of industrial economies (Euro area, Japan, the United States, and the United Kingdom) and emerging-market economies (Brazil, China, Korea, and Mexico), we observe substantial declines from the late 1990s to today. These forecasts have been low and stable in both industrial and many important developing countries in recent years. The surveys thus provide one indication that markets do expect low inflation to persist.\n\nThe volatility of inflation has also declined notably, suggesting that perceived inflation risk may have declined as well. For the industrial countries, inflation volatility (measured as a twenty-quarter rolling standard deviation of consumer price inflation) has declined from the 1980s to the 1990s to the period since 2000. Although it has since drifted up just a bit due to volatility in oil prices, it remains at or near its lowest level in the last quarter-century. For the emerging markets, the decline in volatility is even more dramatic. Brazil, in particular, was off the chart much of the time before the late 1990s. Volatility of inflation in China, Korea, and Mexico is now at levels similar to those of the industrial countries, and volatility in Brazil is not much higher.\n\nOverall, the combination of lower and less volatile inflation around the world has led to a reduction in inflation expectations and lower perceived inflation risk, hence a lower inflation uncertainty premium in long rates. I believe that these factors have been important contributors to the lower long-term yields and the flattening of yield curves, particularly in emerging markets. The existence of markets for long-term nominal government and corporate debt is powerful evidence of the faith that investors place in a future environment of price stability.\n\nFactors Behind the Global Move to Price Stability\nFour broad factors lie behind the move to price stability, especially in emerging markets, and these factors tend to reinforce each other. Each factor affects the cost-benefit tradeoff of pursuing a high-inflation policy.\n\nThe first factor, which gets surprisingly little attention in my view, is financial innovation that alters the ability and incentive of a government to pursue a high-inflation policy.7 I put the innovations into two main categories, developments in information technology and physical dollarization, both of which effectively increase potential competition among currencies. Financial innovations make it easier for citizens to move their assets out of the local currency should their government resort to an inflation tax. The dollarizations that followed the high-inflation episodes in Latin America and the former Soviet Union, for example, significantly reduced the costs of switching away from a local currency for small-value transactions. The specific channels by which financial innovations could have affected competition among currencies are many:\n\nGiven these innovations, a government that pressures a central bank to pursue an inflationary policy gets much less benefit for each unit increase in inflation because people can more easily switch out of the local currency. In other words, the inflation tax becomes much more difficult and costly to levy because citizens can more easily avoid the tax by using an alternative money.\n\nThe second and closely related factor behind disinflation is deregulation and competition in a globalized marketplace. The collapse of the centrally-planned economies has led many countries to turn increasingly to private markets to deliver growth and progress and reduce the role of government. Technology has helped to increase global competition by shrinking the barriers of time and distance. Again, there are several channels by which globalization and competition may have affected the cost-benefit tradeoff in pursing inflation:\n\nThe third factor is that economists and the public have learned from painful experience about the costs of inflation.9 The end of the Bretton Woods gold standard in the early 1970s was associated with the first global and sustained peacetime inflation in history. Although the specific experiences differed across countries, public opinion eventually turned strongly against allowing inflation to continue, and policymakers responded to this pressure by taking stronger measures to achieve price stability. This learning process helped to drive some of the financial innovations that I discussed earlier, which, in turn, helped households and businesses to economize on holding inflationary assets. Economists and central bankers also devoted great attention to understanding the causes and consequences of inflation, providing the intellectual underpinning to policies oriented toward price stability.\n\nThe fourth factor I wish to mention relates to changes in the institutions of central banking that may have increased the costs of pursuing high-inflation policies. The most notable change is the increased independence of many central banks and the corresponding reduced control of the fiscal authorities over monetary policy. Central bank independence reduces the ability of a government to \"raid the cookie jar\" through a surprise inflation tax. In most cases, central bank independence can be reversed by a majority vote of parliament. But having to resort to such a vote is a greater obstacle to inflationary finance than previous arrangements allowed, especially given the public’s increased sensitivity and aversion to inflation.\n\nCentral bank independence has typically been granted in conjunction with an explicit mandate that makes achieving low and stable inflation one of the goals of monetary policy. Central bank independence with a mandate that includes price stability increases the credibility of monetary policy with regard to achieving low inflation. Policy is credible because the central bank’s objectives are clear to the public and the central bank can be held accountable for failing to achieve its objectives.\n\nWhen citizens are more aware of the costs of inflation and when governments would reap lower benefits from a high-inflation policy, institutional reforms that will make central banks more credible and independent may be more likely to be adopted and sustained.10 The fundamental forces I mentioned earlier--financial innovation, deregulation, globalization, and public understanding about the costs of inflation--provided the impetus for fighting inflation and opened the political path to institutional reforms, such as central bank independence, that enhance central bank credibility. Once in place, these reforms made further progress against inflation easier and raised the costs of backsliding. As the benefits of stable prices accrue and as financial markets deepen and become more sophisticated, the benefits of sound economic policies will help to create support for institutional reforms that make returning to inflation harder for future governments.\n\nBenefits of Price Stability\nWhile it is well known that low and stable inflation improves the environment for investment planning and avoids many costs and disruptions associated with frequent price adjustments, I want to focus on a few of the many benefits that are particularly relevant for emerging markets.\n\nPrice stability boosts growth through deepening financial markets. With stable prices, savers and investors have more confidence about the ultimate value of their deposits and loans. Stable prices encourage the growth of financial intermediaries and financial markets. As noted above, many emerging markets have recently experienced a deepening of their local financial markets with greater issuance of longer-dated paper. According to numerous studies, there is a strong link between financial market development and economic growth. Thus, the greater credibility of central banks that permits more development of the local markets can have an economic benefit beyond the financial sector.11\n\nThe development of long-term local-currency bond markets may also help governments and firms plan long-term infrastructure and investment projects that boost economic development. Although such debt markets are only one of many factors that can lower the costs of long-term planning and enhance the ability to undertake long-term investments, the development of these markets, particularly when accompanied by lower real rates, help to support longer-horizon projects and reduce the effect of foreign exchange movements on such activities.\n\nA better fiscal outlook, which might arise from higher and more stable growth as well as better long-term planning, also increases financial market confidence and development and thus further boosts growth and reinforces prospects for continued price stability. This virtuous cycle appears to be happening in key emerging markets that were long plagued with poor fiscal situations, such as Brazil and Mexico. In the 1980s and early 1990s, for example, public sector deficits in these countries often exceeded 10 percent of GDP. Since the late 1990s, deficits have been diminished.\n\nMaintaining this Progress\nAlthough I am an optimist, I would be remiss if I did not point out some risks to this otherwise rosy scenario. The difficulty of reaching agreement in the Doha Round of trade negotiations highlights the risk of renewed protectionism. Trade barriers reduce both domestic and international competition, one of the key factors behind low inflation, and make all countries poorer. Barriers to free flow of goods, services, and capital would also diminish the force of other factors outlined above that help to reduce inflationary pressures.\n\nWe must not forget the examples of high inflation and hyperinflation from the past: They hold important lessons about the costs of not maintaining price stability. That sound policies are the basis for solid economic growth should not be forgotten.\n\nFootnotes\n\n1.  Alan Greenspan (2005), statement before the Senate Committee on Banking, Housing, and Urban Affairs, presenting the Federal Reserve Board’s \"Monetary Policy Report to the Congress,\" February 16. Return to text\n\n2.  Strictly speaking, this calculation requires the use of zero-coupon bonds, but it can be approximated using coupon securities. Return to text\n\n3.  Don H. Kim and Jonathan H. Wright (2005), \"An Arbitrage-Free Three-Factor Term Structure Model and the Recent Behavior of Long-Term Yields and Distant-Horizon Forward Rates,\" Finance and Economics Discussion Series 2005-33 (Washington: Board of Governors of the Federal Reserve System, August). Return to text\n\n4.  Typically we calculate an \"instantaneous\" forward rate, which is the limiting value of a sequence of forward rates with maturities declining toward zero. Return to text\n\n5.  Ben S. Bernanke (2005), \"The Global Saving Glut and the U.S. Current Account Deficit,\" Sandridge Lecture at the Virginia Association of Economists, March 10. Return to text\n\n6.  Randall S. Kroszner (2003), \"Currency Competition in the Digital Age,\" in David E. Altig and Bruce D. Smith, eds., Evolution and Procedures in Central Banking (New York: Cambridge University Press), pp. 275–99. Return to text\n\n7.  I discussed aspects of this factor in my presentation at a May 2001 conference at the Federal Reserve Bank of Cleveland, cited above.  Return to text\n\n8.  Kenneth S. Rogoff (2003), \"Globalization and Global Disinflation,\" in Monetary Policy and Uncertainty: Adapting to a Changing Economy: A Symposium (Federal Reserve Bank of Kansas City, Aug. 28–30), pp. 77–112. Return to text\n\n9.  This hypothesis was raised in the discussion of Rogoff (2003). See Guillermo Ortíz, chair, \"General Discussion: Globalization and Global Disinflation,\" in Monetary Policy and Uncertainty: Adapting to a Changing Economy: A Symposium (Federal Reserve Bank of Kansas City, Aug. 28–30), pp. 119–130. For evidence that voters in Latin America have punished politicians for bad inflation outcomes in recent years, see Eduardo Lora and Mauricio Oliveira (2005), \"The Electoral Consequences of the Washington Consensus,\" Economía, vol. 5 (Spring), pp. 1–61. Return to text\n\n10.  In a paper with Douglas Irwin, I documented a similar dynamic at work in the gradual reversal of protectionist policies in the United States in the 1930s and 1940s. See Douglas A. Irwin and Randall S. Kroszner (1999), \"Interests, Institutions, and Ideology in Securing Policy Change: The Republican Conversion to Trade Liberalization after Smoot-Hawley,\" Journal of Law and Economics, vol. 42 (October), pp. 643–73. Return to text\n\n11.  See Ross Levine (2005), \"Finance and Growth: Theory and Evidence,\" Philippe Aghion and Steven Durlauf, eds., Handbook of Economic Growth (New York: Elsevier); and Randall S. Kroszner and Philip E. Strahan (2006), \"Regulation and Deregulation of the U.S. Banking Industry: Causes, Consequences, and Implications of the Future,\" unpublished paper. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20060616a.htm",
        "title": "Why Are Yield Curves So Flat and Long Rates So Low Globally?",
        "date": "6/16/2006"
    },
    {
        "content": "June 15, 2006\n\nGovernor Randall S. Kroszner\n\nAt the Bankers' Association for Finance and Trade, New York, New York\n\nGovernor Randall S. Kroszner presented identical remarks at the Institute of International Bankers, New York, New York, on June 16, 2006\n\nToday I want to talk about some new and exciting developments in bond markets around the world. The motivation for my discussion is the current puzzling situation of a relatively flat yield curve combined with relatively low real and nominal long-term interest rates, which has occurred both in developed economies and in emerging markets. I will explore some possible explanations for the pattern and will focus on changes in the prospects for and risks to the long-term inflation outlook, particularly in emerging-market economies. In particular, I will highlight how financial innovations and international competitive pressures, combined with a better public understanding of the costs of inflation and changes in the institutions of central banking, have helped improve the credibility of central banks and inflation outcomes in many emerging markets.\n\nUntil recently, many emerging-market countries simply did not have a yield curve because there was effectively no market for debt issued in domestic currency beyond a very short horizon. The credibility of central banks has been crucial to this deepening of the domestic capital market, which is typically associated with higher economic growth.\n\nI am optimistic that these developments will continue, as I believe that the move toward low inflation rates reflects important technological and institutional factors that are likely to persist. Nevertheless, there are still risks, which underscore the importance of continuing to reap the benefits of improved central bank behavior and credibility in emerging markets and around the world.\n\nGlobal Developments in the Bond Market\nIn February 2005, former Federal Reserve Board Chairman Alan Greenspan noted a puzzle in the U.S. economy related to the slope of the yield curve and the level of the long-term interest rate.1 Long-term interest rates had remained low and stable despite a solid economic recovery and a sustained period of monetary policy tightening during which the target federal funds rate went from 1 percent to 2-1/2 percent. When he first publicly noted this \"conundrum,\" as he called it, the ten-year Treasury yield was just over 4 percent. Today, despite an additional 250 basis points of increase in the target federal funds rate, the nominal ten-year Treasury yield is roughly 5 percent, still very low by historical standards, compared to an average of more than 7-1/2 percent since 1980.\n\nThe combination of a rising short rate and a relatively stable long rate has led to a very flat yield curve. During the last quarter-century, for example, the difference between the yield on the ten-year Treasury note and the yield on the three-month Treasury bill has been roughly 1-3/4 percent (or, to be exact, 179 basis points from 1980 to the present). During the last year, that difference has been less than 50 basis points and is currently less than half of that. Thus, this essentially flat slope is atypical in U.S. experience.\n\nI am sure that you are all familiar with the simple relationship between short-term and long-term interest rates. The yield on a ten-year bond, for instance, can be thought of as a series of consecutive forward rates. If you could borrow and lend at the same rate as the U.S. Treasury, then you could lock in a three-month loan ten years from now by borrowing for ten years and three months and simultaneously lending the same principal for ten years. The difference between the interest you pay and the interest you earn on this transaction determines the implied forward rate ten years from today.2 The forward rate reflects not only the market expectation of the future short-term interest rate but also a \"term premium\" to compensate for the risk in committing to extend credit so far in the future, including the risk of future inflation.3\n\nAt any point in time, then, we can calculate the short-term forward rate ten years ahead based on the yield curve of U.S. Treasury coupon securities.4 This \"far forward\" rate makes the conundrum even more puzzling because it reached historically low levels of almost 4-1/4 percent last year, more than 200 basis points below its average since 1990, and has rebounded only somewhat this year. In real terms, the far forward rate calculated from inflation-indexed securities is similarly below its long run average.\n\nThe U.S. bond market conundrum has occurred in parallel with similar developments in foreign bond markets. In major industrial countries, bond yields have trended down, in some cases reaching historical lows recently. Yields are also low in real terms, as measured by inflation-indexed bonds. Far-forward short rates in recent years have also reached unusually low levels in many industrial countries.\n\nThe most interesting and, I believe, perhaps least studied recent developments in the bond markets concern the changes in emerging markets. While it is well known that the yield spreads on dollar-denominated bonds of emerging-market governments included in the EMBI+ index are near all time lows (even taking into account the recent rise), two phenomena in emerging markets have received less attention.\n\nOne is the development of markets for longer-dated fixed-coupon bonds issued in local currencies. This phenomenon is, from my perspective, quite remarkable and belies the assertion that the \"original sins\" of bad policy from the past have doomed the development of domestic currency bond markets in many emerging markets. The recent lengthening of maturities of domestic-currency debt markets has, in many cases, not only extended a yield curve but effectively created a local currency yield curve that simply did not exist earlier.\n\nSince 2000, ten-year nominal fixed-coupon bonds in local currency have been introduced in Brazil, Colombia, Indonesia, Mexico, and Russia, while Korea issued a ten-year fixed coupon bond in 1995. To illustrate in more detail, the governments of Mexico and Korea have been able extend the average maturity of their local-currency debt significantly in just the past few years. The Mexican government issued ten-year maturities in 2001 and then 20-year maturities in 2003. The proportion of local-currency debt in Mexico maturing within one year was nearly 90 percent in 2002 and is now below 75 percent. (I have included floating rate debt in the one-year maturity category.) The Korean government continues to increase the proportion of its domestic currency debt in longer maturities, with the one-year-and-under segment falling from roughly one-half in 1999 to one-quarter by the end of last year.\n\nTwo, bond yields in local currencies of emerging-market countries have also declined. It is perhaps not surprising that, given their high rates of saving and generally high level of economic development, the governments of Hong Kong and Korea can borrow at close to industrial-country levels. More notable, however, is that the Mexican government can borrow in pesos at a ten-year maturity at rates that have averaged roughly 9 percent. And Mexico is not unique in this regard. Other middle-income emerging markets with ten-year local-currency fixed rate bond yields in the single digits include Chile, Malaysia, Russia, and Thailand, to name but a few. For countries with longer maturities, implied short-term interest rates five years ahead also have been declining and have reached very low levels, although there have been some increases in the past few months.\n\nWhat is driving these changes? There are a number of complementary, not alternative, explanations.\n\nExplanations for the Low Real Bond Yields\nChairman Bernanke has suggested that an excess of ex ante global savings relative to global investment, sometimes referred to as a global savings glut, has held down real interest rates around the world and encouraged capital inflows to the United States.5 Some of the factors behind this savings glut include the surge in revenues of oil and commodity exporters, a reduction in fiscal deficits in some Latin American countries, and a retreat in Asian investment demand from the boom that preceded the late 1990s financial crises while saving rates stayed high in Asia. The savings glut story helps to explain the real component of low bond yields as well as the pattern of global capital flows, which was Chairman Bernanke’s focus. Another factor behind declining real yields in some emerging markets is that their improved fiscal situation not only increases national saving but also calms fears about the ability of governments to service their debt.\n\nHowever, there is also a nominal aspect of low global bond yields. In the rest of my talk, I would like to emphasize the worldwide decline of inflation and perceived inflation risk as a key contributor to low nominal bond yields.\n\nExplanations for the Low Nominal Bond Yields \nInflation rates in major industrial and developing regions have trended down over the past twenty-five years. Compared with the period 1980 to 1999, median inflation rates from 2000 to 2004 fell from 5 percent to 2 percent in industrialized countries and from 14 percent to 4-1/2 percent in emerging markets, according to the most recent statistics from the International Monetary Fund. Not long ago, annual inflation rates in Brazil and Mexico at times exceeded 100 percent. But during the past decade, Brazilian and Mexican inflation rates have remained low. In particular, inflation in Brazil did not spike up after its financial crises and sharp currency depreciations in the late 1990s. Given Brazil’s history of hyperinflation, this stability is especially remarkable. Brazil did experience a small spike of inflation around its presidential election in 2002, but even this was minor by historical standards. The pattern of low inflation is seen across many countries, large and small.\n\nA few years ago I did some research that showed how inflation rates around the world have fallen significantly since the 1970s and 1980s, both in terms of averages and medians.6 Indeed, the IMF’s April 2006 World Economic Outlook notes that average inflation rates in both the industrial countries and the developing countries in recent years are at their lowest levels since at least the early 1970s. More important, I found that the worst inflation performers (specifically the 10 percent of the countries of the world experiencing the highest inflation) had much lower inflation rates than the worst performers from the 1970s, 1980s, and 1990s. Thus, the worst behavior is not as bad as it once was.\n\nDo markets expect low inflation to persist in the long run? To answer this question, we can look at measures of expected inflation. Consensus Economics surveys hundreds of professional forecasters in numerous countries each April. The surveys allow us to examine forecasts of inflation around the world six to ten years ahead beginning in 1996. The latest observation, in April 2006, for example, is the forecast of a given country’s average consumer price inflation rate from 2012 through 2016. For both a representative sample of industrial economies (Euro area, Japan, the United States, and the United Kingdom) and emerging-market economies (Brazil, China, Korea, and Mexico), we observe substantial declines from the late 1990s to today. These forecasts have been low and stable in both industrial and many important developing countries in recent years. The surveys thus provide one indication that markets do expect low inflation to persist.\n\nThe volatility of inflation has also declined notably, suggesting that perceived inflation risk may have declined as well. For the industrial countries, inflation volatility (measured as a twenty-quarter rolling standard deviation of consumer price inflation) has declined from the 1980s to the 1990s to the period since 2000. Although it has since drifted up just a bit due to volatility in oil prices, it remains at or near its lowest level in the last quarter-century. For the emerging markets, the decline in volatility is even more dramatic. Brazil, in particular, was off the chart much of the time before the late 1990s. Volatility of inflation in China, Korea, and Mexico is now at levels similar to those of the industrial countries, and volatility in Brazil is not much higher.\n\nOverall, the combination of lower and less volatile inflation around the world has led to a reduction in inflation expectations and lower perceived inflation risk, hence a lower inflation uncertainty premium in long rates. I believe that these factors have been important contributors to the lower long-term yields and the flattening of yield curves, particularly in emerging markets. The existence of markets for long-term nominal government and corporate debt is powerful evidence of the faith that investors place in a future environment of price stability.\n\nFactors Behind the Global Move to Price Stability\nFour broad factors lie behind the move to price stability, especially in emerging markets, and these factors tend to reinforce each other. Each factor affects the cost-benefit tradeoff of pursuing a high-inflation policy.\n\nThe first factor, which gets surprisingly little attention in my view, is financial innovation that alters the ability and incentive of a government to pursue a high-inflation policy.7 I put the innovations into two main categories, developments in information technology and physical dollarization, both of which effectively increase potential competition among currencies. Financial innovations make it easier for citizens to move their assets out of the local currency should their government resort to an inflation tax. The dollarizations that followed the high-inflation episodes in Latin America and the former Soviet Union, for example, significantly reduced the costs of switching away from a local currency for small-value transactions. The specific channels by which financial innovations could have affected competition among currencies are many:\n\nGiven these innovations, a government that pressures a central bank to pursue an inflationary policy gets much less benefit for each unit increase in inflation because people can more easily switch out of the local currency. In other words, the inflation tax becomes much more difficult and costly to levy because citizens can more easily avoid the tax by using an alternative money.\n\nThe second and closely related factor behind disinflation is deregulation and competition in a globalized marketplace. The collapse of the centrally-planned economies has led many countries to turn increasingly to private markets to deliver growth and progress and reduce the role of government. Technology has helped to increase global competition by shrinking the barriers of time and distance. Again, there are several channels by which globalization and competition may have affected the cost-benefit tradeoff in pursing inflation:\n\nThe third factor is that economists and the public have learned from painful experience about the costs of inflation.9 The end of the Bretton Woods gold standard in the early 1970s was associated with the first global and sustained peacetime inflation in history. Although the specific experiences differed across countries, public opinion eventually turned strongly against allowing inflation to continue, and policymakers responded to this pressure by taking stronger measures to achieve price stability. This learning process helped to drive some of the financial innovations that I discussed earlier, which, in turn, helped households and businesses to economize on holding inflationary assets. Economists and central bankers also devoted great attention to understanding the causes and consequences of inflation, providing the intellectual underpinning to policies oriented toward price stability.\n\nThe fourth factor I wish to mention relates to changes in the institutions of central banking that may have increased the costs of pursuing high-inflation policies. The most notable change is the increased independence of many central banks and the corresponding reduced control of the fiscal authorities over monetary policy. Central bank independence reduces the ability of a government to \"raid the cookie jar\" through a surprise inflation tax. In most cases, central bank independence can be reversed by a majority vote of parliament. But having to resort to such a vote is a greater obstacle to inflationary finance than previous arrangements allowed, especially given the public’s increased sensitivity and aversion to inflation.\n\nCentral bank independence has typically been granted in conjunction with an explicit mandate that makes achieving low and stable inflation one of the goals of monetary policy. Central bank independence with a mandate that includes price stability increases the credibility of monetary policy with regard to achieving low inflation. Policy is credible because the central bank’s objectives are clear to the public and the central bank can be held accountable for failing to achieve its objectives.\n\nWhen citizens are more aware of the costs of inflation and when governments would reap lower benefits from a high-inflation policy, institutional reforms that will make central banks more credible and independent may be more likely to be adopted and sustained.10 The fundamental forces I mentioned earlier--financial innovation, deregulation, globalization, and public understanding about the costs of inflation--provided the impetus for fighting inflation and opened the political path to institutional reforms, such as central bank independence, that enhance central bank credibility. Once in place, these reforms made further progress against inflation easier and raised the costs of backsliding. As the benefits of stable prices accrue and as financial markets deepen and become more sophisticated, the benefits of sound economic policies will help to create support for institutional reforms that make returning to inflation harder for future governments.\n\nBenefits of Price Stability\nWhile it is well known that low and stable inflation improves the environment for investment planning and avoids many costs and disruptions associated with frequent price adjustments, I want to focus on a few of the many benefits that are particularly relevant for emerging markets.\n\nPrice stability boosts growth through deepening financial markets. With stable prices, savers and investors have more confidence about the ultimate value of their deposits and loans. Stable prices encourage the growth of financial intermediaries and financial markets. As noted above, many emerging markets have recently experienced a deepening of their local financial markets with greater issuance of longer-dated paper. According to numerous studies, there is a strong link between financial market development and economic growth. Thus, the greater credibility of central banks that permits more development of the local markets can have an economic benefit beyond the financial sector.11\n\nThe development of long-term local-currency bond markets may also help governments and firms plan long-term infrastructure and investment projects that boost economic development. Although such debt markets are only one of many factors that can lower the costs of long-term planning and enhance the ability to undertake long-term investments, the development of these markets, particularly when accompanied by lower real rates, help to support longer-horizon projects and reduce the effect of foreign exchange movements on such activities.\n\nA better fiscal outlook, which might arise from higher and more stable growth as well as better long-term planning, also increases financial market confidence and development and thus further boosts growth and reinforces prospects for continued price stability. This virtuous cycle appears to be happening in key emerging markets that were long plagued with poor fiscal situations, such as Brazil and Mexico. In the 1980s and early 1990s, for example, public sector deficits in these countries often exceeded 10 percent of GDP. Since the late 1990s, deficits have been diminished.\n\nMaintaining this Progress\nAlthough I am an optimist, I would be remiss if I did not point out some risks to this otherwise rosy scenario. The difficulty of reaching agreement in the Doha Round of trade negotiations highlights the risk of renewed protectionism. Trade barriers reduce both domestic and international competition, one of the key factors behind low inflation, and make all countries poorer. Barriers to free flow of goods, services, and capital would also diminish the force of other factors outlined above that help to reduce inflationary pressures.\n\nWe must not forget the examples of high inflation and hyperinflation from the past: They hold important lessons about the costs of not maintaining price stability. That sound policies are the basis for solid economic growth should not be forgotten.\n\nFootnotes\n\n1.  Alan Greenspan (2005), statement before the Senate Committee on Banking, Housing, and Urban Affairs, presenting the Federal Reserve Board’s \"Monetary Policy Report to the Congress,\" February 16. Return to text\n\n2.  Strictly speaking, this calculation requires the use of zero-coupon bonds, but it can be approximated using coupon securities. Return to text\n\n3.  Don H. Kim and Jonathan H. Wright (2005), \"An Arbitrage-Free Three-Factor Term Structure Model and the Recent Behavior of Long-Term Yields and Distant-Horizon Forward Rates,\" Finance and Economics Discussion Series 2005-33 (Washington: Board of Governors of the Federal Reserve System, August). Return to text\n\n4.  Typically we calculate an \"instantaneous\" forward rate, which is the limiting value of a sequence of forward rates with maturities declining toward zero. Return to text\n\n5.  Ben S. Bernanke (2005), \"The Global Saving Glut and the U.S. Current Account Deficit,\" Sandridge Lecture at the Virginia Association of Economists, March 10. Return to text\n\n6.  Randall S. Kroszner (2003), \"Currency Competition in the Digital Age,\" in David E. Altig and Bruce D. Smith, eds., Evolution and Procedures in Central Banking (New York: Cambridge University Press), pp. 275–99. Return to text\n\n7.  I discussed aspects of this factor in my presentation at a May 2001 conference at the Federal Reserve Bank of Cleveland, cited above.  Return to text\n\n8.  Kenneth S. Rogoff (2003), \"Globalization and Global Disinflation,\" in Monetary Policy and Uncertainty: Adapting to a Changing Economy: A Symposium (Federal Reserve Bank of Kansas City, Aug. 28–30), pp. 77–112. Return to text\n\n9.  This hypothesis was raised in the discussion of Rogoff (2003). See Guillermo Ortíz, chair, \"General Discussion: Globalization and Global Disinflation,\" in Monetary Policy and Uncertainty: Adapting to a Changing Economy: A Symposium (Federal Reserve Bank of Kansas City, Aug. 28–30), pp. 119–130. For evidence that voters in Latin America have punished politicians for bad inflation outcomes in recent years, see Eduardo Lora and Mauricio Oliveira (2005), \"The Electoral Consequences of the Washington Consensus,\" Economía, vol. 5 (Spring), pp. 1–61. Return to text\n\n10.  In a paper with Douglas Irwin, I documented a similar dynamic at work in the gradual reversal of protectionist policies in the United States in the 1930s and 1940s. See Douglas A. Irwin and Randall S. Kroszner (1999), \"Interests, Institutions, and Ideology in Securing Policy Change: The Republican Conversion to Trade Liberalization after Smoot-Hawley,\" Journal of Law and Economics, vol. 42 (October), pp. 643–73. Return to text\n\n11.  See Ross Levine (2005), \"Finance and Growth: Theory and Evidence,\" Philippe Aghion and Steven Durlauf, eds., Handbook of Economic Growth (New York: Elsevier); and Randall S. Kroszner and Philip E. Strahan (2006), \"Regulation and Deregulation of the U.S. Banking Industry: Causes, Consequences, and Implications of the Future,\" unpublished paper. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20060615a.htm",
        "title": "Why Are Yield Curves So Flat and Long Rates So Low Globally?",
        "date": "6/15/2006"
    },
    {
        "content": "June 15, 2006\n\nChairman Ben S. Bernanke\n\nBefore the Economic Club of Chicago, Chicago, Illinois\n\nIn my remarks today, I would like to discuss the relationship between energy markets and the economy. As I am certain all of you are aware, the steep increases in energy prices over the past several years have had significant consequences for households, businesses, and economic policy. At least since the time of the first oil shock in October 1973, economists have struggled to understand the ways that disturbances to the supply and demand balance in energy markets influence economic growth and inflation. At the most basic level, oil and natural gas are just primary commodities, like tin, rubber, or iron ore. Yet energy commodities are special, in part because they are critical inputs to a very wide variety of production processes of modern economies. They provide the fuel that drives our transportation system, heats our homes and offices, and powers our factories. Moreover, energy has an influence that is disproportionate to its share in real gross domestic product (GDP) largely because of our limited ability to adjust the amount of energy we use per unit of output over short periods of time. Over longer periods, energy consumption can be altered more easily by, for example, adjusting the types of vehicles that we drive, the kind of homes that we build, and the variety of machines that we buy. Those decisions, in turn, influence the growth and composition of the stock of capital and the productive capacity of the economy.\n\nOver the past thirty-five years, the U.S. economy has experienced some wide swings in energy prices. The oil price increases of the 1970s were followed by price declines in the mid-1980s and then a price spike in 1990, with numerous fluctuations since then. From the mid-1980s until fairly recently, market participants tended to look through these price cycles and did not allow their longer-term expectations for oil prices to be greatly affected by short-run swings in spot prices. But beginning around 2003, futures prices began moving up roughly in line with the rise in spot prices. Thus, unlike in earlier episodes, the significantly higher relative price of energy that we are now experiencing is expected to be relatively long lasting and thus will likely prompt more-significant adjustments by households and businesses over time.\n\nThis higher relative price of energy poses many important questions for economists and policymakers. Why have the prices of oil and natural gas risen so much? What is the outlook for energy supplies and prices in the medium term and in the long term? And what implications does the behavior of energy prices have for the ongoing economic expansion and inflation? I will touch briefly on each of these questions.\n\nDevelopments in Oil Markets\nLet me begin with the market for crude oil. What accounts for the behavior of the current and expected future prices of petroleum? Supply and demand are among the most valuable concepts in the economist's toolkit, and I believe they are the key to understanding recent and prospective developments in oil markets. For the most part, high oil prices reflect high and growing demand for oil and limited and uncertain supplies.\n\nOn the demand side, world oil consumption surged 4 percent in 2004 after rising a solid 2 percent in 2003. The rise in 2004 was much larger than had been expected and was, in fact, the largest yearly increase in a quarter-century. A significant part of the unexpected increase in oil consumption that year reflected rapidly growing oil use in the United States and East Asia, notably China. In 2005, growth of world oil consumption slowed to 1.3 percent, partly reflecting the restraining effects of higher prices. Nonetheless, the level of oil consumption was still high relative to earlier expectations. Thus far this year, underlying demand pressures have remained strong in the context of a global economy that has continued to expand robustly.\n\nOn the supply side, the production of oil has been constrained by available capacity, hurricanes, and geopolitical developments. In 2003 and 2004, as oil consumption and prices rose briskly, Saudi Arabia and other members of the Organization of the Petroleum Exporting Countries (OPEC) pumped more oil. OPEC was able to boost production relatively quickly in response to changing market conditions by utilizing productive capacity that had been idle. By the end of 2004, however, OPEC's spare production capacity was greatly diminished. As a consequence, OPEC's oil production flattened out over the past year even as oil prices continued to soar.\n\nOil production outside OPEC also leveled off last year, contrary to earlier expectations for continued growth. This development in part reflected the devastating effects of last year's hurricanes. Katrina and Rita were enormously disruptive for our nation's production of energy. At the worst point, 1.5 million barrels per day of crude oil were shut in, virtually all of the U.S. production in the Gulf of Mexico and nearly 2 percent of global oil production. Recovery of oil production in the Gulf has been slow, and the disruptions from last year's storms linger even as we enter this year's hurricane season. The cumulative loss in oil production attributable to Katrina and Rita amounts to more than 160 million barrels of oil, a figure equivalent to nearly half the present level of commercial crude oil inventories in the United States.\n\nWith the background of strong demand and limited spare capacity, both actual production disruptions and concerns about the reliability and security of future oil supplies have contributed to the volatility in oil prices. The oil-rich Middle East remains an especially unsettled region of the world, but political risks to the oil supply have also emerged in nations outside the Middle East, including Russia, Venezuela, and Nigeria.\n\nCompounding these difficulties in markets for crude oil have been constraints and disruptions in the refining sector of the energy industry. In the wake of Hurricane Rita, one-quarter of domestic refining capacity was offline, and here, too, the period of recovery has been protracted. Even before last year's hurricanes, however, a mismatch appeared to be emerging between the incremental supply of crude oil, which tended to be heavy and sulfurous, and the demand by refiners for light, sweet crude, which can be converted more easily into clean-burning transportation fuels. These developments have highlighted the need for additional investments in refining capacity to bridge the gap between upstream supply and final demand.\n\nWhat about the longer term? We can safely assume that world economic growth, together with the rapid pace of industrialization in China, India, and other emerging-market economies, will generate increasing demand for oil and other forms of energy. In all likelihood, growth in the demand for energy will be tempered to some extent by continued improvements in energy efficiency which, in turn, will be stimulated by higher prices and ongoing concerns about the security of oil supplies. Such improvements are possible even without technological breakthroughs. For example, Japan is an advanced industrial nation that uses only about one-half as much energy to produce a dollar's worth of real output as the United States does. Of course, the Japanese and U.S. economies differ in important ways, but the comparison nevertheless suggests that there is scope to boost energy efficiency in the United States and other parts of the industrialized world. Newly industrializing economies such as China appear to be quite inefficient in their use of energy; but as they modernize, they can adopt energy-saving techniques already in use elsewhere, and their energy efficiency will presumably improve as well.\n\nStill, as the global economic expansion continues, substantial growth in the use of oil and other energy sources appears to be inevitable. How readily the supply side of the oil market will respond is difficult to predict. In a physical sense, the world is not in imminent danger of running out of oil. At the end of 2005, the world's proved reserves of conventional oil--that is, oil in the ground that is viewed as recoverable using existing technologies and under current economic conditions--stood at more than 1.2 trillion barrels, about 15 percent higher than the world's proved reserves a decade earlier and equal to about four decades of global consumption at current rates. These figures do not include Canada's vast deposits of oil sands, which are estimated to contain an additional 174 billion barrels of proved reserves. In addition, today's proved reserve figures ignore not only the potential for new discoveries but also the likelihood that improved technologies and higher oil prices will increase the amount of oil that can be economically recovered.\n\nThe oil is there, but whether substantial new sources of production can be made available over the next five years or so is in some doubt. Some important fields are in locations that are technically difficult and time-consuming to develop, such as deep-water fields off the coast of West Africa, in the Gulf of Mexico, or off the east coast of South America. In many cases, the development of new fields also faces the challenge of recovering the oil without damaging delicate ecosystems. Perhaps most troubling are the significant uncertainties generated by geopolitical instability, as I have already noted. Much of the world's oil reserves are located in areas where political turmoil and violence have restrained both production and investment.\n\nIn both the developed and the developing world, another factor holding back investment in oil infrastructure has been concern on the part of producers that oil prices might fall back as they did in the 1980s and 1990s. In light of that recognition, some oil producers have been reluctant to launch exploration projects even with today's high prices. Such concerns have been reinforced by the huge reserves of oil in several OPEC countries that could be extracted at very low cost if sufficient resources and expertise were directed toward doing so.\n\nDevelopments in the Natural Gas Market\nThe story for natural gas shares some similarities with the story for oil, but there are important differences as well. In the 1990s, the U.S. spot price of natural gas at the Henry Hub averaged about $2 per million Btu. However, in recent years, the United States has seen a marked increase in the price of natural gas. The average spot price climbed to nearly $9 per million Btu in 2005, with the price spiking to $15 per million Btu following hurricanes Katrina and Rita. So far this year, natural gas prices have fallen back to around $7 per million Btu as an unusually warm winter curtailed consumption and boosted natural gas in storage to record levels. Futures markets currently anticipate that the price of natural gas will be about $9 per million Btu next year.\n\nWhy have natural gas prices risen so sharply over the past few years, and why are they expected to remain elevated? As with oil, high prices of natural gas reflect strong demand and diminished supplies. Unlike the globally integrated market for oil, however, natural gas markets are regional, primarily because of the difficulty in transporting gas by means other than pipelines. Although the world's capacity to trade liquefied natural gas, which is transported by ships, is growing, it is still a small fraction of world supply and is not yet sufficient to fully integrate natural gas markets across continents. Demand for natural gas in North America has remained strong in recent years, particularly as environmental concerns have led clean-burning natural gas to become the fuel of choice for new electricity generation. Moreover, increases in oil prices have boosted the demand for energy substitutes such as natural gas. However, domestic production of natural gas has not kept up. Last year, U.S. production was 7 percent below its 2001 level, with less than half of that decline reflecting the impact of hurricanes Katrina and Rita.\n\nIncreased trade can often mitigate price increases, but net imports of natural gas from Canada, which currently account for around 16 percent of U.S. consumption, have failed to increase in response to higher prices. Between 1988 and 2001, net imports from Canada tripled, but they have since flattened out. Both U.S. and Canadian gas fields have matured and are yielding smaller increases in output, despite the incentive of high prices and a substantial increase in the number of drilling rigs in operation.\n\nTrade in liquefied natural gas, or LNG, is also likely to increase over time, but perhaps at a slower pace than once envisioned. LNG imports into the United States nearly tripled from 2002 to 2004, but they actually fell a bit last year as production disruptions in a number of countries limited supply and as consumers in other countries competed for available cargoes.\n\nThus, natural gas prices are likely to remain elevated for at least the coming few years. It is possible, however, that within a decade new supplies from previously untapped areas of North America could boost available output here, while imports of LNG will increase to more substantial levels as countries seek to bring their isolated natural gas reserves to market. Given time, these developments could serve to lower natural gas prices in the United States significantly. Nonetheless, because of the higher costs of producing these supplies relative to the traditional sources of natural gas, as well as the elevated cost of other energy sources such as oil, natural gas prices seem unlikely to return to the level of the 1990s.\n\nThus, the supply-demand fundamentals seem consistent with the view now taken by market participants that the days of persistently cheap oil and natural gas are likely behind us. The good news is that, in the longer run, we have options. I have already noted the scope for improvements in energy efficiency and increased conservation. Considerable potential exists as well for substituting other energy sources for oil and natural gas, including coal, nuclear energy, and renewable sources such as bio-fuels and wind power. Given enough time, market mechanisms are likely to increase energy supplies, including alternative energy sources, while simultaneously encouraging conservation and substitution away from oil and natural gas to other types of energy.\n\nEconomic and Policy Implications of Increased Energy Prices\nWhat are the economic implications of the higher energy prices that we are experiencing? In the long run, higher energy prices are likely to reduce somewhat the productive capacity of the U.S. economy. That outcome would occur, for example, if high energy costs make businesses less willing to invest in new capital or cause some existing capital to become economically obsolete. All else being equal, these effects tend to restrain the growth of labor productivity, which in turn implies that real wages and profits will be lower than they otherwise would have been. Also, the higher cost of imported oil is likely to adversely affect our terms of trade; that is, Americans will have to sell more goods and services abroad to pay for a given quantity of oil and other imports. For the medium term at least, the higher bill for oil imports will increase the U.S. current account deficit, implying a greater need for foreign financing.\n\nUnder the assumption that energy prices do not move sharply higher from their already high levels, these long-run effects, though clearly negative, appear to be manageable. The U.S. economy is remarkably flexible, and it seems to have absorbed the cost shocks of the past few years with only a few dislocations. And conservation and the development of alternative energy sources will, over the long term, ameliorate some of the effects of higher energy prices. Moreover, ongoing productivity gains arising from sources such as technological improvements are likely to exceed by a significant margin the productivity losses created by high energy prices.\n\nIn the short run, sharply higher energy prices create a rather different and, in some ways, a more difficult set of economic challenges. Indeed, a significant increase in energy prices can simultaneously slow economic growth while raising inflation.\n\nAn increase in oil prices slows economic growth in the short run primarily through its effects on consumer spending. Because the United States imports much of the oil that it consumes, an increase in oil prices is, as many economists have noted, broadly analogous to the imposition of a tax on U.S. residents, with the revenue from the tax going to oil producers abroad. In 2004 as a whole, the total cost of imported oil increased almost $50 billion relative to 2003. The imported oil bill jumped again last year by an additional $70 billion, and given the price increases we have experienced in 2006, it appears on track to increase $50 billion further at an annual rate in the first half of this year. Coupled with the rising cost of imported natural gas, the cumulative increase in imported energy costs since the end of 2003 is shaping up to be $185 billion--equal to almost 1-1/2 percent of GDP. All else being equal, this constitutes a noticeable drag on real household incomes and spending. It is a tribute to the underlying strength and resiliency of the U.S. economy that it has been able to perform well despite the drag from increased energy prices.\n\nAt the same time that higher oil prices slow economic growth, they also create inflationary pressures. Higher prices for crude oil are passed through to increased prices for the refined products used by consumers, such as gasoline and heating oil. When oil prices rise, people may try to substitute other forms of energy, such as natural gas, leading to price increases in those alternatives as well. The rise in prices paid by households for energy--for example for gasoline, heating oil, and natural gas--represent, of course, an increase in the cost of living and in price inflation. This direct effect of higher energy prices on the cost of living is sometimes called the first-round effect on inflation. In addition, higher energy costs may have indirect effects on the inflation rate--if, for example, firms pass on their increased costs of production in the form of higher consumer prices for non-energy goods or services or if workers respond to the increase in the cost of living by demanding higher nominal wages. A jump in energy costs could also increase the public's longer-term inflation expectations, a factor that would put additional upward pressure on inflation. These indirect effects of higher energy prices on the overall rate of inflation are called second-round effects.\n\nThe overall inflation rate reflects both first-round and second-round effects. Economists and policymakers also pay attention to the so-called core inflation rate, which excludes the direct effects of increases in the prices of energy (as well as of food). By stripping out the first-round inflation effects, core inflation provides a useful indicator of the second-round effects of increases in the price of energy.\n\nIn the past, notably during the 1970s and early 1980s, both the first-round and second-round effects of oil-price increases on inflation tended to be large, as firms freely passed on rising energy costs to consumers, workers reacted to the surging cost of living by ratcheting up their wage demands, and longer-run expectations of inflation moved up quickly. In this situation, monetary policymaking was extremely difficult because oil-price increases threatened to result in a large and persistent increase in the overall inflation rate. The Federal Reserve attempted to contain the inflationary effects of the oil-price shocks by engineering sharp increases in interest rates, actions which had the consequence of sharply slowing growth and raising unemployment, as in the recessions that began in 1973 and 1981.\n\nSince about 1980, however, the Federal Reserve and most other central banks have worked hard to bring inflation and expectations of inflation down. An important benefit of these efforts is that the second-round inflation effect of a given increase in energy prices has been much reduced. To the extent that households and business owners expect that the Fed will keep inflation low, firms have both less incentive and less ability to pass on increased energy costs in the form of higher prices, and likewise workers have less incentive to demand compensating increases in their nominal wages.\n\nAs I noted in remarks last week, although the rate of pass-through of higher energy and other commodity prices to core consumer price inflation appears to have remained relatively low in the current episode--reflecting the inflation-fighting credibility built by the Fed in recent decades the cumulative increases in energy and commodity prices have been large enough that they could account for some of the recent pickup in core inflation. In addition, some survey-based measures of longer-term inflation expectations have edged up, on net, in recent months, as has the compensation for inflation and inflation risk implied by yields on nominal and inflation-indexed government debt. As yet, these expectations measures have remained within the ranges in which they have fluctuated in recent years and inflation compensation implied by yields on government debt has fallen back somewhat in the past month. Nevertheless, these developments bear watching.\n\nIn conclusion, energy prices have moved up considerably since the end of 2002, reflecting supply and demand factors. In the short run, prices are likely to remain high in an environment of strong world economic growth and a limited ability to increase energy supplies. Moreover, prices are likely to be volatile in the near term, given the small margins of excess capacity to produce crude oil or natural gas that traditionally have buffered short-run shifts in supply and demand.\n\nHowever, in the long run, market forces will respond. The higher relative prices of energy will create incentives for businesses to create new, energy-saving technologies and for energy consumers to adopt them. The market for alternative fuels is growing rapidly and will help to shift consumption away from petroleum-based fuels. Government can contribute to these conservation efforts by working to create a regulatory environment that encourages the growth in energy supplies in a manner that is consistent with our nation's environmental and other objectives. Given the extraordinary resilience of the U.S. economy, I am confident our nation will be up to this challenge.",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060615a.htm",
        "title": "Energy and the Economy",
        "date": "6/15/2006"
    },
    {
        "content": "June 14, 2006\n\nGovernor Susan Schmidt Bies\n\nAt the Mortgage Bankers Association Presidents Conference, Half Moon Bay, California\n\nGood morning. It is a pleasure to be here today, and I thank you for the invitation. It almost goes without saying that over the past several years, residential housing markets have been attracting considerable attention, and they have been a strong contributor to the overall growth of the U.S. economy. Today I would like to offer some thoughts on the current state of both residential and commercial mortgage markets and then discuss some ways in which U.S. bank supervisors are trying to ensure that bank lending in those markets is safe and sound.\n\nConditions in Real Estate Markets\nResidential Real Estate Markets\nActivity in U.S. housing markets is slowing. Incoming data point to a decided, but so far moderate, cooling. Starts of single-family houses fell appreciably in March and April. Because construction had been spurred in the preceding months by unusually mild weather, some slowing in the spring was natural. However, the level of housing starts lately has fallen below not just the elevated winter pace but also the pace of last fall. Indeed, construction permit issuance for single-family homes, which is less affected by the weather, has been declining since September. Sales of new and existing homes have dropped noticeably from their highs of last year. In addition, inventories of unsold homes have increased, and your own MBA index of loan applications for home purchases has trended lower in recent months.\n\nAlthough a slowdown in housing activity is apparent in a wide range of indicators, it seems to be occurring in a gradual way. Notwithstanding their recent slip, both home construction and home sales are still at relatively high levels. The underlying fundamentals of housing demand also remain favorable. Real disposable income is growing at a solid pace. In the aggregate, household balance sheets appear to be in a good position, even though risks clearly exist for some households. Long-term mortgage rates, although up substantially from last summer's level, remain low relative to their historical experience.\n\nThe latest data on house prices from the Office of Federal Housing Enterprise Oversight suggest that house-price appreciation has moderated but that prices in the aggregate continue to move up. Between the first quarter of 2005 and the first quarter of this year, the price index for existing homes sold in repeat transactions increased 10 percent--a solid gain but down a bit from the record pace for the period mid-2004 to mid-2005. Focusing on the first quarter of 2006 alone, prices increased at an annual rate of 7.3 percent. However, quarterly figures should be treated with a good degree of caution given the volatility of this data series.\n\nGiven the slowing conditions in the housing market, spending for the construction of new housing is unlikely to be an important direct source of overall GDP growth this year, after having contributed close to ½ percentage point last year. In addition, the slackening of house-price appreciation could hold back growth in consumption spending through the so-called wealth effect, or the effect that lower overall housing wealth has on consumption. Estimates from various econometric models of consumer spending suggest that each dollar of change in wealth is associated with a change in consumption of approximately 3½ cents, with roughly half of the effect occurring within a year.\n\nOf course, these consumption estimates are just that--estimates. Beyond the usual issues of measurement and interpretation associated with any statistical estimate, one can easily point to some specific risks in estimating how housing wealth affects spending. First, econometric modeling has had difficulty distinguishing between the effects of movements in housing wealth and movements in other components of household balance sheets, even though housing may be a unique asset in a number of ways. Thus, the estimate of change in consumption that I cited is based on the historical relationship between spending and changes in overall wealth. Changes in housing wealth may have a somewhat different effect. Second, the linkages between housing wealth and consumption may change over time. For example, these linkages may be stronger now than in the past because financial innovation has made it easier and less costly for households to tap their accumulated housing equity. Third, a pronounced deceleration in house prices could have an outsized effect on consumer confidence, and such a decline in confidence could be an additional damping force on consumption.\n\nAnother housing-related issue that bears watching is mortgage debt accumulation. Since the end of 2002, home mortgage debt outstanding has risen about 50 percent. The increase has substantially pushed up homeowners' mortgage payments in relation to their income, in spite of historically low mortgage rates, the growing use of interest-only mortgages, and the lengthening of average loan maturities over the past few years. That said, homeowners appear to be able to manage these higher payments: we have seen only a little deterioration in mortgage credit quality as yet, and overall delinquency rates remain low. Going forward, I expect aggregate homeowner mortgage payments to continue to rise, especially as adjustable-rate mortgages reach their initial reset dates. However, these reset adjustments are expected to be gradual, and only a modest number of outstanding mortgages are expected to reset during 2006 and 2007. To date, consumers appear to be managing changes in their mortgage payments quite well.\n\nOne area of potential concern relates to the portion of home sales accounted for by investors, as opposed to owner-occupants. Historically, only around 5 percent of U.S. homes were purchased each year by investors; in 2005, it appears that figure was considerably higher. In many cases, investors purchased homes because they believed prices were going to rise further, not necessarily because they wanted to retain the property over time for rental income. As prices level off or even decline, it will be important to see whether this investor activity subsides significantly, and if so, the impact on mortgage markets more broadly.\n\nCommercial Real Estate Markets\nIn addition to monitoring residential markets, the Federal Reserve keeps a close watch on developments in commercial real estate markets. Overall, conditions in this sector appear to be improving. Demand for commercial space has been growing moderately, while the construction of new space has generally remained tame, restrained in part by steep land prices and high construction costs. The result has been a widespread decline in vacancy rates over the past few years. Reflecting this improved balance between demand and supply, rents on commercial properties have been increasing after a prolonged period of softness.\n\nAlthough the level of commercial construction remains well below its peak in 2000, the latest data indicate what may be the beginning of a pickup. Census Bureau data on nonresidential construction put in place suggest that real spending rose in April for the sixth consecutive month. Leading indicators of commercial construction spending, such as billings by architectural firms for design work, point to further increases in activity in coming months. An upturn in commercial construction could offset part of what is anticipated to be a waning contribution to GDP growth from the housing sector.\n\nThe performance of commercial real estate loans has generally been very good, due in large part to the low interest rates of recent years and the substantial appreciation of property values that has resulted in sizable equity positions for building owners. Delinquency rates on loans held by commercial banks and life insurance companies remain low by historical standards, and delinquencies on commercial mortgage-backed securities have reversed the modest increase that occurred from 2000 to 2003. The latest information on property prices hints at some moderation in price increases in the first quarter from the rapid price appreciation of last year. But, as in the housing market, commercial real estate prices are continuing to rise in the aggregate.\n\nRecent Supervisory Guidance Relating to Real Estate Lending\nThe Federal Reserve will continue to monitor developments in the residential and commercial real estate markets very closely. In addition to scrutinizing the effect of these developments on the economy, we are also, in our role as bank supervisors, monitoring banks' mortgage lending practices. Last year, the federal bank regulatory agencies issued draft guidance on both residential and commercial mortgage lending. The agencies have received many comments on the proposed guidance, including comments from your association, which we will consider as we discuss what steps to take next. I will address the guidance on residential mortgage lending first.\n\nNontraditional Mortgage Products\nOver the past few years, the agencies have observed an increase in the number of residential mortgage loans that allow borrowers to defer repayment of principal and, sometimes, interest. These loans, often referred to as nontraditional mortgage loans, include interest-only (IO) mortgage loans, for which the borrower pays no loan principal for the first few years of the loan, and payment-option adjustable-rate mortgages (option ARMs), for which the borrower has flexible payment options--and which could result in negative amortization.\n\nIOs and option ARMs are estimated to have accounted for almost one-third of all U.S. mortgage originations in 2005, compared with fewer than 10 percent in 2003. Despite their recent growth, however, it is estimated that these products still account for less than 20 percent of aggregate domestic mortgages outstanding of nearly $9 trillion. Although the credit quality of residential mortgages generally remains strong, the Federal Reserve and the other banking supervisors are concerned that banks' current risk-management techniques may not fully address the level of risk inherent in nontraditional mortgages, a risk that would be heightened by a downturn in the housing market.\n\nMortgages with some of the characteristics of nontraditional mortgage products have been available for many years; however, they have historically been offered to higher-income borrowers. More recently, nontraditional mortgages have been offered to a wider spectrum of consumers, including subprime borrowers, who may be less suited for these types of mortgages and may not fully recognize their embedded risks. These borrowers are more likely to experience an unmanageable payment shock during the life of the loan, meaning that they may be more likely to default on the loan. Further, nontraditional mortgage loans are becoming more prevalent in the subprime market at the same time risk tolerances in the capital markets have increased. Banks need to be prepared for the resulting impact on liquidity and pricing if and when risk spreads return to more \"normal\" levels and competition in the mortgage banking industry intensifies.\n\nSupervisors have also observed that lenders are increasingly combining nontraditional mortgage loans with weaker mitigating controls on credit exposures--for example, by accepting less documentation in evaluating an applicant's creditworthiness and not evaluating the borrower's ability to meet increasing monthly payments when amortization begins or when interest rates rise. These \"risk layering\" practices have become more and more prevalent in mortgage originations. Thus, although some banks may have used some elements of nontraditional mortgage products successfully in the past, the recent easing of traditional underwriting controls and the sale of nontraditional products to subprime borrowers may contribute to losses on these products.\n\nSupervisors are concerned that banks may not be fully aware of the potential risks of using risk-layering practices with nontraditional mortgage products. These practices may have become more widespread over the past couple of years as competition for borrowers and declining profit margins may have forced lenders to loosen their credit standards to maintain their loan volume. In the Federal Reserve Board's most recent Senior Loan Officer Survey, conducted this past April, more than 10 percent of the surveyed institutions reported having eased their underwriting standards for residential mortgage loans. Only one of the surveyed lenders reported having tightened standards. Additionally, information from other sources seems to show continued growth in the number of borrowers purchasing real estate with no equity using simultaneous second liens.\n\nNaturally, we are watching for any signs that defaults may be on the rise. Some industry evidence indicates that delinquencies may be on the uptick; delinquency rates for loans issued in 2005 are, in most cases, higher than those for comparable loans issued in earlier years. Some industry observers believe that the increase in delinquencies for loans issued in 2005 is directly related to the continued easing of underwriting standards and the increased use of risk-layering practices.\n\nThe industry trends I have just described, taken together, were the justification for the issuance of draft guidance on nontraditional mortgage products by the Federal Reserve and the other banking agencies. The proposed guidance emphasizes that an institution's risk-management processes should allow it to adequately identify, measure, monitor, and control the risk associated with these products. It reminds lenders of the importance of assessing a borrower's ability to repay the loan, both now and when amortization begins and interest rates rise. Nontraditional mortgage products warrant a bank having strong risk-management standards as well as appropriate capital and loan-loss reserves. Further, bankers should consider the impact of prepayment penalties for ARMs. Lenders should provide enough information so that borrowers clearly understand, before choosing a product or payment option, the terms of and risks associated with these loans, particularly the extent to which monthly payments may rise and negative amortization may increase the amount owed above the amount originally borrowed. Lenders should recognize that certain nontraditional mortgage loans are untested in a stressed environment; for instance, nontraditional mortgage loans to investors that rely on collateral values could be particularly affected by a housing-price decline. As noted, investors have represented an unusually large share of recent home purchases. Past loan performance has indicated that investors are more likely than owner-occupants to default on a loan when housing prices decline.\n\nWhen credit standards are eased and risks are layered, institutions should compensate for the increased risk with mitigating factors that support the underwriting decision. Among other credit enhancements, these factors generally include requiring borrowers to have higher credit scores, lower loan-to-value and debt-to-income ratios, and significant liquidity and net worth. Finally, lenders should establish appropriate allowances for estimated credit losses in their nontraditional mortgage portfolios and hold capital commensurate with the risk characteristics inherent in these products.\n\nOne final subject that is not addressed explicitly in our draft guidance, but that I believe is still important to supervisors and bankers, is mortgage fraud. There appears to have been a substantial upswing in suspected fraud related to residential mortgages in the past decade. Types of fraud include falsification of loan applications, identity theft, misuse of loan proceeds, and inflated appraisals. According to the Financial Crimes Enforcement Network, there were more than 18,000 reports of suspected mortgage fraud in 2004 (the latest year for which we have complete data), compared with fewer than 2,000 reports in 1997. And in the first six months of 2005 alone, there were more than 11,000 reports of suspected mortgage fraud. The increase may be attributable in part to an increase in the number of originators required to file Suspicious Activity Reports (SARs). Notably, the more widespread use of nontraditional loan products may present greater opportunity for fraud, as these products sometimes lack some of the quality checks typical of more-traditional mortgages. In general, we consider mortgage fraud to be a serious issue and one that bankers and supervisors must continue to confront. Of course, supervisors want to hear the industry's perspective on fraud in mortgage lending.\n\nCommercial Real Estate\nThe U.S. banking agencies recently issued proposed guidance on commercial real estate (CRE) lending. A major portion of that guidance focuses on CRE concentrations.\n\nBefore I discuss the importance of managing CRE concentrations, I want to emphasize that the proposed CRE guidance relates to \"true\" CRE loans. It is not directed at commercial loans for which a bank looks to a business's cash flow as the source of repayment and accepts real estate collateral as a secondary source of repayment. The proposed guidance addresses bank loans for commercial real estate projects for which repayment depends on third-party rental income or on the sale, refinancing, or permanent financing of the property. The latter are \"true\" commercial real estate loans, in that repayment depends on the condition and performance of the real estate market.\n\nI also want to mention up front that the proposed guidance is not intended to cap or restrict banks' participation in the CRE sector but rather to remind institutions that proper risk management and appropriate capital are essential elements of a sound CRE lending strategy. In fact, many institutions already have both of these elements in place and may not need to adjust their practices very much.\n\nI believe we are all aware of the central role that CRE lending played in the banking problems of the late 1980s and early 1990s. One reason supervisors are proposing CRE guidance at this point is that we are seeing high and rising concentrations of CRE loans relative to capital. For certain groups of banks, such as those with assets of between $100 million and $1 billion, the average CRE concentration level is about 300 percent of total capital. In the late 1980s and early 1990s, the concentration level for this same bank group was about 150 percent, or half the current level. Therefore, banks should not be surprised by the emphasis in the proposed CRE guidance on concentrations and the importance of portfolio risk management.\n\nHistorically, CRE has been a highly volatile asset class. In the past, problems in CRE, even at well-managed banks, have generally come at times when the broader market was encountering difficulties. In an effort to generate cash flow, borrowers and bankers with properties in distress may disrupt their local real estate market by cutting rents or offering leasehold improvements and other incentives to attract or keep tenants. These actions can have a negative effect on the entire local real estate market, including good projects. In most years, CRE credit losses are relatively low compared with many other types of bank loans. But in times of stress, the loss rate can jump considerably higher. Because CRE losses tend to be greater during times of stress, bankers must focus more intently on their risk appetite as their CRE concentration grows. Bankers must consider how much capital will be placed at risk if the CRE portfolio hits a stress period and compare that loss exposure with the relative returns of CRE lending. In other words, bankers need to practice risk management.\n\nWhile banks' underwriting standards are generally stronger than they were in the 1980s and 1990s, the agencies are proposing the guidance now to reinforce sound portfolio-management principles that a bank should have in place when pursuing a commercial real estate lending strategy. A bank should be monitoring performance both on an individual-loan basis as well as on a collective basis for loans collateralized by similar property types or in the same markets.\n\nSome institutions' strategic- and capital-planning processes may not adequately acknowledge the risks from their CRE concentrations. CRE lending in recent years has occurred under fairly benign credit conditions, but those conditions are unlikely to continue indefinitely. The ability of banks with significant concentrations to weather difficult market conditions will depend heavily on their risk-management processes and their level of capitalization. From a risk-management and capital perspective, institutions should generally focus on the emerging conditions in their real estate markets and on the potential cumulative impact on their portfolios if conditions deteriorate; they should also take other measures to help identify CRE vulnerabilities. Of course, these measures should vary according to the size of the organization and the level of the concentration. All of these steps are key elements of a sound strategy to manage concentrations.\n\nWhile supervisors continue to underscore the importance of having robust risk-management practices for CRE and other lending concentrations, we do acknowledge that banks may pursue a variety of approaches. In some cases, such as when there is not enough market data available or the relevant geographic market is small, banks may have to turn to less- quantitative approaches. Nonetheless, those approaches should be robust, well documented, and transparent. This is consistent with the broader theme that risk management should be scaled to the institution. Along those same lines, we are not necessarily expecting smaller banks to be able to conduct regular, extensive, and sophisticated quantitative stress tests around their lending concentrations. However, we do want bankers at smaller organizations to have clear and coherent methods for evaluating the various potential outcomes associated with their CRE concentrations and with all their exposures more broadly.\n\nConclusion\nIn the past several decades, real estate markets, both residential and commercial, have affected the U.S. economy in both negative and positive ways. Naturally, the Federal Reserve monitors these markets to gauge their impact on broader economic activity. In addition, because banks are substantially involved in both residential and commercial real estate markets, as a supervisor, we must ensure that bank lending in these markets is conducted in a safe and sound manner.\n\nOne tool we use to help maintain the safety and soundness of banks is supervisory guidance, which can point out areas requiring additional monitoring, suggest ways in which banks can improve risk management, and remind bankers that they should continue to exercise discipline in their lending activities to ensure that they are accounting for all their risks. The recently issued draft guidance on nontraditional mortgage products and on CRE lending, as I have noted, is not an attempt to stifle lending in these sectors, which, if conducted properly, can continue to be profitable businesses for bankers. Indeed, we recognize the important role that banks play in real estate lending. That is why we want to ensure that banks maintain good practices when operating in those markets.\n\nAs a final point, if both sets of guidance are finalized, we aim to implement them as consistently as possible across institutions. We do understand bankers' concerns about this issue. Of course, it is always a challenge to ensure that there is consistent application of guidance throughout the industry, especially when bank-specific factors--such as portfolio concentrations and individual risk-management practices--might affect the manner in which the guidance needs to be applied to each bank. But if the guidance is indeed finalized, we plan to undertake considerable efforts across our agencies, including extensive communication and coordination, so that banks are not subject to needlessly differing treatment.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20060614a.htm",
        "title": "A Supervisor's Perspective on Mortgage Markets and Mortgage Lending Practices",
        "date": "6/14/2006"
    },
    {
        "content": "June 13, 2006\n\nChairman Ben S. Bernanke\n\nAt the Fifth Regional Issues Conference of the Fifteenth Congressional District of Texas, Washington, D.C.\n\nI am pleased to be here to discuss some strategies for helping families, particularly lower-income families, improve their economic and financial well-being. Families today face a financial marketplace that is increasingly complex, with numerous products and service providers from which to choose. Today I will touch on several approaches for helping people of modest means take advantage of these financial opportunities while managing the risks and avoiding possible pitfalls.\n\nToday’s Financial Marketplace\nTechnological advances have dramatically transformed the provision of financial products and services in recent years. To cite just one example, the expanded use of computerized credit-scoring models, by reducing the costs of making loans and by increasing the range of assets that lenders can sell on the secondary market, has made possible the extension of credit to a larger group of borrowers. Indeed, we have seen an increasingly wide array of products being offered to consumers across a range of incomes, leading to what has been called the democratization of credit. Likewise, technological innovation has enhanced financial services, such as banking services, and increased the variety of financial products available to savers.\n\nThe range of providers in consumer financial markets has also increased, with the number of nonbank entities offering credit and other financial services having risen particularly quickly. For example, a recent study of alternative providers of financial services found the number of nonbank check-cashing establishments doubled in the United States between 1996 and 2001.1 Payday lending outlets, a source of credit that was almost non-existent a decade ago, now number more than 10,000. And data from the Survey of Consumers Finances, a triennial survey sponsored by the Federal Reserve Board, indicate that the share of households with a loan from a finance company increased from 13 percent in 1992 to 25 percent in 2004.\n\nFinancial Challenges of Lower-Income Families\nDespite the increased complexity of financial products and the wider availability of credit in many forms, U.S. households overall have been managing their personal finances well. On average, debt burdens appear to be at manageable levels, and delinquency rates on consumer loans and home mortgages have been low. Measured relative to disposable income, household net worth is at a fairly high level, although still below the peak reached earlier this decade.\n\nFamilies with low to moderate incomes, however, face special financial challenges. These families generally have less of a cushion to absorb unanticipated expenses or to deal with adverse circumstances, such as the loss of employment or a serious health problem. Results from the Survey of Consumer Finances show that the median net worth for households in the lowest income quintile--those whose income placed them in the bottom fifth of the population--was only $7,500 in 2004, well below the median for all survey respondents of $93,000.2 The Survey data also indicate that households in the lowest quintile were significantly less likely than the average respondent to maintain a checking or savings account; almost 25 percent of those families were \"unbanked,\" compared to less than 10 percent of families in the other income quintiles. The reasons given for not having an account varied: Some respondents said they would not write enough checks to make having an account worthwhile, but others were dissuaded by minimum balance requirements or said that they did not have enough money to justify opening an account. In some cases, a lack of knowledge about the services that banks offer or even a distrust of banks is likely a factor.\n\nThe Survey also found that lower-income households are less able than others to manage their debts. A greater fraction of these households had debt-to-income ratios of 40 percent or more or had a payment past due at least sixty days. The data also reveal that only 40 percent of families in the lowest quintile own a home, compared with a homeownership rate of 69 percent among all families surveyed. Finally, the data on retirement account ownership show an even larger gap, with only 10 percent of lowest-quintile families holding a retirement account, whereas 50 percent of all families responding to the survey reported participation in some type of retirement savings plan.\n\nHow can these disparities be addressed? Some general approaches to helping families of modest means build assets and improve their economic well-being include community economic development, financial education, and programs that encourage saving and investment. In the remainder of my remarks, I will discuss each of these approaches briefly and offer some insights into their effectiveness based on research and experience.\n\nCommunity Economic Development\nIn my time with the Federal Reserve, I have had a number of opportunities to meet with community economic development leaders--representatives of groups working to assist lower-income families become homeowners, start small businesses, better manage their finances, and save for the future. In fact, my first trip as a Federal Reserve Board member was to Brownsville, Texas, where I saw how a grassroots nonprofit organization is helping to build communities and to provide residents with the chance to build wealth through homeownership. The Community Development Corporation (CDC) of Brownsville works with multiple funding partners--governments at all levels, financial institutions, foundations, and corporations--to construct housing and to design innovative loan products that enable low-income families to qualify for mortgage credit. For example, because of the mix of funding sources, mortgage loans can be offered with features such as down‑payment assistance or a below-market interest rate. The CDC of Brownsville also offers a program that allows prospective homeowners to acquire \"sweat equity\" in a property by working on construction teams to help build their own new home and those of other participating families.\n\nAs in the case of many community development organizations, the Brownsville CDC has also made financial education a critical element of its efforts to help lower-income residents improve their financial status. For example, participation in financial counseling or in an education program is typically required for a borrower to obtain a loan through the CDC or through one of its lending partners. However, the broader aim of these programs is to improve borrowers’ prospects for longer-term success in maintaining their credit and handling their overall finances. Since 1994, through this combination of leveraged financing arrangements and borrower education, the CDC of Brownsville has helped make homeownership possible for more than 2,500 low-income families. I cite the Brownsville example because of the opportunity that I had to learn about their work (and I recently had a similar opportunity to see some impressive community development efforts in the Anacostia neighborhood of the District of Columbia). But this localized approach to community development and wealth-building is playing out in neighborhoods throughout the country, in most cases through strategies tailored to the distinct needs of the particular community.\n\nFinancial Education and Financial Literacy\nFinancial education has not only been integral to community development but has also begun to play a larger role in the broader consumer market. Clearly, to choose wisely from the wide variety of financial products and providers available, consumers must have at least basic financial knowledge. People who understand the financial aspects of purchasing a home or starting a business, or who appreciate the importance of saving for children’s education or retirement, will almost certainly be economically better off than those without that vital information. Financial literacy can be acquired through many channels: in school, on the job, through community programs and counseling, or through self-education and experience.\n\nStudies generally find that people receiving financial education or counseling have better financial outcomes. For example, research that analyzed data on nearly 40,000 mortgage loans targeted to lower-income borrowers found that families that received individual financial counseling were less likely later to become delinquent on their mortgage payments.3 Similarly, another study found that borrowers who sought and received assistance from a credit counseling agency improved their credit management, in particular, by reducing the number of credit accounts on which they carried positive balances, cutting overall debt, and reducing delinquency rates.4 More broadly, the research shows that financial knowledge is correlated with good financial outcomes; for example, individuals familiar with basic financial concepts and products have been found to be more likely to balance their checkbook every month, budget for savings, and hold investment accounts.5\n\nStudies that establish an association between financial knowledge and good financial outcomes are encouraging, but they do not necessarily prove that financial training and counseling are the causes of the better outcomes. It could be, for example, that counseling is associated with better financial outcomes because the consumers who choose to seek counseling are the ones who are already better informed or more motivated to make good financial decisions. In medicine and other fields, researchers gain a better understanding of what causes what by doing controlled studies, in which some subjects are randomly assigned a particular treatment while others do not receive it. To translate this idea to the analysis of the effects of financial counseling, the Federal Reserve Board’s Division of Consumer and Community Affairs is collaborating with the Department of Defense to conduct a three-year study of the effects of financial education. This study will evaluate the impact of various educational programs on the financial decisions of soldiers and their families. It includes a treatment group of those receiving financial education, with the programs each family receives and when they receive it being determined randomly, and a control group of similar soldiers and their families who have not received this formal financial education. Because assignments of individuals to programs will be random, any observed changes in behavior can be more reliably attributed to the type and amount of counseling received. Among other things, the results of this study should help us better understand whether financial education leads to changes in behavior for participants in general or only for those at critical teaching moments, such as the period before making a major financial decision such as choosing a mortgage.\n\nI would like to say just a few words about the Federal Reserve’s broader role in promoting consumers’ understanding of financial products and services. Beyond conducting surveys of consumers and doing research, we work in a number of ways to support consumers in their financial decisionmaking. For example, through our consumer protection rule-writing authority, the Federal Reserve sets requirements that specify the information that must be disclosed to consumers about the terms and fees associated with credit and deposit accounts. These disclosures provide consumers with the essential information they need to assess the costs and benefits of financial services and compare products among different providers. We are currently reviewing many of our disclosures and plan to use focus groups and other methods to try to make these disclosures as clear and as user-friendly as possible.\n\nThe Federal Reserve System also works to promote financial education and financial literacy through various outreach and educational activities. We provide a great deal of substantive financial information, including interactive tools for economic education, on our education website http://www.federalreserveeducation.org/. The website links to a wide variety of financial education resources at the local, regional, and national levels.\n\nAdditionally, the Federal Reserved Board collaborates with educational and community development organizations to support their efforts. Our national partners include the Jump$tart Coalition for Personal Financial Literacy, the Conference of Mayors’ DollarWi$e Campaign, Operation HOPE, the American Savings Education Council, and America Saves, among others. At the regional level, the twelve Federal Reserve Banks work with organizations to support financial education and financial literacy. For example, the Federal Reserve Bank of Cleveland has worked with community financial educators to form regional networks that combine resources and share best practices. The Federal Reserve Bank of Chicago sponsors \"MoneySmart Week,\" partnering with banks, businesses, government agencies, schools, community organizations, and libraries to host activities designed to help consumers learn how to manage money. The Federal Reserve Banks of San Francisco and Minneapolis have worked with leaders in the Native American community to develop financial education materials. My recent testimony to Congress on financial literacy provided information on many other projects and programs. 6 The Federal Reserve will continue to make financial education a priority.\n\nStrategies to Encourage Saving\nEven if people know that they would be better off if they saved more or budgeted more wisely, we all know from personal experience that translating good intentions into action can be difficult. (Think about how hard it is to keep New Year’s resolutions.) The field of behavioral economics, which studies economic and financial decisions from a psychological perspective, has cast new light on consumer behavior and led to recommendations about how to improve people’s financial management. For example, studies of individual choices in 401(k) savings plans strongly suggest that workers do not pay adequate attention to their saving and investment decisions. Notably, despite the tax advantages of 401(k) contributions and, in some cases, a generous employer match, one-quarter of workers eligible for 401(k) plans do not participate. Studies have found, however, that if firms change the presentation of the plan from an \"opt-in\" choice to an \"opt-out\" choice, in which workers are automatically enrolled unless they actively choose to remain out of the plan, participation rates increase substantially.7 The impact of changing from \"opt-in\" to \"opt-out\" is particularly evident for younger and lower-income workers, who may have less financial expertise.\n\nIn addition, participants in savings plans evidently do not understand the various investment options that are offered. A survey by the investment management firm, The Vanguard Group, found that many plan participants cannot assess the risk inherent in different types of financial assets; for example, many did not appreciate that a diversified equity mutual fund is generally less risky than keeping most of one’s wealth in the form of the employer’s stock.8 Indeed, employees appear to invest heavily in their company’s stock despite the fact that their income is already tied to the fortunes of their employer. More than one-quarter of 401(k) balances are held in company stock, and this high share arises not only from an employer match but from voluntary purchases as well.9\n\nThese insights into consumer behavior have prompted some changes in the design of retirement plans and in education programs focused on saving for retirement. More employers now feature automatic enrollment in their 401(k) plans in an effort to boost participation. Also, some have set the default investment option to a diversified portfolio that is rebalanced automatically as the worker ages or have set contribution rates to rise automatically over time in line with salary increases.\n\nHowever, although these changes in program design may boost saving and improve investment choices, they are not a substitute for continued financial education. Employers, including the Federal Reserve Board, offer financial education at the workplace to help their workers gain a better understanding of retirement savings options. Helping people appreciate the importance of saving and giving them the tools they need to translate that knowledge into action remain major challenges.\n\nConclusion\nLet me close by observing that many factors influence consumer financial behavior. Financial education is clearly central to helping consumers make better decisions for themselves and their families, but policymakers, regulators, nonprofit organizations, and financial service providers must all help ensure that consumers have the tools and the information they need to make better decisions. Success can only come through collaborative efforts. I see much interest today in increased collaboration toward these objectives, both in Washington and around the country.\n\nThank you for the opportunity to speak with you today. I encourage you to continue working together to help provide increased economic opportunity in your communities, and I wish you the best of luck in your efforts.\n\nFootnotes\n\n1.  Kenneth Temkin and Noah Sawyer (2004), \"Analysis of Alternative Financial Service Providers (781 KB PDF),\" report prepared for the Fannie Mae Foundation by the Urban Institute Metropolitan Housing and Communities Policy Center. Return to text\n\n2. Brian K. Bucks, Arthur B. Kennickell, and Kevin B. Moore (2006), \"Recent Changes in U.S. Family Finances: Evidence from the 2001 and 2004 Survey of Consumer Finances (448 KB PDF),\" Federal Reserve Bulletin. Return to text\n\n3.  Abdighani Hirad and Peter M. Zorn (2001), \"A Little Knowledge Is a Good Thing: Empirical Evidence of the Effectiveness of Pre-Purchase Homeownership Counseling (466 KB PDF),\" paper presented at \"Seeds of Growth - Sustainable CommunityDevelopment: What Works, What Doesn’t and Why?\"  Return to text\n\n4.  Gregory Elliehausen, E. Christopher Lundquist, and Michael E. Staten (2003), \"The Impact of Credit Counseling on Subsequent Borrower Credit Usage and Payment Behavior (305 KB PDF\" (January), paper presented at \"Seeds of Growth - Sustainable Community Development: What Works, What Doesn’t and Why?\" Return to text\n\n5.  Jeanne M. Hogarth and Marianne A. Hilgert (2003), \"Patterns of Financial Behaviors: Implications for Community Educators and Policymakers (1.7 MB PDF),\" paper presented at \"Seeds of Growth - Sustainable Community Development: What Works, What Doesn’t and Why?\" Return to text\n\n6.  Chairman Ben S. Bernanke, Financial Literacy, Testimony Before the Committee on Banking, Housing, and Urban Affairs, U.S. Senate, May 23, 2006. Return to text\n\n7.  Brigitte Madrian and Dennis Shea (2001), \"The Power of Suggestion: Inertia in 401(k) Participation and Savings Behavior,\" Quarterly Journal of Economics, vol. 116 (November), pp. 1149-87. Return to text\n\n8.  The Vanguard Group (2002), \"Expecting Lower Market Returns in the Near Term,\" Vanguard Participant Monitor. Return to text\n\n9.  Jeffrey R. Brown, Nellie Liang, and Scott Weisbenner (2006), \"401(k) Matching Contributions in Company Stock: Costs and Benefits for Firms and Workers,\" Journal of Public Economics, vol. 90 (August), pp. 1315-46. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060613a.htm",
        "title": "Increasing Economic Opportunity: Challenges and Strategies",
        "date": "6/13/2006"
    },
    {
        "content": "June 12, 2006\n\nGovernor Mark W. Olson\n\nAt the American Bankers Association's Regulatory Compliance Conference, Orlando, Florida\n\nThank you for the invitation to speak today on an issue of great interest to many of us, that is, compliance-risk management and supervisory expectations. Over the last few years, legal and regulatory compliance breakdowns have attracted increased attention across the financial industry. Fortunately, most of you have responded to your evolving compliance risks by investing in effective compliance-risk management programs. However, now and then, headline-grabbing incidents of noncompliance continue to capture public attention, especially when they involve such sensitive areas as fair lending and the Bank Secrecy Act (BSA). Conferences such as this are valuable opportunities for you, as compliance experts, to share experiences and successful approaches to controlling compliance risk.\n\nTo assist you in your efforts to fine-tune your compliance-risk management programs, I'd like to give you a sense of what Federal Reserve examiners look for when they conduct examinations. I will also take a few minutes to address our more focused work in two particularly important areas of regulatory compliance: compliance with BSA requirements and Home Mortgage Disclosure Act (HMDA) data reporting requirements. Otherwise, I will not focus on examinations that look solely at the level of compliance with specific laws and regulations but will focus on how examiners assess the adequacy of a compliance-risk management program and its ability to manage the organization's compliance risk.\n\nCompliance-Risk Management\nOverall, a banking organization's compliance-risk management program should enable it to adequately identify, measure, monitor, and control the compliance risks involved in its various products and lines of business. These are fundamental principles not only for compliance-risk management, but also for sound management of credit, market, liquidity, and operational risk.\n\nIt's worth taking a moment to define compliance risk. It is the risk of legal or regulatory sanctions, financial loss, or damage to reputation and franchise value that may arise when an organization fails to comply with laws, regulations, or standards or codes of conduct of self-regulatory organizations applicable to the business activities and functions of the banking organization.\n\nWhile all banking organizations should have a program in place to effectively manage compliance risk, these programs can vary considerably, depending on the size, complexity, and geographic reach of the banking organization and the inherent risks of its activities. As with other types of risk, large multinational organizations will require more elaborate and formal compliance-risk management systems to address their broader and typically more complex range of financial activities and to provide senior managers and directors with the information they need to monitor and direct activities. Therefore, our supervisory expectations regarding an organization's risk-management program, and more specifically the scope of an examination, will vary according to the organization's size and complexity.\n\nAssessing the Adequacy of Compliance-Risk Management Programs\nThe Federal Reserve's supervisory approach in the area of compliance-risk management is consistent with our long-standing focus on the adequacy of banking organizations' overall management of risk. To this end, Federal Reserve examiners assess the quality of a banking organization's systems for identifying, measuring, and containing its risks. While historically there has been a greater emphasis on risk management in the areas of credit, market, operational, and liquidity risk, because of the growing complexity of banking operations and their regulatory frameworks the Federal Reserve is taking a greater interest in banking organizations' ability to manage their compliance risk.\n\nScoping the Examination\nGenerally, a Federal Reserve examination team begins by defining the scope of the examination; this is when examiners determine the areas of focus and level of scrutiny. The scope of the examination will vary depending on the nature and circumstances of the banking organization. For example, as part of the scoping exercise, examiners will consider previous examination and audit findings to determine whether the organization has a satisfactory history of compliance or whether there have been previous concerns about its compliance-risk management program. The examination team will also review the organization's compliance-risk assessment. Depending on its quality, the risk assessment can also help direct the resources of the examination team. Altogether, the information gleaned from examination and audit findings and a current risk assessment will directly affect the scope of the examination, including the level and area of transaction testing required to assess the adequacy of the compliance-risk management program. At institutions with a less satisfactory record, a more extensive review will be necessary.\n\nFederal Reserve examinations for compliance-risk management are not designed to be gotcha games in which examiners look for one-time breaches of specific regulations or laws. Rather, these examinations are designed to assess the adequacy of the structure and processes the institution uses for managing compliance risk. Examiners are expected to look for the bigger picture and to look at the effectiveness of the program (including policies and processes) for managing the organization's compliance risk. We want to understand whether you have the controls in place to manage the risk of your organization.\n\nAs with all areas of risk management, our expectations--and therefore the scope of many examinations in this area--are framed by an emphasis on\n\nI'll give you a sense of some of the key components that examiners are likely to look for when assessing these fundamental areas.\n\nBoard and Senior Management Oversight\nA successful compliance-risk management program starts at the top of the organization. It is essential that the board of directors takes the lead by requiring a top-to-bottom compliance culture that is incorporated into the organization's day-to-day operations and is well communicated by senior management so that all staff members understand their compliance responsibilities and their roles in implementing the enterprise-wide program. Examiners will look to understand the board and senior management's roles in setting and communicating the compliance culture within the organization.\n\nExaminers will also look to see that roles and responsibilities are clearly defined and communicated throughout the organization and that senior management and staff understand their compliance obligations. In order for the board and senior management to carry out their responsibilities, they need to understand the organization's current compliance risks. We have seen organizations that have experienced challenges as a result of a lack of clarity in this area as they grow and diversify.\n\nExaminers will determine whether the organization has an effective risk assessment that accurately identifies its compliance risks and whether material risks are communicated to the board. Effective risk assessment measures the risk presented by clients, products and services, and geographic exposure within specific business lines or activities and aggregates these risks across the organization.\n\nRisk assessment is critical not only to ensure that the board and senior management is well informed. It also serves as the foundation for risk-based policies, procedures, and internal controls. Examiners will look to understand the organization's risk-assessment process. For example, they will look to see the degree to which the business lines are involved, how frequently the risk assessment is updated, and how it incorporates new products, services, or legal entities.\n\nHuman and financial resources are, of course, critical to effective performance.\nConsequently, examiners will assess whether senior management ensures that the compliance program has sufficient financial resources and a sufficient number of qualified and well-trained staff to carry out its responsibilities effectively.\n\nPolicies and Procedures\nPolicies and procedures essentially define and communicate the key goals and processes of an organization's compliance program. Examiners will look to see whether policies and procedures provide for adequate risk identification, assessment, measurement, and control.\n\nAs I mentioned a few moments ago, clearly communicated roles and responsibilities are a characteristic of an effective compliance program. Toward that end, examiners will also look to determine whether policies clearly delineate accountability and lines of authority across the organization's activities.\n\nExaminers also expect to see a well-defined process for ensuring that when compliance risks or potential breaches are identified they are elevated to the appropriate level, in keeping with the risk to the organization. Procedures for doing so should be well-communicated to staff throughout the organization.\n\nOverall, policies and procedures must be kept current, and, as with the risk assessment, examiners will look to see whether information gleaned from the compliance program operations is used to further tailor compliance policies, procedures, and controls to specifically address the inherent environment as it evolves.\n\nInternal Controls\nInternal controls are a particularly crucial element of a compliance-risk management program. Examiners will verify whether the organization has established and implemented an effective system of internal controls, including appropriate reporting lines and separation of duties, as well as positive and negative incentives.\n\nAn essential part of the internal control framework is periodic testing to determine how well the framework is operating, so that any required remedial actions can be taken. The frequency of testing should be risk-based and should involve, as appropriate, sample transaction testing, the sample size being determined by volume and the degree of risk of the activity.\n\nExaminers will carefully assess the scope and quality of the testing of the compliance program. Part of this assessment will include determining whether the testing was performed with appropriate independence. Examiners will also look to understand the specific delineations of responsibilities between the internal audit, compliance, and other independent functions or third parties. These delineations will vary by organization, but all roles should be clearly defined and communicated.\n\nExaminers will also look at how well compliance-testing exceptions are reported to senior management and resolved by business-line management. They will assess methods for tracking exceptions until the exceptions are resolved; this assessment will include examining the organization's provisions for escalating unresolved exceptions to higher levels in the organization, including the board of directors.\n\nIndependence and separation of duties are also issues of importance beyond compliance testing. For example, in the case of large complex banking organizations that may have a corporate compliance function, examiners will be interested in understanding how the compliance function maintains its independence from the business lines it advises on compliance requirements and the implementation of required controls. In cases in which the compliance function has responsibility for monitoring and testing, examiners will assess whether procedures are established to ensure an adequate degree of independence and objectivity.\n\nMonitoring and Reporting\nAs I mentioned, the fundamental purpose of compliance-risk management programs is to identify, monitor, and manage compliance risk more effectively. Monitoring involves identifying and communicating compliance concerns to the appropriate parties within the organization. Monitoring and reporting enable senior management and the board to effectively carry out their respective responsibilities. We have seen organizations silo critical compliance information rather than share it with all levels of the organization, which can handicap an organization's ability to identify systemic risks. As a result, examiners are interested in whether a compliance program is designed to monitor and report compliance concerns.\n\nThe level of sophistication of banking organizations' monitoring activities generally varies according to the size and complexity of the organization, and examiners' expectations will vary accordingly. For example, large complex banking organizations are typically supported by information systems that provide management with timely reports related to compliance with laws and regulations at the transaction level. Examiners will look to see whether these reports generally address monitoring and testing activities, actual or potential material compliance deficiencies or breaches, and new or changing compliance requirements. They will also assess whether reports are designed to ensure that information on compliance is communicated to the appropriate levels within the organization.\n\nTraining\nTraining on policies, procedures, and associated controls is a component of compliance-risk management that should not be overlooked. Examiners will determine whether the banking organization's training program ensures that compliance policies, procedures, and controls are well understood and appropriately communicated throughout the organization.\n\nWhile the depth and breadth of training that an employee receives depends on that employee's role and responsibilities, examiners generally assess whether staff at all levels understand the organization's compliance culture, general compliance-risk issues, and high-level compliance policies and procedures.\n\nSupervisory Consistency and the Bank Secrecy Act\nAs banking organizations become more complex, consistency in the agencies' supervisory approach has become even more critical. The Federal Reserve views supervisory consistency as a means of enhancing supervision and reducing burden. This is particularly essential in the area of regulatory compliance, and specifically with regard to compliance with the Bank Secrecy Act and its regulations.\n\nThe Federal Reserve includes a review of BSA compliance within every full-scope safety-and-soundness examination. For larger banking organizations that are subject to continuous supervision, the Federal Reserve conducts a series of targeted BSA reviews over the course of the supervisory cycle. This, combined with off-site monitoring, allows the Federal Reserve to maintain a current understanding of BSA compliance within the organizations that are subject to its supervision. On-site examinations are essential to ensure that the BSA program is operating effectively.\n\nBecause of the complexity of banking organizations today, a number of institutions may be subject to the supervision of an increasing number of regulators. A consistent examination approach among regulators is critical in order to achieve a consolidated view of risk management within an organization, and also to reduce burden on banking organizations. Our work with the Federal Financial Institutions Examination Council to develop the Bank Secrecy Act/Anti-Money Laundering Examination Manual, which was released last summer, marked an important step forward in our effort to ensure consistent supervision in the area of BSA compliance. Through the manual, the agencies have emphasized a banking organization's responsibility to establish and implement risk-based policies, procedures, and processes to comply with the BSA and safeguard its operations from money laundering and terrorist financing.\n\nThe agencies are currently updating the manual and plan to release the revised version this summer. I have been told that the revised manual will include not only updates reflecting changes in regulations and supervisory guidance over the course of the past year, but also, among other things, additional guidance on developing a BSA/AML risk assessment, which is the foundation of effective risk-based controls.\n\nHMDA Data and Fair Lending Examinations\nExaminations to evaluate a banking organization's adherence to fair lending laws and regulations are also a routine component of consumer compliance examinations conducted by the Federal Reserve. HMDA data play an important role in examinations of those banking organizations that are required to report the data. Examiners probe that data to understand how the bank is responding to credit needs and serving its community. The data are rich in many respects. They contain information about applicants' and borrowers' race or ethnicity, sex, income level, and property location. And, since 2004, the HMDA data have also included price information about certain loans with prices that exceed thresholds set by the Board.\n\nThe HMDA data help examiners better focus the fair lending examination. Particularly for banks with larger portfolios, the data, including any available pricing data, are incorporated into statistical management systems that analyze lending patterns and help direct the examination process to aspects of the bank's program that may warrant a closer look. Even in smaller banks where a statistical analysis cannot be performed, the HMDA data can be used to start the fair lending review. However, as we know, HMDA data have limitations. For example, the data do not include credit-risk factors such as credit scores and loan-to-value ratios. Because of these limitations, the examination process looks at additional information about a lender's practices, and about particular loans, before any conclusions are drawn. Examiners consider--together with HMDA data--information derived from consumer complaints, risks apparent from various business lines, and the adequacy of the institution's compliance-risk management program.\n\nSince examiners will be looking at the data, it would be advisable for a bank to make a review of the data a component of a comprehensive fair lending compliance program and Community Reinvestment Act strategy. In fact, examiners will look carefully at analyses of HMDA data performed by a bank and talk with the bank to understand the reasons for any disparities in lending patterns. The bank is probably in the best position to understand what the HMDA data suggest about its ability to reach prospective borrowers. Consequently, its own assessment is useful to an examiner establishing the fair lending examination scope. Examiners want to know how banks have addressed any disparities and how the bank's analysis has led to any changes in controls that were made to ensure that policies are followed. I want to emphasize that, as with compliance-risk management programs, the breadth of a banking organization's program and system review should be commensurate with the size and complexity of its operations, the range of its products, and the demographics of its markets.\n\nBeyond this review of HMDA data, examiners evaluate whether an organization's fair lending compliance framework makes it possible to identify, monitor, and effectively control risks. Examiners are looking for a clear articulation by the board of directors of the institution's lending strategy, including defined risk parameters and the execution of appropriate risk-measurement and risk-mitigation initiatives. Examiners will evaluate the extent to which management controls reflect the risk associated with the institution's lending strategy.\n\nAs with the broader area of compliance-risk management, examiners will look closely at how the compliance culture established at the top of the organization filters down into the everyday responsibilities of business-line managers and how those managers are held accountable for compliance.\n\nConclusion\nBecause of the growing complexity of banking organizations, the Federal Reserve is currently considering whether more-tailored guidance in the area of enterprise-wide compliance-risk management is warranted. In the coming months, we will continue to engage with you to better understand your successful approaches to identifying, monitoring, and managing risk across your organizations.\n\nThank you.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/olson20060612a.htm",
        "title": "What Are Examiners Looking for When They Examine Banks for Compliance?",
        "date": "6/12/2006"
    },
    {
        "content": "June 12, 2006\n\nGovernor Susan Schmidt Bies\n\nAt the Financial Women’s Association Washington Briefing, Washington, D.C.\n\nThank you for the invitation to speak here today. I am impressed by the range of interesting subjects covered in your program, and I hope that my remarks on enterprise risk management will be informative as well.\n\nToday I will look at some recent cases in which we believe bankers and supervisors have learned some key lessons about enterprise risk management, or ERM. These lessons demonstrate how good risk management increases business efficiency and profitability. Naturally, what we've learned from the banking industry can be more broadly applied to other industries and sectors. Indeed, one could argue that ERM can improve management of many different types of entities, including government agencies and nonprofit organizations. But before I start discussing particular examples, I want to take a step back and give you my thoughts on ERM generally.\n\nGeneral Thoughts on Enterprise Risk Management\nThe financial services industry continues to evolve to meet the challenges posed by emerging technologies and business processes, new financial instruments, the growing scale and scope of financial institutions, and changing regulatory frameworks. The Federal Reserve, as the supervisor of state member banks and bank and financial holding companies, has been working with other regulators and financial institutions to improve the effectiveness and relevance of regulation and supervision in this changing environment. The Federal Reserve has long emphasized the need for appropriate and strong internal controls in institutions we supervise, and we have taken a continuous-improvement approach to our risk-focused examinations. For many years, enterprise risk management across multiple organizational units within an entity has received increased scrutiny.\n\nIn some cases, firms may be practicing good risk management on an exposure-by-exposure basis, but they may not be paying close enough attention to aggregation of exposures across the entire organization. Rapid growth can place considerable pressure on, among other areas, an organization's management information systems, change-management controls, strategic planning, credit concentrations, and asset/liability management. An organization must also understand how its various business components, some of which can be quite sophisticated and complex, dynamically interact. A successful ERM process can help an organization to meet many of these challenges.\n\nOf course, enterprise risk management is a fairly broad topic that can mean different things to different people. For our purposes here today, I will define ERM as a process that enables management to effectively deal with uncertainty and associated risk and opportunity, enhancing the capacity to build stakeholder value. Borrowing from ERM literature, I would say that ERM includes\n\nSome of you are probably familiar with the ERM framework published over a year ago by the Committee of Sponsoring Organizations of the Treadway Commission, or COSO. The COSO framework provides a useful way to look at ERM and helps generate further discussion.\n\nIn the COSO framework, ERM consists of eight interrelated components derived from the way management runs an enterprise and integrated with the management process: (1) internal environment, (2) objective setting, (3) event identification, (4) risk assessment, (5) risk response, (6) control activities, (7) information and communication, and (8) monitoring. Each of these is described in more detail in the COSO literature.\n\nNotably, the COSO framework states explicitly that, while its components will not function identically within every entity, its principles should apply to all sizes of institutions. Small and mid-size entities, for example, may choose to apply the framework in a less formal and less structured way and scale it to their own needs--as long as quality is maintained. This underscores the message from bank supervisors that good risk management is expected of every institution, regardless of size or sophistication. Naturally, there will still be some tension between what supervisors expect and what bankers do, but we hope that supervisory expectations for risk management are becoming more and more aligned with the way that bankers run their businesses.\n\nI would now like to discuss a few recent examples from banking that highlight the importance of ERM. With the benefit of hindsight, the financial regulators and the industry have been trying to distill the lessons learned from these recent breakdowns in risk management and internal control in the financial services sector.\n\nCompliance Risk\nOne area in which ERM provides tangible value is the area of compliance risk, which can be defined as the risk of legal or regulatory sanctions, financial loss, or damage to an organization's reputation and franchise value. This type of risk may arise when an organization fails to comply with the laws, regulations, or codes of conduct that are applicable to its business activities and functions. The Federal Reserve expects banking organizations to have in place an infrastructure that can identify, monitor, and effectively control the compliance risks that they face. Needless to say, the infrastructure should be commensurate with the nature of the organization's compliance risk. For a large complex banking organization, dealing with compliance risk can be particularly challenging unless it has a well-developed risk-management program.\n\nTo create appropriate compliance-risk controls, organizations should first understand compliance risk across the entire entity. Understandably, this can be a daunting task, but I think most would agree that an effective risk assessment is critical. Managers should be expected to evaluate the risks and controls within their scope of authority at least annually.\n\nAn enterprise-wide compliance-risk management program should be dynamic and proactive. It should constantly assess evolving risks when new business lines or activities are added, when existing activities and processes are altered or when there are regulatory changes. The process should include an assessment of how those changes may affect the level and nature of risk exposures, and whether mitigating controls are effective in limiting exposures to targeted levels. To avoid having a program that operates on autopilot, an organization must continuously reassess its risks and controls and communicate with all employees who are part of the compliance process. If compliance is seen as a one-off project, an organization risks facing a situation down the road where its compliance program has not kept up with the changes in its organization. Also, the board of directors needs to ensure the organization has a top-to-bottom compliance culture that is well communicated by senior management so that all staff members understand their compliance responsibilities. Clear lines of communication and authority help to avoid conflicts of interest.\n\nCompliance-risk management can be more difficult for management to integrate into an organization's regular business processes because it often reflects mandates set out by legislation or regulation that the organization itself does not view as key to its success. For example, bankers understand how vital credit-risk management and interest-rate risk management are to their organizations, because they reduce the volatility of earnings and limit losses. However, regulations enacted for broader societal purposes can be viewed as an expensive mandate. For example, the Patriot Act requires significant reporting of transactions to the government, and many in industry have expressed frustration about the burden associated with such reporting. I can assure you, we recognize banking organizations' investment in and commitment to compliance with regulatory requirements, including those imposed by anti-money-laundering and counter-terrorism regulations. The Federal Reserve will continue to work with our counterparts in the federal government to encourage enhanced feedback on how reporting is contributing to our common fight against money laundering and terrorism.\n\nOperational Risk\nOver the past few years, the Federal Reserve has been increasing its focus on operational risk. For many nonfinancial organizations, the largest share of enterprise risk is likely to be operational risk, as opposed to credit and interest-rate risk. Banks have learned much from the practices that nonfinancial firms have developed over the years. Operational risk has more relevance today for bankers largely because they are able to shed much of their interest-rate and credit risk through sales of loans, use of financial derivatives and sound models to manage the risks that are retained. Further, the revenue streams that are growing the fastest are increasingly related to transaction processing, servicing accounts, and selling sophisticated financial products. To be successful, organizations must have complex systems to execute these activities.\n\nBanks are also utilizing advanced models to estimate and manage credit-risk and market-risk exposures. Growing use of sophisticated models requires stronger risk-management practices since weaknesses in the models' operational design and data integrity can lead to significant losses. Thus, effective risk management requires financial institutions to have more-knowledgeable employees to identify system requirements, monitor their effectiveness, and interpret model results appropriately.\n\nWe have learned quite a bit about operational risk from our examinations of banking organizations. For example, during routine examinations we look at the adequacy of banks' procedures, processes, and internal controls. Such reviews include transaction testing of control routines in higher-risk activities. For example, a bank's wire transfer activities and loan administration functions are often targeted for review, and our experiences have identified some common weaknesses in operational control that are worthy of attention.\n\nWith wire transfers and similar transactions, a banking organization could suffer a significant financial loss from unauthorized transfers and incur considerable damage to its reputation if operational risks are not properly mitigated. A few recurring recommendations from our reviews are to (1) establish reasonable approval and authorization requirements for wire transactions to ensure that an appropriate level of management is aware of the transaction and to establish better accountability; (2) establish call-back procedures, passwords, funds transfer agreements, and other authentication controls related to customers' wire transfer requests; and (3) pay increased attention to authentication controls, since this area may also be particularly susceptible to external fraud.\n\nLoan administration is another area where banking organizations could suffer significant financial losses from inappropriate segregation of duties or lack of dual controls. An institution could also incur considerable damage to its reputation if operational risk factors are not properly mitigated. A few recurring recommendations from these types of reviews that may be applied to corporations more generally are to (1) ensure that loan officers do not have the ability to book and maintain their own loans; (2) confine employee access to only those loan system computer applications that are consistent with their responsibilities; and (3) provide line staff with consistent guidance, in the form of policies and procedures, on how to identify and handle unusual transactions.\n\nOperational Risk Arising In Recent Financial Restatements\nRisks can sometimes quickly appear where they were not traditionally expected. For example, consider the changes we have seen in financial reporting quality of corporations in all industries. In 2005, there were approximately 1,200 restatements of previously filed financial statements by publicly traded companies--twice the rate for 2004. The complexity of generally accepted accounting principles and a more stringent, literal interpretation of the application of those standards by auditors and regulatory bodies, primarily the Securities and Exchange Commission, are two major factors that have led to the restatements.\n\nExamples of prominent restatements include FAS 133 hedge accounting and lease accounting issues. In the area of hedge accounting, the restatements generally resulted from the misapplication of the \"short-cut\" method. The organizations in question did not satisfy all of the criteria for use of the short-cut method but, nonetheless, utilized hedge accounting treatment allowed by this method.\n\nIn the area of lease accounting issues, most companies simply failed to apply longstanding accounting standards related to revenue recognition reserves, accruals and contingencies, and equity accounting. Most companies believed they were actually reporting correctly prior to the restatements. Virtually all of these companies were audited by auditing firms that are now registered with the Public Company Accounting Oversight Board (PCAOB). The PCAOB's inspection process, which involves close scrutiny of registered firms, may be a factor in the increased number of restatements.\n\nSection 404 of the Sarbanes-Oxley Act of 2002 requires each annual report of a public company to include a report by management on the company's internal control over financial reporting. Restatements by banking organizations alone resulted in the revision of a number of material weaknesses in internal control for the 2004 reporting period, fifty-two from the thirty-seven originally reported. This increase implies a significant amount of operational risk associated with the accounting process.\n\nGenerally, examiners review the Sarbanes-Oxley 404 process to determine whether the organization has a clear understanding of the roles of the audit committee, management, internal audit, and the external auditor and whether the organization has implemented an effective plan to achieve the objectives and requirements of Sarbanes-Oxley 404. Examiners also review the Sarbanes-Oxley 404 process to determine whether the organization has an effective follow-up strategy for the remediation of significant deficiencies and material weaknesses. Examiners are encouraged to utilize the results of the Sarbanes-Oxley 404 process, where possible, in their overall assessment of the organization's risk-management and control process and in the risk scoping of safety-and-soundness examinations and inspections.\n\nInformation Security\nIssues involving information security and identity theft have received quite a bit of attention from the federal government over the past several years. In fact, just recently, President Bush signed an executive order that created an Identity Theft Task Force for the purpose of strengthening federal efforts to protect against identity theft. The heads of the federal bank regulatory agencies are designated members of this task force; and as supervisors of financial institutions, I believe we can offer a unique perspective on this issue.\n\nAs you have probably noticed, cyber attacks and security breaches involving nonpublic customer information appear in the headlines almost every week. These events have cost the financial services industry millions of dollars in direct losses and have done considerable reputational damage. The cost of identity theft to affected consumers is also significant. With banking organizations increasingly using the Internet to interact with customers, business partners, and service providers, concerns about the use of the Internet as a communication and delivery channel have resulted in the need for and use of more-sophisticated control mechanisms, such as enterprise-wide firewall protections, multifactor authentication schemes, and virtual private-network connections.\n\nWhile many of the widely publicized information security breaches have involved parties outside the affected banking organization accessing the organization's customer information, organizations also remain at risk for breaches or misuses of information by an insider. During our examination activities, we have seen breakdowns in internal control, resulting in operating losses that were traced back to weak controls over insiders' access to information technology systems interfacing with electronic funds transfer networks. Further investigation into these situations suggests that the duration and magnitude of the fraud and resulting losses is a direct function of the internal party's access to accounting and related systems.\n\nSeveral lessons have emerged. First, institutions should tightly control logical access to funds transfer systems and ensure that access settings enforce separation of duties, dual controls, and management sign-offs. Second, an institution's senior management should be restricted from regular access to business-line functional systems, especially funds transfer systems. When such restriction is impractical, additional controls must be in place and functioning effectively. Finally, effective management of information security risk, even when focused on a specific function, requires an enterprise-wide approach to yield a true and complete evaluation of the associated risks.\n\nMutual Funds\nWell-publicized instances of late trading and market timing at mutual fund firms, and the related investigations, have involved many businesses, including banking, securities, and insurance firms. These types of breakdowns in internal control result in sanctions or financial loss and adversely affect a firm's reputation and franchise value.\n\nI would like to highlight a few lessons learned from our experience in investigating control breaches in these mutual fund cases. One of the most obvious is the need to critically evaluate unusual client relationships that require variances from standard procedures. If a high percentage of compensation is derived from a single client, a red flag should immediately go up. Also, organizations should have a formal process for reviewing and approving unique products, customers, and services at the inception of the client relationship. Furthermore, it is always a good idea to shine some light on areas historically labeled \"low risk\" to validate that assessment. The low occurrence of loss from an activity should not be the only factor considered when assessing risk.\n\nFinally, compensation systems that reward employees for sales without adequately monitoring their internal control breaches can create a conflict between the interest of employees and the interest of the enterprise. As companies move away from straight salaries to more incentive-based systems, it is important that personnel departments be included in an effective enterprise-wide risk-management program to consider how changes in compensation practices affect risks to the enterprise.\n\nCredit Derivatives\nI would now like to turn to one more issue that has relevance to ERM, and that is the importance of companies including an ERM perspective as they design and build new lines of business. As many of you might know, last year a dialogue between supervisors and credit derivatives dealers was initiated to support industry efforts to address weaknesses in the operations surrounding credit default swaps (CDS). While we view these new instruments as an effective way to diversify and mitigate risks related to credit exposures from corporations, an industry-led study, the Counterparty Risk Management Policy Group II report, identified significant weaknesses in the infrastructure supporting sales and risk monitoring of these instruments. While the report identified forty-seven recommendations, regulators in the United States and other countries have focused on two major weaknesses.\n\nOne weakness relates to the lack of discipline in enforcing contract terms. Any time an instrument is traded over the counter, it is important to know with whom you are doing business. Since an exchange does not stand between the two sides of the trade, parties make payments directly to each other to honor the terms of the contracts. The market practice is to use collateral or pricing to mitigate the risk that the other side of the trade cannot perform according to the agreement. The recent industry study also found that competitive pressures were such that brokers were not enforcing the standard CDS agreement, because their counterparties were routinely assigning the trade to another party without the broker's prior consent. As a result, dealers often did not have a real-time understanding of the counterparty exposure. Obviously, this can significantly change the risk profile of a transaction and also make it very difficult to settle payments in a timely manner.\n\nAnother weakness is related to the success of the product. Trading volume has grown so quickly and reached such a significant level that broker-dealers' paper-based systems to record the trades and document the transactions have not been able to keep up. As a result, significant backlogs of confirmations of these over-the-counter derivatives built up. This creates concerns that information feeding risk-management systems--information about the volume, term, and counterparty to the trade--is not complete. This problem would be exacerbated in a stress situation, when positions need to be changed very quickly to mitigate risk.\n\nA few months ago, fourteen major market participants published a letter reiterating their commitment to improving the infrastructure that supports the credit derivatives markets. The market participants are committed to the development and implementation of a set of industrywide guidelines that include a targeted reduction in each market participant's confirmation backlogs and assurance that agreement terms will be enforced. Additionally, the fourteen participants will work to create a largely electronic marketplace in which all trades will be processed through an industry-accepted platform, develop a new set of processing standards for those trades that cannot be confirmed electronically, and establish a new procedure for settlement following a credit event.\n\nWe are generally pleased with both the industry's self-identification of the issues and its commitment to making improvements. But for purposes of our discussion of ERM today, the problems surrounding CDS sales highlight the challenges risk managers face when market pressures make the firm's line management reluctant to initiate appropriate controls on their own. It also illustrates that in new lines of business, sometimes ERM must go outside the enterprise and work with competitors to support the growth of shared systems and standards to mitigate risks.\n\nConclusion\nAt the Federal Reserve, we believe that all banking organizations need good risk management. An enterprise-wide approach is appropriate for setting objectives across the organization, instilling an enterprise-wide culture, and ensuring that key activities and risks are being monitored regularly. In many ways, bankers have learned from nonfinancial industries about ERM. In other cases, banks' application of ERM may hold lessons for entities outside the financial sector. Whichever the case, it is clear that there is always an opportunity to improve upon ERM strategies and maintain the proper discipline to implement them effectively.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20060612a.htm",
        "title": "A Supervisor’s Perspective on Enterprise Risk Management",
        "date": "6/12/2006"
    },
    {
        "content": "June 12, 2006\n\nChairman Ben S. Bernanke\n\nAt the Stonier Graduate School of Banking, Washington, D.C.\n\nGood evening, and thank you for inviting me to speak to you. I am sure that, both in your academic studies and your practical experience, each of you has come to appreciate the rapid pace of change in the financial industry and the increasing complexity of that industry. My remarks will focus on an area that is evolving particularly quickly: the field of risk management. As you know, contemporary banking organizations are exposed to a diverse set of market and nonmarket risks, and the management of risk has accordingly become a core function within banks. Banks have invested in risk management for the good economic reason that their shareholders and creditors demand it. But bank supervisors, such as the Federal Reserve, also have an obvious interest in promoting strong risk management at banking organizations because a safe and sound banking system is critical to economic growth and to the stability of financial markets. Indeed, identifying, assessing, and promoting sound risk-management practices have become central elements of good supervisory practice.\n\nThe evolution of risk management as a discipline has thus been driven by market forces on the one hand and developments in banking supervision on the other, each side operating with the other in complementary and mutually reinforcing ways. Banks and other market participants have made many of the key innovations in risk measurement and risk management, but supervisors have often helped to adapt and disseminate best practices to a broader array of financial institutions. And at times, supervisors have taken the lead, for example, by identifying emerging issues through examinations and comparisons of peer institutions or by establishing guidelines that codify evolving practices.\n\nThe interaction between the private and public sectors in the development of risk-management techniques has been particularly extensive in the field of bank capital regulation, especially for the banking organizations that are the largest, most complex, and most internationally active. The current system of bank capital standards is the so-called Basel I framework, which was established internationally in 1988. Basel I was an important advance that resulted in higher capital levels, a more equitable international marketplace and--most relevant to my theme this evening--closer links between banks' capital holdings and the risks they take. However, as I will discuss, Basel I is becoming increasingly inadequate for our largest and most complex organizations. The activities of these organizations demand that we not only go beyond Basel I but that we continue to improve on today's most advanced methods of risk management. Thus, in the proposed new framework, known as Basel II, supervisors are seeking to draw upon industry best practice while also encouraging the industry to advance the risk-management frontier.\n\nThe Evolution in Risk-Management Practices\nRisk-management practices and bank supervision have both evolved over their long histories, but innovations in information technology and in financial markets have caused the pace of change to increase significantly over the past two decades. In particular, the management of market risk and credit risk has become increasingly sophisticated.\n\nMarket Risk\nFor example, in the area of market risk, advances in data processing have enabled more analytically advanced and more comprehensive evaluations of the interest rate risks associated with individual transactions, portfolios, and even entire organizations. Institutions of all sizes now regularly apply concepts such as duration, convexity, and option-adjusted spreads in the context of analyses that ten years ago would have taxed the processing capabilities of all but a handful of large institutions. From the perspective of bank management and stockholders, the availability of advanced methods for managing interest rate risk leads to a more favorable risk-return tradeoff. For supervisors, the benefit is a greater resilience of the banking system in the face of a risk that figured prominently in some past episodes of banking problems.\n\nOther market risks are those inherent in trading and dealer activities. The management of such risks has also advanced significantly, in large part as a result of the growth and development of over-the-counter derivatives markets. Critical concepts such as value-at-risk and stress testing were pioneered and then became standard practice during the 1990s, advances that, again, were facilitated by the growth of computing power in that decade. Over the past few decades, banks' management of their capital-market risks has evolved from simple methods like the imposition of fixed position limits to increasingly sophisticated techniques that make use of extensive data analyses and a variety of new financial instruments.\n\nSupervisors have encouraged the continuous improvement of banks' systems for managing market risk by emphasizing that bankers bear responsibility for understanding and managing their risk profiles and by issuing guidance that, in some cases, includes industry advances in risk management. A case in point is the 1996 Market Risk Amendment to Basel I, in which supervisors incorporated industry innovations in the calculation of capital requirements for market risk, including the linking of capital charges to the outputs of banks' own value-at-risk models.\n\nCredit Risk\nThe banking industry has also made strides in managing credit risk. Until the early 1990s, the analysis of credit risk was generally limited to reviews of individual loans, and banks kept most loans on their books to maturity. Today, credit-risk management encompasses both loan reviews and portfolio analysis. Moreover, the development of new technologies for buying and selling risks has allowed many banks to move away from the traditional book-and-hold lending practice in favor of a more active strategy that seeks the best mix of assets in light of the prevailing credit environment, market conditions, and business opportunities. Much more so than in the past, banks today are able to manage and control obligor and portfolio concentrations, maturities, and loan sizes, and to address and even eliminate problem assets before they create losses. Many banks also stress-test their portfolios on a business-line basis to help inform their overall risk management.\n\nTo an important degree, banks can be more active in their management of credit risks and other portfolio risks because of the increased availability of financial instruments and activities such as loan syndications, loan trading, credit derivatives, and securitization. For example, trading in credit derivatives has grown rapidly over the last decade, reaching $18 trillion (in notional terms) in 2005. The notional value of trading in credit default swaps on many well-known corporate names now exceeds the value of trading in the primary debt securities of the same obligors. Similarly, between 1990 and 2005, the market for loan syndications grew from $700 billion to more than $2.5 trillion, and loan trading grew from less than $10 billion to more than $160 billion. Asset-backed securitization has also provided a vehicle for decreasing concentrations and credit risk in bank portfolios by permitting the sale of loans in the capital markets, particularly loans on homes and commercial real estate.\n\nRisk-management principles are now ingrained in banks' day-to-day credit allocation activities. The most sophisticated banking organizations use risk-rating systems that characterize credits by both the probability of default and the expected loss given default. Consistent with the principles of the Basel II accord, the largest banks evaluate credit decisions by augmenting expert judgment with quantitative, model-based techniques. For instance, lending to individuals once relied mainly on the personal judgments of loan officers and was thus highly labor-intensive and subjective. Today, retail lending has become more routinized as banks have become increasingly adept at predicting default risk by applying statistical models to data, such as credit scores.\n\nSimilarly, new analytical tools and techniques have made lending to corporate borrowers highly quantitative. Among these tools are models that estimate the risk-adjusted return on capital and thus allow lenders to price relevant risks before loan origination. Other tools include proprietary internal debt-rating models and third-party programs that use market data to analyze the risk of exposures to corporate borrowers that issue stock.\n\nBanks have also come to appreciate the importance of independent controls within the credit review and rating process. Innovations in technology have facilitated significant improvements in bank information systems, a development that the Basel II proposal also has encouraged. These systems increase the ability of bank management to identify, measure, and control key characteristics of portfolio risk.\n\nEvolution of Banking Supervision and Capital Regulation\nTo help fulfill their mandate to monitor and protect the safety and soundness of the financial sector, bank supervisors have consistently encouraged the development of risk management. Through guidance and the supervisory process, they have highlighted advances in sound risk-management practices and encouraged the industry to implement them broadly and consistently. Indeed, the four key elements of sound risk management that are widely accepted today were articulated more than a decade ago by Federal Reserve supervisors in guidance on managing derivatives activities and interest-rate risk. Those four elements are, first, good corporate governance--that is, active oversight by the board and senior management; second, the consistent application of policies, procedures, and limits; third, the use of appropriate risk-measurement techniques and reporting; and, fourth, the adoption of comprehensive internal controls.\n\nSince the mid-1990s, Federal Reserve supervisors have rated banks' risk-management capability as well as their financial condition as part of the examination process. Last year the Federal Reserve introduced a revised rating system for bank holding companies, under which each company receives a rating specifically for the quality of its risk management. This rating includes the four key elements of sound risk management that I just mentioned.\n\nThe increasing supervisory focus on risk-management practices has also had a large influence on the practice of bank supervision. Traditional supervision consisted primarily of periodic assessments of loan quality. In the early 1990s, bank supervisors began to concentrate more on the forward-looking issues of risk and whether the bank has the infrastructure to manage risks. Under this approach, examiners focus their on-site reviews on those activities that appear to pose the greatest risk to the banking organization. The objective is to address weaknesses in management and internal controls before financial performance suffers rather than being satisfied with identifying what went wrong after the fact. At the heart of the modern bank examination is an assessment of the quality of a bank's procedures for evaluating, monitoring, and managing risk, and of the bank's internal models for determining economic capital. These models link capital to risk-taking and help banking organizations compare risks and returns across diverse business lines and locations.\n\nBoth robust risk management and strong capital positions are critical to ensure that individual banking organizations operate in a safe and sound manner that enhances the stability of the financial system. More generally, strong capital helps banks absorb unexpected shocks and reduces the moral hazard associated with the federal safety net.\n\nWhy Basel II?\nIn introducing the concept of risk-based capital ratios, Basel I established the important principle that regulatory capital requirements should be related to risk. At various times, of course, supervisors have also made important adjustments to the Basel I framework, such as the Market Risk Amendment mentioned earlier. Nonetheless, advances in risk management and the increasing complexity of financial activities have prompted international supervisors to review the appropriateness of regulatory capital standards under Basel I, particularly for the largest and most complex banking organizations. The supervisory organizations have agreed that Basel I, with its broad-brush system for setting the risk weights on various classes of bank assets, is increasingly inadequate for measuring risk and the appropriate level of capital for such firms. For example, under Basel I, a bank's regulatory capital requirement takes no account of the specific risk profile of its commercial loan portfolio, deterioration in asset quality, the risks of certain off-balance-sheet transactions or fee-based activities, and actions banks may take to mitigate balance sheet risks. Supervisors recognize that some of the largest and most complex banking organizations have already moved well beyond Basel I in the sophistication of their risk management and internal capital models. As risk-management practices continue to evolve, the gulf between the determinants of minimum regulatory capital under Basel I and what these banks actually do to manage risk will widen. Most important, if the regulatory capital required of these organizations does not adequately reflect the risks they are actually taking, the safety and soundness of the U.S. banking system may be jeopardized.\n\nThe U.S. banking agencies have proposed the adoption of the Basel II accord because it links the risk-taking of large banking organizations to their regulatory capital in a more meaningful way than does Basel I and encourages further progress in risk management. It does this by building on the risk-measurement and risk-management practices of the most sophisticated banking organizations and providing incentives for further improvements. Moreover, by providing a framework to be applied consistently across banks, Basel II will make it easier for supervisors to identify banks whose capital is not commensurate with their risk levels and to evaluate emerging risks in the banking system as a whole.\n\nBroadly, the Basel II framework encompasses three pillars. Pillar 1 is risk-focused minimum regulatory capital requirements, pillar 2 is supervisory review, and pillar 3 is market discipline. Under pillar 1, the risk sensitivity of minimum risk-based capital requirements would be much greater than under the current accord. This greater sensitivity would be achieved by linking each banking organization's capital requirement to empirically based measures of credit and operational risk; these measures would be determined in part by risk parameters estimated by the banks, such as a loan's probability of default and its expected loss given default. The methods used to construct these estimates would be subject to regulatory requirements and supervisory guidance and review, including a requirement that the risk parameters used for pillar 1 be consistent with risk assessments actually used by the bank for its internal risk management. The pillar 1 treatment of credit risk also reflects more accurately the risk-reducing effects of guarantees, credit derivatives, and securitization, thus improving regulatory capital incentives for banks to hedge credit risks. The incorporation of operational risk in pillar 1 is based on the recognition that, indeed, operational failures are a potentially important risk that banks should seek to minimize.\n\nPillar 2 of the new accord provides a consistent framework for improving supervisory assessments of capital adequacy and risk management. Under pillar 2, a bank would be required to maintain capital in excess of the regulatory minimums to capture the full set of risks to which the bank is exposed. These include liquidity risk, interest rate risk, and concentration risk, none of which are reflected in pillar 1. Currently, U.S. banking regulators assess a bank's overall capital adequacy as a normal part of the examination process. But the overall quality of assessments of capital adequacy, both by supervisor and by each bank, should improve greatly under Basel II because of the expanded information that will be available from pillar 1, from supervisory reviews under pillar 2, and from the bank's own analyses.\n\nUnder pillar 3, banks will be required to disclose to the public the new risk-based capital ratios and more-extensive information about the credit quality of their portfolios and their practices in measuring and managing risk. Such disclosures should make banks more transparent to financial markets and thereby improve market discipline.\n\nTaken together, these three pillars provide a broad and coherent framework for linking regulatory capital to risk, for improving internal risk measurement and management, and for enhancing supervisory and market discipline at large, complex, internationally active banks. The three pillars build on the risk-management approaches of well-managed banks and better align regulatory and supervisory practices with the way the best-run banks are actually managed. As a result, Basel II will be better able than the current system to adapt over time to innovations in banking and markets. In addition, Basel II sets standards for the measurement and management of risk and for related disclosures that will give banks ongoing incentives to improve their practices in these areas.\n\nAlthough the Basel II framework provides the basis for modernizing the supervision of large, internationally active banks, I emphasize that it remains in many ways a work in progress.1 Important details remain to be worked out, and much work remains to be done by both banks and supervisors to ensure that the system works as intended. The Federal Reserve Board has only recently approved a notice of proposed rulemaking, which invites comments from interested parties on all aspects of the proposed rules. The Federal Reserve and the other bank supervisors will review these comments carefully and will continue to consult widely. Under current plans, the transition to the new system will be gradual--no U.S. bank will have its capital requirement determined unconditionally by Basel II before 2012--and implementation will be subject to a number of safeguards. The supervisory agencies are also committed to continued review and adjustment of the system as experience accumulates.\n\nProposals to Amend Basel I\nMany of you here today have also been following discussions about possible changes to the existing Basel I framework, proposals known collectively as \"Basel IA.\" Only the very largest banking organizations will be required (or will choose) to adopt the Basel II framework. The vast majority of U.S. banks would be able to continue operating safely under Basel I as amended through the rulemaking process. The Basel I framework has already been amended more than twenty-five times in response to changes in the banking environment. The agencies believe that now is another appropriate time to amend the Basel I rules.\n\nLast fall, the U.S. banking agencies issued preliminary proposals that outline suggested changes to Basel I. In part, these proposed changes are meant to address concerns about the potential adverse competitive effects of Basel II. The Federal Reserve takes concerns about competitive effects seriously and has conducted substantial research on the topic. During the process to amend Basel I, we sought input from the industry and other interested parties. In view of those concerns, regulators have proposed changes to enhance the risk sensitivity of U.S. Basel I rules; we also remain vigilant about identifying potential competitive distortions that might be created with the introduction of Basel II.\n\nWe are also mindful that amendments to Basel I should not be too complex or too burdensome for the multitude of smaller banks to which the revised rules would apply. That is, in amending Basel I for these institutions, we are trying to find the right balance between added risk sensitivity and regulatory burden. That balance is not necessarily easy to find. For example, one way to tie regulatory capital more closely to risk under Basel I would be to expand the number of factors used to determine risk weights--for example, to include credit scores or external credit ratings. The tradeoff is that incorporating additional risk measures is likely to increase the burden of calculating regulatory capital. The comments received suggest that institutions differ on how best to make this tradeoff. We will continue to evaluate this tradeoff and solicit further comments on how to proceed.\n\nConclusion\nWe expect that risk management and banking supervision will continue to develop along parallel tracks. The Basel II framework represents an important effort by supervisors to integrate leading-edge risk management practices with the calculation of regulatory capital requirements. The ongoing work on this framework has already led large, complex banking organizations to improve their systems for identifying, measuring, and managing their risks. Indeed, banking organizations of all sizes have made substantial strides over the past two decades in their ability to measure and manage risks. The banking agencies will continue to promote supervisory approaches that complement and support banks' own efforts to enhance their risk-management capabilities.\n\nFootnotes\n\n1.  I addressed this theme in more detail in my recent speech Basel II: Its Promise and Its Challenges. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060612a.htm",
        "title": "Modern Risk Management and Banking Supervision",
        "date": "6/12/2006"
    },
    {
        "content": "June 09, 2006\n\nChairman Ben S. Bernanke\n\nAt the Massachusetts Institute of Technology 2006 commencement, Cambridge, Massachusetts\n\nPresident Hockfield, members of the faculty, alumni, families and friends of graduates, and, especially, members of the 2006 graduating class: I am honored to speak at the 140th commencement exercises of this distinguished institution.\n\nIt is wonderful to be back at MIT. I graduated from the Institute with a Ph.D. in economics in 1979. That year, President Weisner gave the commencement address. He spoke about, among other things, the nation’s transition from an era of cheap energy to one of energy scarcity and about the need for new technologies to aid in this transition. Obviously, these issues still confront us. One cannot help but wonder whether that theme will feel as current twenty-seven years from now as it does today.\n\nAs for today, you may have been surprised at some point to learn that an economist rather than an engineer or scientist would be serving as your commencement speaker. But in my remarks, I hope to illustrate that this address continues a long and productive tradition of collaboration at MIT between economics and the engineering and scientific disciplines for which the Institute is so well-known. Building on that theme, I will discuss the essential complementarity of technology and economics in modern economies. Finally, I will have a few words to say about what you, as MIT graduates, can do to strengthen our economy and our society even as you pursue your personal and professional goals.\n\nA Short History of Economics at MIT\nIf you will bear with me, I would like to begin with a short history of economics at MIT. The MIT Economics Department is, of course, the part of the Institute that I know best, and I hope to persuade you that it has played a special and unique role in this institution.\n\nMIT’s connection to economics dates at least to 1881, when Francis A. Walker became the institution’s third president. To say that Walker had already had a distinguished career would be an understatement. He was named a brevet brigadier general at the end of the Civil War, at the age of twenty-four. He served as the superintendent of the 1870 and 1880 Censuses of the United States and was one of the leading economists of his era. The year he arrived at MIT, he taught the first economics course ever offered at the Institute. The course covered political economy and was so popular that it was soon accorded its own course classification as \"Course IX, General Studies.\" Walker helped found the American Economic Association, still the leading professional association for economists. During his tenure at MIT, he moonlighted both as the first president of that association and as president of the American Statistical Association.\n\nIn the early twentieth century, the economics program at MIT aimed to prepare undergraduates for leadership roles in business. During those years, economics as a discipline was gaining greater prominence both here and abroad. But the modern era of economics at MIT began in 1940--the year that Paul Samuelson, not yet having even received his doctorate, was persuaded to emigrate here from a somewhat less technically proficient institution located on another stretch of the Charles River. In part, Samuelson was willing to leave Harvard because his Foundations of Economic Analysis--a book now universally recognized by economists as inaugurating the modern mathematical approach to economics--was not well received by the old guard at the Harvard Economics Department.\n\nMIT’s Ph.D. program in economics was established a year after Samuelson arrived. Right from the start, the department attracted strong graduate students: The very first of these, Lawrence Klein, received the Nobel Prize in Economics in 1980 for his work in econometric modeling. With support from MIT’s administration, the department expanded rapidly after World War II, and MIT led the development of a more mathematically rigorous approach to economics. Given the emphasis on quantitative reasoning at MIT, it makes perfect sense that the Economics Department here was in the vanguard of those using mathematics as a framework for organizing economic thought.\n\nThese developments laid the foundation for economics as a discipline in the second half of the twentieth century, and the department quickly rose to the top of national rankings. Besides Samuelson, many economists contributed to the department’s outstanding reputation--Franco Modigliani, Robert Solow, Charles Kindleberger, Rudiger Dornbusch, and Stanley Fischer, to name just a few. Modigliani, Samuelson, and Solow won Nobel Prizes for their research. In addition, nine other economists with MIT connections have won Nobels.\n\nMathematical approaches to economics have at times been criticized as lacking in practical value. Yet the MIT Economics Department has trained many economists who have played leading roles in government and in the private sector, including the current heads of four central banks: those of Chile, Israel, Italy, and, I might add, the United States. One of my teachers at MIT, Stan Fischer, is a sterling example of what MIT training can produce. Stan followed a brilliant career as a researcher and teacher at MIT with important work as a public servant, including top positions at the World Bank, the International Monetary Fund, and, currently, the Bank of Israel.\n\nWhy did economics at MIT become so successful? Perhaps Paul Samuelson and the people he helped to attract here could have been equally successful anywhere. But I suspect that the placement of economics in a milieu where quantitative reasoning and the scientific method are the coin of the realm was an important contributing factor. The Sloan School, with its close links both to the Economics Department and to other parts of the Institute, has benefited from the same milieu and has been the source of many fundamental advances as well. Notably, in recent years the global financial industry has been transformed by new quantitative approaches to pricing complex financial instruments such as derivatives and to measuring and managing risk. This transformation stemmed from the application of formal tools of mathematical economics that were developed to a substantial extent by faculty at the Sloan School, including Fischer Black, Robert Merton, and Myron Scholes--the latter two of whom won Nobel Prizes for their work.\n\nAs MIT economics has benefited from its proximity to the scientific and engineering expertise of MIT, so the Institute has benefited from the presence of a world-class economics department, over and above the addition of still more luster to the MIT name. The exposure of students and faculty from other divisions to the discipline and approaches of economics has stimulated creative thinking about how technology can be used to improve the economic welfare of the average person. That thought brings me to my second topic: the link between technology and economic growth.\n\nTranslating Technological Advances into Economic Growth\nAs has always been the case, technological change and innovation are today in large part driving economic growth and the improvement of living standards. But it is important to understand that even the very best ideas in science or engineering do not automatically translate into broader economic prosperity. In large measure, the material benefits of innovation spring from complementarities between technology and economics, where I include in \"economics\" not only economic ideas but also economic policies and, indeed, the entire economic system. When the economics is right, scientific and technological advances promote economic development, which in turn, in a virtuous circle, may provide resources and incentives that help to foster more innovation. A negative example is the former Soviet Union, which certainly did not lack for scientific and engineering talent but which had an economic system that was poorly suited for translating scientific advances into economic progress.\n\nThe experience of the United States over the past decade illustrates the essential complementarity of technology and economics, in my view. Before the mid-1990s, the growth of productivity--the amount of output produced per worker or per hour of work--had been relatively sluggish for more than two decades in this country. As productivity is perhaps the single most important determinant of average living standards--a country in which the average worker can produce a lot is usually also a place in which the average person can consume a lot--the so-called productivity slowdown of that earlier period was the source of much concern among economists and policymakers. In the mid-1990s, however, productivity growth picked up in the United States. The growth rate of productivity increased still further around the turn of the century and remains strong today. This productivity revival augurs well for the future of the U.S. economy. But why did it happen?\n\nYou, of all people, will not be surprised to hear that the research suggests that the pickup in U.S. productivity growth in the mid-1990s was importantly related to advances in information and communication technologies.1 But these technical advances in and of themselves can’t be the whole story. For example, even though the new technologies are widely available around the world, many other countries appear not to have derived the same degree of economic benefit from them as has the United States. Notably, productivity in Europe increased rapidly in the decades after World War II but then decelerated around the mid-1990s, at about the same time that U.S. productivity growth began to increase. Thus the gap between productivity levels in the United States and Europe, which had nearly closed by 1995, has been widening since then. What accounts for the apparently disparate effects of technology on growth here and abroad?\n\nDifferences in economic policies and systems likely account for some of the differences in performance--another example of the complementarity of technology and economics. One leading explanation for the strong U.S. productivity performance is that labor and product markets in the United States tend to be more flexible and competitive, and that these market characteristics have allowed the United States to realize greater economic benefits from the new technologies. For example, taking full advantage of new information and communication technologies may require extensive reorganization of work practices, reassignment and retraining of workers, and ultimately some reallocation of labor among firms and industries. Regulations that raise the costs of hiring and firing workers and that reduce employers’ ability to change work assignments--like those in a number of European countries, for example--may make such changes more difficult to achieve. Likewise, in product markets, a high degree of competition and low barriers to the entry of new firms in most industries in the United States provide strong incentives for firms to find ways to cut costs and to improve their products. In some other countries, in contrast, the prominence of government-owned firms with a degree of monopoly power, together with a regulatory environment that protects large incumbent firms and makes the entry of new firms difficult, reduces the competitive pressure for innovation and the application of new ideas. Competition is one of the key benefits of free and open trade; companies that are exposed to global competition tend to be much more efficient and to produce goods of higher quality than companies that are sheltered from international competition.\n\nOther economic factors have probably been important in translating technological change into material progress.2 Some observers point to the depth, liquidity, and sophistication of American financial markets as contributing to recent productivity gains. Sizable markets for venture capital and ready access to equity financing facilitate start-up enterprises, which are often the best means of bringing new technologies to the market. The United States also benefits from its high-quality research universities, which have shown both the willingness and the ability to collaborate with the private sector and, in some cases, with the government as well, in the development and commercialization of new ideas. For example, Intel was co-founded by an MIT graduate, and MIT graduates played key roles in designing and developing the Internet.\n\nManagement practices also differ across countries, and these differences may also matter for productivity. A recent study found that business establishments in the United Kingdom that are owned by U.S. multinationals get higher productivity from information technology than do other establishments, and it tied this differential to the management and organization of U.S. firms.3 Finally, relatively more positive attitudes toward competition and entrepreneurship in the United States--a factor that spans economics and sociology--may also stimulate innovation and its commercial application as well as economic policies that support innovation. Of course, there are factors that may restrain both technological innovation and its commercialization in the United States as well: I would put at the top of the list the relatively poor performance of our K-12 educational system in stimulating interest in and providing solid training in the sciences.\n\nOne interesting feature of the U.S. and global experience with major innovations is that often a significant amount of time passes between the initial development and diffusion of new technologies and the realization of the associated productivity benefits. Computers were first commercialized in the 1950s, for example, and personal computers came into widespread use beginning in the early 1980s. But until the mid-1990s these developments had little evident effect on measures of productivity. Indeed, MIT’s Robert Solow famously quipped in 1987 that \"computers are everywhere except in the productivity statistics.\" Moreover, despite the sharp decline in information-technology investment after the meltdown of tech-sector stocks earlier this decade, the growth rate of productivity actually increased further in recent years, as I mentioned. These long lags raise additional questions about the nature of the links between new technologies and the resulting productivity gains.\n\nPerhaps the answer lies in taking the longer view. Some research by economists has drawn an analogy between modern information and communication technologies and earlier so-called general-purpose technologies such as the steam engine, the electric motor, and the internal combustion engine. General-purpose technologies have broad application and thus have the potential both to revolutionize methods of production and to make a host of new goods and services available to businesses and consumers.4 For example, when smaller electric motors replaced single-power sources, such as steam or water power, in manufacturing facilities, it became feasible to reorganize the layouts of plants to optimize the flow of materials rather than the distribution of power.5 And the advent of air conditioning significantly expanded opportunities for economic development in the warmer regions of the United States and the world. However, in all cases, these developments evolved over a long period and required firms to make collateral investments in research and development, organizational structure, and employee training. These investments in learning how to make the best use of new technologies have been dubbed intangible capital, to distinguish them from investments in physical goods like new machines or facilities.\n\nIn the case of information and communication technologies, new economic research suggests that the investments in associated intangible capital--figuring out what to do with the computer once it’s out of the box--are quite important indeed.6 In my view, important investments in intangible capital remain to be made, as much still remains to be learned about how to harness these technologies most effectively. Thus, it should not be surprising that the benefits of these technologies have taken a long time to show up in the productivity statistics. This research also suggests that the current productivity revival still has some legs, as the full economic benefits of recent technological changes have not yet been completely realized.\n\nLooking to the Future\nAs graduates of MIT, you will be at the heart of this critical process of developing new technologies and taking them to the marketplace. We are in an age in which technology and its fruits will be a dominant force not only in our economic lives but in the cultural, social, political, and personal aspects of our lives as well. Your training at MIT equips each of you exceptionally well to take the fullest advantage of the professional and personal opportunities that technological innovation and change will create.\n\nEach of you, because of your youth, your talent, your demonstrated commitment to learning, and your personal and intellectual attainments during your time at MIT, will soon find--to paraphrase Shakespeare--that the world is your oyster. I hope that you will contribute in some measure to economic progress, whether in the United States or elsewhere; and I hope you find some measure of financial reward. But the world has a great deal more to offer than money, and a key question each of you will face repeatedly in your lives is how to use the talent and education that you have been given and the knowledge that you have attained. With respect to your professional lives, I hope that when you make career choices, you will look first for opportunities that excite you intellectually, that allow you to use your creative powers to the fullest extent, and that let you continue to learn and grow. I hope you will not be afraid to be unconventional, to do something nobody else has thought of before. Remember that the path to success and fulfillment may not be well marked, the scaling of some predetermined ladder; it may instead be a road without signs or maps. And remember that it is OK to fail--really: New opportunities will always arise for those who seek them. If you remain nimble in searching out new and unexpected opportunities, it will not only benefit you, but it will also benefit the economy and our society, as long experience has shown that dynamism and creativity are the seeds of innovation and of progress.\n\nIn the personal sphere, as you make your way in the world, I hope you will not forget the importance of your family and how much it has already contributed to your journey through life. Remember, too, family members are the ones who will still love you even when things aren’t going so well. Even as you focus intensively on your professional interests, I hope you will remain intellectually broad--well-read, well-informed, and open to new experiences. And finally, I hope you will remain engaged with the broader society. That may involve entering public service at some point, as many MIT graduates have chosen to do. But it need not. There are always opportunities to make a difference in the world, through volunteering, civic participation, charitable activities, or the type of work you choose to do.\n\nI congratulate all graduates and your families for what you have accomplished and wish you the very best for the future.\n\nFootnotes\n\n1. See Stephen Oliner and Daniel Sichel (2000), \"The Resurgence of Growth in the Late 1990s: Is Information Technology the Story?\" Journal of Economic Perspectives, vol. 14, pp. 3-22. Also see Dale W. Jorgenson and Kevin J. Stiroh (2000), \"Raising the Speed Limit: U.S. Economic Growth in the Information Age,\" Brookings Papers on Economic Activity: 1, pp. 125-211. Return to text\n\n2.  For a discussion of some of these other factors, see Robert J. Gordon (2004), \"Why Was Europe Left at the Station When America’s Productivity Locomotive Departed?\" NBER Working Paper Series 10661 (Cambridge, Mass.: National Bureau of Economic Research, August). Return to text\n\n3.  See Nick Bloom, Raffaella Sadun, and John Van Reenen (2006), \"It Ain’t What You Do, It’s the Way that You Do I.T.--Testing Explanations of Productivity Growth Using U.S. Affiliates\" working paper (London: Centre for Economic Performance, London School of Economics, March). Return to text\n\n4.  See Timothy Bresnahan and Manuel Trajtenberg (1995), \"General Purpose Technologies: 'Engines of Growth?'\" Journal of Econometrics, vol. 65, pp. 83-108. Return to text\n\n5.  See Paul David (1990), \"The Dynamo and the Computer: An Historical Perspective on the Modern Productivity Paradox,\" American Economic Review, vol. 80 (May), pp. 355-61. Return to text\n\n6.  See Carol A. Corrado, Charles R. Hulten, and Daniel E. Sichel, (2006), \"Intangible Capital and Economic Growth,\" NBER Working Paper Series 11948 (Cambridge, Mass.: National Bureau of Economic Research, January). Also see Erik Brynjolfsson, Lorin M. Hitt, and Shinkyu Yang (2002), \"Intangible Assets: Computers and Organizational Capital\" Brookings Papers on Economic Activity: 1, pp. 137-198. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060609a.htm",
        "title": "Commencement address",
        "date": "6/9/2006"
    },
    {
        "content": "June 06, 2006\n\nGovernor Susan Schmidt Bies\n\nAt the Western Independent Bankers Annual CFO & Risk Management Conference, Coronado, California\n\nI am pleased to be here today and thank you very much for the invitation.  The focus of this conference is risk management, which I think is an excellent discussion topic for bankers, regardless of their institution’s size.   In fact, I am quite pleased to see more and more conferences devoted exclusively to risk management, analyzing its different facets and exploring ways to tailor it to specific institutions and situations.  There is growing understanding that good risk management can be an integral part of running any type of business.\n\nRisk Management in Financial Services\nWith respect to financial services, the Federal Reserve Board, as the primary supervisor of state member banks and the consolidated supervisor of financial holding companies, has been working with other regulators and financial institutions to improve the effectiveness of banks’ risk management in order to keep pace with changing business practices and strategies.  The Federal Reserve has long emphasized the need for appropriate and strong internal controls in the institutions we supervise, and we have taken a continuous-improvement approach to our risk-focused examinations.  For many years, enterprise risk management across an entire entity has received increased scrutiny.\n\nOf course, bankers are the ones who have led the way in continuing to improve the risk-management and risk-measurement processes at their institutions.  To be more effective competitors and to control and manage their losses, banks have created many new techniques to improve their risk management and internal economic capital measures.  By more clearly defining risk exposures, identifying the causes of their losses, and establishing controls to limit future losses, bank managers have been better able to integrate decisions about risk-taking into their strategic and tactical decision making.  Banks that integrate risk measurement into their business-line goals often find that this helps them to implement their strategic plans more effectively.  That is because strategic planning tends to focus more on alternative “most likely” scenarios.  By including a risk management analysis in the strategy discussion, bankers can more clearly identify the inherent operational and environmental factors that can significantly affect the realization of strategic goals.  Thus, banks can design in, at the start, the appropriate internal controls and management information systems to improve the execution of the strategy.\n\nIn some cases, firms may be practicing good risk management on an exposure-by-exposure basis, but they may not be paying close enough attention to the aggregation of their exposures or concentrations that arise in the resulting portfolios of business.  Also, rapid growth can place considerable pressure on an organization's management information systems, change-management controls, strategic planning, credit concentrations, and asset/liability management, among other areas.  Many of the companies that have attracted public attention in recent years due to serious breaks in controls failed to focus on process changes and critical investments in their risk management and control systems that were needed to successfully support their business plans.  An organization must also understand how its various business components dynamically interact.  A successful enterprise-wide risk management process can help to meet many of these challenges.\n\nAt the same time, it is clear that risk management practices need to be applied in a manner appropriate to the size and complexity of the organization.  While the leading-edge risk-management practices used at the largest, most complex banks may have some applicability at smaller, less complex banks, data and cost limitations require greater use of more generalized models and the use of outsourcing to supplement the knowledge base of the institution.  And as most of you know, running a smaller or less complex bank presents different types of challenges and requires a risk-management framework appropriately tailored to the institution.  For example, transactions may be conducted more on a relationship basis and may be less data-intensive.  In such a case, bank management needs to develop risk-management tools that allow it to ensure that risks are still being appropriately addressed.  Further, smaller organizations often face a challenge of ensuring independent review of processes and decisions since officers and staff often have multiple responsibilities that can present conflicts of interest.\n\nMany of you are probably familiar with the enterprise risk management framework published over a year ago by the Committee of Sponsoring Organizations of the Treadway Commission, or COSO.  The COSO framework provides a useful way to look at enterprise risk management.  Notably, the COSO framework states explicitly that, while its components will not function identically in every entity, its principles should apply to institutions of all sizes.  Small and mid-size entities, for example, may choose to apply the framework on a less-formal and less-structured basis and scale it to their own needs--as long as quality is maintained.  This underscores the message from bank supervisors that good risk management is expected of every institution, regardless of size or sophistication.  Naturally, some tension will still exist between what supervisors expect and how bankers want to run their business.  But we hope that supervisory expectations for risk management are becoming more and more aligned with the way that bankers run their businesses.\n\nI would now like to turn to two examples highlighting the importance of risk management for smaller banks:  credit concentrations and business disruptions.\n\nLending Concentrations\nAs any banker worth his or her salt knows, lending concentrations must be carefully identified, monitored, and managed.  It is one of the basics of banking to understand the consequences of placing all your eggs in one basket.  Naturally, supervisors from time to time have concerns about growing credit risk concentrations at banks and bankers’ ability to manage them.  A current example is commercial real estate (CRE).  The U.S. banking agencies recently issued proposed guidance on CRE lending, and a major portion of that guidance is directed at CRE concentrations.  The agencies have received many comment letters on the proposed guidance.  These comments will be very helpful as we discuss what steps to take next.\n\nBefore I discuss the importance of managing CRE concentrations, I want to emphasize that the proposed CRE guidance is intended to encompass “true” CRE loans.  It is not focused on commercial loans for which a bank looks to a business’s cash flow as the source of repayment and accepts real estate collateral as a secondary source of repayment.  That is, the proposed guidance addresses bank loans for commercial real estate projects in which repayment is dependent on third-party rental income or on the sale, refinancing, or permanent financing of the property.  These are “true” commercial real estate loans in that repayment depends on the condition and performance of the real estate market.\n\nI also want to mention up front that the proposed guidance is not intended to cap or restrict banks’ participation in the CRE sector but rather to remind institutions that proper risk management and appropriate capital are essential elements of a sound CRE lending strategy.  In fact, many institutions already have both of these elements in place and may not need to adjust their practices very much.\n\nI believe we are all aware of the central role that CRE lending played in the banking problems of the late 1980s and early 1990s.  One reason supervisors are proposing CRE guidance at this point is that we are seeing high and rising concentrations of CRE loans relative to capital.  For certain groups of banks, such as those with assets of between $100 million and $1 billion, average CRE concentrations are about 300 percent of total capital.  In the late 1980s and early 1990s, the concentration level for this same bank group was about 150 percent, or half the current level.  Therefore, banks should not be surprised by the emphasis in the proposed CRE guidance on concentrations and the importance of portfolio risk management.\n\nHistorically, CRE has been a highly volatile asset class.  In the past, problems in CRE even at well managed banks have generally come at times when the broader market encounters difficulties.  Borrowers and bankers with properties in distress can disrupt their local real estate market by cutting rents or offering leasehold improvements and other incentives to attract or keep tenants in an effort to generate cash flow.  This can negatively affect the local real estate market as a whole, and adversely affect even good projects.  CRE is a highly volatile asset class in that credit losses in most years are relatively low compared with many other types of bank loans.  But in times of stress, the loss rate on CRE can jump considerably higher relative to the good years, compared with the behavior of other types of loans.  Since CRE losses tend to be concentrated in these times of stress, bankers must focus more intently on their risk appetite for losses as their concentration grows.  This means considering how much capital can be placed at risk if the portfolio of CRE loans hits a stress period and comparing that loss exposure to the  relative returns in CRE lending, i.e., practicing risk management.\n\nWhile banks’ underwriting standards are generally stronger than they were in the 1980s and 1990s, the agencies are proposing the guidance now to reinforce sound portfolio-management principles that a bank should have in place when pursuing a commercial real estate lending strategy.  A bank should be monitoring performance both on an individual-loan basis as well as on a collective basis for loans collateralized by similar property types or in the same markets.\n\nSome institutions’ strategic- and capital-planning processes may not adequately acknowledge the risks from their CRE concentrations.  CRE lending in recent years has occurred under fairly benign credit conditions and, naturally, those conditions are unlikely to continue indefinitely.  The ability of banks with significant concentrations to weather difficult market conditions will depend heavily on their risk-management processes and their level of capitalization.  From a risk-management and capital perspective, institutions should generally focus on the emerging conditions in their real estate markets and on the potential cumulative impact on their portfolios if conditions deteriorate, and they should take other measures to help identify CRE vulnerabilities.  Of course, these measures should vary according to the size of the organization and the level of the concentration.  All of these steps are key elements of a sound strategy to manage concentrations.\n\nIn evaluating the impact of their CRE concentrations, bankers should also pay attention to geographic factors.  Many banks conduct successful CRE lending within a certain geographic area, but problems can arise when banks begin to lend outside their market or “footprint,” where they normally have better market intelligence.  In recent years, supervisors have observed banks lending outside their established footprint--to maintain a customer relationship--into real estate markets with which they have less experience.  The challenge is heightened when the borrower is also venturing into a new market.  These practices led to significant losses in prior CRE credit downturns.\n\nI noted that CRE underwriting appears substantially better compared with the late 1980s and early 1990s.  However, we have noticed some slippage recently.  Therefore, the proposed CRE guidance underscores the existing interagency guidance on real estate lending standards.  That is, it offers some reminders about risk-management practices for individual exposures.  For example, banks may occasionally be inclined to make some compromises and concessions to borrowers in order to attract new business and sustain loan volume.  As supervisors, we want to ensure that loan-to-value standards and debt-service-coverage ratios are meeting the organization’s policies--and that there is not an undue increase in the exceptions to those standards and ratios.  We also continue to monitor whether lenders routinely adjust covenants, lengthen maturities, or reduce collateral requirements.  To be clear, we have not yet seen underwriting standards fall to unsatisfactory levels on a broad scale, but we are concerned about some of the downward trend in these standards.\n\nIt is important to note that no element of the proposed guidance is intended to act as a “trigger” or “hard limit” for immediate cutback or reversal of CRE lending; rather, the guidance is a reminder to institutions that certain risk-management standards are vitally important for banks involved in the business.  Additionally, the agencies intend to use the proposed thresholds in the guidance only as a “first cut” or “screen” to identify institutions that may have heightened CRE concentration risk.  The thresholds are intended to serve as benchmarks to identify banks where further information on portfolio risk management is needed.  In some cases, after more careful review, supervisors may actually find that given the characteristics of its CRE portfolio an institution has sound risk management and is holding appropriate capital.  In general, the proposed guidance is intended to be applied quite flexibly and in a manner consistent with the size and complexity of each organization.\n\nWhile supervisors continue to underscore the importance of having robust risk-management practices for CRE and other lending concentrations, we do acknowledge that banks may pursue a variety of approaches.  In some cases, such as when there is not enough market data available or the relevant geographic market is small, banks may have to turn to less quantitative approaches.  Nonetheless, those approaches should be robust, well documented, and transparent.  This is consistent with the broader theme that risk management should be scaled to the institution.  Along those same lines, we are not necessarily expecting smaller banks to be able to conduct regular, extensive and sophisticated quantitative stress tests around their lending concentrations.  However, we do want bankers at smaller organizations to have clear and coherent methods for evaluating the various potential outcomes associated with such concentrations, and their exposures more broadly.\n\nManaging Business Disruptions\nA number of events in the past half decade, including terrorist attacks and natural disasters, have reminded us of the importance of planning and preparation.  Most recently, Hurricane Katrina underscored the critical role of business-continuity planning and disaster response for small businesses and small banks in local communities.  Most financial institutions in the affected areas responded admirably to the extreme challenges posed by the hurricane and subsequent flooding, and the benefits of planning and preparation showed.  I am sure that many of you in the audience today have studied the lessons from Katrina and improved your own plans for dealing with business disruptions.  But I want to offer a supervisory perspective on potential lessons for bankers.\n\nBanks, like businesses everywhere, can be subject to wide-scale disruptions resulting from both natural and man-made disasters.  Potential problems include destruction of facilities, missing personnel, power and communications outages, lack of transportation and fuel, interruption of mail and other delivery services, and health and safety crises.  In short, services and activities normally taken for granted can be suddenly disrupted--and in some cases for an extended time.\n\nIn 2003, U.S. supervisors issued revised guidance on business continuity planning that explicitly advises banks to factor the risk of a wide-scale disruption into their business continuity and disaster response plans.  The experiences of bankers during and after Katrina confirmed the essential elements of good business-continuity management laid out in the guidance.  As the first step in preparing for business disruptions, large or small, the guidance advises bankers to conduct a full evaluation of what it takes to run their business effectively and provide necessary services to their customers.  This evaluation should include a variety of scenarios and possible events that could cause a business disruption.  Bankers should then analyze the business impact of these possible disruptions, which could include a prolonged recovery period, and fashion appropriate responses.  Once the business-continuity plan is developed, it should be implemented and tested regularly.  It should also be updated whenever the bank expands or changes its business activities and when it gathers new information from tests or real-life events.\n\nWhen developing business-continuity plans, bankers need to understand that people are the most vital resource.  Bankers should plan for ways to track and communicate with personnel through a range of channels, including ways to reach personnel if phone and electrical services are down, as they were after Katrina.  For banks operating in smaller geographic markets, it may be worthwhile to establish communication contacts outside of the region to be used by both employees and customers.  Depending on the cause of the disruption, bankers should also expect that some of their personnel may be dealing with family emergencies that will limit their ability to work.  Therefore, it is especially important to identify and train backup personnel to handle critical operations and services.\n\nBusiness-impact analysis and planning requires that bankers understand not only their business lines but also the systems and processes that support those business lines.  The bank’s planning should address how these support systems and processes could be recovered if they are disrupted, including the effect such a disruption would have on the bank’s facilities, equipment, and other physical property.  The bank may have to operate from backup or some type of recovery facilities for an extended period in order to provide critical services to customers.  Employees may also need to be prepared to perform services manually if computer systems become unavailable.\n\nHurricane Katrina also reminds us that unlike a fire, which may interrupt only the bank’s own activities as the community continues business as usual, a more widespread event causes banks to serve as agents of recovery for both their immediate and larger communities.  I am sure that all of you understand, first, the importance of providing financial services in any community and, second, that you have a responsibility to provide those services to your customers and neighbors during a crisis.  Accordingly, you should try to understand and coordinate your plans with the disaster-response programs for your neighborhood, city, and state.  In fact, bankers’ knowledge of their critical systems needs can very often assist local government and utility company managers in better evaluating the impact of their preparedness on local customers.  By the same token, you can improve your institution’s ability to respond by understanding the strengths and limitations of infrastructure around your bank, and the manner in which the community’s disaster-response efforts may unfold.\n\nNaturally, we cannot expect bankers to prepare for every conceivable event or plan for them with equal intensity.  As with any aspect of risk management, bankers should assess the probability of an event and its potential consequences.  We certainly understand that planning, preparation, and testing consume time, energy, and money.  Accordingly, institutions should determine the most cost-effective way to mitigate risks and continue to assess which possible events deserve greater attention and preparation.\n\nConclusion\nOur ongoing supervision of banking organizations indicates that a preponderance of institutions continue to be sound and well managed.  This strong performance has occurred concurrently with institutions’ continued efforts to improve their risk-identification and management strategies.  That said, certain areas in banking operations, such as credit concentrations and business continuity planning, are placing pressures on risk-management systems.  In turn, supervisors are increasingly scrutinizing these and other relevant areas to ensure that management is fully aware of their risks and has made any necessary risk-management upgrades.\n\nOf course, bankers may be somewhat concerned about the impact that supervisory initiatives--even proposed guidance--could have on their business.  We hear your concerns about regulatory burden, but I think it is helpful to remember that our job as regulators is to ensure that the United States has a safe and sound banking system.  In other words, supervisors are in the business of monitoring “downside risk” to the financial system, so we must act appropriately when we see possibly excessive risk taking or inappropriate risk management.  We also have a role in helping banks to prepare for potentially disruptive events.  While most U.S. banking organizations today operate in a safe and sound manner and enjoy substantial profitability, they need to remember that continued business success depends on their ability to prepare for unexpected, and potentially much less favorable, events and outcomes.\n\nAs institutions continue to offer new products and services, they face the challenge of incorporating the associated risks into their existing risk-management framework.  This is true for institutions of all sizes.  But the manner in which risk-management challenges are addressed can--and should--vary across institutions, based on their size, complexity, and individual risk profile.  Additionally, as supervisors, we want to ensure that institutions are not only identifying, measuring, and managing their risks but also developing and maintaining appropriate corporate-governance structures to keep up with their business activities and risk taking.  Our hope is that the guidance we offer to bankers on these various topics is becoming more consistent with their own practices for running an effective and profitable business.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20060606a.htm",
        "title": "Challenges of Conducting Effective Risk Management in Community Banks",
        "date": "6/6/2006"
    },
    {
        "content": "June 05, 2006\n\nChairman Ben S. Bernanke\n\nAt the International Monetary Conference, Washington, D.C.\n\nI am pleased to be here this afternoon to participate in the International Monetary Conference.  In my remarks, I will provide a brief update of the economic outlook for the United States and discuss the implications of that outlook for monetary policy.\n\nIt is reasonably clear that the U.S. economy is entering a period of transition.  For the past three years or so, economic growth in the United States has been robust, reflecting both the ongoing re-employment of underutilized resources as well as the expansion of the economy’s underlying productive potential, as determined by factors such as productivity trends and the growth of the labor force.  Although we cannot ascertain the precise rates of resource utilization that the economy can sustain, we can have little doubt that, after three years of above-trend growth, slack has been substantially reduced.  As a consequence, a sustainable, non-inflationary expansion is likely to involve some moderation in the growth of economic activity to a rate more consistent with the expansion of the nation’s underlying productive capacity.  It bears emphasizing that productivity growth seems likely to remain strong, supported by the diffusion of new technologies, capital investment, and the creative energies of businesses and workers.  Thus, productive capacity should continue to expand over the next few years at a rate consistent with solid growth of real output.\n\nReal gross domestic product grew rapidly in the first quarter of this year, but the anticipated moderation of economic growth seems now to be under way.  Consumer spending, which makes up more than two-thirds of total spending, has decelerated noticeably in recent months.  One source of this deceleration is higher energy prices, which have had an adverse impact on real household incomes and weighed on consumer attitudes.  As had been expected, recent readings also indicate that the housing market is cooling, partly in response to increases in mortgage rates.  To be sure, the data on home sales and construction have been somewhat erratic from month to month, reflecting weather conditions, statistical noise, and other factors.  However, overall, housing activity has softened relative to the high levels of last summer, and the rate of house-price appreciation appears to have lessened.  A slowing of the real estate market will likely have the effect of restraining other forms of household spending as well, as homeowners no longer experience increases in the equity value of their homes at the rapid pace seen in recent years.\n\nGains in payroll employment in recent months have been smaller than their average of the past couple of years, and initial claims for unemployment insurance have edged up.  These developments are consistent with the softening in the pace of overall economic activity that seems to be under way.  That said, going forward, relatively low unemployment and rising disposable incomes may counter to some extent the factors tending to restrain household spending.\n\nAlthough spending by the household sector is showing signs of moderation, other sectors of the economy retain considerable momentum.  According to the available data, business investment appears to have risen briskly, on net, so far this year.  In particular, investment in nonresidential structures, which had been weak since 2001, seems to have picked up appreciably, raising the possibility that increased nonresidential construction may absorb some of the resources released by the slowing housing sector.  Spending on equipment and software is also on a strong upward trend, and backlogs of orders for capital goods are still rising.  Business investment is being supported by high rates of profitability and capacity utilization.  Credit conditions for businesses are favorable:  Although market participants appear to have become more attuned to risks in recent weeks, corporate bond spreads remain low, and banks are well capitalized and willing to lend.\n\nGlobally, output growth appears poised to exceed 4 percent for the fourth consecutive year--a strong performance that will support the U.S. economy by continuing to stimulate our exports of goods and services.  The buoyant global economy does present some challenges, however.  In particular, the increased world demand for crude oil and other primary commodities, together with the limited ability of suppliers to expand capacity in the short run, has led to substantial increases in the global prices of those goods.  Those price increases are a partial offset to the forces supporting global growth and are also a source of inflationary pressure.\n\nLongstanding concerns about global imbalances remain with us as well.  Along with greater national saving in the United States, increased domestic demand in countries with current account surpluses and a greater flexibility of exchange rates more broadly would help to reduce those imbalances over time.  Should U.S. economic growth moderate as expected, sustaining the global expansion will require a greater reliance by our trading partners on their own domestic spending as a source of growth.\n\nConsumer price inflation has been elevated so far this year, due in large part to increases in energy prices.  Core inflation readings--that is, measures excluding the prices of food and energy--have also been higher in recent months.  While monthly inflation data are volatile, core inflation measured over the past three to six months has reached a level that, if sustained, would be at or above the upper end of the range that many economists, including myself, would consider consistent with price stability and the promotion of maximum long-run growth.  For example, at annual rates, core inflation as measured by the consumer price index excluding food and energy prices was 3.2 percent over the past three months and 2.8 percent over the past six months.  For core inflation based on the price index for personal consumption expenditures, the corresponding three-month and six-month figures are 3.0 percent and 2.3 percent.  These are unwelcome developments.\n\nAlthough the rate of pass-through from the higher prices of energy and other commodities to core consumer price inflation appears to have remained relatively low, the cumulative increases in energy and commodity prices have been large enough that they could account for some of the recent pickup in core inflation.  Despite recent increases in spot oil prices, futures markets imply that oil prices are not expected to continue rising.  The realization of that outcome would reduce one source of upward pressure on inflation.  However, the volatility of these and other commodity prices is such that possible future increases in these prices remain a risk to the inflation outlook.  Subdued growth in most broad measures of nominal labor compensation and the ongoing expansion of labor productivity have held down the rise in unit labor costs, the largest component of business costs.  Anecdotal reports suggest, however, that the labor market is tight in some industries and occupations and that employers are having difficulty attracting certain types of skilled workers.  Finally, some survey-based measures of longer-term inflation expectations have edged up, on net, in recent months, as has the compensation for inflation and inflation risk implied by yields on nominal and inflation-indexed government debt.  As yet, these expectations measures have remained within the ranges in which they have fluctuated in recent years, but these developments bear watching.\n\nWith the economy now evidently in a period of transition, monetary policy must be conducted with great care and with close attention to the evolution of the economic outlook as implied by incoming information.  Given recent developments, the medium-term outlook for inflation will receive particular scrutiny.  There is a strong consensus among the members of the Federal Open Market Committee that maintaining low and stable inflation is essential for achieving both parts of the dual mandate assigned to the Federal Reserve by the Congress.  In particular, the evidence of recent decades, both from the United States and other countries, supports the conclusion that an environment of price stability promotes maximum sustainable growth in employment and output and a more stable real economy.  Therefore, the Committee will be vigilant to ensure that the recent pattern of elevated monthly core inflation readings is not sustained.\n\nToward this end, and taking full account of the lags with which monetary policy affects the economy, the Committee will seek a trajectory for the economy that aligns economic activity with underlying productive capacity.  Achieving this balance will foster sustainable growth and help to forestall one potential source of inflation pressure.  In addition, the Committee must continue to resist any tendency for increases in energy and commodity prices to become permanently embedded in core inflation.  The best way to prevent increases in energy and commodity prices from leading to persistently higher rates of inflation is by anchoring the public’s long-term inflation expectations.  Achieving this requires, first, a strong commitment of policymakers to maintaining price stability, which my colleagues and I share, and, second, a consistent pattern of policy responses to emerging developments as needed to accomplish that objective.\n\nOur economy has reaped ample rewards in recent years from the achievement and maintenance of price stability.  Although challenges confront us, as they always do, I am confident that we will be able to preserve those hard-won benefits while promoting sustainable economic growth.",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060605a.htm",
        "title": "Panel Discussion: Comments on the Outlook for the U.S. Economy and Monetary Policy",
        "date": "6/5/2006"
    },
    {
        "content": "May 25, 2006\n\nGovernor Mark W. Olson\n\nAt the Social Compact Visionary Awards, Washington, D.C.\n\nI appreciate the opportunity to join you this evening to recognize the successes of an impressive list of community revitalization projects. Having observed the revitalization of Washington, D.C., over a number of years, I am particularly pleased to be here at the Tivoli Theatre and see first-hand the work that has been done to restore this historic theater and bring new activity and investment to the Columbia Heights neighborhood.\n\nAs I look around, it is hard to believe that such an architectural gem sat vacant for some twenty-eight years. The restoration of the Tivoli honors the past while helping serve the residential, commercial, retail, and cultural needs of Columbia Heights residents. It is a fine example of the transformative power of investments in older, urban neighborhoods. Projects such as this would not be possible without the strategic vision and commitment of community, business, and government leaders. It is that vision we are here to celebrate this evening.\n\nI would like to begin by offering a review of the overall economic climate today and the prospects for business investment generally. Then, to borrow a phrase from Social Compact, I will \"drill down\" to highlight a number of the elements that I believe are important in making sound investment decisions, particularly in underserved areas of our inner cities. Finally, I will offer a few other observations about the state of the community development industry as I see it.\n\nMacroeconomic Conditions\n\nThe recent news on the U.S. economy has been, on the whole, quite positive. Adjusted for inflation, gross domestic product increased at a brisk rate in the first quarter of the year. Gains in consumer spending and business investment were particularly strong. To be sure, most forecasters are expecting the overall pace of economic activity to moderate to a more sustainable pace in coming quarters as housing markets gradually cool and the delayed effects of higher interest rates and energy prices temper domestic demand. However, with economic activity abroad expanding at a solid pace, export sales should provide some support for domestic production.\n\nOn balance, the outlook for business investment should remain quite favorable even as the pace of overall activity moderates. Against a backdrop of sustained growth in sales, businesses should be well positioned to undertake potentially profitable projects. They have enjoyed robust earnings for the past several years, and their balance sheets are strong. The reports on first-quarter earnings have been quite positive, and available measures of credit quality, such as credit ratings and loan defaults, show few signs of stress.\n\nFinancing costs have moved up since last fall, but the Federal Reserve’s most recent survey of bank lending practices, which was conducted in April, indicates that domestic banks are noticing increases in requests for business loans. They also indicated an increased willingness to supply business loans in an environment of brisk competition from other lenders, a liquid secondary market for business loans, and an increased tolerance for risk. In that regard, research conducted by Federal Reserve economists notes that more banks are offering loans to businesses in markets where the banks do not have branch offices; the research suggests that new technology has spurred competition for some types of small business lending in recent years. More broadly, businesses currently have access to plentiful debt and equity financing from a variety of sources in addition to banks.\n\nThe challenge that community development practitioners face in working with businesses looking for new investment opportunities is to demonstrate through careful analysis that promising inner-city neighborhoods can be good investments. That the current overall conditions for business investment are positive is thus good news for the work of Social Compact and its partners.\n\nThe Importance of Data to Investment Decisions\n\nCommunity development has come a long way since I first began following it as a young congressional staffer thirty-five years ago. Whereas government programs once funded the bulk of inner-city development, public-private partnerships now dominate. In many places, community-based organizations have been able to \"jump start\" markets by facilitating private investment. The focus of neighborhood revitalization has shifted increasingly toward market-based approaches. In that regard, organizations that seek to promote neighborhood revitalization must focus on what makes their projects competitive.\n\nSocial Compact is perhaps best known for its focus on one important input into the investment process--information. Timely and accurate information is what project planners need to bring entrepreneurs, investors, and lenders together to objectively assess the proposed undertaking. Having been a banker, I know very well how much more efficient the process can be when decisionmakers are presented with a clear assessment of the projected returns and a balanced analysis of risk.\n\nAnother related feature of market-based investment decisions is the importance of built-in measures of accountability. Hard data on results are important in establishing the credibility of projects. With data, and results, such investments can also serve as catalysts for additional private investment in underserved areas. A number of methods have been derived for evaluating the effectiveness of community development projects. The one with which I am most familiar, through my work on the board of NeighborWorks America, is the Success Measures Data System. This system uses more than forty indicators and a variety of data-collection tools--including surveys, interviews, and focus groups--to quantify the effects of housing, economic development, and community building programs at the personal, organizational, and community levels. This array of tools allows NeighborWorks to measure success not just quantitatively but qualitatively, looking beyond the numbers to detect sustainable positive change. The underlying philosophy is that a community development initiative cannot be judged successful just because it puts a family in a home; rather, it must be able to say that, by doing so, it has improved the community, the family’s quality of life, and the family’s economic potential.\n\nThese data are also useful to corporate partners insofar as they demonstrate business performance against internal goals and commitment to the community. Businesses recognize that demonstrable success enhances their reputation, increases brand recognition, and improves their ability to compete successfully for future partnership opportunities. Thus, being able to determine the return on a business’ investment in a community development initiative in both quantitative and qualitative terms is the next logical step in attracting further private investment.\n\nBut the social and economic needs of our inner-city neighborhoods extend beyond business investment. Effective partnerships with local officials and community leaders are another key element of the revitalization process. At the state and local levels, public officials contribute to the attractiveness of investment conditions by maintaining sound fiscal policies. These policies allow local officials to take the lead from Social Compact and begin to market local neighborhoods using the analysis that demonstrates the buying-power residing within these more challenging neighborhoods. Their willingness to address other neighborhood needs, such as housing, infrastructure, education, health care, and other social supports will also be important to investors.\n\nIn general, when community development is a cooperative effort, it can lead to more sustainable outcomes along both economic and social dimensions. While the return on investment can be a sufficient gauge of their economic success, success in broader terms can be viewed by assessing the commitment of all segments of the community to the goal of revitalization. Moreover, the benefits of such cooperative actions are not always readily measurable in dollars and cents. Greater opportunities in local neighborhoods evidence themselves through better schools and stronger ties forged within the local community.\n\nThe Importance of Vision\n\nSo far, my remarks have focused on the tangible aspects of planning and investment. But I do not want to leave out a critical element that is perhaps impossible to measure but underlies each of your undertakings. That element is vision. The Washington Post has reported that when Joe Horning walked through this theater eight years ago, he was surrounded by broken windows, rotted floor boards, and leaks in the roof. That’s what he saw. But what he envisioned was quite different. He envisioned a beautifully restored theater that would not only serve the community but, more important, also serve as a symbol that Columbia Heights could be revitalized.\n\nIndeed, in the past eight years, that vision has been realized, with Columbia Heights today a vibrant and viable neighborhood. As in similar neighborhoods in the District of Columbia and throughout the United States, there is much to celebrate in the results of community investments. The increase in street traffic and economic activity makes the neighborhood a livelier and safer place. Property values are rising, and that allows homeowners here to experience the wealth-building power of homeownership that their peers in more-affluent areas have experienced for years. It also enables them to invest in the rehabilitation of their homes. This additional investment improves the look and feel of the entire neighborhood. Of course, the higher home prices and rents that result from the increased desirability of this neighborhood also limit the availability of affordable housing. These pressures on the market are being felt here in the Washington metropolitan area as well as in other expanding real estate markets.\n\nThe question of gentrification in Columbia Heights was interestingly portrayed in the Washington Post earlier this month through the eyes of a pair of twelve-year old girls who have grown up in this neighborhood. The girls entered a citywide essay contest on gentrification, a word that they had not previously known, and observed very keenly the effects of the development going on around them. They noted their increased feeling of safety as they walked to and from school thanks to the rehabilitation of previously vacant rowhouses and the exciting new diversity of individuals living in their neighborhoods. But they also missed the friends whose families were forced to move out of the neighborhood when it became too expensive to stay. They were saddened by the loss of some of the smaller shops that will be replaced by larger retailers. In the end, the girls concluded that gentrification is a complicated issue, and its results are \"not always bad and not always good.\"\n\nI would have to agree that gentrification is complicated and requires thorough thought and planning as neighborhood revitalization strategies are developed. Many strategies are being developed at the local level to address the increasing need for affordable housing; these strategies include housing trust funds, private activity bonds, and community land trusts, to name just a few. The Federal Reserve Bank of Richmond is currently studying patterns of home price changes and gentrification in neighborhoods here in Washington, D.C. The work is intended to assist city governments, community development corporations, and funders in anticipating and mitigating the effects of gentrification on lower-income residents. As difficult as the challenges seem, I am confident that if policymakers, researchers, and public and private community development practitioners continue to work together, they will be able to balance new investments with strategies to encourage affordable housing options for sustainable and equitable community development.\n\nIn the meantime, my colleagues at the Board of Governors and I are committed to sustaining a noninflationary economic expansion and a healthy financial system, which should create a solid foundation for the development of new business opportunities. Of course, although a strong economy and vibrant capital markets are necessary to establish a solid market-based foundation for economic development, they are not sufficient. Successful community development projects require vision, commitment, tough choices, and hard work. Indeed, these are the qualities that each of the projects honored here tonight share. I am pleased to be able to give my personal congratulations to tonight’s honorees and to Social Compact for leading the way in the effort to uncover the market potential of inner-city neighborhoods through innovation, risk taking, and the sharing of results.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/olson20060525a.htm",
        "title": "Community Development and the Bigger Picture",
        "date": "5/25/2006"
    },
    {
        "content": "May 24, 2006\n\nGovernor Randall S. Kroszner\n\nAt the National Association for Business Economics Professional Development Seminar for Journalists, Washington, D.C.\n\nAccurate and timely statistics are fundamental to sound financial and economic decisionmaking in both the private and public sectors. The quality of the economic statistics in the United States is among the best, if not the best, in the world, but that should not make us complacent. The U.S. economy is extremely dynamic, and improvements in our economic statistics should reflect--and measure--that ever-present change.\n\nNaturally, markets work best when people are well informed. Business people rely on economic statistics when they make and execute plans for production, investment, and hiring. Academics and researchers around the world also use economic statistics to evaluate alternative models and theories that further our understanding of how the economy and financial markets operate.\n\nReliable and timely economic statistics are crucial to the formulation and evaluation of budgetary and monetary policies. Measuring U.S. gross domestic product (GDP), for example, is an enormously difficult task, but accurate measurements are extremely important for policymakers' decisions. An error of just 1/10 percentage point in projections of long-term real GDP growth, for instance, can result in an error of approximately $270 billion in a ten-year budget forecast.\n\nMonetary policy relies crucially upon sound statistics. I am sure that many of you have heard the analogy comparing a monetary policy maker to the driver of a car. The policymaker is faced with the decision of whether to tap on the accelerator or the brake to maintain the proper speed for the economy. In 2004, then-Governor Bernanke commented that, in part given the imperfect nature of economic statistics, \"if making monetary policy is like driving a car, then the car is one that has an unreliable speedometer, a foggy windshield, and a tendency to respond unpredictably and with a delay to the accelerator or the brake.\"1 Trying to fix that speedometer and to clear some of the fog from the windshield is what I want to focus on in my remarks today.\n\nLet me start with a couple of concrete examples of the importance of improving economic statistics for monetary policy. The first concerns the measurement of productivity, that is, the amount of output for each hour of labor. Productivity is a key element in the evaluation of how rapidly the economy can grow before raising concerns about inflationary pressures. Errors in measuring either output or hours of work--unless they are in both and just happen to exactly offset each other--will lead to distortions in the measurement of productivity. Such a distortion occurred in the mid-1990s. Work done at the time by the Board's staff suggested that measured productivity in some sectors of the economy was implausibly low.2 As a consequence, Chairman Greenspan urged the Federal Open Market Committee (FOMC) to pursue a more accommodative monetary policy than the published productivity statistics, at the time, might have suggested. The result was a period of high economic growth and low inflation.\n\nPrices are another set of data that are important for monetary policy. The primary responsibility of monetary policy is to ensure that we have a low and stable rate of inflation and the maximum sustainable rate of employment growth. Inaccurate price measurement could result in inaccurate forecasts of the direction of inflation and, thereby, lead to policy choices that are too tight or too accommodative.\n\nAs a researcher, I find that timely and reliable information is critical for analyzing, measuring, and calibrating the ways in which the various parts of our complex economy operate.\n\nAlthough most people think of banks, money, and interest rates when they hear mention of the Federal Reserve, the central bank is also an important producer of economic statistics. Among other things, the Federal Reserve publishes data on interest rates, industrial production, and financial accounts for the nation--the flow of funds system. The Federal Reserve also has a strong commitment to promoting economic education and financial literacy. Recently we have begun using the latest Internet-based technology to make the data we publish more accessible and easier to understand. In particular, last month we added to our web site a new data download application, which provides interactive access to Federal Reserve statistical data in a variety of electronic formats. We view this new feature as a key element in our mission of promoting financial education.\n\nA Cost-Benefit Framework\nAs an economist, I like to think about the issues surrounding the improvement of economic statistics using the framework of cost-benefit analysis. As applied to economic statistics, cost-benefit analysis essentially says that because the federal government has limited resources, it's important that the statistical agencies and the nation get the biggest bang possible for the buck. The goal is to allocate resources most efficiently and effectively, focusing on the data that are most important for private and public decisionmaking.3\n\nImproving the cost-benefit tradeoff for economic statistics has several elements, and I will focus on two in my remarks. The common theme is that innovations in economic statistics must keep pace with innovations in the economy.\n\nOne element to improving the cost-benefit tradeoff is removing legislative barriers that raise costs without providing benefits. Sometimes individual federal statistical agencies must separately collect information from the public on the same subject because the agencies are prohibited by law from sharing the information with each other. In 2002, the Congress enacted an important piece of legislation called the Confidential Information Protection and Statistical Efficiency Act (CIPSEA). As a member of the President's Council of Economic Advisers at the time, I helped lead the effort to urge passage of CIPSEA.4\n\nCIPSEA tackled two important issues. First, the act strengthens the safeguards that protect the confidentiality of information provided by the public. The legislation applies clear and uniform statutory restrictions on the use of confidential statistical information. In particular, information about individuals or organizations acquired for exclusively statistical purposes and under a pledge of confidentiality can be used only for statistical purposes. The act replaced a patchwork of safeguards with consistent tough penalties for the unauthorized disclosure of confidential statistical information.\n\nSecond, the act authorized the limited sharing of business data among the Bureau of the Census, the Bureau of Economic Analysis (BEA), and the Bureau of Labor Statistics (BLS) for statistical purposes. Allowing the agencies to share certain businesses data has improved the accuracy and reliability of economic statistics. In particular, enhanced data sharing among the agencies has improved the ability of the Census Bureau, the BEA, and the BLS to track rapidly changing trends in the U.S. economy. In addition, it has helped to reduce the duplicative paperwork burdens imposed on businesses.\n\nEven with the 2002 legislation, however, more can be done to realize the full benefits of data sharing. What I'm about to discuss is going to sound very arcane, but it is quite important for the improvement of economic statistics. So please bear with me.\n\nThe Census Bureau conducts a large number of business surveys the results of which are key inputs into the estimation of the nation's GDP. The Census Bureau selects businesses to survey from a list of establishments called a \"business register.\" The Census Bureau uses tax reports as a way of identifying establishments to include in its business register. It's very important to note that the Census Bureau is not rummaging through individual line items on company tax reports but rather is using the reports only to identify companies for inclusion in its surveys.\n\nMeanwhile, every month the BLS conducts a survey of establishments to find out about such things as employment and earnings. The BLS also uses a business register to decide which establishments to contact. However, the BLS register comes from reports filed by firms to state unemployment insurance offices.\n\nHere's the rub: In many instances establishments show up as being part of different industries in the two registers. As a result, industry analyses that use survey data on employment or prices from the BLS and survey data on shipments from the Census Bureau may well provide unreliable characterizations of changes in real output and productivity for particular industries.\n\nHow important is this problem? Let me give you a few examples. A limited study compared the Census Bureau's and BLS's business registers for 1994 and found that 30 percent of the same single-establishment firms had been assigned different detailed industry codes. Staff work at the Federal Reserve Board found that, according to the BLS in 1997, 1.1 million workers were employed in the industry category known as Management of Companies, whereas the Census Bureau tallied employment at 2.6 million for that year!5 In other industries, the differences in 1997, though not as large, still are dramatic. In the Oil and Gas Extraction industry, the employment counts differ by more than 30 percent. In the Computer and Electronic Product Manufacturing industry, the BLS and Census counts differ by more than 12 percent, and so on.\n\nThe reason the two business registers have not been harmonized is that current law prevents the BLS from having the same access as the Census Bureau to business identifiers on tax forms. Removing this barrier to sharing such data for statistical purposes would have significant payoffs. Sharing of business registers would help provide for more accurate measures of industry output, compensation, and productivity trends. It would permit the statistical agencies to keep abreast of our dynamic economy by producing statistical samples that are consistently adjusted for the entry and exit of new businesses in a timely manner, and it would allow the agencies to correct errors quickly and efficiently. This is especially important for fast-growing and innovative industries such as information technology. Such improvements would improve our ability to perceive emerging trends in the economy and more accurately forecast economic activity.\n\nThe BEA, the statistical agency responsible for estimating GDP and the other components of the national income and product accounts, cannot currently access information from business taxes. Allowing the BEA to obtain certain aggregate numbers from business tax returns for statistical purposes would let the BEA significantly improve its estimates of the role of noncorporate businesses in the economy. This is important, in part, because many entrepreneurial start-up firms, which are a source of dynamism in the economy, begin their lives organized as sole proprietorships or partnerships--that is, not as corporations. Permitting the BEA to use limited business tax information for statistical purposes could appreciably improve the measurement of this vibrant part of the economy and, once again, improve our ability to spot new trends and forecast economic activity. Of course, in obtaining this information, the BEA would continue to be subject to the strict confidentiality requirements and disclosure penalties embodied in CIPSEA. Protecting data privacy is of utmost concern, and I would certainly not suggest changes that would compromise confidentiality.\n\nI now would like to turn to the second element I want to emphasize in improving the cost-benefit trade off for economic statistics, namely, that the statistical agencies hasten their move toward a more effective allocation of current resources. The usefulness of economic statistics--the benefit side of the cost-benefit tradeoff--is the key. The economy is constantly changing. In the past, there has been a tendency to persist in a full and expensive compilation of detailed data for heretofore cutting-edge industries far beyond the time at which those industries stopped being prominent in the economy. Consequently, statistical agencies need to be constantly assessing whether their data reflect the changes--that is, you don't want to be spending all your resources on collecting information about buggy whips in the age of the automobile.\n\nAn innovative economy needs to have innovative statistics. This means that the statistical agencies need to be on the cutting edge in measuring the most dynamic parts of the economy. Typically, we think of dynamism in terms of those segments of the economy--usually industries and sometimes occupations--where output or employment are growing rapidly. But output and employment growth are not the only criteria for dynamism. Rapid increases in what economists call \"intangible capital\" are another characteristic of dynamic firms. This includes such things as expenditures on scientific research and development (R&D) as well as the breakthrough improvements to businesses processes that have made so many American companies successful. Dynamic firms are also characterized by the proliferation of new products that incorporate large amounts of R&D as well as by rapid improvements in the quality of products.\n\nThese brisk gains in intangible capital and the quality of products are important because they are an important source of productivity growth. In other words, dynamic firms are the ones making the largest contributions to the nation's overall productivity growth, which is, at bottom, the fundamental source of rising standards of living. Accordingly, it is important that the economic statistical system do a good job of measuring not only output and employment growth but also intangibles, new cutting-edge products, and quality changes.\n\nGiven the speed and short histories of many of these changes and innovations, however, it is particularly challenging to create statistics for new and dynamic parts of the economy that will be as reliable as those in the more traditional sectors. We need to be mindful of the difficult burden that the statistical agencies face in trying to maintain their very high standards for quality in such areas.\n\nRe-allocation of resources is rarely easy since it means cutting back on something. It is not appropriate for the Federal Reserve to get into the details of how the Congress or the statistical agencies should spend the taxpayers' money. However, as consumers of the data produced by the agencies, it is fitting for the Federal Reserve to indicate improvements to economic statistics that would enhance their usefulness for the analyses and forecasts upon which we base our decisionmaking. Indeed, in our role as data consumers, the agencies often seek our recommendations.6 So, with that in mind, let me offer some specifics.\n\nI'll start with the way in which the Census Bureau classifies the outputs of the economy into particular categories for data collection purposes. The Census Bureau creates a list of so-called \"product codes\" that identify the types of goods and services that companies produce and sell. They group similar products into larger categories, but innovations in the economy have made those groupings woefully out of date.\n\nFor the past several years, the Census Bureau has been developing more extensive product codes for services, and that effort is to be applauded. It is precisely the right response to a dynamic economy in which services have become much more important over time. In addition, improving product codes for communications equipment and other high-tech components of manufacturing also should be given high priority.\n\nTo illustrate, consider the product category \"broadcast, studio, and related equipment.\" The Census Bureau publishes sixteen subcategories of product data--items such as AM and FM radio transmitters, for which shipments in 2004 were valued at $103 million; or cable-TV subscriber equipment (decoders, switches, and so forth), with shipments worth $41 million; or studio transmission links (the hardware to bring a live reporter's feed back to the studio), for which shipments were worth $18 million.7\n\nNow, contrast this with the product code called \"data communications equipment.\" Unlike \"broadcast, studio, and related equipment,\" there are no break-downs into subcategories. Such leading-edge products as large enterprise routers, gateways, bridges, and terminal servers are combined into a single category. In 2004, shipments from manufacturers of data communications equipment totaled more than $10-1/2 billion. The current system of product codes thus provides as much information about a well-established technology with shipments worth $18 million as a grouping of rapidly changing technology products with shipments worth more than $10-1/2 billion.\n\nSimilarly, the current system of product codes tell us as much about shipments of public pay telephone as it does about shipments of cellular system equipment--an industry that is more than 230 times larger ($43 million versus nearly $10 billion). The task of updating product lists is resource intensive and time consuming, but it is critical to gaining a more comprehensive understanding of developments in the most vibrant sectors of our economy.\n\nIn addition to updating product codes to reflect change, another data challenge of our dynamic economy is to adjust the prices of high-tech equipment for improvements in quality. By adjusting for quality, I mean price statistics that recognize that a computer that costs $1,000 today is several times more powerful than a computer that cost $1,000 ten years ago. Accordingly, after taking account of the quality improvements, one can say that today's computers cost far less than computers cost ten years ago because you get much more for your money today. Government price data for computers do try to adjust for quality improvements, and the agencies must be applauded for undertaking such adjustments to keep pace with innovations in the economy.\n\nMany dynamic sectors, however, still await adjustments for quality improvements. As a result, the statistical measurement system is not fully capturing these critical technology improvements, which are a key source of productivity gains in the IT sector.8 Many types of logic chips that are now common in everything from cell phones to DVD players have been improving rapidly. Their prices, however, have not been adjusted for quality and such chips are simply lumped in with \"other semiconductors.\" Because semiconductor prices are often used as a gauge of technological progress--the faster the prices fall, the faster technology is improving--inadequate price measurement may be leading to inadequate assessment of the pace of technological progress.\n\nAnother product for which improved price statistics could be particularly valuable is medical diagnostic equipment--the forgotten part of the high-tech equipment sector. 9 Government price statistics combine all medical diagnostic equipment (CT scanners, MRI scanners, and the like) into a single bundle. The pace of technological advances for these types of equipment has been breathtaking and reflects, in part, ongoing miniaturization afforded by the increased use of embedded computer-like components. Gathering additional information about high-tech medical equipment would not only round out our picture of high-tech equipment and components but also shed light on some, though certainly not all, of the questions that have been posed about the contribution of medical technology to health-care costs.10\n\nLet me leave price measurement and conclude with another area where innovative statistics are valuable for enhancing our understanding of the economy. As I mentioned previously, intangible capital is an important characteristic of dynamic firms. Currently, however, the government publishes no comprehensive statistics on intangible capital. In their pathbreaking work, Federal Reserve economists Carol Corrado and Daniel Sichel, along with University of Maryland economist Charles Hulten, identify three broad types of intangible capital: computerized information--primarily software; innovative property--knowledge acquired through scientific R&D and nonscientific inventive and creative activities; and economic competencies--knowledge embedded in firm-specific human and structural resources.11\n\nLater this year the BEA plans to issue a satellite account that will treat scientific R&D as investment. This is a significant and valuable step forward. Scientific R&D is measured by an important survey sponsored by the National Science Foundation and conducted by the Census Bureau. As Corrado, Sichel, and Hulten note, entrepreneurs and businesses also devote a broad array of \"nonscientific\" resources, including the development of entertainment and artistic originals, to develop new products and processes. Further work to investigate the feasibility of capturing such information at reasonable cost could produce benefits of increasing our understanding of productivity and growth trends.12\n\nIn applying a cost-benefit framework, I think that it would be valuable for the statistical agencies to continue to seek opportunities to partner with the private sector in order to boost efficiencies.13 Perhaps the private sector could help collect data and even help to process and disseminate it. Retail chains have extensive electronic data systems on the details of consumer purchases--a wealth of data on consumer spending patterns that is now being analyzed for statistical purposes.14 And high-tech firms have excellent information on inventories, sales, and prices, which could help to provide a better snapshot of innovations that are driving the most dynamic parts of the economy. To some extent, the statistical agencies, including the Federal Reserve, already use data--both public and proprietary--that are collected by the private sector. The key issue is finding more opportunities for private-public partnerships that are efficient, mutually beneficial, and do not compromise the high quality of federal statistics that we have come to expect.\n\nSummary and Conclusion\nAs I noted in my introduction, the U.S. enjoys among the best, if not the best, economic statistics in the world. My remarks have been focused on building on that excellent foundation using a cost-benefit framework. Innovations in economic statistics must keep pace with innovations in the economy. Barriers to useful sharing of information across statistical agencies should be removed to reduce costs and enhance benefits, but in no way should we compromise the high standards of privacy protection embodied in CIPSEA and other statutes. And allocating resources to better measure the most dynamic parts of the economy will help us get the most bang per buck in economic statistics by enhancing our ability to spot trends and improve forecasts of the direction of the economy.\n\nPolicymakers, businesses, and average Americans are able to make better decisions with economic statistics that reflect the latest innovations in the economy. Achieving an innovation-sensitive statistical system can be fostered by removing legislative barriers that impose costs on the system with no (or little) benefit. Also, statistical agencies need to be nimble at recognizing and responding promptly to emerging trends in the structure of the economy. One way of achieving nimbleness is by getting feedback from users in government, in the academy and in business. The Federal Economic Statistics Advisory Committee and the BEA advisory committee are good examples of statistical agencies trying to increase the amount of feedback they receive. And organizations like the National Association for Business Economics can communicate more widely the importance of innovative statistics for a dynamic economy. Together we can try to fix the speedometer and clear the fog from the windshield to improve both public and private decisionmaking.\n\nFootnotes\n\n1.  Ben S. Bernanke (2004), \"The Logic of Monetary Policy\", speech delivered before the National Economists Club, December 2, 2004.  Return to text\n\n2.  Carol Corrado and Lawrence Slifman (1999), \"Decomposition of Productivity and Unit Costs,\" American Economic Review, vol. 89 (May), pp. 328 -32.  Return to text\n\n3.  Former Federal Reserve Chairman Alan Greenspan, in comments submitted to the Senate Banking Committee in 2002, said: \"I am reluctant to support increased spending. In the case of certain economic statistics, however, the benefits are so large relative to cost that there should be little question as to its desirability\" (Alan Greenspan, 2002, \"Response to Written Questions,\" in Federal Reserve's Second Monetary Policy Report for 2002, hearing before the Senate Committee on Banking, Housing, and Urban Affairs, U.S. Senate, July 16, 107 Cong. (Washington: Government Printing Office), p. 47.  Return to text\n\n4.  Randall S. Kroszner (2002), \"Prepared Statement\", in H.R. 5215, Confidential Information Protection and Statistical Efficiency Act of 2002, hearing before the Subcommittee on Government Efficiency, Financial Management and Intergovernmental Relations of the Committee on Government Reform, U.S. House of Representatives, Sept. 17, 107 Cong. (Washington: Government Printing Office), pp. 36-39.  Return to text\n\n5.  Remarks of Carol Corrado in \"Monetary Policy and Research at the Federal Reserve,\" in chap. 3 of Caryn Kuebler and Christopher Mackie, rapporteurs (forthcoming), Improving Business Statistics Through Interagency Data Sharing: Summary of a Workshop (Washington: National Academies Press).  Return to text\n\n6.  For example, Lawrence Slifman (2002), \"Bureau of Economic Analysis' Strategic Plan for 2001-2005: Comments\" (636 KB PDF), Survey of Current Business (May), pp. 9-10.  Return to text\n\n7.  U.S. Census Bureau (2005), \"Communication Equipment: 2004\" (293 KB PDF), Current Industrial Reports, MA334P(04)-1 (Washington: Census Bureau, August).  Return to text\n\n8.  See Stephen D. Oliner and Daniel E. Sichel (2000). \"The Resurgence of Growth in the Late 1990s: Is Information Technology the Story?\" Journal of Economic Perspectives, vol. 14 (Fall), pp. 3-22; and Dale W. Jorgenson and Kevin J. Stiroh (2000), \"Raising the Speed Limit: U.S. Economic Growth in the Information Age,\" Brookings Papers on Economic Activity, 1, pp. 125-211.  Return to text\n\n9.  The discussion of medical equipment is based on Jack E. Triplett and Barry P. Bosworth (2004), Productivity in the U.S. Services Sector (Washington: Brookings Institution Press), pp. 304-20.  Return to text\n\n10.  Refer, for example, to Kevin M. Murphy and Robert H. Topel (2003), \"The Economic Value of Medical Research,\" in Kevin M. Murphy and Robert H. Topel, eds., Measuring the Gains from Medical Research: An Economic Approach (Chicago: University of Chicago Press), pp. 41-73.  Return to text\n\n11.  Carol Corrado, Charles Hulten, and Daniel Sichel (2005), \"Measuring Capital and Technology: An Expanded Framework,\" in Carol Corrado, John Haltiwanger, and Daniel Sichel, eds., Measuring Capital in the New Economy (Chicago: University of Chicago Press ), pp. 11-45.  Return to text\n\n12.  Carol Corrado, Charles Hulten, and Daniel Sichel (2006), \"Intangible Capital and Economic Growth\", NBER Working Paper Series 11948 (Cambridge, Mass.: National Bureau of Economic Research, January).  Return to text\n\n13.  Randall S. Kroszner (2002), \"Bureau of Economic Analysis' Strategic Plan for 2001-2005: Comments\" (636 KB PDF), Survey of Current Business (May), pp. 10-11  Return to text\n\n14.  For example, Robert C. Feenstra and Matthew D. Shapiro, eds. (2003), Scanner Data and Price Indexes, Studies in Income and Wealth, vol. 64 (Chicago : University of Chicago Press); and Erik Hurst and Mark Aguiar (2005), \"Lifecycle Prices and Production\", NBER Working Paper Series 11601 (Cambridge, Mass.: National Bureau of Economic Research, September).  Return to text\n\nData Download",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20060524a.htm",
        "title": "Innovative Statistics for a Dynamic Economy",
        "date": "5/24/2006"
    },
    {
        "content": "May 18, 2006\n\nChairman Ben S. Bernanke\n\nAt the Federal Reserve Bank of Chicago’s 42nd Annual Conference on Bank Structure and Competition, Chicago, Illinois\n\nI am pleased to speak to you this morning at what has become, over more than four decades, perhaps the most prestigious conference for bankers, academics, and bank supervisors in the United States. In every year but one of his tenure, Chairman Greenspan spoke at this meeting, and he sometimes used the occasion to advocate major changes to the bank regulatory system. Notable examples include the revisions to deposit insurance and bank capital standards that were embedded in the Federal Deposit Insurance Corporation Improvement Act of 1991 (FDICIA), the relaxation of restrictions on interstate banking in the Riegle-Neal Act of 1994, and the repeal of Glass-Steagall's limits on combinations of commercial and investment banking implemented by the Gramm-Leach-Bliley Act of 1999.\n\nToday, I will discuss the importance of implementing another major change in the way we regulate and supervise banking organizations, especially our largest, most complex, and most internationally active firms.1 That change is the modernization of our approach to assessing the adequacy of bank capital, within the context of the framework that has come to be known as Basel II. The current system of bank capital standards, the so-called Basel I framework agreed upon internationally in 1988, was a major step forward in that it embodied the important principle that regulatory capital requirements should be tied to the risks taken by each banking institution.2 However, the relatively crude method of assigning risk weights to assets, as well as an emphasis on balance-sheet risks as opposed to other risks facing financial firms, limits the overall responsiveness of capital requirements to risk under Basel I, which renders that system increasingly inadequate for supervising the largest and most complex banking organizations. For these organizations, we need to move beyond Basel I to a more risk-sensitive and more comprehensive framework for assessing capital adequacy. Basel II represents the concerted efforts of the supervisory community, in consultation with banks and other stakeholders, to develop such a framework.\n\nThe broad principles of Basel II have become increasingly clear over many years of discussion and consultation, and I will review them briefly. I will argue that a framework built on these principles is the right one for supervising the largest and most complex institutions. Moreover, the Basel II framework is inherently dynamic; it will be able to adapt to ongoing innovation and change. But I also want to convey today that Basel II remains very much a work in progress. As we proceed toward the implementation of this framework, success will require that bank regulators, the banking industry, the Congress, and other relevant parties engage in an ongoing and frank dialogue, and that policymakers be open-minded, flexible, and ready to make needed adjustments. For our part, the Federal Reserve is committed to getting Basel II right.\n\nFinancial Innovations in Mortgage and Other Markets\nAs in years past, the Chicago Fed has chosen wisely in selecting the theme of the conference. Innovations in the financing of homes and other real estate have come at a remarkable pace over the past decade or more, leading to more sophisticated and flexible instruments, more liquid markets, and better risk-sharing. Taking full advantage of these innovations has required banks and other institutions to make important improvements in risk measurement and risk management.\n\nFinancial innovations and improved risk management have not been limited to real estate finance, of course. Securitization, improved hedging instruments and strategies, more-liquid markets, greater risk-based pricing, and the data-collection and management systems needed to implement such innovations have also occurred in other retail and wholesale markets.\n\nIn my judgment, these developments, on net, have provided significant benefits. Borrowers have more choices and greater access to credit; lenders and investors are better able to measure and manage risk; and, because of the dispersion of financial risks to those more willing and able to bear them, the economy and the financial system are more resilient. To be sure, rapid financial innovation carries some risks, if new instruments are used improperly or if the market infrastructure that facilitates the trading of those instruments is inadequate. Regulators must be aware of and ready to mitigate those risks. Overall, however, the public interest is served both by the prudent use of new financial instruments and by the improvements in risk-measurement and risk-management practices that prudent use requires.\n\nImplications for Financial Regulation\nAs market participants innovate and markets become more efficient and sophisticated, bank regulators and supervisors must ensure that they do not fall behind. Indeed, a regulatory and supervisory system that is increasingly divorced from actual business practice may well become counterproductive. A regulatory and supervisory system that is not in tune with practice may increase the costs of regulation, stifle efficiency and innovation, and ultimately be less effective in mitigating the moral hazard problems associated with the financial safety net.\n\nMost regulatory changes are incremental, which is entirely appropriate given the inherent uncertainties about how best to respond to market developments and the desire to avoid unintended consequences. Examples of incremental regulatory change are common. For example, the U.S. banking agencies have made more than twenty-five amendments to the Basel I framework in response to ongoing changes in banking and financial markets.\n\nBut sometimes a more fundamental rethinking of the regulatory framework is needed. Sometimes a crisis is the catalyst for such rethinking, as in the case of the deposit insurance and capital policy reforms in FDICIA that I mentioned earlier. At other times, major change is motivated by the gradual accumulation of factors, such as the blending of commercial and investment banking that evolved before passage of the Gramm-Leach-Bliley Act. I would put the need for Basel II squarely in this latter category. No immediate crisis requires us to move toward Basel II, but the gradual evolution of market practice and the emergence of very large and increasingly complex banking organizations operating on a global scale require that we make significant changes in the way we assess capital adequacy at these organizations. Indeed, waiting for a crisis to force change would be foolish; by moving forward now, we have the luxury of being deliberate in the development and introduction of a system that promises significant benefits.\n\nWhy Basel II?\nMany aspects and possible effects of Basel II have been debated, including its potential effects on banks' costs and on the competitive landscape. I will discuss some of those aspects shortly. It is important to keep in mind, however, that the core goal of Basel II is to promote the stability of the U.S. financial system by ensuring the safety and soundness of U.S. banks. Its ability to promote that objective is the first criterion on which the proposed Basel II framework should be judged.\n\nI stated that the core goal of Basel II is to promote the stability of the U.S. financial system; but, as everyone here understands, the U.S. banking and financial system is increasingly interwoven with that of the rest of the world. Indeed, increasing globalization was a major factor that led the U.S. banking agencies to negotiate the Basel I agreement in 1988. Of course, the extent of globalization of both financial and real product markets is even more extensive today, and systemic financial problems will not respect national borders. Thus, it remains very much in the interest of the United States to continue to encourage international cooperation and consistency in regulating and supervising those banks that pose the greatest potential systemic risk. For this reason, we have chosen to work with our international and domestic colleagues on the Basel Committee to develop a new capital framework for the largest, most complex, and most globally active banking organizations.\n\nTo maintain U.S. and global financial stability, we want to ensure that banks, particularly our largest and most complex institutions, will remain able to serve their customers and meet their obligations as lenders and counterparties during periods of economic or financial stress. That requires, of course, that banks have both adequate capital and strong risk management. Because confidence promotes stability, it is also important that supervisors and market participants are able to assess for themselves the financial soundness and risk-management capabilities of these institutions.\n\nBasel II is a comprehensive framework for improving bank safety and soundness by more closely linking regulatory capital requirements with bank risk, by improving the ability of supervisors and financial markets to assess capital adequacy, and by giving banking organizations stronger incentives to improve risk measurement and management. The framework encompasses three elements: risk-focused regulatory capital requirements, supervisory review, and market discipline. These are the so-called three pillars of Basel II.\n\nUnder Pillar 1, the risk sensitivity of minimum risk-based capital requirements would be much greater than under the current accord. This greater sensitivity would be achieved by linking each banking organization's capital requirement to empirically based measures of credit and operational risk, as determined in part by risk parameters estimated by that organization, such as a loan's probability of default and its expected loss given default. The methods used to construct such estimates would be subject to supervisory standards, guidance, and reviews, including a requirement that the risk parameters used for Pillar 1 be consistent with risk assessments actually used by the bank for its internal risk management. The Pillar 1 treatment of credit risk also reflects more accurately the risk-reducing effects of guarantees, credit derivatives, and securitization, thus improving regulatory capital incentives for banks to hedge portfolio credit risks. The incorporation of operational risk in Pillar 1 is also a significant step, which recognizes that operational failures are indeed a potentially important risk for many banks, one that they should actively seek to minimize. In addition, Pillar 1 incorporates a more comprehensive treatment of trading account risk.\n\nWe should not underestimate the importance of strong minimum capital requirements. Strong capital helps banks absorb unexpected shocks and reduces the moral hazard associated with the federal safety net. A key lesson of the banking and thrift crises of the late 1980s and early 1990s is that prudent and explicit minimum regulatory capital requirements are needed to ensure that banks maintain adequate capital and to anchor an effective supervisory system. For example, explicit minimum regulatory capital requirements that accurately reflect a bank's risk provide more-effective triggers for prompt corrective action.\n\nBesides making regulatory capital ratios more risk-sensitive, Basel II provides a consistent framework for improving supervisory assessments of capital adequacy and risk management. Under Pillar 2, a bank would be required to maintain a capital cushion above the regulatory minimums to capture the full set of risks to which the bank is exposed. These include liquidity risk, interest rate risk, and concentration risk, none of which is reflected in Pillar 1. Currently, the U.S. banking agencies assess a bank's overall capital adequacy as a normal part of the examination process. But the overall quality of both the supervisors' and each bank's assessments of capital adequacy should improve greatly under Basel II because of the expanded information that will be available from Pillar 1, from supervisory reviews of a bank's systems for implementing Pillar 1 and Pillar 2, and from the bank's own analyses.\n\nUnder Pillar 3, banks will be required to disclose to the public the new risk-based capital ratios and more-extensive information about the credit quality of their portfolios and their practices in measuring and managing risk. Such disclosures should make banks more transparent to financial markets, thereby improving market discipline.\n\nTaken together, these three pillars provide a broad and coherent framework for linking regulatory capital to risk, for improving internal risk measurement and management, and for enhancing supervisory and market discipline at large, complex, and internationally active banks. The three pillars build on the economic capital and other risk-management approaches of well-managed banks and better align regulatory and supervisory practices with the way the best-run banks are actually managed. As a result, Basel II will be better able than the current system to adapt over time to innovations in banking and markets. In addition, Basel II sets standards for risk measurement and management and for related disclosures that will give banks ongoing incentives to improve their practices in these areas. Indeed, we have already seen significant progress in risk measurement and management at many banks in the United States and elsewhere as a result of the Basel II development process. More progress can be expected as we move forward.\n\nThe Need for Ongoing Dialogue and Flexibility\nBasel II has the potential to provide significant benefits, but any policy change as fundamental as Basel II inevitably creates uncertainties and raises difficult and complex tradeoffs. Successfully dealing with these challenges requires that the banking agencies, the industry, and other stakeholders maintain a frank and ongoing exchange of views and remain flexible and open-minded as we tackle difficult issues.\n\nIn this spirit, I urge the industry and other interested parties to comment thoroughly on the Basel II Notice of Proposed Rulemaking (NPR), proposed supervisory guidance that will come out in the near future, and the upcoming proposed changes to Basel I that would apply to the vast majority of banks not subject to Basel II. In an effort to aid this process, I would like to touch on a number of concerns that have been raised regarding Basel II.\n\nThe first issue is the complexity and potential cost of the framework. The Basel II NPR is long and detailed. The draft agreed upon among the banking agencies and approved unanimously by the Federal Reserve's Board of Governors in late March is just short of 450 pages. The length and complexity of that document, not to mention the additional supervisory guidance that will be needed, have led some to fear that the costs of implementing Basel II in the United States will outweigh any benefits it brings in terms of greater safety and soundness of banks, improved risk measurement and management, and better market discipline.\n\nI understand these concerns. We must recognize, however, that the Basel II proposal is complex for the good reason that modern risk measurement and risk management are inherently complex activities. Indeed, some commenters have argued that Basel II is overly simplistic in certain areas. The draft proposals also represent many judgments about how best to deal with sometimes competing objectives. On the one hand, the system must be enforceable and it must allow for reasonable comparability of regulatory capital ratios across large, complex, and diverse institutions. Thus, some standardization is required. Regulators and supervisors also owe it to the industry and other market participants to be as clear as possible about what we mean by such core concepts as the probability of default and loss given default and about our expectations for how banks go about estimating such parameters.\n\nOn the other hand, the diversity of practices across banking organizations, the absence of any single, definitive \"best practice,\" and the need to provide strong incentives for improving risk measurement and management require that the system be flexible enough to allow the exercise of judgment by supervisors and bankers. This need for flexibility and the use of judgment is the major reason that we have long emphasized that the supervisory reviews in Pillar 2 are a necessary complement to the explicit minimum regulatory capital requirements set forth in Pillar 1.\n\nNaturally, we would like to see the framework implemented as cost-effectively as possible. The desire to avoid unnecessary regulatory costs is another reason why we have tried to build Basel II on what banks are already doing. However, Basel II does ask banks to make some infrastructure investments whose primary purpose is to help supervisors validate and compare banks' systems--investments that many banks would not otherwise make. Feedback from the industry regarding our proposals in this area would be very helpful.\n\nA key mechanism in Basel II for balancing the inevitable tensions that arise when attempting to achieve sometimes competing objectives is the so-called use test. Under the use test, the systems and processes that a bank uses for regulatory capital purposes must be consistent with those used internally. Note that I use the word \"consistent,\" not \"identical.\" For example, minimum regulatory capital need not equal the economic capital requirements computed internally by a bank, but our intent is that the two will be highly correlated. In addition, Basel II seeks to accommodate a range of risk- measurement and risk-management practices, a range that can change over time. No regulator wants to impose a single definition of \"best practice\" or to set current practice in stone. In light of questions about the use test, I would highlight our desire for specific feedback on how best to implement this important principle.\n\nMore generally, I hope the industry will work with the agencies to identify aspects of the framework that are neither cost-effective nor supportive of sound risk management. Relative to the Advanced Notice of Proposed Rulemaking issued in 2003, the current NPR incorporates many specific suggestions from the industry along these lines. Important examples include the definition of default and the treatments of expected losses, loan-loss reserves, credit derivatives, and securitization. If more adjustments are needed, we are ready to listen and adjust.\n\nAnother concern that has been expressed regarding Basel II is that it will unfairly tilt the competitive playing field. This concern has two aspects. First, some have argued that the bifurcated application of Basel II within the United States could allow domestic banks that adopt the framework both lower capital charges on certain activities and lower overall regulatory capital requirements compared with other domestic banks. Lower regulatory capital charges would, it has been argued, translate into a cost advantage for adopters that would place non-adopters at a competitive disadvantage. In addition, some fear that adopters would use any newly created excess regulatory capital to acquire smaller banks.\n\nIt is an important principle that differences in capital rules among institutions should not distort financial markets or create artificial competitive advantages for any particular class of banks. With this principle in mind, the Federal Reserve has conducted research on the potential competitive impacts of Basel II, and all of the U.S. banking agencies have received comments from many sources. Based on this information, the banking agencies have announced their intention to propose revisions to the existing Basel I capital standards that would aim to mitigate competitive inequities. Proposals should be released in the not-too-distant future, and I urge interested parties to give us specific advice regarding what else we may need to do to reduce any unintended competitive consequences of Basel II.\n\nThe second competitive equity concern relates to the international consistency with which Basel II will be implemented. Inconsistency in international standards of implementation and enforcement, it is sometimes said, will put internationally active U.S. banks at a competitive disadvantage and may also hurt purely domestic U.S. banks vis-à-vis the U.S. subsidiaries of foreign banks.\n\nAll bank regulators recognize that achieving international consistency will be a challenge. However, this problem is not really new. Companies operating across national borders, and their supervisors, are familiar with the challenges of complying with sometimes conflicting legal and regulatory requirements. Still, we recognize that some international implementation issues will be more complex than those we currently face. U.S. regulators are working hard through the Basel Committee and with individual firms and national supervisors to address international implementation issues. A great deal more effort and cooperation will be needed, but I believe that, as in the past, we can craft an acceptable set of agreements and work out means of resolving future issues.\n\nThe final concern I will discuss is the worry that Basel II could lead to a substantial decline in minimum regulatory capital requirements at adopting banks. I emphasized earlier that, for supervisors, an overarching lesson from the banking and thrift crises of the late-1980s and early-1990s is the importance of prudent minimum regulatory capital standards. All the banking agencies are committed to this principle.\n\nAt present, we cannot quantify precisely how much Basel II, once fully implemented, will affect banks' risk-based capital requirements relative to Basel I levels. Although our quantitative impact studies have been useful, they have been conducted using bank systems and measurements that generally would not be expected to meet the Basel II standards. We will learn more as the process moves forward, the standards and guidance come into sharper focus, and banks upgrade their risk-management systems.\n\nBecause of the irreducible uncertainty in this process, the implementation plan set forth in the NPR incorporates extensive safeguards to limit the potential for unintended consequences, including any possibility of a large decline in required capital levels. These safeguards include a minimum one-year parallel run for each bank, during which the bank will calculate what its risk-based capital requirement would be under Basel II, even though its actual requirement will be determined using the Basel I rules. Following the parallel run, each bank will be subject to a transition period of at least three years during which capital floors based on the Basel I rules will ensure that there is no sharp decline in regulatory capital. In addition, the agencies have committed themselves, throughout the transition process and beyond, to continually evaluate the effects of the new framework and to make any needed adjustments to ensure prudent levels of capital. Finally, I would note that, even when Basel II is fully implemented, all banking organizations will continue to be subject to the current minimum leverage-ratio requirement and prompt-corrective-action rules.\n\nThis step-by-step implementation plan, which all the U.S. banking agencies support, should ensure that banking organizations maintain strong capital positions throughout the transition years and after. Moreover, safety and soundness depends not only on the absolute level of capital in the banking system but on how well that capital is deployed. The Basel II framework should make any given level of bank capital work harder, so to speak, by aligning capital more closely with risks taken, by providing incentives for banks to improve their risk management, and by enhancing market discipline through greater transparency. To reiterate, however, to make that framework achieve what is intended requires getting the details right. Therefore, I once more urge the banking industry and other members of the public to review the NPR and the draft supervisory guidance closely and to provide the agencies with constructive comments on all aspects of the proposed framework.\n\nConclusion\nIn summary, I believe the time has come to move to the next stages of implementing Basel II. This framework will modernize bank supervision and bring supervisory practice into line with best industry practice. Substantial benefits will ensue--most importantly, a safer and sounder banking system--but uncertainties remain. Satisfactory resolutions of these uncertainties will require hard work and close cooperation among bank regulators, the industry, Congress, and other key players. There is a long way to go, but the Federal Reserve is committed to this task. It is very much in the best interest of all Americans for the next generation of bank capital standards to be as effective as possible in promoting the stability of the U.S. banking and financial system.\n\nFootnotes\n\n1.  In my talk, I will use the terms \"bank\" and \"banking organization\" interchangeably. Return to text\n\n2.  A second major contribution of Basel I was that it created international standards for measuring bank capital. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060518a.htm",
        "title": "Basel II: Its Promise and Its Challenges",
        "date": "5/18/2006"
    },
    {
        "content": "May 18, 2006\n\nGovernor Donald L. Kohn\n\nAt the Conference on New Directions for Understanding Systemic Risk, New York, New York\n\nI am very pleased to participate in a conference seeking new directions for understanding systemic risk. Let me begin by thanking the Federal Reserve Bank of New York for hosting the conference and for organizing, along with the National Academy of Sciences, an impressive program that will facilitate interdisciplinary discussion on a topic of intense interest to central bankers.1\n\nThe Evolution of Institutions and Markets\nMaintaining the stability of the financial system and containing the systemic risk that may arise in financial markets has been central to the Federal Reserve’s mission for as long as there has been a Federal Reserve. Indeed, Congress passed the Federal Reserve Act in 1913 to provide the nation with a safer and more stable monetary and financial system.\n\nThus, the focus of this conference--understanding systemic risk--is not new, though some of the names have changed. At the beginning of the twentieth century, banks were virtually the only financial intermediary available. Periodically, these highly leveraged institutions lost the confidence of depositors, fell into crisis, and reduced their lending to creditworthy borrowers, thereby accentuating economic downturns. The tools that were developed to prevent and to manage financial crises--lender-of-last-resort facilities, supervision and regulation aimed at bank safety and soundness, deposit insurance, and the provision of payment system services--were geared toward a bank-denominated system.\n\nToday, multiple avenues of financial intermediation are available, and the financial system has become much more market-dominated. This evolution of financial institutions and markets probably has made the financial system more resilient. Financial innovations, such as the development of derivatives markets and the securitization of assets, have enabled intermediaries to diversify and manage risk better. Moreover, as markets have become more important, ultimate borrowers have acquired more avenues to tailor their risk profiles and are less dependent on particular lenders, and savers have become better able to diversify and to manage their portfolios. Consequently, the economy has become less vulnerable to problems at individual types of institutions.\n\nHowever, the evolution of financial institutions and markets has not removed the underlying risks and uncertainty associated with financial transactions. Financial institutions and other market participants must still make decisions and take actions with incomplete knowledge about the condition of their counterparties. Actual (or perceived) information asymmetries can increase enormously during times of heightened market volatility or economic stress. In addition, in such circumstances, investors may become very uncertain about the behavior of other market participants and the resulting path of asset prices and extent of market liquidity, creating a situation in which each market participant waits to see how other investors will react.\n\nWhen this happens, the typical correlations derived from historical experience tend to break down, liquidity may be more costly to obtain or unavailable, normal risk management strategies may no longer be useful, questions about counterparty solvency escalate, and the result can be a financial crisis. In times of such high uncertainty, it is human nature to act very cautiously. Investors uncertain about what shocks other market participants have experienced and unsure about what those other market participants are likely to do tend to withdraw from potentially risky financial activities and to hold safer and more liquid assets, such as government securities.\n\nAs a consequence, creditworthy borrowers may not be funded, just like in the old days when banks fell into crisis. These disruptions in the flow of credit in turn can have real effects on the macroeconomy. Moreover, these real effects can create feedback effects on market participants that exacerbate the original financial crisis.\n\nThese motivations and reactions persist in the twenty-first century, and thus so does the possibility of financial crisis, even if the key propagation mechanisms for crisis in our financial system may have changed. The shift from a bank-dominated financial system to a combination of centralized and decentralized markets has increased the importance of market liquidity for determining asset prices and for managing portfolios. The increased importance of markets also has created new and complex interactions among participants in the financial system and heightened the reliance on key market utilities.\n\nWe are in the process of adapting the tools that were developed to prevent and manage financial crises in a bank-dominated financial system to prevent and manage financial crises in the more market-dominated financial system. This is an ongoing process that is very much informed by our experience with past financial crises. In that regard, the 1987 stock market crash and the liquidity crisis in the fall of 1998 are likely to be more typical of modern financial events. In the rest of my talk, I will discuss the following aspects of these two events: their common elements, the Federal Reserve’s response, lessons learned for crisis management, and implications for crisis prevention in the more market-driven financial system.\n\nTwo Modern Financial Crises\nThe 1987 stock market crash may have been the first modern financial crisis in the United States. Unlike previous financial crises, the 1987 stock market decline was not associated with a deposit run or any other problem in the banking sector. Instead, the 20 percent decline on October 19, 1987, in the Standard & Poor’s 500 index--the largest single-day decline since the Great Depression--was driven by investor decisions to reduce equity exposures and by the resulting chaotic trading in the stock markets. Indeed, trading was so chaotic that stock prices were difficult to determine.\n\nThe liquidity crisis in the fall of 1998 was triggered by the Russian debt default in August and then aggravated by the problems at the hedge fund Long-Term Capital Management. During the 1998 crisis, risk spreads widened sharply, stock prices fell, and liquidity became so highly valued that the spread between on- and off-the-run Treasury securities widened substantially. Of even more concern, the capital markets virtually seized up as market participants retreated from risk taking, and, for a time, credit was simply unavailable to many private borrowers at any price.\n\nThe events of 1987 and 1998 had several common elements. First, they began with sharp movements in asset prices. Second, these price movements were exacerbated by market participants trying to protect themselves--with portfolio insurance in 1987 and by closing out positions in 1998. Third, market participants became highly uncertain about the dynamics of the market, the “true” value of assets, and the future movement of asset prices. As a consequence, with their standard risk-management systems seemingly inapplicable, they pulled back from making markets and taking positions and further exacerbated the price action. Fourth, the large and rapid price movements called into doubt the creditworthiness of counterparties, which could no longer be judged by now obsolete financial statements; credit decisions were further complicated by uncertainty about the value of collateral. In turn, the defensive behavior of market participants escalated and reinforced adverse market dynamics. Finally, the decline in asset prices reduced wealth and raised the cost of capital, which seemed likely to reduce both consumption and investment.\n\nNaturally, the Federal Reserve’s response to these crises also had several common elements. We publicly acknowledged that a crisis was under way, and through this recognition and our follow-up actions, we sought to reassure participants that we were doing what we could to limit the possible systemic effects on the economy. The hope was that such reassurance might encourage a return to risk-taking. Accordingly, open market operations were directed toward an easing of reserve-market conditions and were guided by day-to-day developments. For example, in October 1987, open market operations accommodated substantially enlarged desires for excess reserves and a large increase in required reserves associated with a sharp rise in transactions deposits. We monitored the flow of credit through the financial system, and we pointed out to lenders their collective interest in avoiding a credit gridlock that would only tend to accentuate asset price movements. We worked with other agencies to identify weak points in the market and impediments to its functioning. We also eased the stance of monetary policy to counteract the systemic consequences of the ongoing financial crisis.\n\nPrinciples to be Drawn From Recent Financial Crises\nWhat are the principles for Federal Reserve actions that we can draw from these experiences, as well as from other recent, but less severe periods of financial turmoil? First and foremost, the management of financial crisis is easier, and the consequences are less severe, if the economy is in healthy condition. Partly as a consequence in neither 1987 nor in 1998 did the crisis lead to a recession. As monetary policy makers, we improve the odds of financial stability over time when we focus on the fundamentals of maintaining price stability and sustainable growth. When the Federal Reserve conducts monetary policy so as to create low and stable inflation and to firmly anchor expectations that inflation will be contained, it best positions itself to forcefully offset the ill effects of a financial crisis without generating deep concerns that such actions will result in sharply higher inflation. Some argue that economic stability can set the stage for financial instability by breeding complacency about risk. Perhaps, but surely the benefits of macroeconomic stability far outweigh its possible costs.\n\nAnother principle is that a healthy banking system works hand-in-hand with a healthy economy to substantially mitigate the consequences of financial crisis. Banks continue to play critical roles in the market-dominated system, including financing market participant’s holdings of securities and clearing and settling their trades. Healthy banks can be bulwarks against the propagation of financial turmoil; in contrast, questions about the health of banks would raise additional concerns during the course of a crisis.\n\nA third principle is that the actions taken to prevent a crisis should not raise the odds of creating more problems in the future. In particular, the problem of moral hazard is a significant concern. If market participants begin to rely too much on regulators and central bankers to manage possible future crises, they may act in a way that has the effect of raising the risk of a financial crisis. For example, they may fail to engage in adequate due diligence when extending credit to other market participants or to maintain adequate capital for the risks they undertake. And they might come to believe that the government possesses more tools and resources than are actually available to shield them from the consequences of poor risk management.\n\nThe central bank can draw from a menu of actions that involve varying degrees of moral hazard. Those actions range from loans to individual institutions that otherwise would not have access to credit, to moral suasion, to open market operations, which allow the markets to distribute credit and should not cause distortions in private-sector decisionmaking.\n\nHeightened uncertainty is the key characteristic of episodes of financial instability. The central bank may not have any more information than market participants do; in economic models based on such uncertainties, it is the central bank’s willingness to act in the face of uncertainty that differentiates it from other market participants and gives it a positive role to play during financial crises. Its willingness to act ameliorates the negative effects of the uncertainty of other market participants about the solvency of counterparties and the status of markets.\n\nStill, the more information the central bank has, the easier it is for it to choose the best course of action. Policymakers want to choose the path with the lowest moral-hazard consequences to leave market participants with incentives to manage risk appropriately. But policymakers are in a difficult position in a crisis. The costs of not acting forcefully enough will be immediate and obvious--additional disruption to the financial markets and the economy. The costs of acting too forcefully--of interfering unnecessarily in markets and creating moral hazard--manifest themselves over a longer period and may never be traceable to a particular policy choice. The natural inclination to take more intrusive actions that minimize the risks of immediate disruption is probably exacerbated by ignorance and uncertainty; the less you know, the easier it is to imagine bad outcomes and the more reliant you may be on people in the market whose self-interest may well color the information they are giving you.\n\nIn a bank-dominated financial system, the critical information would have come from fellow bank regulators with whom we had been working or from the institutions we collectively had been examining for some time. In a world of financial institutions with a presence in many lines of business crossing national boundaries, obtaining such information, developing cogent analyses, and deciding on a course of action in a crisis requires widespread cooperation among many disparate agencies. Moreover, I should note that, even for possible bank-centric crises, the financial landscape in the United States has changed. The large bank holding companies are involved in a myriad of businesses that cross a multitude of regulatory boundaries. In addition, any future financial crisis that does involve a large, complex banking organization might need to grapple with the “systemic risk exception” that was passed by the Congress in 1993, which requires the boards of the Federal Reserve and of the Federal Deposit Insurance Corporation, the Secretary of the Treasury, and the President to act jointly.\n\nEstablishing the needed coordination across many agencies could potentially create delays during a time when quick action is needed. Moreover, different regulators can have different supervisory philosophies. These potential impediments to a timely response suggest that regulators need to be discussing the possibilities associated with financial crisis on an ongoing basis and long before an actual event. These discussions should encompass the possible actions that might be needed and the information that should be on hand to shape those actions in the public interest.\n\nReducing the Odds on Financial Crises\nThis brings me to my final and most important principle--an ounce of prevention is worth many pounds of cure. As we’ve seen in recent years, macroeconomic stability has not forestalled large and unexpected movements in asset prices. Shocks occur, or reality catches up to unreasonable expectations, and market prices adjust, sometimes very substantially and suddenly. And although the financial system has become more resilient to such unanticipated developments, it is still subject to bouts of increased uncertainty that could build on themselves to disrupt the normal functioning of markets and send asset prices further away from fundamentals, with potentially adverse implications for the economy. Drawing on our experiences, the Federal Reserve has been working with other regulatory agencies and the private sector both here and abroad to strengthen the financial system in order to lower the odds that a sharp change in prices or questions about a major market participant would lead to a systemic financial crisis. Our collective efforts have been in three areas: enhancing market discipline; encouraging sound risk management; and strengthening clearing and settlement systems.\n\nMarket discipline through vigilance among private parties is always to be preferred to regulatory dictates as the prime source of constraint on possible crisis-causing behavior. It lessens the odds of a financial crisis and any potential moral hazard that might be created by governmental actions taken during periods of financial market turmoil. Reliance on market discipline becomes even more of an imperative when key market participants are subject to widely varying degrees and types of regulation from different regulators. For market discipline to be effective, counterparties must have a clear understanding of each others’ risk profile. Such transparency can be promoted through sound policies regarding accounting and, where necessary and appropriate, public disclosure. However, a meaningful understanding of risk profiles often requires information that market participants regard as proprietary. Confidentiality agreements between counterparties may be necessary to make them comfortable sharing such information. Counterparties that cannot obtain sufficient information should limit more strictly the amount of credit they supply through, for example, requiring higher margins.\n\nIn a market-based system, sound risk management by all market participants is essential to protect against the risk that a low-probability--or “tail”--event could cause a financial crisis. Such practices enhance financial stability without increasing moral hazard. Indeed, sound risk management by all market participants would reduce moral hazard. Market participants familiar with the risk metrics, the stress-test methodologies (and the associated market scenarios), the models, and other analytics used by their counterparties’ risk managers would be more likely to continue to provide access to credit during periods of systemic and institutional stress. Of course, the more that privately provided credit continues to flow in such circumstances, the lower the odds on a crisis and on the need for government intervention.\n\nIt is a tall order to “plan” for the unexpected, improbable, and unknowable nature of future financial market crisis, but actions by the authorities, such as supporting risk-management policy groups and promoting stress-testing, should help. For example, bringing together practitioners can potentially improve both the markets’ and the central bank’s understanding of how leverage, liquidity, and concentration are interrelated. In addition, risk-management policy groups can potentially lead to improvements in the reporting of risk information to counterparties and allow for the transfer of best practices across market participants with respect to valuation, exposure measurement, limit setting, and internal checks and balances. Indeed, a lesson drawn by the President’s Working Group from the 1998 crisis was that a lack of basic risk management, and the resulting ability of risk managers to avoid counterparty discipline, was an important factor that enabled key market participants to become so large that their troubles could disrupt entire markets.\n\nStress tests employ either historical data from asset price distributions or hypothetical scenarios that would provide insights into the downside financial risks associated with investments and associated hedging strategies. Focusing market participants on the structure of stress, and thus their risks beyond the range of recent historical experience, might assist in limiting the affects of a highly infrequent but significantly costly tail event.\n\nThe regulatory capital framework proposed in Basel II would require the largest internationally active banking organizations to enhance measurement and management of their risks, including credit risk and operational risk. It would also require these entities to have rigorous processes for assessing overall capital adequacy in relation to their total risk profile and to publicly disclose information on their risk profile. Because such actions would likely provide market participants with a better sense of how others might act during a crisis, these actions would likely help mitigate the adverse consequences created by financial market turmoil.\n\nRisk cannot be managed if participants are uncertain about their exposures to counterparty credit risk and to changes in prices. It is important for trades to be cleared and settled when expected at the agreed upon price with the anticipated counterparties. As a result, the central bank must promote robust payment and settlement systems. Sound risk-management practices and stress testing by operators of the settlement infrastructure are critically important in this domain. If payment and settlement systems have adequate collateral and liquidity to settle during turbulent periods, they reduce the threat of contagion across institutions and markets. To this end, the Federal Reserve actively participates in international groups, such as the Committee on Payment and Settlement Systems, that identify and promulgate best practices.\n\nCounterparty discipline, sound risk management, and strong and resilient clearing and settlement are all in the interest of private parties. Nonetheless, government has a role to play, especially when it senses moral hazard is weakening market discipline on risk taking and leaving the broader interests of society inadequately protected. Regulators may need to insist on minimal capital levels and on actions to strengthen private systems. In addition, they may need to work with disparate private parties and help market competitors take collective actions to solve problems that such competitors might otherwise find difficult to solve by themselves. For example, the Federal Reserve Board recently endorsed the chartering of a dormant bank, referred to as NewBank, which would be available for activation to clear and settle U.S. government securities. Such activation would occur if an existing clearing bank could not operate and no well-qualified bank stepped forward to purchase its clearing business. Similarly, the Federal Reserve Bank of New York, and other regulatory bodies have met with major dealers to strengthen the clearing and settlement infrastructure for the credit derivatives markets.\n\nConclusion\nMaintaining the stability of the financial system and containing the systemic risk that may arise in financial markets is central to the Federal Reserve’s mission. Recent financial crises within the United States have been well contained. However, with the rapid evolution of the financial system, there is a real possibility that markets are evolving faster than the efforts by market participants to better control risk and to improve the payment and settlement infrastructure.\n\nNew markets have been tested to some extent--by the stock market crash, by the widening of spreads in 2001 and 2002, and by the problems of the auto companies and their suppliers. So far, despite some glitches, markets have adapted and changed when deficiencies became obvious. No doubt, markets and institutions have become more flexible and resilient. But perhaps we have also been lucky; prudence dictates that we identify points of weakness and strengthen them.\n\nAnd, in a rapidly evolving financial system, we need to think creatively about where the weakness might be and how disturbances might be transmitted and amplified. Thus, this conference has the potential to contribute materially to our ability to continue to fulfill our most basic mandate: To foster an efficient, stable financial system in support of our nation’s economic welfare.\n\nFootnotes\n\n1.  The views expressed are my own and do not necessarily reflect the views of other members of the Board of Governors or the Federal Reserve more generally. Diana Hancock and Wayne Passmore, of the Board’s staff, contributed to these remarks. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20060518a.htm",
        "title": "The Evolving Nature of the Financial System: Financial Crises and the Role of the Central Bank",
        "date": "5/18/2006"
    },
    {
        "content": "May 16, 2006\n\nGovernor Susan Schmidt Bies\n\nAt the Global Association of Risk Professionals' Basel II & Banking Regulation Forum, Barcelona, Spain\n\nThank you for the invitation to speak here today. I am honored to be with this distinguished group of risk-management professionals from around the world. In my remarks, I will focus primarily on the choices and challenges associated with Basel II implementation. In particular, I want to reaffirm the Federal Reserve’s commitment to Basel II and the need for continual evolution in risk measurement and management at our largest banks and then discuss a few key aspects of Basel II implementation in the United States. Given the international audience here today, I also plan to offer some thoughts on cross-border implementation issues associated with Basel II, including so-called home-host issues.\n\nMoving to Basel II\nBy now most of you are aware that on March 30 the Federal Reserve Board approved a draft of the U.S. notice of proposed rulemaking (NPR) on the Basel II capital framework. The NPR is expected to be issued in the Federal Register once all of the U.S. banking agencies have completed their individual review and approval processes, at which time it will be “officially” out for comment. We recognize the significance of this development to the industry, the U.S. Congress, and others who have waited for greater specificity on the proposed revisions. But before commenting further on the NPR and the U.S. Basel II process, I want to reiterate our rationale for pursuing Basel II.\n\nRationale for Moving to Basel II\nThe current Basel I capital framework, adopted nearly twenty years ago, has served us well but has become increasingly inadequate for large, internationally active banks that are offering ever more complex and sophisticated products and services. We need a better capital framework for these large, internationally active banks, and we believe that Basel II is such a framework.\n\nOne of the major improvements in Basel II is the closer link between capital requirements and the way banks manage their actual risk. The current Basel I measures have very limited risk-sensitivity and do not provide bankers, supervisors, or the marketplace with meaningful measures of risk at large complex organizations. Under Basel I, a bank’s capital requirement does not adequately reflect gradations in asset quality and does not change over time to reflect deterioration in asset quality. Further, there is no explicit capital requirement for the operational risk embedded in many of the services from which the largest institutions generate a good portion of their revenues.\n\nIn addition to strengthening the link between regulatory capital and the way banks manage their actual capital, Basel II should make the financial system safer by encouraging continual improvement in risk-measurement and risk-management practices at the largest banks. Basel II is based on many of the economic capital principles used by the most sophisticated banks and therefore brings minimum regulatory capital requirements closer to banks’ internal capital models. By providing a consistent framework for the largest banks to use, supervisors will more readily be able to identify portfolios and banks whose capital is not commensurate with their risk levels. Through ongoing and regular dialogue, this process will in turn help management to be better informed about how their proprietary models compare to the range of practices currently in use so they can better prioritize where enhancements are needed. We have already seen some progress in risk measurement and management at many institutions in the United States and around the globe as a result of preparations for Basel II. Admittedly, banks have told us that some of the costs for Basel II would have been incurred anyway. But if anything, Basel II has accelerated the pace of this change.\n\nBasel II can also provide supervisors with a more conceptually consistent and more transparent framework for evaluating systemic risk in the banking system through credit cycles. Thus it improves on Basel I, which requires banks to hold the same level of capital for a given portfolio, no matter what its inherent risk may be. Further, as bankers gain experience with the advanced approaches under Basel II, they will have better information on how their risk taking may vary through credit cycles. Therefore, Basel II establishes a more coherent relationship between how supervisors assess regulatory capital and how they supervise banks, enabling examiners to better evaluate whether banks are holding prudent capital levels, given their risk profiles.\n\nThe reasons I’ve just given for pursuing Basel II also provide justification for the recent Basel revisions to the 1996 Market Risk Amendment (MRA). Since adoption of the MRA, banks’ trading activities have become more sophisticated and have given rise to a wider range of risks that are not easily captured in their existing value-at-risk (VaR) models. For example, more products related to credit risk, such as credit default swaps and tranches of collateralized debt obligations, are now included in the trading book. These products can give rise to default risks that are not captured well in methodologies required by the current rule specifying a ten-day holding period and a 99 percent confidence interval. The inability of VaR calculations to adequately measure the risks of certain traded positions may give rise to arbitrage opportunities between the banking book and the trading book because of the lower capital charge that may be afforded trading positions under a VaR approach that is not optimally risk-sensitive. The U.S. banking agencies are in the process of developing a notice of proposed rulemaking to implement the market risk revisions in the United States. These revisions will apply to those banks with significant trading activity, regardless of their Basel II status.\n\nBridging the Gap between Regulatory Capital Requirements and Internal Bank Practice\nWith Basel II, U.S. supervisors are attempting to use the internal risk-measurement and -management information produced by large complex institutions to manage their own risks in such a way as to augment the risk sensitivity and overall meaningfulness of minimum regulatory capital measures. Basel II, by tying regulatory capital calculations to bank-generated inputs, offers greater transparency about risk-measurement and management practices that stand behind the inputs provided by banks and exactly how they are calculated. Supervisors, through their analysis of bank inputs to Basel II, will develop an even better assessment of institutions’ risk-measurement and risk-management practices. Furthermore, the added transparency in Pillar 3 disclosures is expected to give market participants a better understanding of an institution’s risks and its ability to manage them.\n\nOf course, we understand that the extent that internal inputs from bankers can be used in regulatory capital requirements is limited, for a variety of reasons. Today’s banks have highly customized models for running their businesses, which of course is entirely appropriate. But, as supervisors, we need to ensure adequacy and enforceability of our minimum regulatory requirements and maintain some consistency across banks. Naturally, as we seek to develop a common framework that will work for large complex banks globally, we recognize an inherent tension between our regulatory rules and internal bank practice. We are working to strike the right balance to achieve our goals without making Basel II purely a compliance exercise and creating undue burden.\n\nNeed for Strong Capital\nBasel II is intended to improve regulatory capital requirements, especially for large complex organizations, through greater risk sensitivity of regulatory capital and improved linkage to banks’ actual capital risk management. That is why we have chosen to adopt only the most advanced options for credit risk and operational risk minimum regulatory capital calculations in the United States, and to limit the requirement of Basel II to only a small number of banking institutions that fit the definition of large, complex, and internationally active. It is also important to recognize that Basel II is a complete capital framework consisting of three pillars. While much of the focus to date has been on the calculation of minimum regulatory capital in Pillar I, it should be remembered that Pillar 2, which provides for supervisory oversight of an institution’s overall capital adequacy, and Pillar 3, which requires enhanced transparency via disclosure, are also important parts of this new framework.\n\nLet me assure you that we at the Federal Reserve would not be pursuing Basel II if we thought that it would in any way undermine the strong capital base that U.S. institutions now have. As a central bank and a supervisor of banks, bank holdings companies, and financial holding companies, the Federal Reserve is committed to ensuring that the Basel II framework delivers a strong and risk-sensitive base of capital for our largest and most complex banking institutions. That is why we supported moving ahead with the NPR, which includes modifications to address concerns identified in the fourth quantitative impact study, known as QIS4, and additional safeguards to ensure strong capital levels during the transition to Basel II. We will remain vigilant in monitoring and assessing Basel II’s impact on individual and aggregate minimum regulatory capital levels on an ongoing basis. As an extra degree of precaution, the U.S. banking agencies also decided to delay for a year the start of the parallel-run period.\n\nStarting with the parallel run, and both during and after the transition to Basel II, the Federal Reserve will rely upon ongoing, detailed analyses to evaluate the results of the new framework to ensure prudent levels of capital. Basel II represents a major shift in how we think about regulatory capital, especially as we will implement it in the United States. It is complex, reflecting the complexity of risk measurement and management for the largest, most complex banking institutions, and the banking institutions and the supervisors will need to have ongoing dialogue and work diligently to make sure it is working as we expect it to. But we believe it is a powerful approach to making regulatory capital more risk-sensitive. To be quite clear, the Federal Reserve believes that strong capital is critical to the health of our banking system, and we believe that Basel II will help us continue to ensure that U.S. banks maintain capital levels that serve as an appropriate cushion against their risk-taking.\n\nSome Aspects of U.S. Proposals\nAs you know, the draft U.S. Basel II NPR is based on the 2004 framework issued by the Basel Committee and adheres to the main elements of that framework. But the U.S. agencies have exercised national discretion and tailored the Basel II framework to fit the U.S. banking system and U.S. financial environment, as have their counterparts in other countries. For example, as I have just mentioned, the U.S. agencies continue to propose that we implement only the advanced approaches of Basel II, namely the advanced internal-ratings-based approach (AIRB) for credit risk and the advanced measurement approaches (AMA) for operational risk.\n\nThe U.S. agencies also included in the NPR a more gradual timetable and a more rigorous set of transition safeguards than those set forth in the 2004 Basel II framework. For instance, the U.S. agencies are proposing three years of transition floors below which minimum required capital under Basel II will not be permitted to fall, relative to the general risk-based capital rules. The first transition year would have a floor of 95 percent, the second 90 percent, and the third 85 percent. Part of the justification for implementing a more gradual transition timetable was the recognition that banks needed more time to prepare and we as supervisors needed more time to analyze transition information and ensure there would be no unintended consequences.\n\nAs you are aware, the QIS4 exercise identified some areas requiring further clarification by regulators and additional work by bankers on risk models and databases. One of the key areas in the NPR influenced by these results pertains to banks’ estimates of loss given default (LGD). Many QIS4 participants had difficulty computing LGDs, which must reflect downturn conditions, in part because their data histories were not long enough to capture weaker parts of the economic cycle. As a result, the agencies have proposed a supervisory mapping function that can be used by those institutions unable to estimate appropriate LGDs. The mapping function allows an institution to take its average LGDs and “stress” them to generate an input to the capital calculation that conforms to the Basel II requirements and hence produces a more appropriate capital requirement. The Federal Reserve believes this supervisory mapping function is an important component of Basel II because the QIS4 results showed the difficulty some banks are likely to have in producing acceptable internal estimates of LGD that are sufficient for risk-based capital purposes. The bank will shift from use of the mapping function to its own internal estimates of LGDs when they become reliable.\n\nAnother key area in the U.S. Basel II proposals relates to regulatory reporting and data requirements. The agencies expect to issue information about this aspect of our proposals soon, so I will offer only a few general thoughts here.\n\nAs you know, risk managers need to be able to discern whether fluctuations in risk exposures and capital are due to external effects, such as changes in the economy and the point in the economic cycle where decisions are being made, or are more related to their individual business decisions, including product characteristics, customer mix and underwriting criteria. We will continue to expect bankers to anticipate the effects of such economic fluctuations and business decisions, not just analyze them after the fact. As we move toward greater risk sensitivity in our regulatory capital framework, and greater alignment with what banks are doing internally to manage risk, the way in which we as supervisors assess the adequacy of capital levels must consider the sources of these fluctuations more than ever before. This requires both bankers and supervisors to place a greater emphasis on high-quality data and sound analysis. For example, data should contain enhanced look-back capabilities, so that we and bankers will be able to assess fluctuations within an institution over time. Unfortunately, in our QIS4 analysis we were unable to decompose changes we observed into those attributable to the economic cycle and those attributable to a bank’s individual portfolio composition because the QIS4 data were collected at a single point in time. Even comparisons of QIS4 information to previously collected QIS3 data were limited because there was no direct link between the two data samples. As part of the move toward greater risk sensitivity, and noting that different institutions have different risk profiles, we expect to place increased emphasis on sound economic analysis that focuses on changes observed at a single institution over time, as well as more traditional analysis across institutions.\n\nBasel I Modifications\nAt this point I would like to say just a few words about ongoing efforts to revise existing Basel I regulatory capital rules for non-Basel II institutions. We expect only one or two dozen banks to move to the U.S. version of Basel II in the near term, meaning that the vast majority of U.S. banks would be able to continue operating under Basel I, which will be amended through a separate rulemaking process. The Basel I framework has already been amended more than twenty-five times in response to changes in banking products and the banking environment and as a result of a better understanding of the risks of individual products and services. The U.S. agencies believe that now is another appropriate time to amend the Basel I rules. The U.S. agencies have issued an advance notice of proposed rulemaking discussing possible changes to enhance the risk sensitivity of U.S. Basel I rules and to mitigate potential competitive distortions that might be created by introducing Basel II. We are now in the process of reviewing the comments and working on a draft notice of proposed rulemaking. We are mindful that amendments to Basel I should not be too complex or too burdensome for the large number of banks to which the revised rules will apply.\n\nWith regard to both Basel II proposals and proposed Basel I amendments, we understand the need for full transparency. For that reason, we expect to have overlapping comment periods for both the Basel II NPR and the NPR for the proposed Basel I amendments. In fact, we want all interested parties to compare, contrast, and comment on the two proposals in overlapping timeframes. Accordingly, our proposals could change as a result of comments received or new information gathered by the U.S. agencies.\n\nCross-Border Implementation of Basel II\nAs I noted earlier, each country must implement Basel II as appropriate for the particular jurisdiction. To that end, the U.S. agencies are taking actions to ensure that implementation in the United States is conducted in a prudential manner and without generating competitive inequalities in our banking sector. We recognize that the differing approaches to Basel II that are being adopted by various countries may create challenges for banking organizations that operate in multiple jurisdictions. It is good to remember that cross-border banking has always raised specific challenges that supervisors from various countries have worked hard to address. Let me assure all bankers here that supervisors are aware that the process of change to new national versions of Basel II has heightened concerns about home-host issues. The Federal Reserve and other U.S. agencies have, for many years, worked with international counterparts to limit the difficulty and burden that have arisen as foreign banks have entered U.S. markets and as U.S. banks have established operations in other jurisdictions.\n\nThe U.S. is working to complete its national standard setting process since we recognize that the lack of a final rule raises uncertainty for both banks and foreign supervisors about exactly what will be required. As you are aware, the Accord Implementation Group has been working for the past few years identifying issues arising from differences in national standards of the Basel II framework. All of the supervisory bodies participating in that effort are committed to making the transition to Basel II successful.\n\nWe have heard from bankers that they are concerned about home-host issues. The U.S. banking agencies all encourage regular meetings between bankers and supervisors. These meetings provide a forum for bankers to make supervisors aware of implementation plans and progress at individual banks, and for supervisors to make bankers aware of current supervisory expectations. They also provide bankers opportunities to raise specific implementation issues. Of course, all Basel-member countries have their own rollout timelines and their own ways of addressing items that are left to national discretion under the Accord, which is entirely appropriate. We also want you to let us know any concerns you have about cross-border implementation. We would be grateful if you could be as specific as possible about your concerns, since that would greatly assist in the resolution of the issues.\n\nConclusion\nIn conclusion, we are encouraged by the progress that international supervisors and banking organizations have made in preparing for the implementation of Basel II, and we look forward to the continuing dialogue which will help inform further refinements to our approach. The preparations for Basel II have already had a positive impact on banks’ efforts to update their risk-measurement and -management processes. As risk management continues to become more complex and quantitative, it will underscore the importance of further improvements in data architecture and information technology systems development. Of course, a lot of work remains as we move toward a final rulemaking in the United States. We actively seek comments on our proposed rule and encourage an open dialogue with the banking industry and other interested parties, since such communications will undoubtedly improve the proposal. Substantial benefits can be derived from the more risk-sensitive approach to regulatory capital and the continual improvement in risk measurement and management that are the central themes of Basel II.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20060516a.htm",
        "title": "Implementing Basel II: Choices and Challenges",
        "date": "5/16/2006"
    },
    {
        "content": "May 16, 2006\n\nGovernor Mark W. Olson\n\nTo the Financial Services Roundtable and the Morin Center for Banking and Financial Services, Washington, D.C.\n\nThank you for the opportunity to share my perspective on effective compliance risk management for diversified financial institutions. This area of risk management has been receiving a good deal of attention lately. Risk, to state the obvious, is inherent in all activities in the financial services industry. Because the industry is heavily regulated, compliance risk is an important element of the activities in which you are engaged.\n\nToday, the financial services industry is experiencing tremendous growth, diversification, and innovation. This growth should be embraced as we consider the future of our financial sector. However, our \"embrace\" should be in the form of a sound risk-management framework. My remarks today will focus on what Federal Reserve supervisors see as the key components of a sound and effective compliance-risk management framework.\n\nBefore I begin, let me clarify what I mean by \"compliance risk.\" Compliance risk can be defined as the risk of legal or regulatory sanctions, financial loss, or damage to reputation and franchise value that arises when a banking organization fails to comply with laws, regulations, or the standards or codes of conduct of self-regulatory organizations applicable to the banking organization's business activities and functions.\n\nA consolidated--or \"enterprise-wide\"--approach to compliance risk management has become \"mission critical\" for large, complex banking organizations, for several reasons. First, because compliance failures have touched many businesses, including banking, securities, and insurance firms, it has become clear that companies operating in more than one type of business must have a compliance strategy that is both globally consistent and locally effective. Increasingly, large, complex organizations are taking an enterprise-wide compliance-risk management approach to augment and better coordinate what had been fragmented and duplicative compliance activities. Such an approach puts local compliance activities within individual business lines into an integrated, global program, makes possible an understanding of compliance requirements and performance across an organization, and promotes consistency in responsibility, expectations, documentation, assessment, and reporting. I have been told that this more-integrated approach to compliance risk management by industry is already having a positive effect on risk identification and mitigation.\n\nSecond, the need for an enterprise-wide approach to compliance risk management at larger, more complex firms is suggested by the diversity of laws and regulations that span business lines, legal entities, and geographic boundaries--for example, in the areas of Bank Secrecy Act compliance and anti-money laundering controls, fair lending, information security, privacy, transactions with affiliates (Regulation W), and conflicts of interest. A more-integrated approach may be warranted for other matters, as well, because similar laws and regulations exist across the various jurisdictions in which the organization operates, both in the United States and overseas.\n\nBecause of the nature and levels of risks inherent to their business activities, complex banking organizations should have in place a compliance-risk management framework that makes it possible to identify, monitor, and effectively control the compliance risks facing their entire organization. Of course, such a framework needs to be commensurate with the nature and level of the organization's compliance risk. It should evolve with the ever-changing product lines and business activities of any growth-oriented organization. And, of course, it needs to stay on top of regulatory developments.\n\nSetting the Tone at the Top--The Role of the Board of Directors\nA successful compliance-risk management program starts at the top of the organization. It is essential that the board of directors take the lead by requiring a top-to-bottom compliance culture that is well-communicated and incorporated into the organization's day-to-day operations by senior management, in order to ensure that all staff members understand their compliance responsibilities and their roles in implementing the enterprise-wide program. A strong compliance culture is evidenced by the extent to which employees work together both to raise concerns about compliance risks and to design and establish effective controls.\n\nA sound and effective enterprise-wide compliance-risk management program has strong board and senior management oversight. However, it is important to note that the board of directors and senior management have distinct, though complementary, roles in ensuring the program's success. As I mentioned, the board of directors is responsible for requiring a strong compliance culture. This environment should assure that compliance is an integral part of day-to-day operations. To this end, the board should approve the key elements of the program and then entrust responsibility for embedding the culture and implementing the program to managers and staff at all levels of the organization. Periodically, the board should be apprised of the extent to which predetermined benchmarks are being met.\n\nMany of the largest and most complex banking organizations have established a corporate-level function to oversee their enterprise-wide compliance-risk management program, and the Federal Reserve views this trend very favorably. In these cases, the central function has been staffed with experienced compliance officers who are able to provide substantive guidance to business line managers and to keep senior management and boards of directors informed on how well the program is functioning, including by alerting them promptly about any material compliance breaches. Overall, such a central function can contribute a great deal toward ensuring that the key elements of a sound and effective compliance-risk management program are present and functioning as intended.\n\nCorporate Standards--Tailored within the Business Lines\nHow does a board-mandated \"compliance culture\" permeate a banking organization? Among other ways, the culture and expectations are communicated through enterprise-wide compliance-risk management standards or objectives--established by senior management within a corporate-level compliance function--that reflect the expectations of the board.\n\nRegardless of the compliance-risk management framework employed, the business line managers continue to \"own\" the compliance risk. That is, they are responsible for achieving compliance in their business lines and suffer the consequences in the event of compliance failures or missteps. Business line managers convert the corporate compliance risk standards or objectives into policies and procedures tailored to the specific type and level of compliance risk inherent in their activities and to the specific laws of the jurisdictions in which they operate.\n\nTo create appropriate compliance risk controls, organizations seek first to understand compliance risk across the entire entity. Managers assess and evaluate the risks and controls within their scope of authority at least annually. However, an enterprise-wide compliance-risk management program is dynamic and proactive, meaning that risks are assessed whenever new business lines or activities are added or existing activities and processes are altered. Once a particular business line has identified and assessed its compliance risks, it can design policies, detailed day-to-day procedures and processes, and associated risk-mitigating internal controls. The corporate compliance function plays an important role in advising business line managers as they assess risk. Such counsel adds to consistency in approach and helps control for mis-estimated risk. The corporate compliance function's involvement also increases the organization's ability to aggregate and better understand risks across the organization. Understanding of risk helps both the business lines and the compliance function develop internal controls that are reasonably designed to mitigate the risk.\n\nWhile the industry trend toward more-consolidated compliance risk management is a favorable development, it is critical that business line management remain engaged. As I noted, senior managers in the business lines remain the \"owners\" of the risk, and compliance risk mitigation must be integrated into their overall business processes. The sense of ownership can be reinforced by factoring compliance ratings into performance measures and rewards. I am aware that, increasingly, organizations are tying compensation to management's ability to maintain strong internal controls and compliance processes. This practice helps keep the organization focused on managing compliance risk. To ensure that issues are escalated as they arise, the compliance staff embedded within the business lines should be independent and should have a clear reporting relationship to the corporate compliance-risk management function. In addition, to be most effective, the business line compliance staff should be \"right-sized\" to the level of risk within that business line.\n\nClear lines of reporting, authority, and communication are key to the success of any compliance-risk management framework. Policies and procedures should spell out for business line staff how to escalate compliance concerns, should delineate responsibilities (avoiding overlapping roles and conflicts of interest), and should ultimately ensure that the board and senior management are fully apprised of material compliance events.\n\nMonitoring and Reporting\nAs I mentioned at the start, the fundamental purpose of the enterprise-wide compliance-risk management framework is to identify, monitor, and manage compliance risk more effectively. Among other things, monitoring is the means of identifying and communicating compliance breaches to the appropriate points within the organization. Banking organizations are establishing processes for monitoring the implementation of compliance policies, procedures, and controls, at both the consolidated and business line levels.\n\nSound compliance-risk monitoring activities at large, complex banking organizations are supported by information systems that provide management with timely reports related to compliance with laws and regulations at the transaction level. These reports generally address monitoring and testing activities, actual or potential material compliance deficiencies or breaches, and new or changing compliance requirements. Reports such as these ensure that information on compliance is communicated to the appropriate levels within the organization. Monitoring and reporting enable senior management and the board to effectively carry out their respective responsibilities.\n\nAt the frontier of compliance risk monitoring, larger, more complex banking organizations are moving toward the collection and analysis of quantitative information relating to compliance risk activities at the transaction level. Bearing in mind that this is an evolving area, banking organizations are investing in the development and application of such information as key risk indicators and key performance indicators in order to facilitate more accurate and more timely monitoring. Increased development and use of key compliance risk and performance indicators will ultimately enable better measurement, monitoring, and control of compliance risk.\n\nTesting\nAs with other control functions within an organization, independent testing should be conducted to verify that compliance-risk mitigation activities, including training and internal controls, are in place and functioning as intended throughout the organization. Whereas monitoring is an ongoing process, testing is a \"point-in-time\" event, a critical check of performance. The frequency of testing should be based on risk--on such factors as whether significant deficiencies were identified through a recent examination or during the last testing; whether the business line's products or activities have grown or changed significantly; or whether a high risk that may directly affect the organization's activities has been identified.\n\nExceptions to the corporate-wide risk standards or objectives are reported to senior management, and I understand that this is most effectively done through reporting channels not under the direct control of business line management; however, resolution of the exceptions is the responsibility of business line management. An exception is tracked until its resolution has been validated. Importantly, a sound testing program contains provisions for escalating unresolved exceptions to higher levels in the organization, including the board of directors. Of course, the program should clearly define and communicate the roles of the internal audit, compliance, and other independent functions or third parties, although these roles will vary by organization.\n\nTesting provides important feedback on how well the internal control framework is operating in practice, pointing the way toward any remedial actions that need to be taken. This essential part of an internal control framework can be performed by compliance personnel, audit personnel, or third-party firms that have specialized expertise. While banking organizations sometimes outsource compliance testing or other compliance function tasks, the organization itself remains accountable and must exercise appropriate oversight.\n\nThe \"War for Talent\"\nWe've heard industry leaders refer to their compliance recruitment process as a \"war for talent.\" Running an effective enterprise-wide compliance-risk management program requires more than policies, procedures, and management information systems. Decisions are made by senior management on the basis of advice from their staff. Consequently, talent is required at all levels. In assessing the risk management of individual organizations, we have noted the importance of staff who have experience and expertise consistent with the scope and complexity of an organization's business activities. Staff must also have the integrity, ethical values, and competence consistent with a prudent management philosophy and operating style. We understand that hiring and retaining qualified and experienced compliance staff is a key challenge across the industry. Successful organizations are focusing not only on recruiting talent, but also on developing staff from within. Training and other staff-development tools are essential in this time of \"war for compliance talent.\"\n\nThe Role of the Regulators\nThe business lines and activities of financial services firms can cross the jurisdictional boundaries of many legal entities, making it more difficult for a supervisory agency acting alone to determine what went wrong and what processes may need to be improved. In these cases, the Federal Reserve, as the consolidated supervisor of the banking organization in question, works closely with the primary bank regulator and the Securities and Exchange Commission (SEC) to investigate control breakdowns. The agencies exchange information and compare their findings. For example, findings from SEC examinations of broker-dealers, investment advisers, and mutual fund distributors are analyzed together with findings from targeted joint reviews of bank holding companies and banks conducted by the Federal Reserve and the primary bank regulators. This is the way the functional regulators should and will work in the environment of diversified financial institutions.\n\nConclusion\nControlling compliance risk may be easier said than done. Certainly, establishment of an effective enterprise-wide compliance-risk management function and program is not done overnight. Nor is it done without costs--both monetary costs and the intangible costs inherent in real cultural change. However, it is clear that all organizations must find ways to effectively manage compliance risk--and there is growing consensus within the industry that for some of the largest and most complex organizations, an enterprise-wide approach to controlling compliance risk is no longer just a \"nice thing to have.\" Rather, it has become an essential element of effective risk management.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/olson20060516a.htm",
        "title": "Compliance Risk Management in a Diversified Environment",
        "date": "5/16/2006"
    },
    {
        "content": "May 16, 2006\n\nChairman Ben S. Bernanke\n\nAt the Federal Reserve Bank of Atlanta’s 2006 Financial Markets Conference, Sea Island, Georgia\n\nThank you for inviting me to speak today. In keeping with the theme of this conference, I will offer some thoughts on the systemic risk implications of the rapid growth of the hedge fund industry and on ways that policymakers might respond to those risks.\n\nThe collapse of Long-Term Capital Management (LTCM) in 1998 precipitated the first in-depth assessment by policymakers of the potential systemic risks posed by the burgeoning hedge fund industry. The President's Working Group on Financial Markets, which includes the Federal Reserve, considered the policy issues raised by that event and, in 1999, issued its report, Hedge Funds, Leverage, and the Lessons of Long-Term Capital Management. The years since then have offered an opportunity to consider whether the Working Group's recommendations for addressing those issues have been effective and whether new concerns have arisen that warrant an alternative approach.\n\nLTCM and the Working Group's Recommendations\nAs the title of the report indicated, the Working Group focused on the potential for leverage to create systemic risk in financial markets. The concern arises because, all else being equal, highly leveraged investors are more vulnerable to market shocks. If leveraged investors default while holding positions that are large relative to the markets in which they have invested, the forced liquidation of those positions, possibly at fire-sale prices, could cause heavy losses to counterparties. These direct losses are of concern, of course, particularly if they lead to further defaults or threaten systemically important institutions; but, in addition, market participants that were not creditors or counterparties of the defaulting firm might be affected indirectly through asset price adjustments, liquidity strains, and increased market uncertainty.\n\nThe primary mechanism for regulating excessive leverage and other aspects of risk-taking in a market economy is the discipline provided by creditors, counterparties, and investors. In the LTCM episode, unfortunately, market discipline broke down. LTCM received generous terms from the banks and broker-dealers that provided credit and served as counterparties, even though LTCM took exceptional risks. Investors, perhaps awed by the reputations of LTCM's principals, did not ask sufficiently tough questions about the risks that were being taken to generate the high returns. Together with the admittedly extraordinary market conditions of August 1998, these risk-management lapses were an important source of the LTCM crisis.\n\nThe Working Group's central policy recommendation was that regulators and supervisors should foster an environment in which market discipline--in particular, counterparty risk management--constrains excessive leverage and risk-taking. Effective market discipline requires that counterparties and creditors obtain sufficient information to reliably assess clients' risk profiles and that they have systems to monitor and limit exposures to levels commensurate with each client's riskiness and creditworthiness. Placing the onus on market participants to provide discipline makes good economic sense; private agents generally have strong incentives to monitor counterparties as well as the best access to the information needed to do so effectively.\n\nFor various reasons, however, creditors may not fully internalize the costs of systemic financial problems; and time and competition may dull memory and undermine risk-management discipline. The Working Group concluded, accordingly, that supervisors and regulators should ensure that banks and broker-dealers implement the systems and policies necessary to strengthen and maintain market discipline, making several specific recommendations to that effect. The Working Group's recommendations on this point have largely been followed. Domestically, regulatory authorities issued guidance on risk-management practices, and bank supervisors now actively monitor and conduct targeted reviews of banks' dealings with hedge funds. The Securities and Exchange Commission (SEC) intensified its risk-management inspections of the larger broker-dealers after LTCM. Internationally, both the Basel Committee on Banking Supervision and the International Organization of Securities Commissions produced papers on sound practices in dealings with highly leveraged institutions, and the Basel Committee conducted a series of follow-up studies.\n\nAn alternative policy response that the Working Group considered, but did not recommend, was direct regulation of hedge funds. Direct regulation may be justified when market discipline is ineffective at constraining excessive leverage and risk-taking but, in the case of hedge funds, the reasonable presumption is that market discipline can work. Investors, creditors, and counterparties have significant incentives to rein in hedge funds' risk-taking. Moreover, direct regulation would impose costs in the form of moral hazard, the likely loss of private market discipline, and possible limits on funds' ability to provide market liquidity.\n\nIn focusing on counterparty risk management in its recommendations, the Working Group did not intend to prevent failures in the hedge fund industry. Hedge funds offer their investors high prospective returns but also high levels of risk. Experienced investors know, or should know, that in any given year some hedge funds lose money for their investors and some funds go out of business. Those occurrences are only normal and to be expected in a competitive market economy. The Working Group's recommendations were aimed, instead, at ensuring that when hedge funds fail, as some inevitably will, the effects will be manageable and the potential for adverse consequences to the broader financial system or to real economic activity will be limited.\n\nEffectiveness of the Working Group's Approach\nHas the approach proposed by the President's Working Group worked? Any answer must be provisional, but, to date, it apparently has been effective. Since the LTCM crisis, ongoing improvements in counterparty risk management and the resultant strengthening of market discipline appear to have limited hedge fund leverage and improved the ability of banks and broker-dealers to monitor risk, despite the rapidly increasing size, diversity, and complexity of the hedge fund industry. Many hedge funds have been liquidated, and investors have suffered losses, but creditors and counterparties have, for the most part, not taken losses. The general perception among market participants is that hedge funds are less highly leveraged today than in 1998 though, to be sure, meaningful and consistent measurements of leverage are not easy to come by and many newer financial products embed significant leverage in relatively nontransparent ways.\n\nAccording to bank supervisors and most market participants, counterparty risk management has improved significantly since 1998. Some of this progress is due to industry-led efforts, such as two reports by the Counterparty Risk Management Policy Group (CRMPG) that lay out principles that institutions should use in measuring, monitoring, and managing risk. Reviews conducted by bank supervisors in 2004 and 2005 indicated that banks have become more diligent in their dealings with hedge funds. In most cases, substantial resources have been devoted to expanding and improving the staffing of the risk-management functions related to hedge fund counterparties. Dealers universally require hedge funds to post collateral to cover current credit exposures and, with some exceptions, require additional collateral, or initial margin, to cover potential exposures that could arise if markets moved sharply. Now, risk managers can more accurately measure their current and projected exposures to hedge fund counterparties, and more firms use stress-testing methodologies to assess the sensitivity of their exposures to individual counterparties if the market moves substantially.\n\nDespite this progress, some concerns about counterparty risk management remain and may have become even more pronounced given the increasing complexity of financial products. I will note four of these concerns. First, hedge funds are profitable customers for dealers, and our supervisors are concerned that competition for hedge fund business has eroded initial margin levels. Second, given the increasing volume of complex transactions with hedge funds, we are also concerned whether counterparty exposures in such complex transactions are being measured accurately. Supervisors are monitoring banks with these issues in mind. Third, our supervisors are concerned that more extensive stress-testing should be done. Although stress-testing of exposures at the level of the individual hedge fund counterparty is becoming more common, still-wider application of this technique would be useful. Similarly, aggregate stress tests--by which a dealer evaluates its exposure to the hedge-fund sector in the event of a large market move--merit wider use. Aggregate stress tests are a desirable complement to stress tests of individual hedge fund counterparties because funds sometimes imitate each others' strategies or choose strategies that are affected by common market factors. Supervisors are encouraging the expanded use of stress-testing when it is appropriate. Fourth, supervisors are concerned that the assessment of counterparty risks should be better tied to the amount of transparency offered by hedge funds. In particular, good risk management should link the availability and the terms of credit granted to a hedge fund to the fund's willingness to provide information on its strategies and risk profile. Our supervisors are pushing banks to clearly link transparency with credit terms and conditions.\n\nSince the Working Group report was issued, hedge funds have greatly expanded their activities and strategies, and their interactions with counterparties and creditors have accordingly become more complex. The continuing challenge for supervisors, counterparties, and hedge funds is to ensure that rigorous and appropriate methods of risk management are brought to bear even as institutions, instruments, and markets change. Two recent challenges of note are the spread of prime brokerage services and the emergence of operational issues in the settling of trades in newer types of over-the-counter (OTC) derivatives, particularly credit derivatives.\n\nHedge funds have long used arrangements that allow them to execute trades with several dealers but then to consolidate the clearing and settlement of their trades at a single firm, the \"prime broker.\" The prime broker typically provides financing and back-office accounting services to the hedge fund as well as settlement services. In the past couple of years, prime brokerage has expanded beyond cash trades for securities to include foreign exchange and OTC derivative trades, and more firms are offering prime brokerage services.\n\nPrime brokerage poses some unique challenges for the management of counterparty credit and operational risk. Prime brokers must ensure that they have adequate information and controls to protect against counterparty credit risk arising both from the client and from the executing dealer. They also must implement internal controls to monitor and track transactions executed as part of the prime brokerage agreement and to ensure that the transactions meet the terms of the agreement. Supervisors of firms that offer prime brokerage services, particularly supervisors of new entrants, must ensure that the firms are fully aware of the risks involved and effectively manage them.\n\nThe proliferation of new financial products also poses risk-management challenges, including challenges on the operational side. For example, trading in credit derivatives has grown dramatically in recent years, and firms have had difficulties in processing and settling these and other OTC derivative trades in a timely way. These problems are not limited to hedge funds but affect all participants in the OTC derivatives market and all dealers in credit derivatives. Recently, supervisors in several jurisdictions, working with the Federal Reserve Bank of New York, have pushed firms to improve their processes for confirming and assigning trades. So far, good progress has been made, with private-sector participants meeting most of their objectives for reducing backlogs. Commitments are in place to effect still further improvement.\n\nA noteworthy feature of these efforts is the cooperation among authorities. The Federal Reserve has devoted more effort in recent years to maintaining a dialogue with international supervisors, such as the U.K. Financial Services Authority, and we will continue to do so. Domestically, the Federal Reserve is coordinating with the SEC, which is the primary regulator of several large firms that deal in OTC derivatives or engage in prime brokerage activities.\n\nProposals for Creating a Database of Hedge Fund Positions\nFollowing the LTCM crisis and the publication of the Working Group's recommendations, the debate about hedge funds and the broader effects of their activities on financial markets abated for a time. That debate, however, has now resumed with vigor--spurred, no doubt, by the creation of many new funds, large reported inflows to funds, and a broadening investor base. Renewed discussion of hedge funds and of their benefits and risks has in turn led to calls for authorities to implement new policies, many of which will be topics of this conference. I will briefly discuss one of these proposals: the development of a database that would contain information on hedge-fund positions and portfolios.\n\nIt is commonly observed that hedge funds are \"opaque\"--that is, information about their portfolios is typically limited and infrequently provided. It would be more accurate to say that the opacity of hedge funds is in the eye of the beholder; the information a fund provides may vary considerably depending on whether the recipient of the information is an investor, a counterparty, a regulatory authority, or a general market participant. From a policy perspective, transparency to investors is largely an issue of investor protection. The need for counterparties to have adequate information is a risk-management issue, as I have already discussed. Much of the recent debate, however, has focused on the opacity of hedge funds to regulatory authorities and to the markets generally, which is viewed by some as an important source of liquidity risk. Liquidity in a particular market segment might well decline sharply and unexpectedly if hedge funds chose or were forced to reduce a large exposure in that segment.\n\nConcerns about hedge fund opacity and possible liquidity risk have motivated a range of proposals for regulatory authorities to create and maintain a database of hedge fund positions. Such a database, it is argued, would allow authorities to monitor this possible source of systemic risk and to address the buildup of risk as it occurs. Various alternatives that have been discussed include a database maintained by regulators on a confidential basis, a system in which hedge funds submit position information to an authority that aggregates that information and reveals it to the market, and a public database with nonconfidential information on hedge funds.\n\nI understand the concerns that motivate these proposals but, at this point, remain skeptical about their utility in practice. To measure liquidity risks accurately, the authorities would need data from all major financial market participants, not just hedge funds. As a practical matter, could the authorities collect such an enormous quantity of highly sensitive information in sufficient detail and with sufficient frequency (daily, at least) to be effectively informed about liquidity risk in particular market segments? How would the authorities use the information? Would they have the authority to direct hedge funds or other large financial institutions to reduce positions? If several funds had similar positions, how would authorities avoid giving a competitive advantage to one fund over another in using the information from the database? Perhaps most important, would counterparties relax their vigilance if they thought the authorities were monitoring and constraining hedge funds' risk-taking? A risk of any prescriptive regulatory regime is that, by creating moral hazard in the marketplace, it leaves the system less rather than more stable.\n\nA system in which hedge funds and other highly leveraged market participants submit position information to an authority that aggregates that information and reveals it to the market would probably not be able to address the concern about liquidity risk. Protection of proprietary information would require so much aggregation that the value of the information to market participants would be substantially reduced. Timeliness of the data would also be an issue.\n\nA public database of nonproprietary information could provide the public with a general picture of hedge-fund activity without creating the false impression that the authorities were engaged in prudential oversight of hedge funds. Such a public database might demystify hedge funds, but it would not address the central policy concern that opacity creates liquidity risk.\n\nI expect discussion and analysis of the potential costs and benefits of increased disclosures will continue, as well as suggestions about how such disclosures might be structured and disseminated. The important challenge is to structure any disclosures in a way that does not generate moral hazard or weaken market discipline.\n\nConclusion\nIn the final analysis, authorities cannot entirely eliminate systemic risk. To try to do so would likely stifle innovation without achieving the intended goal. However, authorities should (and will) try to ensure that the lapses in risk management of 1998 do not happen again. Private market participants, too, have their role to play in ensuring that such lapses do not recur. The principles articulated in the CRMPG's reports are a good starting place for firms, and senior management should rigorously assess their operations against those principles and commit the resources to address deficiencies. Authorities' primary task is to guard against a return of the weak market discipline that left major market participants overly vulnerable to market shocks. Continued focus on counterparty risk management is likely the best course for addressing systemic concerns related to hedge funds. This public policy approach does not entail the moral hazard concerns created by authorities' monitoring of positions using a private database. Rather, a focus on counterparty risk management places the responsibility for monitoring risk squarely on the private market participants with the best incentives and capacity to do so.",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060516a.htm",
        "title": "Hedge Funds and Systemic Risk",
        "date": "5/16/2006"
    },
    {
        "content": "May 11, 2006\n\nGovernor Donald L. Kohn\n\nAt the 2006 Payments Conference, Federal Reserve Bank of Chicago, Chicago, Illinois\n\nI am pleased to have the opportunity to speak at this conference on investing in payments innovations.1 We are living in a period of rapid innovation and technological change. The productivity of the U.S. economy accelerated in the mid-1990s and productivity has grown at an even more rapid pace this decade, as new technologies and increasingly sophisticated capital equipment have made it possible to produce and deliver goods and services more efficiently and cost effectively. Technological advancements have made the payments system a very dynamic part of the U.S. economic infrastructure. The increasing availability and convenience of payment alternatives, along with generally lower costs, have affected how people choose to pay for their purchases. Payments that a few years ago were being made in paper form--by checks or in cash--are today being made electronically. Indeed, a recent Federal Reserve study found that the number of retail electronic payments in the United States--such as credit cards, debit cards, or automated clearinghouse (ACH) payments--exceeded check payments for the first time in 2003. On at least one network, debit card payments have recently surpassed credit card payments, reflecting the rapid growth in debit card use that has been evident for a number of years. Given this changing environment, the Federal Reserve is planning to repeat its triennial retail payments survey next year to help quantify recent trends.\n\nThe check-processing system is also evolving rapidly. The changing character of the payments business is challenging long-held assumptions about the payments infrastructure, as well as business relationships in the payments industry. My remarks today, which reflect my own thoughts and not necessarily those of the Federal Reserve Board, will provide an overview of some of the recent changes in retail payments, the challenges of innovation, and the Federal Reserve’s role in the retail payments system.\n\nChanges in the Retail Payments System\nFor many reasons, I believe the pace of change we have experienced in the retail payments system over this past decade will continue or even accelerate. Much of the needed electronic infrastructure is in place today, both in homes and at businesses. As new generations grow up in the PC age and as older generations become increasingly computer literate, people are becoming more comfortable with using electronic technology in their daily lives. At the same time, the emergence of interstate banking has removed institutional barriers to the development of national electronic debit systems. Innovation in payments systems has also become a global phenomenon, and successful innovations outside the United States will likely influence developments here. These and other factors will likely spur further changes in our payments system in the years to come.\n\nNew technologies are at the heart of this evolution. Laws, regulations, and rules have also been modified to remove barriers to innovation and experimentation in the payments system. These factors are driving the changes we see today. Consumers and businesses are increasingly using debit and credit cards instead of paper checks and cash at the point of sale, and technology has allowed the payments industry to develop new payment instruments to meet new business needs. For example, over the past decade, Internet-based payment options, such as PayPal, have grown rapidly and facilitated commerce on the web. In addition, closed-network, prepaid payment alternatives, which use chip-based technology, have been successful in collecting tolls on the nation’s highways.\n\nLikewise, the combination of new technologies and changes to the rules and regulations governing the ACH network has facilitated the use of this network for one-time, nonrecurring payments. In the past, the ACH was mainly used for recurring payments, such as payroll, mortgage, and utility payments. Today, consumer purchases at stores, over the telephone, or over the Internet can be completed using the ACH. Regulatory changes have also facilitated the use of the ACH to convert checks that consumers mail to businesses into electronic payments. These new uses of the ACH for one-time payments have driven continuing double-digit growth rates of ACH transaction volume.\n\nLegal and technological changes are also affecting how banks process checks. For example, the Check Clearing for the 21st Century Act, or Check 21, has facilitated the ability of banks to collect checks electronically. Depository institutions are using the authority granted under Check 21 to apply communications and imaging technologies to long-established check-collection processes. Ultimately, these changes will not only reduce check-processing and transportation costs but also diminish the importance of geography in check processing. However, banks seeking to implement a full end-to-end electronic check-processing environment must reengineer their back-office systems to integrate the more complex aspects of electronic check receipt into their core banking and risk-management systems. We are beginning to see evidence that depository institutions are starting this reengineering process. Generally, Check 21 volumes at the Reserve Banks are increasing rapidly, reflecting the efficiency gains that will inexorably lead to the electronic processing of almost all checks in the not-too-distant future.\n\nThe greater use of electronic payments has not been uniform across all market segments. A closer look at payment trends reveals that consumers and businesses may have different perspectives on electronic payments. Businesses have been encouraging consumers to use electronic payment alternatives and have been the driving force behind the conversion of consumer checks into ACH payments. In turn, consumers, recognizing the convenience of electronic payments, have allowed their checks to be converted into ACH payments and have initiated an increasing number of their payments electronically.\n\nBusinesses, on the other hand, have been more cautious about having their own checks converted into ACH payments or switching from paper to electronics. There are some obvious reasons for this behavior. First, businesses often have cash-management processes or legacy accounting systems in place that rely on paper checks. A payments conference hosted by the Federal Reserve in 2003 found that businesses face organizational and technical challenges in moving to electronics. For example, businesses may need to make significant investments in their back-office payment, billing, and accounting systems before they can use electronic payments more frequently. In addition, many businesses are reluctant, from a control perspective, to allow others to debit funds electronically from their accounts. If businesses can overcome these challenges, they should be able to make and receive a much larger portion of their payments electronically. At the same time, the financial incentives for businesses to make these changes will increase as the relative costs of electronic versus paper payments continue to shift in favor of electronic payments.\n\nChallenges of Payment System Innovation\nIn addition to low costs, successful innovations must satisfy at least three criteria: meet end-user needs, address network externalities, and provide sufficient controls over risks. To be successful, new payment instruments, and changes to existing instruments, must ultimately be more convenient and cost-effective for end users. Consumers and businesses are not likely to modify their payment behavior unless they are shown how they will benefit from a change. A Federal Reserve Board staff study conducted in 2002, “The Future of Retail Electronic Payments Systems,” highlighted this observation. The study was based on interviews with payment system innovators, many of whom had experience during the years of the Internet boom. The interviews emphasized that providers of payment services cannot assume that an innovative service will generate significant demand simply because it provides new and creative technical capabilities. Providing net benefits to one or more key participants in a transaction, while not materially affecting other participants negatively, appears to be the most important requirement for any innovation to be successful. The benefits may accrue to the providers of payment services in the form of lower costs, but they cannot adversely affect end users. This interaction between the preferences of payments users and the choices offered by payments providers determines which payment instruments will become successful in the long run.\n\nSuccessful electronic payment instruments, by their very nature, require large networks with wide reach. That is, the instruments require a critical mass of both payers and payees. Developing large networks and attaining critical mass, however, are often difficult tasks. Consumers may be cautious about using new payment methods and will have particular concerns about how providers protect users’ funds, security, and privacy. In complex payment systems, users may need a significant amount of time to become familiar with and learn to trust a new system. Moreover, potential providers of new payment services may be cautious about investing before they understand how many consumers would want to use a new payment instrument or system. As a result, innovators face the classic “chicken and egg” problem when assessing the network economics of establishing new systems. In the late 1990s, attempts to establish large-scale stored-value card networks in the United States faced these problems and were ultimately not successful.\n\nHowever, the likelihood of overcoming these network problems appears to be higher if a particular innovation meets a significant need that existing instruments do not. For example, the growth of Internet commerce created a new need for a payment method that would allow both consumers and businesses to make and receive relatively low-value electronic payments. While credit cards and debit cards have become the primary means of conducting these transactions, a segment of the market, specifically individuals and small businesses, could not readily accept card payments from others. This demand for an alternative way to make payments on the Internet eventually led to the development of new payment options, such as PayPal.\n\nThe economics of networks and of scale may also interact and accelerate change in traditional systems. For example, the clearing system for paper checks includes a high-fixed-cost physical transportation system. Declines in check volumes generally, as well as declines in the collection of checks in paper form, will likely result in higher unit costs to collect paper checks, as the fixed costs of the network are spread over ever-decreasing volumes. The increases in unit costs, in turn, will cause banks to examine less expensive electronic processing alternatives, as check-collection intermediaries raise prices to recover their costs. As a result, market dynamics will continue to lead to the greater use of electronic check processing.\n\nManaging risk is also critical to successful payments innovation. These risks include fraud, operational, financial, and legal risks. Understanding the relationship between innovation and risk is particularly important in today’s evolving payment system because concerns about risk can inhibit the adoption of innovative payment products and services. Payment innovations can potentially reduce risk within a payment system, but in a complex environment, innovation may also shift risks or even increase them. Both the design of payments systems and the way participants use them can affect risk. For example, because a depository institution’s ACH and check systems may not be integrated, stop payments on checks may not work as intended, thereby increasing the risk that fraud may slip through the system. Innovations in the use of payment networks, such as the ACH, have led to greater complexity in roles and responsibilities. Both depository institutions and ACH operators are actively examining their risk-management capabilities to determine whether new risks resulting from innovation are being adequately managed. The use of Check 21 authority to process checks has altered operational, legal, and fraud risks, which has led depository institutions to reexamine and, as necessary, modify their processes and procedures to ensure that these risks continue to be well managed.\n\nIn this time of transition in the payment system, both providers and users of payments systems will need to manage their risks more comprehensively. Traditionally, risk has been managed with a segmented, payment-specific approach. This made sense when payments systems were largely independent of one another. Today, the conversion of checks to electronic payments, as well as the technical integration of electronic networks and systems, requires providers and users to adopt a more strategic approach to managing products, infrastructure, and risk across traditional payments silos. They need to consider not only the financial and business cases for adopting a particular payments strategy but also how the risks associated with the strategy will be managed. The payments industry has been actively discussing this issue, and I expect that the industry will develop effective and comprehensive risk-management approaches.\n\nThe Federal Reserve’s Role in the Payments System\nThe Federal Reserve will continue to play an important role in fostering a smoothly functioning payments system that is safe, efficient, and accessible. We also need to be flexible in carrying out our traditional functions within the payments system--as a provider of payment services, regulator, and catalyst for change--in this rapidly changing environment.\n\nIn its role as service provider, the Federal Reserve will continue to promote the efficiency of the nation’s payments system. The Reserve Banks are now pricing their check services to encourage the greater use of electronic check products relative to paper check products. The Reserve Banks are also leaders in providing Check 21 services to encourage depository institutions to shift to the greater use of electronics in check processing and have been working collaboratively with the industry on electronic check standards and other technical issues. Most importantly, the Reserve Banks will continue to compete as payment services providers on a fair and equitable basis by pricing their services to recover their costs, including imputed profits and taxes, as required by the Monetary Control Act of 1980.\n\nIn its role as a regulator, the Federal Reserve will need to be alert to the application of regulations in changing circumstances. In some cases, regulations may impede the ability of service providers and consumers to take advantage of new technology. For example, the Federal Reserve has clarified the application of Regulation E to the new ACH services I discussed earlier involving check conversions. In some other cases, payments innovations can expose consumers to new risks--risks with which they may not be familiar. In addressing these situations, the Federal Reserve must ensure that consumers have adequate protection and that regulations are consistent with the changing technological environment. Changing payment practices can also expose financial institutions to growing risks. Last year, for example, the Federal Reserve modified its Regulation CC to reallocate the liability for unauthorized, remotely created checks among depository institutions, shifting liability to institutions that are better positioned to influence and mitigate those risks.\n\nFinally, in its role as a catalyst for change, the Federal Reserve will work with the private sector to identify and, when appropriate, address barriers to payments system innovation. Last month, the Federal Reserve combined the duties of two of its internal committees that deal with payments issues. The newly expanded Payments System Policy Advisory Committee will provide the Federal Reserve with an overall view of strategic developments in both wholesale and retail payment systems. In addition to supporting sound policy development, the committee will sponsor research on payments issues that will help inform policymakers, the industry, and the public. We will also promote dialogue with a wide range of participants in the retail and wholesale payments systems to better understand a variety of perspectives on key issues. We will continue to sponsor different types of forums as an important part of our public outreach activities.\n\nConclusion\nDespite the many challenges associated with the rapid evolution I have discussed today, the United States continues to enjoy a safe, efficient, and reliable payment system. The strength of its financial institutions, as well as its payments and settlement systems, are the bedrock of the country’s financial infrastructure. As innovations occur in the payments system, market forces will determine which of these innovations will ultimately best serve the needs of consumers and businesses. We, at the Federal Reserve, need to continue to address barriers to innovation to give the private sector scope to experiment with new payment services, while we continue to fulfill our responsibilities to foster a robust payments system that protects and benefits its participants. At this time of strategic change, I believe that dialogue among payments system participants and users will help all of us identify and address issues of innovation and risk in a balanced and thoughtful manner. This conference is a welcome and constructive element in this important dialogue.\n\nFootnotes\n\n1.  Edwin J. Lucio, Helena L. Tenenholtz, and Jeffrey S. H. Yeganeh, of the Board’s staff, contributed to this speech.  Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20060511a.htm",
        "title": "Investing in Payment Innovations: A Federal Reserve Perspective",
        "date": "5/11/2006"
    },
    {
        "content": "May 04, 2006\n\nGovernor Susan Schmidt Bies\n\nAt the Bank Administration Institute Treasury Management Conference, Orlando, Florida\n\nThank you for the invitation to speak today. I know we have a mix of bankers in the audience from institutions of all sizes that are engaged in asset/liability and treasury management. So I am going to discuss some issues that are relevant to banks of various types. First, I am going to comment on the proposed guidance for commercial real estate and nontraditional mortgages. Then, I want to discuss changing risk exposures in market risk and the related capital regulations. Finally, I will focus on recent regulatory actions that relate to small bank holding company capital and trust preferred securities.\n\nProposed Supervisory Guidance\nIt is my understanding that many of you are aware of proposed supervisory guidance relating to commercial real estate and nontraditional mortgages. We have received many comments on both proposals--perhaps even from some of you. Naturally, bankers may be somewhat concerned about the effect that this proposed guidance could have on their business. But I think it is helpful to remember that the primary responsibility of regulators is to ensure that the United States has a safe and sound banking system. When we see possibly excessive risk-taking or inappropriate risk management or controls, we must act. While most U.S. banks operate in a safe and sound manner, we must always be vigilant for problems that may arise in the future.\n\nCommercial Real Estate\nFirst, I would like to underscore that the proposed commercial real estate (CRE) guidance focuses on \"true\" CRE loans. It does not concern commercial loans for which a bank looks to a borrower's cash flow as the source of repayment and accepts real estate collateral as a secondary source of repayment. Rather, it addresses bank lending on commercial real estate projects for which repayment is dependent on third-party rental income or the sale, refinancing, or permanent financing of the property. With \"true\" commercial real estate lending, repayment depends on the condition and performance of the real estate market.\n\nI would also like to mention up front that the proposed guidance is not intended to cap or restrict banks' participation in the commercial real estate sector, but rather to remind institutions that proper risk management and adequate capital are essential components of a sound CRE lending strategy. In fact, both of these components are already in place at many institutions. No element of the proposed guidance is intended to act as a \"trigger\" or \"hard limit\" signaling the need for an immediate cutback in or reversal of CRE lending; rather, the thresholds in the proposed guidance are intended as benchmarks identifying cases for further review.\n\nSupervisors focus on commercial real estate because that sector played a central role in the banking problems of the late 1980s and early 1990s and has historically been a highly volatile asset class. Past problems in the sector have generally come at times when the broader market encountered difficulties. Therefore, banks should not be surprised by the emphasis of the proposed guidance on the importance of portfolio risk management and concentrations. One reason supervisors are proposing CRE guidance at this point is that we are seeing high and rising concentrations of CRE loans relative to capital. For certain groups of banks, such as those with assets between $100 million and $1 billion, average CRE concentrations are about 300 percent of total capital. This is twice the concentration level of about 150 percent in the late 1980s and early 1990s for this same bank group, when we last went through the bottom of a CRE credit cycle.\n\nWhile banks' underwriting standards are generally stronger now than in the 1980s and 1990s, the agencies are proposing the guidance now to reinforce sound portfolio-management principles that a bank should have in place when pursuing a commercial real estate lending strategy. In addition to monitoring the performance of individual loans, bankers should also be monitoring the mix of property types and performance of their portfolio as a whole and the performance of local real estate markets in which they are lending. A bank targeting commercial real estate lending as a primary business activity needs to consider that the risk exposure arising from the performance of its total CRE loan portfolio--the concentration risk--depends on broader real estate market conditions. For example, if a bank has several borrowers with similar projects that encounter problems--such as longer absorption periods, higher marketing costs, or higher vacancy rates--weaknesses in the broader real estate market can have a cascading effect on the quality of the bank's CRE portfolio as real estate values erode.\n\nIn evaluating the impact of their commercial real estate concentrations, bankers should also pay attention to geographic factors. A bank may lend successfully within a certain geographic area but may encounter problems when it begins to lend outside its market, or \"footprint,\" for which it typically has better market intelligence. In recent years, supervisors have observed banks, in order to maintain a customer relationship, going beyond their established footprint and lending in real estate markets with which they have less experience. The challenge can be even greater when the borrower is also venturing into a new market. In prior CRE credit downturns, such practices have led to significant losses.\n\nNontraditional Mortgage Products\nOver the past few years, the agencies have observed an increase in the number of residential mortgage loans that allow borrowers to defer repayment of principal and, sometimes, interest. These loans, often referred to as nontraditional mortgage loans, include \"interest-only\" (IO) mortgage loans, on which the borrower pays no loan principal for the first few years of the loan, and \"payment-option\" adjustable-rate mortgages (option ARMs), for which the borrower has flexible payment options--and which could also result in negative amortization.\n\nIOs and option ARMs are estimated to have accounted for almost one-third of all U.S. mortgage originations in 2005, compared with less than 10 percent in 2003. Despite their recent growth, however, these products, it is estimated, still account for less than 20 percent of aggregate domestic mortgages outstanding of $8 trillion. While the credit quality of residential mortgages generally remains strong, the Federal Reserve and other banking supervisors are concerned that current risk-management techniques may not fully address the level of risk inherent in nontraditional mortgages, a risk that would be heightened by a downturn in the housing market.\n\nMortgages with some of the characteristics of nontraditional mortgage products have been available for many years; however, they have historically been offered to higher-income borrowers. More recently, they have been offered to a wider spectrum of consumers, including subprime borrowers, who may be less suited for these types of mortgages and may not fully recognize the embedded risks. These borrowers are more likely to experience an unmanageable payment shock during the life of the loan, meaning that they may be more likely to default on the loan. Further, nontraditional mortgage loans are becoming more prevalent in the subprime market at the same time risk tolerances in the capital markets have increased. Banks need to be prepared for the resulting impact on liquidity and pricing if and when risk spreads return to more \"normal\" levels and competition in the mortgage banking industry intensifies.\n\nSupervisors have also observed that lenders are increasingly combining nontraditional mortgage loans with weaker mitigating controls on credit exposures--for example, by accepting less documentation in evaluating an applicant's creditworthiness and not evaluating the borrower's ability to meet increasing monthly payments when amortization begins or when interest rates rise. These \"risk layering\" practices have become more and more prevalent in mortgage originations. Thus, while some banks may have used elements of the product structure successfully in the past, the easing of traditional underwriting controls and sales of products to subprime borrowers may have unforeseen effects on losses realized in these products.\n\nIn view of these industry trends, the Federal Reserve and the other banking agencies decided to issue the draft guidance on nontraditional mortgage products. The proposed guidance emphasizes that an institution's risk-management processes should allow it to adequately identify, measure, monitor, and control the risk associated with these products. It reminds lenders of the importance of assessing a borrower's ability to repay the loan including when amortization begins and interest rates rise. These products warrant strong risk-management standards as well as appropriate capital and loan-loss reserves. Further, bankers should consider the impact of prepayment penalties for ARMs. Lenders should provide borrowers with enough information to clearly understand, before choosing a product or payment option, the terms of and risks associated with these loans, particularly the extent to which monthly payments may rise and that negative amortization may increase the amount owed above the amount originally borrowed. Lenders should recognize that certain nontraditional mortgage loans are untested in a stressed environment; for instance, nontraditional mortgage loans to investors that rely on collateral values could be particularly affected by a housing price decline. Investors have represented an unusually large share of home purchases in the last two years. Past loan performance indicated that investors are more likely to default on a loan when housing prices decline, than owner occupants.\n\nOngoing Efforts to Update and Improve the Market Risk Amendment\nLet me now describe the changes that are occurring in market risk capital. While only banks with large trading books hold capital for market risk, the issues that the proposed changes are designed to address can be considered by treasurers, asset/liability managers, and traders to determine whether their risk management practices are keeping pace with the changing nature of risks.\n\nAs you are no doubt aware, about a decade ago the U.S. banking agencies developed regulatory capital requirements specifically for market risk. These requirements, set forth in the Market Risk Amendment (MRA) to the Basel Capital Accord, were an attempt to keep up with the financial innovation that was occurring in the industry. The Basel MRA is only applied to banks with sizable market risk exposures. The U.S. banking agencies adopted the internal models approach to capturing the market risk arising from traded exposures. The internal models approach, which is the only one offered in the United States, is based on the widely used value-at-risk (VaR) methodology with a uniform ten-day holding period and a 99 percent confidence interval. At the time the Market Risk Amendment was adopted, the soundness standard set forth in the internal models approach worked well for the traded positions of a large number of banks. However, even then, banks and supervisors recognized that certain risks, such as fat tails and model risks, were not well captured in VaR models. Supervisors were led to impose a multiplier of 3 on internally modeled estimates of general market risk and a multiplier of 4 on internally modeled estimates of specific market risk that do not adequately capture event and default risk.\n\nSince adoption of the Market Risk Amendment, banks' trading activities have become more sophisticated and have given rise to a wider range of risks that are not easily captured in a VaR model. For example, more products related to credit risk, such as credit default swaps and tranches of collateralized debt obligations, are now included in the trading book. These products can give rise to default risks that are not captured well in models specifying a ten-day holding period and a 99 percent confidence interval, particularly if the bank has concentrations across various trading portfolios, such as bonds, credit derivatives, and other structured credit products. In addition, structured and exotic traded products may give rise to liquidity, correlation, concentration, and skew risks, which are difficult to capture adequately in a VaR model. The inability of VaR calculations to adequately measure the risks of certain traded positions may give rise to arbitrage opportunities between the banking book and the trading book because of the lower capital charge that may be afforded trading positions under a VaR approach that is not optimally risk sensitive.\n\nWhen the Basel Committee published its revised capital framework in June 2004,1 the focus was on banks' credit and operational risks, not on their trading activities. However, in releasing the revised framework, the Basel Committee indicated that work should begin immediately on applying the revised framework to banks' exposures arising from trading activities. Thus was convened a working group that included representatives of both the Basel Committee and the International Organization of Securities Commissions (IOSCO), given the interest of the securities regulators in the same issues related to trading book risks. The Basel-IOSCO working group issued a consultative paper in April 2005 and subsequently received comments from banks, investment firms, industry associations, supervisory authorities, and other interested parties. The working group considered these comments in formulating revisions to the MRA, which were published in July 2005.2 The revisions attempt to enhance the risk sensitivity of the capital charges for positions assigned to the trading book by promoting improved market risk methodologies.\n\nThe U.S. banking agencies are in the process of developing a notice of proposed rulemaking to implement the Basel-IOSCO market risk revisions in this country. We expect that the current market risk revisions, like the current U.S. rules implementing the MRA for U.S. banking organizations, will apply only to banking organizations meeting the current threshold criteria for application of the market risk rules--that is, to those organizations having aggregate trading assets and liabilities, as reported on the Call Report, of (1) 10 percent or more of total assets or (2) $1 billion or more. These revised market risk amendments would be applied by banking organizations meeting either criterion, whether or not the organization is adopting the credit and operational portions of the Basel II framework. We expect that a small number of banking organizations will remain under the Basel I rules (as amended) but will also compute a market risk charge. Banking organizations that do not meet one of the criteria would not be subject to a market risk capital charge, regardless of whether they apply the Basel I or Basel II rules for credit and operational risk. This expectation is consistent with our view--since the inception of the market risk rules--that imposing the burden of calculating a market risk capital charge is not appropriate when market risk exposure does not meet the threshold criteria.\n\nThese Pillar 1 revisions in the area of market risk are supplemented and supported by revisions in the areas of supervisory review (Pillar 2) and market discipline (Pillar 3), just as are the revisions for credit risk and operational risk. The Pillar 2 changes seek to strengthen firms' assessments of their internal capital adequacy in the area of market risk, taking into account the output of their VaR models, valuation adjustments, and stress tests. Internal capital assessments must factor in: illiquidity; concentrated positions; nonlinear and deep out-of-the-money products; events and jumps-to-default; and significant shifts in correlations. The Pillar 3 changes increase the robustness of trading book disclosures; specifically, firms must demonstrate how they combine their risk measurement approaches to arrive at an overall internal capital assessment. We expect to issue this NPR sometime this summer.\n\nInnovations in Capital Instruments\nNaturally, in determining capital adequacy ratios, supervisors focus not just on defining risk positions--the denominator--but also on defining the components of capital--the numerator. Over the years there have been a number of innovations in capital instruments; one particularly innovative area has been the structuring of preferred stock and hybrid securities for inclusion in tier 1 capital. The Federal Reserve refined its framework for the components of tier 1 capital in a final rule issued in March 2005. The final rule allows bank holding companies to continue to include trust preferred securities in tier 1 capital, much of which they invest in the common stock of their depository institution subsidiaries. However, the final rule also imposes stricter quantitative limits and qualitative standards on trust preferred securities and on other restricted core capital elements included in tier 1 capital. The Federal Reserve's goal in framing the final rule was to allow innovation and adaptation in capital funding for bank holding companies--much of which are aimed at increased cost effectiveness--while ensuring consistency with the Federal Reserve's focus on the quality and strength of institutions' capital bases. To this end, the final rule states that common stock and noncumulative perpetual preferred stock should make up no less than three quarters of a banking organization's tier 1 capital. We believe that the continued strength of the capital base of U.S. bank holding companies is a critical component of the safety and soundness of the industry we supervise.\n\nThe Federal Reserve considers many factors when it evaluates eligible capital for BHCs, especially for innovative capital instruments. Thus, our decision to allow BHCs to continue to include trust preferred stock in tier 1 capital came only with stricter quantitative standards that apply to a range of non-common equity capital elements. In our view, the continued inclusion of trust preferred securities in tier 1 capital is merited because our experience has shown that this instrument can provide financial support to banking organizations if their financial condition deteriorates. Also, from a competitive point of view, poolings of trust preferred stock have enabled smaller BHCs to enter the capital markets for tier 1 capital, which larger BHCs have long been able to access.\n\nA recent innovation that banking organizations have been considering is a modified trust preferred security that continues to receive tier 1 capital treatment but is given increased equity credit from the rating agencies. Although the Federal Reserve is working with institutions on possible modifications to conventional trust preferred securities that meet this objective, our focus is on ensuring that the instruments continue to provide capital strength and do not give rise to supervisory problems. Our experience with conventional trust preferred securities has been largely positive, and we hope to maintain that track record with any modified trust preferred securities we eventually approve. Thus, while we are cognizant that a large volume of trust preferred securities will begin to become callable at the end of this year, making institutions especially keen to find a replacement security with higher rating-agency equity credit, we intend to move judiciously in approving modifications. As always, we will do our best to respond to the business needs of the banking organizations we supervise in a timely manner and to accommodate them to the extent possible within the bounds of our prudential framework.\n\nChanges to the Small Bank Holding Company Policy Statement\nFinally, knowing that many of you represent smaller banks, I want to highlight a recent change in the Federal Reserve's Small Bank Holding Company Policy Statement. That statement was originally issued in 1980 to facilitate the transfer of ownership of small community-based banks in a safe and sound manner. It permits the formation and expansion of small bank holding companies (BHCs) that have debt levels higher than would be permitted for larger BHCs. The statement previously applied to those BHCs (qualifying small BHCs) that had pro forma consolidated assets of less than $150 million and met certain qualitative criteria.\n\nThe Federal Reserve follows the general principle that bank holding companies should be a source of strength for their subsidiary banks. When a bank holding company incurs debt and relies on the earnings of its subsidiary banks to repay the debt, the probable effect on the financial condition of the holding company and its subsidiary bank or banks becomes a concern. The Federal Reserve believes that a high level of debt at the parent holding company level can impair the BHC's ability to provide financial assistance to its subsidiary bank or banks; in some cases, the servicing requirements on such debt may be a significant drain on the bank's resources. Nevertheless, the Federal Reserve recognizes the need for flexibility in the formation and expansion of small bank holding companies that have debt levels higher than would be permitted for larger bank holding companies. Notably, approval of the higher debt levels has been given on the condition that the small bank holding companies demonstrate the ability to service debt without straining the capital of their subsidiary banks and, further, that the companies restore their ability to serve as a source of strength for their subsidiary bank within a relatively short period.\n\nIn September 2005, the Federal Reserve requested comment on proposed amendments to the rule. After reviewing the comments on the proposal, the Federal Reserve in February 2006 approved a final rule very similar to the proposal. The final rule raised the small BHC asset size threshold from $150 million to $500 million for determining whether a small BHC would be eligible for the Policy Statement and exempt from the Capital Guidelines. The Federal Reserve also adopted several modifications to the qualitative criteria under which a BHC not exceeding the asset size threshold nevertheless would be ineligible for application of the Policy Statement and would be subject to the Capital Guidelines. These modifications were intended to ensure that factors related to safety and soundness, not just to size, are also taken into account. The final rule also clarified that subordinated debt associated with issuances of trust preferred securities generally would be considered debt for most purposes under the Policy Statement, but provided a five-year transition period for subordinated debt issued before the date of the proposed rule.\n\nThe new threshold and amended qualitative criteria are designed to reflect changes in the industry since the Policy Statement was first issued in 1980. With respect to the amended qualitative criteria, the final rule excludes from Policy Statement eligibility any small BHC that is engaged in significant nonbanking activities; is engaged in significant off-balance sheet activities, including securitizations or assets under management; or has a material amount of debt registered with the Securities and Exchange Commission or equity securities outstanding. Federal Reserve staff expect that relatively few small BHCs will be excluded under these criteria.\n\nWhereas the treatment of subordinated debt associated with trust preferred securities was not previously defined, the final rule clarifies that such subordinated debt is considered debt for most purposes under the Policy Statement. It provides for a five-year transition period, however, to give qualifying small BHCs sufficient time to conform their debt structures. The rule also allows small BHCs to refinance existing issuances of trust preferred securities without losing the exempt status of the related subordinated debt during the transition period, so long as the amount of the BHC's subordinated debt does not increase in the aggregate.\n\nFinally, the notice informs the public that the Federal Reserve expects to review the asset threshold at least once every five years to determine whether further adjustments might be appropriate.\n\nConclusion\nThe Federal Reserve believes that ensuring strong capital levels and good risk management at U.S. banking organizations is critical to the health of our banking and financial system. Our regulatory and supervisory efforts support this broad objective. That is why we provide guidance around emerging risk issues, such as the current proposals for commercial real estate and nontraditional mortgages. We also work to make sure innovations in financial instruments are encouraged and used in appropriate risk-management frameworks, and we periodically will change regulatory policy to support sound risk management. At the end of the day, it is our job as bank supervisor and central bank to ensure that banks are operating in a safe and sound manner, and that financial stability is maintained.\n\nThank you.\n\nFootnotes\n\n1.  Basel Committee on Banking Supervision, International Convergence of Capital Measurement and Capital Standards: A Revised Framework (Basel, June 2004), available online at www.bis.org/publ/bcbs107.htm.  Return to text\n\n2.  See Bank for International Settlements, Basel Committee on Banking Supervision, The Application of Basel II to Trading Activities and the Treatment of Double Default Effects (July 2005), www.bis.org.  Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20060504a.htm",
        "title": "Supervisory Perspective on Current Bank Capital, Market Risk, and Loan Product Issues",
        "date": "5/4/2006"
    },
    {
        "content": "May 03, 2006\n\nChairman Ben S. Bernanke\n\nAt Operation HOPE’s Anacostia Economic Summit, Washington, D.C.\n\nI would like to thank Operation HOPE and the host committee for inviting me to participate in today's summit. I would also like to congratulate John Bryant and the staff of the Anacostia HOPE Center on this anniversary and on your many contributions to this community over the past year. I recently visited the HOPE Center and saw some of the services it offers in financial education, small business development, job training, and computer literacy. These activities contribute to the revitalization of this community by helping residents gain access to resources they can use to improve their economic situations and prospects. Institutions such as the HOPE Center and THEARC (Town Hall Education, Arts, and Recreation Campus) bring people together and give them a sense of being part of a larger community.\n\nToday, I will share some thoughts on economic revitalization, highlighting two general themes. The first theme is the importance in community economic development of strategic collaboration among public, private, and nonprofit organizations. Working together, these three groups of actors can achieve much more than they could on their own. The second theme is the need for a comprehensive approach to revitalizing communities, one that focuses on the economic and cultural viability of the community as a whole, not only on the construction or rehabilitation of individual homes and businesses. In my remarks I will focus on the experience of the Anacostia neighborhood in the District of Columbia, which offers useful illustrations of both themes.\n\nAnacostia: Looking Back\nTo think about Anacostia's future it helps first to recall its past. I recently toured some of the neighborhoods southeast of the Anacostia River and gained an appreciation for both the rich history of the area and its great promise. Many of the buildings in this area are architectural treasures. The older houses date from the late 1800s, when Anacostia was home to a large working-class community. The residents included, of course, Frederick Douglass, known as the Sage of Anacostia, a man whose dedication to lifelong learning and to the battle for racial equality still has the capacity to inspire.\n\nIn the early to middle twentieth century, Anacostia offered affordable homes and wide open spaces--at that time, it had only 5 percent of the District's total population but 40 percent of the District's vacant land. In the 1920s, Anacostia had a higher percentage of homeowners than other sections of the District, with apartments accounting for less than 1 percent of its dwellings. Indeed, during a six-year period of rapid expansion following World War I, when Anacostia's population surged 56 percent, more than 1,800 detached and row houses were constructed in the area, but only four apartment buildings.1\n\nHowever, the pattern of residential construction and homeownership in Anacostia changed dramatically over the following decades. During the 1920s, 1930s, and 1940s, policies designed to eradicate tenements in central D.C. resulted in Anacostia, with its relatively large tracts of undeveloped land, becoming home to large numbers of lower-income families displaced from other parts of the city. In the 1950s and 1960s, the growth of the city's population, the expansion of the central business district to accommodate the burgeoning federal government, and urban renewal projects led to substantial increases in the demand for new housing in the city, particularly for lower-income residents. By 1967, the need for affordable housing in the District led the planning commission to call for the construction of 65,000 units of new housing, with 30,000 of those units to be located in Anacostia.\n\nThe result of these developments was an apartment-construction boom that ultimately changed the character of the area. As the community's stock of public and rental housing increased, Anacostia shifted from being a community of homeowners to one of renters, with a high concentration of lower-income households. As we have seen in other cities, large-scale public housing projects have often become liabilities to the community. Anacostia struggled as public housing complexes fell into disrepair, many homes were abandoned, crime rates increased, and poverty rates climbed. The economic vitality of this historic community was undermined, and previously healthy neighborhoods were destabilized.\n\nAnacostia Today\nToday, however, Anacostia looks to be on the way back. Leaders in the public, private, and nonprofit sectors have a new vision for the area--one of mixed-income neighborhoods, vibrant commercial and retail centers, expanded neighborhood amenities, and strong community institutions. These partners are playing different but complementary roles, bringing both capital and expertise and ushering in a new wave of economic development in the area.\n\nIn the public sector, both the federal and city governments are supporting development in Anacostia through their investments, often leveraged with private money. On the federal side, for example, the Department of Homeland Security's new communications center and other planned development on the St. Elizabeth's property will bring jobs and economic activity to the neighborhood. The city of Washington is making a number of important investments here as well. For example, the Anacostia Gateway project will include two buildings, one that will house offices of the D.C. government, the other (which will be a joint venture of the Anacostia Economic Development Corporation and the National Capital Revitalization Corporation) providing commercial office space. The city has also made a major commitment for the construction of a baseball stadium, as you know. In the sphere of housing, an entirely new community is rising up at Henson Ridge, bringing residents with a range of incomes. The city took a leadership role in this project, demolishing the public housing previously on the site and then arranging the financing of the new construction through a combination of public bond issuance, private equity, and a successful competitive proposal to the U.S. Department of Housing and Urban Development.\n\nThe promise of these investments for Anacostia's future is supported by recent research, which has highlighted how carefully targeted public investment can help to jump-start urban revitalization.2 An illustration not too far from here can be found in Richmond, where the city played a lead role in finding the funds to develop mixed-income housing in seven distressed neighborhoods. After five years, the housing values in those communities increased nearly 10 percent per year faster than in the city of Richmond as a whole.3\n\nBut public investments alone are generally not sufficient to re-establish the economic viability of a community. For development to be truly sustainable, private capital is also needed. Successful private investment in economically challenged areas can be rewarding, but it requires substantial expertise, local knowledge, and a vision of what the community can become. In this community, several innovative private developers have shown what is possible by collaborating with the city, financial institutions, and nonprofit organizations to rehabilitate and construct thousands of new mixed-income housing units for both renters and owners.\n\nA key element in the success of much of this private-led development is the insight that, to achieve economically viable communities, building housing units is not enough. For people to find an area an attractive place to live, they need a range of services, community institutions, and places to shop and work. Accordingly, developers building in Anacostia have included in their plans community amenities such as day care centers, shuttle services, and recreation programs for resident children. Developments like Asheford Court, a new mixed-use, mixed-income community, will include a supermarket, restaurants, and shops. Even as they make communities more attractive, these amenities create new jobs and provide opportunities for small business development.\n\nTogether with actors in the public and private sectors, leaders in the nonprofit community have an important role in the redevelopment of Anacostia as well, particularly in creating the social infrastructure that improves the quality of life. I have already alluded to Operation HOPE and the important work that it does in helping residents become more financially literate, which paves the way for more people to own homes and to start small businesses. Nonprofit community development organizations, such as East of the River Community Development Corporation, the Anacostia Economic Development Corporation, and the Marshall Heights Community Development Organization have supported retail development. With the support of the Local Initiatives Support Corporation, over the past twenty years these organizations have worked to bring projects such as the Good Hope Marketplace and the future Shops at Park Village to the area. Guided by leaders who have a personal commitment to the success of their communities, these nonprofit organizations have led the way in meeting the needs of residents for affordable housing and in helping to attract private investment and development.\n\nNonprofit educational and cultural organizations are also at the heart of this community's revitalization. The network of nonprofits that worked together to create THEARC--the Washington Ballet, the Corcoran Gallery of Art, the Levine School of Music, and the Boys and Girls Club--is a model for strategic partnerships. THEARC also provides vital services, such as the Children's Hospital family wellness center, a significant resource in a community that previously had no health-care facilities.\n\nAgain, the development philosophy that we see at work in Anacostia is one that focuses not only on the construction of individual homes and businesses but on the broader social and economic environment in the community. As every successful developer knows, real estate markets are driven not only by the characteristics of the physical structures, though those are important, but also by the accessibility of goods and services that current and future residents want, such as schools, shops, and transportation.\n\nData on business patterns offer some insight into the positive changes in Anacostia, as well as the remaining challenges. In the communities which I had the opportunity to visit, the number of business establishments increased by about 7 percent between 1998 and 2003.4 In light of the public and private investments in this community, it is not surprising that some of the most rapid growth was in the number of firms in construction (64%) and in the finance and insurance industries (47%). Industry data for these communities reveal significant increases in the number of establishments in educational services, such as academic and arts schools and training centers (a 118% gain); professional, scientific, and technical services such as offices of lawyers and engineers (a 35% increase); and health care and social assistance including medical care and daycare centers (a 15% gain).5 Although the rate of homeownership in Ward 8 is still only about half the average for the city as a whole, progress has been made on that count as well. From 1990 to 2000, the homeownership rate in Ward 8 increased by 22 percent, and I have little doubt that further improvement has occurred since 2000, given the completion of 765 owner-occupied housing units since 2001 and an additional 210 under construction due to be completed by 2006.6\n\nBeyond Anacostia\nWhat does Anacostia's experience offer for other communities confronting economic decline, given the reality of limited resources and the financial risks associated with redeveloping distressed areas? One lesson is that public, private, and nonprofit development partners must be increasingly innovative in their work. First, they must identify the strategic investments that have the potential to transform neighborhoods and stimulate ongoing private investment and economic activity. As I have noted today, success in community development requires a comprehensive approach--one based on the recognition that vibrant communities offer their residents not only a place to live, but also access to services, to community institutions, and to places to shop, work, and enjoy recreation as well. Many of the development initiatives that have taken place in Anacostia have made good use of this insight, combining housing development with other amenities such as recreational areas, retail outlets, or cultural institutions like THEARC.\n\nSecond, the community leaders, government officials, lenders, and developers now involved in helping to rebuild communities must keep working to find new partners and new sources of capital. In this respect, it is encouraging to see how much more professional the whole field of community development finance has become. For example, over the past twenty-five years, innovative lenders at banks and at community development financial institutions have demonstrated that investments in community economic development can be rewarding in the financial sense as well as in the social sense. With that demonstration, new financing structures may continue to emerge that can help mitigate decreases in government funding. The expansion of secondary markets for affordable housing and community development loans will, I hope, provide increasing liquidity that allows the redeployment of capital for new development efforts. What we see today in Southeast D.C. demonstrates that smart public and private investment can create a virtuous circle of economic growth and opportunity. I am optimistic that the positive changes that we see in Anacostia can be replicated in economically challenged communities throughout the country.\n\nMy optimism also stems from the commitment demonstrated by the participation today of leaders from all levels of government, banking, the corporate sector, and nonprofit institutions. Today's summit underscores your commitment to increasing opportunity for lower-income individuals and communities. I join you in celebrating the ongoing revitalization of Anacostia and the potential for similar communities throughout our nation. I commend you for your leadership and look forward to your continued success.\n\nFootnotes\n\n1.  American Studies at the University of Virginia, \"The Changing Face of Anacostia: Public Housing and Urban Renewal.\"  Return to text\n\n2.  Bruce Katz, Brookings Institution, \"Transformative Investments: Unleashing the Potential of American Cities (3.1 MB PDF),\" April 5, 2006.  Return to text\n\n3.  John Accordino, George Galster, Peter Tatian, \"The Impacts of Targeted Public and Nonprofit Investment on Neighborhood Development: Research based on Richmond, Virginia's Neighborhoods in Bloom Program (3.1 MB PDF),\" July 2005.  Return to text\n\n4.  U.S. Census Bureau, North American Industry Classification System, zip codes 20020 and 20032.  Return to text\n\n5.  Ibid. Return to text\n\n6.  D.C. Office of Planning and NeighborhoodInfo DC, Neighborhood Profiles.  Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060503a.htm",
        "title": "Community Revitalization: Lessons from Anacostia",
        "date": "5/3/2006"
    },
    {
        "content": "April 28, 2006\n\nGovernor Susan Schmidt Bies\n\nAt the Enterprise Risk Management Roundtable, North Carolina State University, Raleigh, North Carolina\n\nThank you for the invitation to speak today. As you likely know, I am quite interested in the discipline of enterprise risk management (ERM) and believe roundtables such as this are a useful way to share perspectives and approaches to sound ERM. I know that these roundtables cover a wide range of industry sectors, but today I will, naturally, focus on ERM from a banking perspective. I will use some recent cases in which we believe bankers and supervisors have learned some key lessons about ERM and describe how the lessons learned can be more broadly applied to other industries. But before I start discussing particular examples, I want to take a step back and give you my thoughts on ERM more broadly.\n\nGeneral Thoughts on Enterprise Risk Management\nThe financial services industry continues to evolve to meet the challenges posed by emerging technologies and business processes, new financial instruments, the growing scale and scope of financial institutions, and changing regulatory frameworks. The Federal Reserve Board, as the primary supervisor of state member banks and the consolidated supervisor of financial holding companies, has been working with other regulators and financial institutions to improve the effectiveness and relevance of regulation and supervision in this changing environment. The Federal Reserve has long emphasized the need for appropriate and strong internal controls in institutions we supervise, and we have taken a continuous-improvement approach to our risk-focused examinations. For many years, enterprise risk management across multiple organizational units within an entity has received increased scrutiny.\n\nIn some cases, firms may be practicing good risk management on an exposure-by-exposure basis, but they may not be paying close enough attention to aggregation of exposures across the entire organization. Rapid growth can place considerable pressure on, among other areas, an organization's management information systems, change-management controls, strategic planning, credit concentrations, and asset/liability management. An organization must also understand how its various business components, some of which can be quite sophisticated and complex, dynamically interact. A successful ERM process can help to meet many of these challenges.\n\nOf course, enterprise risk management is a fairly broad topic that can mean different things to different people. For our purposes here today, I will define ERM as a process that enables management to effectively deal with uncertainty and associated risk and opportunity, enhancing the capacity to build stakeholder value. Borrowing from ERM literature, I would say that ERM includes\n\n\n\nMany of you are probably familiar with the ERM framework published over a year ago by the Committee of Sponsoring Organizations of the Treadway Commission, or COSO. The COSO framework provides a useful way to look at ERM and helps generate further discussion--just what this ERM roundtable is trying to promote.\n\nIn the COSO framework, ERM consists of eight interrelated components that are derived from the way management runs an enterprise and that are integrated with the management process: (1) internal environment, (2) objective setting, (3) event identification, (4) risk assessment, (5) risk response, (6) control activities, (7) information and communication, and (8) monitoring.\n\n\n\nHaving laid out some general thoughts about ERM within the COSO framework, I would now like to discuss a few recent examples from banking in which the importance of ERM has been highlighted. With the benefit of hindsight, the financial regulators and the industry have been trying to distill the lessons learned from these recent breakdowns in risk management and internal control in the financial services sector.\n\nCompliance Risk\nOne area in which ERM provides tangible value is the area of compliance risk, which can be defined as the risk of legal or regulatory sanctions, financial loss, or damage to an organization's reputation and franchise value. This type of risk may arise when an organization fails to comply with the laws, regulations, or codes of conduct that are applicable to its business activities and functions. The Federal Reserve expects banking organizations to have in place an infrastructure that can identify, monitor, and effectively control the compliance risks that they face. Needless to say, the infrastructure should be commensurate with the nature of the organization's compliance risk. For a large complex banking organization, dealing with compliance risk can be particularly challenging unless it has a well-developed risk-management program.\n\nTo create appropriate compliance-risk controls, organizations should first understand compliance risk across the entire entity. Managers should be expected to evaluate the risks and controls within their scope of authority at least annually. I should also emphasize the need for the board of directors to establish a top-to-bottom compliance culture that is well communicated throughout the organization by senior management so that all staff members understand their compliance responsibilities and their role in implementing the compliance program. Clear lines of communication and authority help to avoid conflicts of interest.\n\nAn enterprise-wide compliance-risk management program should be dynamic and proactive, meaning it constantly assesses evolving risks when new business lines or activities are added or when existing activities and processes are altered. The process should include an assessment of how those changes may affect the level and nature of risk exposures, and whether mitigating controls are effective in limiting exposures to targeted levels. This understanding must be constantly evolving, to keep up with not only the organization's own products and business strategies, but also regulatory changes. To avoid having a program that operates on autopilot, an organization must continuously reassess its risks and controls and communicate with its business lines. Also, if compliance is seen as a one-off project, undertaken only when a new regulation or product is introduced, a banking organization is placing itself at risk that, down the road, the lack of a compliance process will not sustain the effectiveness of the program as the organization changes.\n\nCompliance-risk management can be more difficult for management to integrate into an organization's regular business processes because it often reflects mandates set out by legislation or regulation that the organization itself does not view as key to its success. For example, bankers understand how vital credit-risk management and interest-rate risk management are to their organizations, because they reduce the volatility of earnings and limit losses. On the other hand, regulations that are enacted for broader societal purposes, but that the enterprise sees as of little benefit to its revenue growth, can be viewed as an expensive mandate. For example, the Patriot Act requires significant reporting of transactions to the government, and many in industry have expressed frustration about the burden associated with such reporting. I can assure you, we recognize banking organizations' investment in and commitment to compliance with regulatory requirements, including those imposed by anti-money-laundering and counter-terrorism regulations. The Federal Reserve will continue to work with our counterparts in the federal government to encourage enhanced feedback on how reporting is contributing to our common fight against money laundering and terrorism.\n\nOperational Risk\nOver the past few years, the Federal Reserve has been increasing its focus on operational risk. For those of you in nonfinancial organizations, the largest share of your enterprise risk is likely to be operational risk, as opposed to credit and interest-rate risk. Banks have learned much from the practices that you have developed over the years. Operational risk has more relevance today for bankers largely because they are able to shed much of their interest-rate and credit risk through sales of loans, use of financial derivatives and sound models to manage the risks that are retained. Further, the revenue streams that are growing the fastest are increasingly related to transaction processing, servicing accounts, and selling sophisticated financial products. To be successful, organizations must have complex systems to execute these activities.\n\nBanks are also utilizing advanced models to estimate and manage credit-risk and market-risk exposures. Growing use of sophisticated models requires stronger risk-management practices since weaknesses in the models' operational design and data integrity can lead to significant losses. Thus, effective risk management requires financial institutions to have more-knowledgeable employees to identify system requirements, monitor their effectiveness, and interpret model results appropriately.\n\nWe have learned quite a bit about operational risk from our examinations of banking organizations. For example, during routine examinations of activities that pose operational risk, we look at the adequacy of banks' procedures, processes, and internal controls. Such reviews include transaction testing of control routines in higher-risk activities. For example, a bank's wire transfer activities and loan administration functions are often targeted for review, and our experiences have identified some common weaknesses in operational control that are worthy of attention.\n\nWith wire transfers and similar transactions, a banking organization could suffer a significant financial loss from unauthorized transfers and incur considerable damage to its reputation if operational risks are not properly mitigated. A few recurring recommendations from our reviews are to (1) establish reasonable approval and authorization requirements for wire transactions to ensure that an appropriate level of management is aware of the transaction and to establish better accountability; (2) establish call-back procedures, passwords, funds transfer agreements, and other authentication controls related to customers' wire transfer requests; and (3) pay increased attention to authentication controls, since this area may also be particularly susceptible to external fraud.\n\nLoan administration is another area where banking organizations could suffer significant financial losses from inappropriate segregation of duties or lack of dual controls. An institution could also incur considerable damage to its reputation if operational risk factors are not properly mitigated. A few recurring recommendations from these types of reviews that may be applied to corporations more generally are to (1) ensure that loan officers do not have the ability to book and maintain their own loans; (2) limit employee access to those loan system computer applications that are inconsistent with their responsibilities; and (3) provide line staff with consistent guidance, in the form of policies and procedures, on how to identify and handle unusual transactions.\n\nOperational Risk Arising In Recent Financial Restatements\nRisks can sometimes quickly appear where they were not traditionally expected. For example, consider the changes we have seen in financial reporting quality of corporations in all industries. In 2005, there were approximately 1,200 restatements of previously filed financial statements by publicly traded companies--twice the rate for 2004. The complexity of GAAP accounting standards and a more stringent, literal interpretation of the application of those standards by auditors and regulatory bodies, primarily the Securities and Exchange Commission, are two major factors that have led to the restatements.\n\nExamples of prominent restatements include FAS 133 hedge accounting and lease accounting issues. In the area of hedge accounting, the restatements generally resulted from the misapplication of the \"short-cut\" method. The organizations in question did not satisfy all of the criteria for use of the short-cut method but, nonetheless, utilized hedge accounting treatment allowed by this method.\n\nIn the area of lease accounting issues, most companies simply failed to apply longstanding accounting standards related to revenue recognition reserves, accruals and contingencies, and equity accounting. Most companies believed they were actually reporting correctly prior to the restatements. Virtually all of these companies were audited by auditing firms that are now registered with the Public Company Accounting Oversight Board (PCAOB). The PCAOB's inspection process, which involves close scrutiny of registered firms, may be a factor in the increased number of restatements.\n\nSection 404 of the Sarbanes-Oxley Act of 2002 requires each annual report of a public company to include a report by management on the company's internal control over financial reporting. Looking only at banking organizations, as a result of restatements, the number of material weaknesses in internal control for the 2004 reporting period has been revised up to 52 from the 37 originally reported. This increase implies a significant amount of operational risk associated with the accounting process.\n\nGenerally, examiners review the Sarbanes-Oxley 404 process to determine whether the organization has a clear understanding of the roles of the audit committee, management, internal audit, and the external auditor and whether the organization has implemented an effective plan to achieve the objectives and requirements of Sarbanes-Oxley 404. Examiners also review the Sarbanes-Oxley 404 process to determine whether the organization has an effective follow-up strategy for the remediation of significant deficiencies and material weaknesses. Examiners are encouraged to utilize the results of the Sarbanes-Oxley 404 process, where possible, in their overall assessment of the organization's risk management and control process and in the risk scoping of safety-and-soundness examinations and inspections.\n\nInformation Security\nCyber attacks and security breaches involving nonpublic customer information appear in the headlines almost every week. These events have cost the financial services industry millions of dollars in direct losses and have done considerable reputational damage. The cost of identity theft to affected consumers is also significant. With banking organizations increasingly using the Internet to interact with customers, business partners, and service providers, concerns about the use of the Internet as a communication and delivery channel have resulted in the need for and use of more-sophisticated control mechanisms, such as enterprise-wide firewall protections, multifactor authentication schemes, and virtual private-network connections.\n\nWhile many of the widely publicized information security breaches have involved parties outside the affected banking organization accessing the organization's customer information, organizations also remain at risk for breaches or misuses of information by an insider. During our examination activities, we have seen breakdowns in internal control, resulting in operating losses that were traced back to weak controls over insiders' access to information technology systems interfacing with electronic funds transfer networks. Further investigation into these situations suggests that the duration and magnitude of the fraud and resulting losses is a direct function of the internal party's access to accounting and related systems.\n\nSeveral lessons have emerged. First, institutions should tightly control logical access to funds transfer systems and ensure that access settings enforce separation of duties, dual controls, and management sign-offs. Second, an institution's senior management should be restricted from regular access to business-line functional systems, especially funds transfer systems. When such restriction is impractical, additional controls must be in place and functioning effectively. Finally, effective management of information security risk, even when focused on a specific function, requires an enterprise-wide approach to yield a true and complete evaluation of the associated risks.\n\nMutual Funds\nWell-publicized instances of late trading and market timing at mutual fund firms, and the related investigations, have involved many businesses, including banking, securities, and insurance firms. These types of breakdowns in internal control result in sanctions or financial loss and adversely affect a firm's reputation and franchise value.\n\nI would like to highlight a few lessons learned from our experience in investigating control breaches in these mutual fund cases. One of the most obvious is the need to critically evaluate unusual client relationships that require variances from standard procedures. If a high percentage of compensation is derived from a single client, a red flag should immediately go up. Also, organizations should have a formal process for reviewing and approving unique products, customers, and services at the inception of the client relationship. Further, it is always a good idea to shine some light on areas historically labeled \"low risk\" to validate that assessment. The low occurrence of loss from an activity should not be the only factor considered when assessing risk.\n\nFinally, compensation systems that reward employees for sales without adequately monitoring their internal control breaches can create a conflict between the interest of employees and the interest of the enterprise. As companies move away from straight salaries to more incentive-based systems, it is important that personnel departments be included in an effective enterprise-wide risk-management program to consider how changes in compensation practices affect risks to the enterprise.\n\nCredit Derivatives\nFinally, I would now like to turn to one more issue that has relevance to ERM, and that is the importance of companies including an ERM perspective as they design and build new lines of business. As many of you might know, last year a dialogue between supervisors and credit derivatives dealers was initiated to support industry efforts to address weaknesses in the operations surrounding credit default swaps (CDS). While we view these new instruments as an effective way to diversify and mitigate risks related to credit exposures from corporations, an industry-led study, the Counterparty Risk Management Policy Group II report, identified significant weaknesses in the infrastructure supporting sales and risk monitoring of these instruments. While the report identified 47 recommendations, regulators in the United States and other countries have focused on two major weaknesses.\n\nOne weakness is related to the success of the product. Volume of trades has grown so quickly and reached such a significant volume that broker-dealers are not able to keep up with their paper-based systems to record the trades and document the transactions. As a result, significant backlogs of confirmations of these over-the-counter derivatives have built up. This creates concerns that information feeding risk-management systems--information about the volume, term, and counterparty to the trade--is not complete. This problem would be exacerbated in a stress situation, when positions need to be changed very quickly to mitigate risk.\n\nAnother weakness relates to the lack of discipline in enforcing contract terms. Any time an instrument is traded over the counter, it is important to know who you are doing business with. Since an exchange does not stand between the two sides of the trade, parties make payments directly to each other to honor the terms of the contracts. The market practice is to use collateral or pricing to mitigate the risk that the other side of the trade cannot perform per the agreement. The recent industry study also found that competitive pressures were such that brokers did not enforce the standard CDS agreement and allowed counterparties to assign their side of the trade to another party without notifying the broker. Obviously, this can significantly change the risk profile of a transaction and also make it very difficult to settle payments in a timely manner.\n\nAbout six weeks ago, fourteen major market participants published a letter reiterating their commitment to improving the infrastructure that supports the credit derivatives markets. The market participants are committed to the development and implementation of a set of industrywide guidelines that include a targeted reduction in each market participant's confirmation backlogs and assurance that agreement terms will be enforced. Additionally, the fourteen participants will work to create a largely electronic marketplace where all trades will be processed through an industry-accepted platform, develop a new set of processing standards for those trades that cannot be confirmed electronically, and establish a new procedure for settlement following a credit event.\n\nWe are generally pleased with both the industry's self-identification of the issues and its commitment to making improvements. But for purposes of our discussion of ERM today, the problems surrounding CDS sales highlight the challenges risk managers face when market pressures make the firm's line management reluctant to initiate appropriate controls on their own. It also illustrates that in new lines of business, sometimes ERM must go outside the enterprise and work with competitors to support the growth of shared systems and standards to mitigate risks.\n\nConclusion\nAt the Federal Reserve, we believe that all banking organizations need good risk management. An enterprise-wide approach is appropriate for setting objectives across the organization, instilling an enterprise-wide culture, and ensuring that key activities and risks are being monitored regularly. Senior management must be involved in ERM, since they are the ones who decide the level and types of risk the organization is comfortable with accepting and what controls and risk mitigants will be employed to ensure that risk exposures stay within the agreed-upon levels.\n\nIn addition, it is important for organizations to make sure they do not ignore or accidentally overlook lower-profile activities that still might bear substantial risks. As I noted, such activities can include financial statement reporting, information security, and back-office systems. And operational risk, more broadly, has the potential to create disruptions for the organization that could reduce the value of the organization. Often, the solutions to these problems are basics such as training, developing internal controls, and establishing the appropriate culture across the organization. Therefore, organizations should look at the discipline of enterprise risk management as a way to ensure that they effectively deal with uncertainty and the associated risk and opportunity.\n\nThank you.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20060428a.htm",
        "title": "A Bank Supervisor's Perspective on Enterprise Risk Management",
        "date": "4/28/2006"
    },
    {
        "content": "April 27, 2006\n\nGovernor Donald L. Kohn\n\nAt the Forecasters Club of New York Luncheon, New York, New York\n\nThe members of any group calling itself the Forecasters Club don't need an elaborate justification for my focus today on business fixed investment. The outlook for business investment is always a key element in any economic forecast. It can be a highly volatile component of aggregate demand, with variations in investment often accounting for a good share of the fluctuations in economic activity. That lesson was demonstrated again in the past decade when strength in business investment contributed to the vigorous expansion of the second half of the 1990s, and then a marked and prolonged weakening in capital spending contributed to recession and sluggish growth in 2001 and 2002. Business investment also has important implications for the supply side of the economy through its influence on the rate of increase in labor productivity and thus the economy's sustainable level of potential output.\n\nJust three years ago, Chairman Bernanke talked to you about investment, laying out a structure for forecasting and using it to comment on the economic outlook. I thought that now would be a propitious moment to revisit the subject. What I would characterize as the standard forecast for this year and next has the economy slowing a bit to trend. That slowing arises in substantial part from the effects that a cooling in housing markets has on residential construction and on consumption. But, at the same time, the standard forecast sees growth as being supported by a continued robust expansion of business investment. However, the range of views on investment seems a little wider than usual; the variation in large part reflects whether forecasters foresee a resolution of some apparent anomalies in investment behavior observed over the past several years. In particular, the weakening in investment in 2001 and 2002 was larger and lasted longer than many had anticipated, and although investment growth has picked up in recent years, the level of investment has not fully recovered from the earlier weakness. My remarks today examine what we have--and have not--learned about this shortfall and what we might expect for business fixed investment over the next few years. I must emphasize that these views are my own and do not necessarily reflect the views of my colleagues on the Federal Open Market Committee.1\n\nBusiness fixed investment has risen at a robust annual rate of nearly 9 percent on average over the past two years, and the real level of investment at the end of last year, $1.3 trillion, was nearly 6 percent higher than the peak reached five years earlier. However, real gross domestic product (GDP) expanded nearly 14 percent over the same period. To be sure, the investment peak in 2000 was unusually high; still, the nominal share of business fixed investment in GDP, at 10-3/4 percent at the end of 2005, was well below its forty-year average.\n\nOf course, comparisons of these simple ratios and growth rates do not account for other influences on investment in the macroeconomic environment, such as interest rates, the prices of capital goods, or the rate of increase in final spending. However, a more rigorous exercise using a standard model favored by many forecasters yields a similar conclusion. Using lagged net investment, changes in business output, and changes in the user cost of capital to predict the level of current net investment, we find that the model did not forecast the plunge in investment in 2001. And, despite its ability to predict recent growth rates reasonably well, a dynamic simulation of such a model starting in 2000 indicates that the current level of investment is still considerably lower than expected. To be sure, we would not have expected investment to snap back right away, judging from experience. Still, investment over the past few years is showing no signs of returning to the path that we would have expected from historical relationships through 1999.\n\nBusiness financial statements also reflect evidence of restrained business spending behavior. Normally, businesses are heavy net users of savings generated by the rest of the economy. The financing gap--the level of capital spending over the level of internal funds--is a measure of that reliance. But it was close to zero in 2002 and 2003 and remained unusually low last year (after adjustment for tax-induced flows of repatriated foreign earnings), which suggests that businesses didn't see enough profitable investment projects to warrant tapping the markets for external financing, even at low long-term interest rates. To be sure, profit margins and cash flow have been high, but that would also seem to be an environment that should encourage expansion. In fact, businesses appear to be using some of their very large holdings of cash for other purposes. Corporations have increased their share repurchases, which hit a record level last year. They have also increased share retirements through cash-financed mergers and acquisitions, which have been boosted by a surge in buyouts. Evidently, corporate managers view prospective returns from these uses of cash flow as comparing favorably with those from new capital spending projects.\n\nThe low level of investment has not been unique to the United States. Gross investment in other member countries of the Organisation for Economic Co-operation and Development was sluggish in the early years of this recovery, and the nominal share of nonresidential fixed investment in GDP in these countries is still barely higher than its twenty-five-year trough in 2003. This pattern persists despite a low cost of capital and ample cash flows in many countries as well as business sentiment that improved markedly last year. Although real investment in Japan has moved up fairly steadily for nearly three years, capital spending was weak for nearly a decade as profits were channeled to clean up balance sheets rather than expand productive capacity. Euro-area investment has also languished, in part because of relatively slow growth prospects. In both Japan and the euro area, investment likely also is being curtailed to some extent because of a demographic shift toward a more elderly population: As the share of the population that is of working age declines, the rise in the capital stock needed to equip the labor force decreases.\n\nInvestment in the East Asian countries is still lower than before the crises of the late 1990s, although investment rates in the region are generally higher than those in advanced economies. Also, lower rates of investment in some of these countries may reflect some shifting of production to China (where investment rates have been quite high in recent years).\n\nRatios and equations are at best only rough guides to the investment that we might expect on the basis of past behavior in similar circumstances. Still, looking across a variety of indicators and a variety of countries, it does appear that the level of investment is unusually low for this stage of the business cycle. A more difficult task is determining why. As we shall see, there are a number of possible explanations, but no single one seems to hold the entire answer.\n\nAn explanation that has received a great deal of attention is that a capital overhang, usually thought of as concentrated in high-tech equipment, developed in the late 1990s and subsequently has been dragging down investment spending. In the late 1990s, firms invested in high-tech goods at a very rapid pace, spurred at first by plunging prices and robust business output growth and eventually by an apparently overly optimistic view about the returns on those investments. Subsequently, high-tech investment dropped at a double-digit rate in 2001 and fell further in 2002. This sharp decline, combined with the high depreciation rates on these types of goods, severely curtailed growth of the capital stock, and any overhang seems likely to have been eliminated relatively quickly. However, desired or optimal capital stocks are notoriously difficult to specify and measure, and hence so are overhangs, even several years after the episode. Consequently, we cannot definitively rule out the possibility that the excess capital built up during the late 1990s is restraining investment to some extent today.\n\nAnother possibility is that business investment has been held down in recent years because relative prices of capital goods are no longer falling at the same pace at which they declined in the late 1990s. At least some of the deceleration may reflect a slowing pace of technological improvement--that is, less-rapid downward shifts in the supply curve of capital goods. However, to the extent that these price changes are well measured and our econometric models are well specified, the implications of slower price declines should already be captured by our models. In addition, the effects of less rapidly falling prices on the growth rate of the user cost of capital appears to have been substantially offset over much of this period by declines in real interest rates.\n\nAnother explanation that received attention in the early years of this decade is that businesses were unusually cautious after the most recent recession in expanding their productive capacity. Both hiring and capital investment lagged the usual recovery pace. One possible source of this caution was said to be questions about the strength and sustainability of the recovery, accentuated by concerns about terrorism and other geopolitical uncertainties. Many periods of recovery have been accompanied by concerns about economic growth and political turmoil. But surveys suggest that managers did experience a prolonged sense of gloom, with measures of sentiment dropping to low levels and staying that way for a year or two beyond the business cycle trough. However, the economy has been expanding at an above-trend pace for about two years now, and the durability of the recovery should no longer be an issue. Indeed, most surveys of business confidence and capital spending plans have reached, and in some cases exceeded, the levels of the late 1990s.\n\nStill another possibility is that conditions created by corporate governance scandals and the regulatory response to those events led firms to hold back on capital spending. The scandals and the market's reaction were said to have contributed to a more conservative attitude toward risk taking. Moreover, complying with the Sarbanes-Oxley Act of 2002 may have affected capital spending as firms scrambled to meet the 2004 and 2005 deadlines. Clearly, compliance costs have been substantial, perhaps diverting funds and attention away from capital spending plans. However, capital spending to update information systems to address the enhanced auditing needs may be offsetting at least a portion of any damping effect the legislation may have had. In any event, the market effects of corporate governance scandals appear to have faded some time ago. And, at larger companies, where systems have been adapted to the new requirements, compliance now should be more routine, freeing time and attention to concentrate on business strategy and expansion. If these types of influences have had any restraining effects, they should be receding.\n\nChanging replacement cycles are another potential downward influence on the pace of investment. Before 2000, many firms invested in new technologies to replace those not compatible with the century date change. This effort tended to speed up replacement cycles (and thus depreciation), boosting gross investment at that time. The resulting bunching of purchases may have contributed to the drop-off in investment in 2001 as firms with relatively new, efficient capital goods saw less reason to upgrade. Also, during 2001 and 2002, anecdotal reports suggested that many firms saw no need to upgrade equipment because no compelling new technology or application had been released, which would have tended to lengthen the replacement cycle. If replacement cycles since then have remained longer than in previous decades, firms would respond with a lower level of gross investment. And, anecdotes and surveys suggest that replacement cycles have in fact lengthened in this century compared with the late 1990s. But the implied drop in the rate of depreciation is much too small to explain the low-investment puzzle.\n\nSome have posited that low investment in the United States reflects firms' decisions to meet expanding demand by investing overseas rather than at home. Economic theory suggests that in countries where labor is cheap and abundant, all else equal, we would expect the marginal product of capital to be relatively high, making these economies attractive places in which to invest. Thus, countries such as China ought to be seeing an influx of direct investment. However, the dollar value of U.S. direct investment into China averaged about $2 billion per year in the first five years of this decade, much less than 1 percent of domestic investment spending and not enough to be a major influence on investment spending trends. Looking at flows to all developing economies, the share of outward direct investment going to these destinations has been about flat over the past decade. Foreign direct investment, as a whole, has been rising relative to domestic investment, but gradually over several decades--a trend that was not picked up in recent years.\n\nClearly, none of these explanations is the sole cause of the relatively restrained level of investment. Most likely, some combination of these factors along with others we have not identified accounted for the sharp decline in investment in 2001 and 2002 and has contributed to keeping investment spending rising along a lower track subsequently, both domestically and abroad. However, as I noted, several of those factors are of questionable quantitative import, and others no longer seem to provide a rationale for the failure of investment spending to rebound more vigorously. Yet, most indicators in hand do not point to a surge in business fixed investment that will restore the trend derived from earlier relationships.\n\nInstead, the latest reads on business spending and intentions point to continued solid growth in capital spending, supported by favorable fundamentals of steady increases in final demand and a relatively damped cost of capital. Over the past three quarters, both orders and shipments of capital equipment (excluding the volatile aircraft category) have continued to move up at roughly the steady pace seen since 2003. Moreover, orders remained above shipments in the first quarter of this year, leading to another increase in the backlog of orders. In addition, surveys indicate that businesses' capital spending plans and their outlook for sales remain, on balance, in the elevated range that they have occupied for several quarters. A slowing in the growth of consumption and residential investment associated with a cooling in the housing market will exert some restraint on capital investment, but business sales should receive some support from improved markets for our exports.\n\nMoreover, business spending on structures finally seems to be picking up momentum. The construction data that we have in hand for the first two months of the year suggest a bounceback from the anemic growth in spending on nonresidential buildings that has prevailed over the past few years. This pickup should persist, responding to the recent declines in office and industrial vacancy rates. And expenditures on drilling and mining structures are likely to remain strong, given the current market expectations for elevated energy prices. Spending on structures should also get a boost this year from rebuilding in the areas hard hit by last year's hurricanes.\n\nThe outlook for solid increases in investment spending has both upside and downside risks associated with it. On the downside, the cooling off that we are currently observing in housing markets could become more severe, and both residential construction and consumer spending could take a larger hit than expected. A substantial slowing in these two categories of final demand would likely induce some businesses to curtail or delay investment projects. On the upside, we cannot say exactly why the level of investment has remained low for the past few years, so we certainly cannot rule out a return to previous higher trends. In that regard, we do seem to be seeing a strengthening in global demand, which could signal a more pervasive change in attitudes and expectations.\n\nBecause capital spending influences not only aggregate demand today but also influences aggregate supply and productivity over the medium term, it is a key element of any forecast. The focus in current commentary is mostly on the outlook for housing and consumption, but I suspect that business fixed investment will again play a central role in shaping the path of the economy. The experience of the past several years does not seem to have greatly clarified the reasons for the extent of the fall in investment in 2001 and 2002 and its subsequent failure to return to previous trends. The persisting puzzle has the effect of increasing uncertainty around any projection. But it also suggests the potential for substantial returns to further analysis and research for forecasters, like those in this club and us in the Federal Reserve.\n\nFootnotes\n\n1.  Stacey Tevlin and Charles S. Struckmeyer, of the Board's staff, contributed to these remarks.   Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20060427a.htm",
        "title": "Business Capital Spending",
        "date": "4/27/2006"
    },
    {
        "content": "April 20, 2006\n\nChairman Ben S. Bernanke\n\nAt the Greenlining Institute's Thirteenth Annual Economic Development Summit, Los Angeles, California(via satellite)\n\nI would like to thank Greenlining for the opportunity to participate in today's conference. In my time at the Federal Reserve, I have had a number of opportunities to meet with community economic development leaders to discuss issues of mutual concern and learn about the valuable role that community development organizations play in economically distressed areas across the country. I have been particularly impressed, and heartened, by the increasingly high degree of professionalism in the field. In this area, as in social policy generally, good intentions are not enough. Successful community development requires knowledge--knowledge about the particular community in question and about what has worked in similar communities in the past--and community development organizations are working assiduously and with sophisticated tools to help develop that knowledge.\n\nOf course, knowledge bearing on community economic development has both qualitative and quantitative aspects, and it can be gained through diverse channels, from talking to people in a neighborhood to performing a regression analysis. Today, I will focus on the progress that is being made on the quantitative side--in particular, the remarkable strides that have been made in developing and analyzing social and economic data at the community level. The information that can be extracted from detailed data profiles of individual communities supports economic development in several distinct ways. First, by making companies, entrepreneurs, and investors aware of new opportunities and by promoting competition in underserved areas, such information helps put market forces in the service of community development. Second, both government policymakers and community development organizations need the reality check that only hard data can provide. To know whether our policies and programs are delivering the desired results, we need to be able to measure inputs and outcomes, program by program and community by community. Better information increases accountability and promotes good governance in both the public and the nonprofit sectors. Third, the increased availability of community-level data facilitates independent research, which is vital to informing the public policy debate and to developing further community development efforts, both public and private.\n\nHistorically, government agencies have been the source of the most-comprehensive social and economic data bearing on community development. An important example is the data collected by the Federal Reserve under the Home Mortgage Disclosure Act (HMDA). The HMDA data set provides extensive information on home mortgage applications to virtually all U.S. lenders, including approval rates, the socioeconomic characteristics of applicants, and most recently, mortgage pricing information. As all good social scientists know, the data never \"speak for themselves,\" and the HMDA information, like any data set, must be interpreted with care and insight. Still, for nearly three decades, the HMDA data have provided valuable information about mortgage lending patterns, contributed to significant changes in mortgage credit practices, informed regulatory policies, and supported fair-lending enforcement.\n\nAlthough government agencies continue to be an important source of data on community development, data collection and data analysis in this area is increasingly becoming the province of the private and nonprofit sectors, notably including community development organizations themselves. In recent years, we have seen a series of data-collection initiatives outside the public sector, with objectives that include the improvement of development strategies, the identification of new opportunities, the quantification of risk, and the exertion of influence on the direction of public policy. Many of these efforts have already had significant payoffs.\n\nIn the rest of my remarks, I will discuss some specific ways data and quantitative measurement have been used in community development. To be clear, I do not believe that all aspects of economic development can or should be quantified; and, as I have already noted, the data never speak for themselves but must be interpreted with care. Still, improving the measurement of inputs and outcomes is critical to better development policy. In this regard, it is interesting to observe that we have seen some convergence between best practices in community economic development and in economic development policy at the international level. I will conclude by noting a few of those parallels and their implications.\n\nDiscovering Market Potential\nGood data support community growth and development by helping to identify previously unrecognized market opportunities. Free markets can be a powerful source of economic development, but markets work less effectively when information about potential opportunities is absent or costly for private actors to obtain. Several noteworthy initiatives have helped to provide better information about the economic potential of lower-income and underserved communities. For example, the Local Initiative Support Corporation's (LISC) MetroEdge initiative seeks to demonstrate the market potential of diverse communities through customized data analyses of each community's demographics and buying power. Such analysis can provide investors with a different perspective when they assess a neighborhood's viability for investment. In one instance, a national home-improvement retailer used MetroEdge data as the basis for its decision to establish a store in inner-city Chicago, even though the retailer's own site-selection model presented discouraging indications of profit potential for that neighborhood. With access to new market data, the company could justify its investment in the community, and sales performance was triple what was expected within the first six months of operation. 1\n\nSimilarly, Social Compact's Neighborhood Market DrillDown methodology uses a multilayered research process to provide profiles of the market potential of high-density, lower-income communities. This approach focuses on business indicators--buying power, market size, unmet needs, and market risks--rather than on the deficiency statistics typically used to describe inner-city neighborhoods, such as rates of poverty, crime, and overcrowding. Social Compact, a coalition of business leaders, has applied its DrillDown approach to 101 neighborhoods over the past five years, beginning with Chicago neighborhoods and, most recently, in Santa Ana, California. By tapping existing public records and conducting intensive economic and demographic surveys, the DrillDown analyses of these 101 neighborhoods in eight cities have, in the aggregate, revealed additional income and buying power averaging nearly $6,000 per household, which is not captured by traditional sources of community-level data.2 Such information may attract private-sector investors to areas that had once been deemed untenable for investment. For example, following Social Compact's study of neighborhoods in Jacksonville, Florida, a developer announced plans to invest $45 million in a multi-use entertainment complex there. A DrillDown study in inner-city Houston revealed a population that was 25 percent larger than Census estimates, resulting in the redevelopment of a 750,000 square foot retail center that brought 2,000 jobs to a neighborhood that had not had new construction in fifty years. This shopping center is now one of the busiest retail centers in the city. 3\n\nWork to improve the measurement of market potential in inner-city communities is continuing. In one such project, Social Compact and the Brookings Institution's Urban Markets Initiative group are collaborating in reviewing methods for measuring the size and composition of economies in urban areas around the world. The objectives of the review are to develop new tools for measuring economic activity at the local level and to identify areas for future research.\n\nInforming Investors in Community Development\nThe growth and maturation of community development financial institutions (CDFIs) provide another impetus for data development and analysis at the community level. CDFIs are private-sector financial intermediaries with community development as their primary mission. Like banks and other more-conventional financial intermediaries, CDFIs are in the business of attracting funds and putting those funds to work in productive ways. Also like conventional intermediaries, CDFIs depend heavily on the production of accurate information both to guide investment decisions and to provide a basis for attracting new funding. It is difficult to overstate the importance of adequate and accurate information for attracting capital. Managers of pools of capital have many choices, and they tend to be extremely wary when they cannot fully assess the level of risk presented.\n\nWith an appreciation for the need for such information, managers and others with an interest in the CDFI industry have invested substantial effort in designing tools for data collection and analysis that focus on measuring the financial performance--the risks and returns--of CDFI portfolios. An important motivation for these efforts is the need to diversify funding sources for community development, which has relied heretofore largely on grants from government and foundations. To attract more return-oriented investors, including both conventional investors and those with social as well as financial goals, CDFIs must demonstrate financial viability as well as the ability to fulfill the broader development mission.\n\nFor example, the Opportunity Finance Network's CDFI Assessment and Rating System (CARS) gathers data to evaluate a CDFI's overall creditworthiness and its effectiveness in using its financial resources to achieve its development objectives. A CDFI is rated for its financial strength and performance in the areas of capital, assets, management, earnings, and liquidity, in a manner broadly analogous to the way a supervisory agency would rate a commercial bank. The financial analysis is supplemented by an evaluation of how well the CDFI is fulfilling its mission, including an assessment of its procedures for tracking the outcomes of its work. To date, more than forty CDFIs have chosen to be evaluated under the CARS, and thirty-one analyses have been completed. Thus far, fifteen potential investors have subscribed to the CARS database, including socially responsible investment funds, brokerage houses, large financial institutions, and national foundations.4 Although still in its early stages, this initiative, if successful, will have the double benefit of attracting more funds into community development and helping to ensure that those funds are effectively used.\n\nMore generally, the movement toward quantifying the performance, risk, and community impact of CDFIs is essential to the growth and sustainability of the field, in my view. By demonstrating both financial viability and social impact through hard data, CDFIs are better positioned to obtain the funding necessary to maintain their operations and to respond to emerging needs and opportunities. Indeed, progress has been made in recent years in the rating and securitization of community development portfolios, a development that should provide CDFIs with increased access to the capital markets and to new sources of liquidity. If the new data and evaluation methods of CDFI performance bear scrutiny, investors will gain confidence in using this information for matching their investment choices with their priorities and risk tolerances. In the community development field, to be sure, financial returns and social returns are not necessarily the same, which is why measurement should include both financial and social indicators. Potential investors, including public-sector and foundation sources of funds, will naturally differ on the weights they put on financial and social returns. To attract the widest range of funding, both types of information should be provided.\n\nEvaluating Policy and Practice\nQuantitative information plays yet another important role: increasing the effectiveness of policies and programs. The systematic collection and analysis of data on program inputs and outputs is an increasingly important part of learning about what works. For policymakers, data on program results help guide policy development and improve the allocation of scarce public funds. For community development organizations, participation in broad-based data-gathering serves at least two goals. First, in the long run, their analyses of the activities and the associated outcomes in diverse communities will help them achieve the greatest impact for resources expended. Second, such analyses help community development organizations demonstrate their effectiveness to public and private funders.\n\nA number of methods for evaluating community development projects are currently in use, with more in development. The NeighborWorks America's® Success Measures Data System documents the effect of community development programs throughout the country. Using forty-four indicators and a range of data-collection tools, the system quantifies the effects of housing, economic development, and community building programs at the individual, organization, and community levels. By sharing this knowledge, practitioners, funders, and policymakers can identify programs that achieve the best outcomes and gain insights into the reasons they work. Broad access to this information promotes replication of the most effective programs and may diminish the costs associated with trial-and-error learning.5\n\nAnother tool available to CDFIs is the Community Investment Impact System developed by the Department of Treasury's CDFI Fund. This system collects detailed information on institutions and transactions, allowing the CDFI Fund to measure community effects and to associate those effects with institutions working in that area. These results can help inform funding decisions, develop programs, establish performance benchmarks, and communicate societal benefits attributable to specific policy. For example, using data from the system, the CDFI Fund found that in a recent year, CDFIs leveraged financial program awards by the fund at a ratio of 20 to 1, using multiple sources of debt and equity financing from banks, local and state governments, private investors, and borrower equity to structure project financing.6\n\nEach of these data-driven initiatives share the goal of increasing understanding of opaque markets to support investment, policy, and research. The need for data and tools is the driving force behind the Brookings Institution's Urban Markets Initiative. In establishing this policy center, Brookings acknowledged that limited access to data that captures the viability of urban communities constrains investment in these markets. The think tank is focusing on initiatives that can demonstrate untapped market potential.7 One such effort is the National Infrastructure for Community Statistics. It will include a central web-based repository that integrates data from federal, state, and local governments and from commercial sources. The ultimate goal of this project, which is under development in collaboration with more than 100 participants from government, nonprofits, and private-sector industries, is to aggregate and to make accessible the data needed to inform decisions about economic development activities.8\n\nParallels to International Economic Development\nThe usefulness of microeconomic data in community development raises an interesting parallel to recent analyses of international economic development. Although the U.S. context is obviously different in important respects from that of developing countries, domestic community organizations and providers of international aid both face the challenge of fostering economic development in low-income areas. In the United States, our experience in community development over the past thirty years has resulted in an evolution from a centralized, federal-government-driven approach to a heavy reliance on the involvement of community-based organizations and agencies for project development and implementation. In light of this experience, it is quite interesting that some new thinking on international development has rejected the traditional approach to aid, with its emphasis on large-scale projects and top-down planning, in favor of micro-level, bottom-up approaches that use local information and systematic analyses of inputs and outcomes.\n\nCritics of traditional development aid programs, such as New York University economist William Easterly, argue that such programs have not succeeded because those implementing the programs do not have the information necessary to make effective use of resources.9 For example, a World Bank report describes an irrigation project that was being designed by technical staff for an area of Nepal that was thought to be unirrigated. A delay in the project led to the discovery that, in fact, eighty-five fully functioning farmer-managed irrigation systems existed in the \"unirrigated\" area. Further, another irrigation program actually reduced productivity because it undermined pre-existing arrangements among farmers.10 Quite obviously, those planning these projects needed local input to make better use of the project resources.\n\nEasterly advocates a more decentralized, grass-roots approach that involves local groups and emphasizes feedback and accountability. Illustrative of this point, a World Bank study of rural water supply projects found that, of those projects with a high level of participation by local beneficiaries, more than two-thirds were successful whereas, among those projects with little local beneficiary participation, only 12 percent were successful.11 Both feedback and accountability depend, of course, on accurate measurement of results. In practice, measuring results is easier at the local level, in part because comparisons can be drawn to other localities that have not received aid. Incentives also matter; and smaller, more-tailored projects for which responsibilities are well defined are likely to provide better incentives to the people who carry them out than those that large, diffuse projects will provide. Follow-up is important as well. Easterly criticizes, for instance, situations in which foreign aid has been used to build highly visible projects, such as new roads, without providing resources or incentives to do the less-glamorous work of maintaining them.\n\nThe themes emphasized by Easterly and other analysts of international aid programs are useful, I think, in the context of domestic community development. Although national initiatives have their place, often the most effective programs take place at the level of the individual community, using local information and local participation. Accountability and feedback, facilitated by data development and quantitative analysis as well as by more-qualitative information, are critical for success. Goals should be modest at first; but knowledge is cumulative, and sometimes good results can be replicated at larger scales. Research, both quantitative and qualitative, furthers learning. None of this is easy, particularly since the data have a way of challenging our views about what works and what doesn't. But a great deal is at stake both internationally and domestically and serious empirical analysis has no substitute. The development of more and better data on economically distressed communities, together with sophisticated tools for analyzing those data, is essential for continued progress in community economic development.\n\nFootnotes\n\n1.  Local Initiatives Support Corporation, \"LISC Adds Market Research Initiative to Arsenal of Community Development Tools.\" MetroEdge, Case Studies, \"World's Largest Home Improvement Retailer.\"   Return to text\n\n2.  Social Compact. \"Social Compact Completes DRILLDOWN in One Hundredth Underserved Neighborhood,\" DrillDown aggregate statistics provided by Social Compact.  Return to text\n\n3.  Social Compact, News and Events, \"Image Upgrade for Santa Ana's Core,\" The Orange County Register, February 7, 2006. Social Compact, News and Events, \"The Immigrant Dollar: A Driving Force at Gulfgate,\" The Houston Chronicle, April 9, 2006.   Return to text\n\n4.  Opportunity Finance Network; National Community Capital Association, CARS, the CDFI Assessment and Rating System.  Return to text\n\n5.  NeighborWorks America, Success Measures Data System.  Return to text\n\n6.  United States Department of the Treasury (2005), Community Development Financial Institutions Fund, Impact Data and Reports, \" CDFIs Leverage CDFI Program Awards Nearly $20 to $1!\" (May).   Return to text\n\n7.  Brookings Institution, Urban Markets Initiative.  Return to text\n\n8.  Brookings Institution, National Infrastructure for Community Statistics.  Return to text\n\n9.  William Easterly (2001), The Elusive Quest for Growth: Economists' Adventures and Misadventures in the Tropics, (Cambridge, Mass.: MIT Press).  Return to text\n\n10.  World Bank (1998), \"Assessing Aid--What Works, What Doesn't, and Why,\" Policy Research Reports (November).   Return to text\n\n11.  World Bank (1998), \"Assessing Aid.\"  Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060420a.htm",
        "title": "By the Numbers: Data and Measurement in Community Economic Development",
        "date": "4/20/2006"
    },
    {
        "content": "April 17, 2006\n\nVice Chairman Roger W. Ferguson, Jr.\n\nAt the Conference on Modern Financial Institutions, Financial Markets, and Systemic Risk, Federal Reserve Bank of Atlanta, Atlanta, Georgia\n\nI am very pleased to open this conference on Modern Financial Institutions, Financial Markets and Systemic Risk. Let me begin by thanking the Federal Reserve Bank of Atlanta for hosting this conference and for organizing, along with the International Association of Financial Engineers, an impressive program that is filled with high-quality papers on topics of keen interest to central bankers. Before proceeding, I must indicate that the views I am about to express are my own and do not necessarily reflect the views of other members of the Board of Governors or the Federal Reserve more generally.\n\nFew subjects are more important for central bankers than the efficiency and stability of our financial system. The term \"financial instability\" is often poorly defined. Some argue that financial instability occurs when imperfections or externalities in the financial system are substantial enough to create significant risks for real aggregate economic performance. Others argue that financial stability is potentially absent, or that financial instability is on the horizon, when they perceive that some important set of financial asset prices seem to have diverged sharply from fundamentals. Finally, many observers have used the term \"financial instability\" to describe their perception that market functioning seems to have been significantly distorted or impaired. Regardless of the definitions used for financial instability, they lead us to a strong interest in ensuring that our financial infrastructure is robust and that our supervisory operations are sound and up-to-date.\n\nIronically, our interest in financial stability seem to have increased in recent years even as real (that is, inflation-adjusted) variability in economic aggregates seems to have decreased. Since 1985, the volatility of real growth in gross domestic product (GDP) has been only about half of what it was during the preceding twenty-five years. In addition, as shown in a number of papers, the volatility of many components of GDP and of other measures of aggregate economic activity also declined sharply between these periods.\n\nThe source of the moderation in the real economy is unclear. Changes in data construction do not seem to be responsible. Fiscal policy has not become appreciably more countercyclical, and the shift of the economy toward producing more services appears to have played only a small role. The leading explanations of the moderation are that (1) economic shocks have been milder; (2) inventory management has improved; (3) financial innovations such as improved risk assessment and risk-based pricing have made credit more widely available, even during economic downturns; and (4) monetary policy has been better.\n\nThe first explanation--milder economic shocks--has seemed less persuasive following the events of the late 1990s and early 2000s. From the Asian financial crisis to the September 11 attacks to the corporate governance scandals to the surge in oil prices, powerful economic shocks have marked the past few years. Yet, the economy has performed rather well, on balance, over this period.\n\nAs for the second explanation--better inventory management--changes in inventory dynamics have indeed contributed significantly to the reduced volatility of GDP growth. Those changes are consistent with anecdotal evidence and case studies about the use of information technology and better inventory management practices to catch incipient inventory overhangs before they become a problem.\n\nRegarding the third explanation--better availability of credit--Karen Dynan, Doug Elmendorf and Dan Sichel, of the Board's staff, present evidence in a recent paper that financial innovation has been partly responsible for the reduced variability of real activity of the past two decades or so.1 According to their work, the greater availability and use of credit over time may have reduced economic volatility by reducing the sensitivity of household spending to downturns in income and cash flows and to fluctuations in interest rates, with the result that consumer spending and home purchases have become less sensitive to contemporaneous income.\n\nLet me focus for a moment on the fourth explanation, that monetary policy has been better. I think it has indeed been better. We are better at understanding how the economy operates (and therefore, at evaluating the appropriate stance of monetary policy) and we are more determined to pursue the goal of price stability. But secondarily, I think the greater dominance of market-based finance, combined with a greater transparency by the Federal Reserve, has made both the mechanism of monetary policy and the intentions of the central bank more understandable to market participants.\n\nThe mechanism of monetary policy is clearer with greater market-based finance relative to bank-dominated finance because the direct effects of policy on corporate and household balance sheets are more easily observed by both policymakers and market participants. In contrast, bank-dominated finance involves more complicated interactions between depositor behavior, loan underwriting standards, and interest rates.\n\nThe greater transparency of central banks also seems to have led to improved economic performance. Market expectations are more likely to remain anchored in the face of various shocks when investors can see more clearly that central bankers are committed to long-run objectives such as price stability and sustainable economic growth. This commitment feeds into the planning and execution of investments by firms and households, which are more likely to undertake such investments given greater certainty about the commitment of the central bank. Moreover, with this greater certainty, prices and pricing decisions more clearly communicate the desires of households and firms.\n\nSome evidence for this view is found in the decline of inflation volatility relative to real interest rate volatility. Both inflation volatility and bond term premiums have declined significantly in recent years. Research at the Federal Reserve Board by Don Kim and Jonathan Wright, as well as work by others outside the Federal Reserve, have suggested that inflation expectations that are more firmly anchored, combined with the reduction in the volatility of real activity, seem to be a significant part of the explanation for the decline in term premiums.2 I would argue that the greater transparency of central banks has played a role in communicating and emphasizing to the markets our commitment to price stability.\n\nThus, the moderation in aggregate economic volatility seems somewhat understandable. But why, then, the seemingly greater concern these days about financial market instability? This anxiety appears to be driven by three factors: First, some asset prices, such as housing prices, seem to be high by historical standards. Given the substantial decline of stock prices beginning in 2000, many observers worry that greater boom or bust cycles in some asset prices could be the \"flip-side\" of the moderation of real economic volatility during recent decades.\n\nAsset prices are the key channel through which monetary policy is transmitted to the real economy. Moreover, because asset prices embody the expectations of forward-looking investors, they might contain information of value for the policy-setting process. But from the Federal Reserve's perspective, asset prices must ultimately be seen through the lens of long-term growth and price stability. If inflation seems contained and the prospects for economic growth are good, then it's unclear why the policymaker should set aside these direct signals in preference for signals from asset prices that may or may not be out of line with their historical relationships to fundamentals--the very fundamentals, I should add, that we look at directly in judging the health of the economy. Indeed, even in retrospect, our knowledge of what drove the price-earnings ratios for U.S. equities so high in the late 1990s and our ability to estimate what a more \"appropriate\" level for the price-earnings ratio might have been are very incomplete and, frankly, probably will not improve substantially.\n\nAdditionally, in the current conjuncture, some have expressed a concern that an unwinding of global imbalances, should it occur, might be disorderly and associated with financial instability. Others question whether the simultaneous removal of monetary accommodation by central banks in several major economies could possibly trigger a period of financial instability emanating from the inevitable rebalancing of portfolios. Should events such as these occur, central bank communication and understanding market participants' reactions will certainly be important considerations for maintaining financial stability.\n\nA third source of anxiety concerning financial market instability arises because some of the more recent crises have been financial in nature. Although their effects on the real economy in the United States have been relatively limited, the economies of other nations have been significantly affected, and there is concern that a financial crisis might, at some point, have more severe consequences for the real economy in the United States. When we review these recent cases of financial market turmoil, it appears that each is a unique event. But some common lessons can be learned, and I will outline them after I briefly review two of these crises that have been important in the United States during the past decade.\n\nThe market turmoil in the fall of 1998 was touched off by the Russian debt default in August and then exacerbated by the well-publicized travails of Long-Term Capital Management. During this time, nearly all financial indicators portrayed a dour picture of economic prospects--risk spreads widened sharply, stock prices fell, and banks reported tightening the terms and lending standards on business loans. In addition, market reports indicated that the capital markets were seizing up as dealers and other marketmakers recoiled from risk taking. In response, the Federal Open Market Committee (FOMC) lowered its target for the federal funds rate 75 basis points in three equal steps and maintained the lower rate through June of the subsequent year. This response mainly reflected FOMC concerns that these financial instabilities had either signaled or created significant downside risks to the economic outlook, particularly for business investment. The FOMC's significant aversion to the possible negative outcomes associated with these risks was part of a risk-management perspective--that is, that the economic recovery from a financial shock could be more difficult to manage than the financial shock itself.\n\nAs for events after 1998, it is more difficult to identify a \"pure\" financial crisis. The devastating terrorist attacks in 2001 caused tragic loss of life and major damage to the physical infrastructure of a number of key firms central to trading and market-making activities. Although there were many important differences, this crisis mimicked a financial meltdown in the sense that important financial markets could not operate because of the cessation of activities by some firms.\n\nThe Federal Reserve responded in a manner that was appropriate to the nature of the crisis. We issued a statement that we were up and running and ready, if needed, to extend loans from the discount window. Depository institutions took up the offer; their borrowing surged to more than $45 billion but dropped quickly after a few days. We also worked jointly with foreign central banks to provide funds to promote the smoother operation of foreign exchange transactions and established swap lines that channeled funds to institutions that needed dollars. In addition, the Federal Reserve took a variety of other actions, including waiving daylight overdraft fees, extending the operating hours for Fedwire, and easing the limits on securities lending to reduce the pressure on firms requiring securities that were made scarce because of settlement difficulties. All these measures were taken quickly, maintained temporarily, and wound down in an orderly manner as the need for them receded.\n\nAfter the initial rush of activity, we focused on the nonfinancial economy. Evidence of a weakening economy had already emerged before the terrorist attacks; the decline in stock prices, the widening of risk spreads, and the impairment of market functioning created by the attacks caused many policymakers to worry that this weakening would accelerate. Again reflecting the risk-management perspective I described earlier, the FOMC lowered its target for the federal funds rate 50 basis points before the reopening of the markets on Monday, September 17, 2001. In explaining its action, the FOMC pointed to a less sanguine economic outlook and to significant downside risks associated with that outlook.\n\nBesides the crisis of 1998 and the September 11 terrorist attacks, other episodes of financial turmoil were important, but these episodes did not raise the same level of concern that the negative shock might be transmitted to the nonfinancial economy in a rapid and disorderly fashion. For example, the significant decline of stock prices starting in 2000 was not accompanied by a major market malfunctioning, and the resulting loss of equity wealth did not seem likely to have negative ramifications for the real economy that were so immediate and severe as to be considered a crisis. Similarly, the major accounting and corporate scandals of 2002 led to a significant widening of risk premiums and much anxiety about the veracity of many corporations' financial statements. But for the most part, the markets again functioned smoothly and risks seemed to be priced normally. Finally, the more pronounced interest rate volatility during the summer of 2003, which appears to have been significantly amplified by mortgage hedging, created some short-lived market difficulties. But again, this volatility seemed unlikely to have significant effects on the real economy.\n\nDespite the rarity of internally generated financial crisis, some argue that ongoing trends in the United States should be examined closely for their potential effects on financial stability. Four trends are often mentioned.\n\nThe first is increased concentration in the financial services industry. In particular, consolidation has resulted in a smaller number of firms doing a larger share of the bank lending throughout the world. For example, the origination and servicing of consumer loans have become more concentrated. For the most part, these rising levels of concentration appear to be motivated by cost savings that are often attributed to economies of scale, or by expectations of greater revenue stability derived from either greater diversification of products or greater geographic diversification. While the risks to financial stability that arise from the creation of a small number of large and complex firms are obvious, there may be benefits as well. Greater concentration in financial services has the potential to have some positive impact on financial stability because lower costs can allow firms to build the capital reserves that help insulate them from shocks, and greater diversification can reduce firm risk. Moreover, the market and financial supervisors are requiring the adoption of more sophisticated and comprehensive techniques for the management of risks associated with larger and more complex firms. However, the benefits of lower costs, greater diversification, and better risk management at large, complex firms depend on many particulars, including robust infrastructure and a reduction in the opaqueness that results from increased firm complexity. In this regard, infrastructure is one area in which the increase in concentration has received attention. The creation of NewBank, which I describe later, is a recent private-sector response to the concentration of clearing and settlement activities in the market for government securities.\n\nThe second trend is that the pricing and management of credit continues to become more market oriented. This development should increase financial stability, because market pricing and the management of credit risk via marketable securities would be expected to promote a more robust system for risk management. In this scenario, a broad-based and diversified group of rational market participants would determine the success or failure of financial products through an evolutionary process, allowing the available set of financial assets to gradually become more useful and comprehensive. However, some hold to a more pessimistic scenario that envisions smaller groups of market participants, with short time horizons and an excessive interest in mark-to-market profitability, who create more volatility because of their high sensitivity to the latest rumors and news. I tend to adopt the more optimistic view, but in any case the central bank will need to maintain its focus on markets as more credit is intermediated through them.\n\nThe third trend is similar to the second. The ongoing increase in the scope and availability of financial instruments is probably providing many firms and households with improved methods of risk diversification and hedging and with greater access to credit. As I noted earlier, such financial innovations have likely been partly responsible for the lowered variability in many real economic aggregates over the past two decades. That said, the increasing complexity of these instruments raises a host of policy questions regarding, to name just a few items, financial education for households, and, for financial institutions, operational procedures, valuation practices, accounting treatments, disclosure policies, and capital provisions. Moreover, these financial innovations often rely on the ready availability of market liquidity, an assumption that likely will not hold during a financial crisis. Therefore, one hopes that all market participants who are involved in these complex instruments have liquidity plans in place.\n\nThe final trend is the ongoing and increasing globalization of markets. Make no mistake; I think such a trend is to be welcomed because it brings about the usual gains from trade. But we must be mindful that borrowers are raising funds in multiple financial centers in multiple currencies across diverse legal and political systems; that investors are taking on greater international exposure; and that arbitrageurs are establishing leveraged positions across currencies and international markets. These actions increase cross-border interdependence and thus in some circumstances might propagate financial problems more quickly and widely.\n\nGiven these trends, what roles should a central bank play with regard to financial stability? I would suggest three. First and foremost, the central bank's role is to maintain a focus on the possible effects of financial instability for its two core objectives, namely price stability and long-run real growth. Any actions to promote financial stability need to be seen through this lens. We must always ask: Do our potential actions credibly mitigate a risk of inflation or a threat to the real economy? Such a standard helps reduce the danger that we might pursue financial stability to the point of changing the behavior of market participants in counterproductive ways, such as increasing moral hazard, which would, in turn, create problems for the real economy. This objective also suggests that the central bank needs to continually monitor financial developments, including those regarding financial accounting and reporting standards, to be able to appropriately assess the effect of these developments on the real economy.\n\nSecondly, I would argue that the examples of the recent past, combined with our understanding of how markets function, suggest that much of the central bank's work lies in bank supervision, including promoting better risk management and the avoidance of operational risks on the part of other financial institutions, and emphasizing the importance of backup and contingency arrangements. That is, the central bank can assist in getting market participants to consider and focus on the management of risk in general and of the risk of low probability, but high cost, outcomes in particular.\n\nAlong these lines, we have encouraged banks to adopt the most modern risk-management techniques, and we have encouraged all financial institutions to ensure the robustness of their systems. We have also strived to bring our capital regulations up-to-date and make them more risk sensitive through the Basel II process and the effort to revise Basel I; both of these efforts are intended to modernize capital regimes as part of our ongoing effort to improve safety and soundness and, ultimately, financial stability. And following our own advice, the Federal Reserve has implemented additional layers of backup and contingency arrangements for our key payment system operations.\n\nMost recently, the Federal Reserve Board endorsed the creation of a dormant bank, referred to as NewBank, which would be available for activation to clear and settle U.S. government securities. Such activation would occur if a credit or legal problem caused the market to lose confidence in an existing clearing bank and no well-qualified bank stepped forward to purchase that bank's clearing business. Similarly, the Federal Reserve System, operating through the Federal Reserve Bank of New York, has met with major dealers to improve the practices of the credit derivatives industry in various ways, including implementing procedures to improve the settlement process for credit default swaps, establishing targets for the reduction of confirmation backlogs, and insisting that dealers obtain the consent of the original counterparty before accepting an assignment of a contract.\n\nThe final role I would suggest for a central bank is to research the implications of longer-term financial trends for the economy more generally. As I mentioned above, the consolidation of financial services, increasing market intermediation of credit, the greater complexity of financial instruments, and increasing globalization of financial institutions and markets all might raise concerns about our financial system. Although my assessment, and that of many other observers, is that these forces and developments support financial stability, they merit ongoing study.\n\nIn a more proactive vein, once the central bank identifies a longer-run concern, it can try to raise the awareness of other policymakers regarding the potential problems. Recently, for example, the government-sponsored enterprises, which lack the normal market discipline to check the growth of their portfolios, have been a concern that the Federal Reserve Board has been highlighting before the Congress. As another example of being proactive, I would suggest that the efforts to increase the transparency of central bank actions that I discussed at the beginning of my talk have, in some part, been motivated by a desire to enhance financial stability.3\n\nOne lesson that stands out from our experience gained during the past decade is that only looking backward is not useful. Prudent central bankers must be forward-looking, searching for developments that might become significant problems under some circumstances. What would be useful from a risk-management perspective is more information along the lines of what we have for inflation--market instruments that allow us to measure, to some extent, market participants' expectations. The absence of such direct measures of financial stability, however, suggests that we should continue to present our views of potential financial risks and their associated propagation mechanisms, both to other public-sector colleagues and to private-sector analysts and observers. Participation in official organizations such as the President's Working Group, the Financial Stability Forum, and the Committee on the Global Financial System, which are little known to the general public but are well regarded by the official community, offers the Federal Reserve such engagement. Moreover, we should develop theoretical and empirical models to help us understand potential risks. That is why conferences such as this one, which bring together researchers, policymakers, and practitioners to discuss issues related to financial stability, are so important.\n\nFootnotes\n\n1.  Karen E. Dynan, Douglas W. Elmendorf, and Daniel E. Sichel (2006), \"Can Financial Innovation Help to Explain the Reduced Volatility of Economic Activity?\" Journal of Monetary Economics, vol. 53, pp. 123-50.  Return to text\n\n2.  Don H. Kim and Jonathan H. Wright (2005), \"An Arbitrage-Free Three-Factor Term Structure Model and the Recent Behavior of Long-Term Yields and Distant-Horizon Forward Rates,\" Finance and Economy Discussion Series 2005-33 (Washington: Board of Governors of the Federal Reserve System, August)  Return to text\n\n3.  Indeed, when the FOMC began issuing its policy decisions more than a decade ago, it did so in order to avoid possible misunderstandings of its intentions and a consequent overreaction in the markets.  Return to text",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/ferguson20060417a.htm",
        "title": "Thoughts on Financial Stability and Central Banking",
        "date": "4/17/2006"
    },
    {
        "content": "April 13, 2006\n\nGovernor Donald L. Kohn\n\nAt the Bankers and Business Leaders Luncheon, Federal Reserve Bank of Kansas City, Oklahoma City Branch, Oklahoma\n\nI am pleased to have this opportunity to talk with the business leaders of Oklahoma City. The Federal Reserve has deep roots in this community: The Federal Reserve Bank of Kansas City has had a Branch here since 1920. Consistent with the experience of most other enterprises these days, the Federal Reserve is having to change the way we do business in response to rapid changes in technology. We can see the effects of these changes on the payment system every day and all around us as we make and receive payments that depend less and less on the use of checks. To hold down costs, we have been required to reduce the number of our offices delivering payments services to banks, including check and cash operations at Oklahoma City.\n\nBut closing those operations by no means implies that we are reducing our commitment to the community. Indeed, Tom Hoenig and his colleagues at the Federal Reserve Bank in Kansas City are taking steps to strengthen our roots here. Our most important ties to local communities have never been primarily through the services we deliver to their banks. Rather they are established through the exchange of information between those communities and the Federal Reserve, and the Oklahoma City Branch will remain our eyes and ears in this region. Tom will continue to bring to our policy deliberations the information he gathers from Tyree Minner and his colleagues on the board of directors of the Oklahoma City Branch, from Bob Funk and his colleagues on the head office board, and from many other contacts in the region. The Federal Reserve Bank of Kansas City and the Oklahoma City Branch are increasing their efforts to reach out to this community to make sure that we are listening as effectively as possible to your impressions of the current economic circumstances, to explain our policies, to help educate the public in the use of the increasingly complex financial instruments available to them, and to work with community leaders to identify strategies that will help sustain a vibrant economy. The Board of Governors in Washington strongly supports these efforts by Kansas City and other Reserve Banks to deepen their ties to their communities, and I am pleased to be here as tangible evidence of that support.\n\nI thought I would take advantage of this opportunity to discuss the U.S. economy and the conduct of monetary policy. We are at an especially interesting juncture, so this is a good time to take stock of where we are and to peer ahead through the prognosticator's always murky lens. I must emphasize that these views are my own and do not necessarily reflect those of my colleagues on the Federal Open Market Committee.1\n\nOn balance, the economy has been performing quite well in recent quarters, with growth strong and underlying inflation stable despite the considerable rise in energy prices over the past few years. I say \"on balance\" because a number of unusual factors have been influencing the pattern of economic activity lately, adding to the normal difficulty of discerning underlying trends from inherently volatile data and other information. These factors include not only last year's devastating hurricanes but also the unusually warm weather in the early part of this year, which very likely provided a fillip to housing construction and consumer spending. Smoothing through these events, however, the economy has been expanding at a pretty good clip by historical standards--probably an annual rate of about 3-1/2 percent since midyear 2005.\n\nSeveral factors have propelled reasonably strong economic growth. A critical element has been supportive financial conditions. Despite the tightening of monetary policy that began in mid-2004, short-term interest rates were at fairly low levels until recently, and the effects of those low rates have continued to spur household and business spending. In addition, although longer-term yields have moved up notably in recent weeks, they too have been low by historical standards. Moreover, credit to businesses and households has remained readily available at narrow spreads in markets and on favorable terms and conditions at banks, reflecting, in part, robust earnings and strong balance sheets at businesses along with good performance on outstanding household and business loans.\n\nThe pace of economic growth in the United States has also been supported by a firming of activity abroad and the associated increases in demand for our exports. Among industrial countries, expansion of output has become more firmly established in both Japan and the euro area, while growth continues at a solid pace in the United Kingdom and Canada. Furthermore, economic growth in most of the emerging economies of Asia has been robust. U.S. exports were disrupted by last summer's hurricanes in the Gulf, but smoothing through this volatility, exports appear to have been contributing substantially to the growth of gross domestic product (GDP).\n\nOur economy has been able to register this good performance despite rising energy prices. This audience knows well that since late 2003 the price of West Texas intermediate crude oil, for delivery at Cushing, has soared from about $30 per barrel to nearly $70 recently. Nonetheless, the rise in energy prices has apparently had only a limited negative effect on the national economy. Energy costs are not nearly as important today as they once were. Since the 1970s, the energy intensity of production in the United States has fallen dramatically; indeed, on an inflation-adjusted basis, it takes roughly half as many Btu of energy to produce a dollar of GDP today than it did at the time of the 1973 oil crisis. This sharply reduced share of energy costs in total business expenses has likely limited the influence of these costs on profits, on overall consumer prices and real income, and on the economy more generally. A rough estimate puts the reduction in real GDP growth from the increases in energy prices since late 2003 at between 1/2 percent and 1 percent per year. Of course, reactions to higher energy prices are hard to predict, but the measured response of activity over the past couple of years suggests that the most recent price increases will have, at most, only a small effect on economic growth during this year.\n\nThe good performance of the economy has led to a further narrowing in the margin of unutilized resources. For example, the Federal Reserve's measure of capacity utilization in manufacturing has moved up 2 percentage points during the past nine months, to around 80-1/2 percent, a touch above its longer-run average. Similarly, the unemployment rate has fallen almost 1/2 percentage point from early 2005 to around 4-3/4 percent last month.\n\nNevertheless, the underlying rate of inflation has remained moderate. Headline inflation, of course, has been boosted by the jump in energy prices. However, the run-up in the prices of energy and other commodities appears to have had only a modest effect on prices for non-energy goods and services. The inflation rate for prices of consumer goods and services excluding food and energy, as measured by the core personal consumption expenditures (PCE) price index, has been running a bit under 2 percent, only about 1/2 percentage point above its rate two years ago before the spurt in energy prices began.\n\nInflation has been restrained, in part, by the margin of slack in labor and product markets that has persisted through much of this period. Another reason for this favorable outcome is that longer-term inflation expectations remain well contained. For example, the median expected inflation rate during the next five to ten years, as reported in the University of Michigan's survey of consumers, has barely edged up in recent years, even as short-term inflation has been boosted by rising energy prices. Meanwhile, inflation compensation for investors implied by the spreads between the rates on nominal and CPI-indexed Treasury notes at both five- and ten-year maturities also has not shown any tendency to move higher on balance.\n\nIn recent years, declining prices of imports and the threat of import competition have also probably held down costs and prices to a degree. Moreover, increases in compensation costs have generally been modest. To be sure, average hourly earnings of production and nonsupervisory workers have been accelerating in recent quarters. However, the broadest measures of compensation have not picked up, suggesting that competition in labor markets has been intense. Nonetheless, with labor markets tightening, some pickup in compensation increases for the broad measures would not be surprising. Nor would a pickup necessarily be inflationary, given the very good growth in labor productivity that we have experienced in recent years.\n\nDespite the relatively moderate increases in prices and costs that we have observed lately, the capacity utilization rate and the unemployment rate have recently reached zones that on occasion in the past have been associated with the beginnings of upward pressure on inflation. Of course, the past is not always a good guide to the future, in part because a great deal of uncertainty surrounds the relationship of resource utilization and inflation. For instance, we cannot directly observe full capacity of either labor or production resources; consequently, we can never be certain what level of activity represents the full utilization of capacity.\n\nIn addition, measurement issues aside, the empirical evidence of the past half-century suggests that the relationship between utilization and inflation can shift over time. Unfortunately, we typically are only imperfectly aware of the changes and their magnitudes in real time. In the 1990s, that relationship was affected by, among other things, changing trends in the growth rate of productivity and innovations in the structure of labor markets, such as increased use of temporary help supply.\n\nThese uncertainties mean that we, as policymakers, need to keep not only an open mind about estimates of the economy's potential but also a close eye on the various indicators of costs and prices so that we can recognize incipient price developments and react to them as early as possible. But we also must recognize that, by the time evidence of accelerating prices becomes definitive, containing inflation pressures could entail disruptive economic adjustments. So despite the uncertainties, we must evaluate all the evidence and make our best judgments about the oncoming risks to sustained good economic performance. In the current circumstances, as the Federal Open Market Committee has said, the economic climate appears to be one in which further increases in resource utilization, in combination with the elevated prices of energy and other commodities, have the potential to add to inflation pressures.\n\nThe available evidence suggests that the pace of economic expansion may moderate a little from its average over recent quarters, keeping resource utilization in line with recent levels. Maintaining economic growth around this pace will likely reflect a balancing of opposing forces. The rise in interest rates we have experienced will tend to restrain demand, offsetting the effects of sustained economic expansion in our trading partners and the reduced drag on U.S. growth from oil prices, assuming that those prices roughly flatten out as participants in futures markets seem to expect.\n\nIf the past is any guide, the effect of rising interest rates is likely to be felt most visibly in housing markets. The rate for a thirty-year, fixed-rate mortgage is up 70 basis points from its level in the middle of last year, and one-year adjustable-rate mortgages have risen more than 100 basis points over the same period. In addition, house prices have increased considerably relative to rents, incomes, and returns on alternative assets. Already there have been signs that housing demand has begun to moderate. Sales of both new and existing homes are down substantially from their levels last summer, and information on mortgage applications and pending home sales point to further softening in the next few months. With demand slowing, house prices also seem likely to decelerate. Indeed, we are beginning to see hints of moderation in some of the data on housing prices.\n\nAs a consequence, spending for new housing construction, after contributing nearly 1/2 percentage point to overall GDP growth last year, may not increase much this year. Moreover, the slowdown in house price increases could well hold back growth in consumption spending on a wide variety of goods and services. The rapid run-up in prices over the past few years and hence in household wealth, perhaps combined with the increasing ease of tapping that wealth, probably has been a major reason that households have been saving so little of their current flow of income. As house-price appreciation slows, the personal saving rate likely should begin a gradual ascent.\n\nTo be candid, however, the behavior of the housing market and the response of spending are among the great uncertainties about the economic outlook. I have sketched a benign scenario of gradual adjustment that lines up very nicely with the Federal Reserve's assessment that overall growth should slow to a sustainable pace. But our ability to predict asset prices is very limited, especially when the trajectory of those prices is shifting, as that of house prices appears to be doing right now. Moreover, we have particular difficulty in assessing how consumers will respond to changes in their perceptions of future capital gains and actual home prices. The housing market and its effects on spending will be among the areas that Tom and I and our colleagues on the FOMC will be monitoring most closely as we try to discern the emerging pattern of economic activity and inflation.\n\nAt this time, even with housing markets cooling, the fundamentals remain favorable for solid gains over the coming months and quarters in both consumer spending and business investment. In part, that assessment reflects the sizable increases in employment that we have been seeing over the past year or so. Just last week, the Labor Department reported that payroll employment rose 211,000 in March. If the growth of aggregate demand moderates as we expect, increases in employment should also slow but still be sufficient to absorb new entrants into the labor market. Moreover, the gains in wage and salary income associated with those employment increases should provide ongoing support to household spending. The purchasing power of those gains will go further than it has in recent years if, as anticipated in futures markets, energy prices level out.\n\nMeanwhile, in the business sector, order books for nondefense capital goods are full, sales prospects appear good, profits have been strong, balance sheets are in healthy shape, and companies are flush with cash. As the growth of consumption eases back a little, so too should the increase in capital spending as firms come to anticipate slower growth in sales. But judging from rising global commodity prices and equity valuations abroad, foreign demand looks to be increasing, and rising exports should offset some of the scaling back of domestic sales prospects. In addition, technological advances will continue to boost demand for capital equipment by reducing its costs and increasing its usefulness in improving efficiency.\n\nIf, as I anticipate, economic growth moderates a touch and pressures on labor and product markets do not intensify substantially further, I believe that underlying inflation should remain roughly stable. That sanguine picture is reinforced if crude oil prices do, in fact, turn out to be relatively flat over the remainder of this year. Such a flattening of oil prices would reduce headline inflation directly and would diminish the threat of higher energy prices becoming more deeply embedded in the inflation process by raising inflation expectations. And, to date, inflation expectations have been well anchored. As I have already mentioned, it would not be surprising to see some pickup in hourly compensation, but such an acceleration may not add to price pressures. In today's competitive environment, and with profits generally robust, some of the cost increases might well be absorbed in margins. Moreover, further productivity gains should act to damp the effects on overall unit costs.\n\nLike all forecasts, this expectation of stable inflation is only the middle of a wide range of possible outcomes. For example, a further spurt in energy and commodity prices could be passed through into core inflation to a greater extent than seems to have been occurring recently; the threat of that outcome probably is especially great when the economy is already operating at a high level. If the economy does not moderate somewhat, pressures on resources will increase, further raising the odds of higher inflation. And as I discussed earlier, historical patterns suggest that resource utilization is already in a zone that at times has been associated with the emergence of inflation pressures. But the risks are not all one-sided. Price and compensation inflation have, in fact, remained moderate at high levels of resource utilization, despite rapid increases in energy and commodity prices, suggesting that some forces not yet fully identified may be helping to keep them contained. Those forces might include robust underlying productivity growth here at home or competition from abroad. It is also possible that the economy could cool more than expected if housing markets weaken quite substantially, consumption is cut back significantly, and as a consequence businesses pare capital spending plans.\n\nMy job as a policymaker is to work with my colleagues to identify the path of short-term interest rates that has the best chance of realizing that favorable central-tendency forecast of solid growth and continued low inflation. I do not know how much policy firming will be needed to accomplish this objective.\n\nMy forecast is that the economy is in transition to a sustainable pace of growth, in which case policy likely will be in transition as well. At this juncture, given the apparent strength in demand and the narrowing margin of unused resources, I am focused on making sure that inflation and inflation expectations remain well anchored. A tendency for inflation to move higher would put economic stability and the long-term performance of the economy at risk. Accordingly, for me, the critical indicators in the time ahead will be the ones that signal whether growth is indeed likely to proceed at a sustainable pace and whether inflation remains on a favorable track. This is a judgment my colleagues and I will need to make meeting by meeting as the incoming information--both the data and, critically, the timely feel for developments that we get from the Reserve Banks' contacts in the community--help us assess the paths for the economy and price pressures.\n\nFootnotes\n\n1.  Wendy Dunn and Lawrence Slifman, of the Board's staff, provided valuable assistance in the preparation of these remarks. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20060413a.htm",
        "title": "Economic outlook",
        "date": "4/13/2006"
    },
    {
        "content": "April 13, 2006\n\nGovernor Mark W. Olson\n\nAt the University of Arkansas at Little Rock, Little Rock, Arkansas\n\nRecent trends in the U.S. economy have been, on the whole, quite favorable. Real gross domestic product (GDP) increased 3-1/4 percent during 2005, on the heels of two years during which it rose at an annual rate of almost 4 percent. By the end of last year, businesses had added another 2 million jobs to their payrolls, and the unemployment rate had dropped to its lowest level in five years. Although top-line consumer price inflation was boosted by the run-up in energy prices, core consumer price inflation remained relatively steady, at about 2 percent for the year.\n\nOne of the significant challenges for a policymaker is to be able to discern the underlying trends in real activity and inflation in the midst of the inevitable noise and shocks that come along. The recent period has presented a number of those types of unanticipated events: Most notably, the devastating hurricanes that hit the Gulf Coast last fall destroyed residential and business capital and tragically uprooted the lives of many people. The resulting disruptions to economic activity, particularly in the energy and petrochemical sectors, held down the rise in real GDP in the fourth quarter. The restarting and rebuilding that began late last year should be providing a boost to activity this year, but the size of that effect and the extent to which it will be distributed in coming quarters remain uncertain. Another special factor at work early this year was the atypically warm weather that very likely advanced homebuilding and perhaps other spending that might otherwise have stretched into the spring. As a result, most forecasters believe that the incoming information has been adding up to a sizable, but quite likely temporary, acceleration in real GDP in the first quarter after the small increase in the final quarter of 2005.\n\nAnother, more persistent economic challenge in recent years has been the climb in world oil prices. As a result, last year marked a third consecutive year of rapidly rising--and sometimes volatile--domestic energy costs. To be sure, higher energy prices cut into households' purchasing power and the profits of non-energy firms. While recognizing the difficulty of identifying the reactions of consumers and businesses to higher energy prices, Chairman Bernanke has recently noted that the surge in energy prices since late 2003 probably reduced the growth of real GDP between 1/2 percent and 1 percent per year. One important change in the economic landscape that has limited the current effect of an energy shock compared with the effects in our earlier experience is that energy costs today are not nearly as important as they were during the 1970s, when the energy intensity of U.S. production was significantly higher.\n\nAfter taking into consideration the unusual factors that have been influencing the pattern of economic activity of late, most forecasters are projecting that, after a sizable increase in the first quarter, production and spending will return to a moderate and sustainable pace over the remainder of the year. Several fundamental factors support that view. First, although longer-term yields have begun to move up recently, they remain low by historical standards, and financial conditions remain favorable. Second, business balance sheets are strong, and corporate earnings have been robust. Third, household credit quality has shown few signs of stress. Finally, households' real income has been receiving a boost from the recent strong gains in employment, and the ongoing increases in house prices and gains in the stock market have kept the ratio of wealth to income at a high level.\n\nAll told, prospects for ongoing gains in consumer spending and business investment appear good, and the recent indicators are positive. This morning's report on retail sales, together with the information on motor vehicle sales released last week, points to a solid rise in real consumer spending in March and a very strong reading for the first quarter as a whole. The outlook for moderate increases in personal consumption expenditures in the near term is consistent with the latest readings on consumer sentiment. In addition, the continued low level of new claims for unemployment insurance bodes well for further net gains in employment and, thus, income. A survey of businesses' capital spending plans conducted by the National Association of Business Economists at the end of last year pointed to another solid year of spending on plant and equipment. And, through February, backlogs of orders for nondefense capital goods were still moving up.\n\nAnother important factor in the outlook for the United States is the likelihood of a continued solid expansion in economic activity abroad. Notably, the Japanese economy has strengthened, and economic activity in emerging Asian economies has continued to rise at a brisk pace, while the outlook in Canada calls for sustained moderate growth.\n\nDomestic economic activity should also continue to receive a boost in coming quarters from reconstruction in the Gulf region. In part, the reconstruction represents some temporary federal fiscal stimulus from the hurricane relief that the Congress has provided. However, despite the additional support to construction from hurricane-related repair and rebuilding, residential construction overall appears likely to cool a bit this year. The most recent indicators suggest that demand and home-price inflation have begun to moderate.\n\nWith respect to inflation, although forecasting energy prices is risky, I should note that the futures market suggests that crude oil prices will move up a bit further in coming months before flattening out at $70 per barrel. If so, the drag on real income and spending from rising energy costs should diminish over time, as should the risk of additional energy cost pressure on underlying, or \"core,\" inflation.\n\nThe general contour of economic activity that most forecasters are expecting, in which they see little change in resource utilization over the year, should be consistent with relatively stable core inflation. As I noted earlier, the unemployment rate is now at a five-year low of 4-3/4 percent. More important, it and other indicators of resource utilization--such as the industrial capacity utilization rate--are now at levels at which, in the past, little or no economic slack remained. At this point, we have seen few signs of upward pressure on labor compensation or core inflation. Although we have experienced run-ups in prices of commodities, such as building supplies, that are more sensitive to changes in supply and demand conditions and in prices of energy-intensive commodities and services, the pass-through to core inflation appears to have been limited. Important in that regard is the fact that longer-run inflation expectations have been well anchored, as is apparent in the stability of the five-to-ten-year inflation expectations of households in the Michigan SRC survey and in the reading implicit in Treasury-inflation-protected securities.\n\nThe observations I've just shared illustrate the evolving economic forces that the Federal Open Market Committee (FOMC) will consider as it makes monetary policy decisions over the remainder of the year. The FOMC has raised the target federal funds rate 25 basis points at each of the past fifteen meetings over roughly two years. During much of that time, we described our actions as \"removing accommodation\" at a \"measured pace.\" At our March meeting, we indicated that some additional tightening may be necessary to keep the risks to the attainment of both sustainable economic growth and price stability roughly in balance. Thus, the future path for the target federal funds rate will depend importantly on how incoming information on economic activity and inflation affect our assessment of these risks.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/olson20060413a.htm",
        "title": "Update on the U.S. economy",
        "date": "4/13/2006"
    },
    {
        "content": "April 10, 2006\n\nGovernor Mark W. Olson\n\nAt the Fiduciary and Investment Risk Management Association’s Twentieth Anniversary Training Conference, Washington, D.C.\n\nThank you for giving me the opportunity to speak to you about one of the broad issues that you will be discussing during the next several days--the nature of risk and risk management. Today, I will offer my perspectives about risk management in general and then focus on enterprise-wide compliance-risk management, an area of risk management that has been receiving a good deal of attention lately. Risk, to state the obvious, is inherent in all activities in your industry. With the benefit of hindsight, the financial services regulators and the industry have been trying to distill the lessons learned from recent internal control breakdowns in the financial services sector. My remarks today will delve into some of the implications those breakdowns have for compliance-risk management.\n\nOver the past two decades, we have seen remarkable changes in the global financial system. Among these changes are (1) the financial sector's increasing reliance on risk-transfer strategies and (2) the dizzying array of new products being offered to customers. Investment options continue to proliferate, as new investment products are developed and alternative investments such as hedge funds, which now total $1 trillion by some estimates, continue to grow.\n\nRisk Management in General\nOne of the biggest risks facing businesses and governments today is the risk of not preparing for how the world will change over the next five years. Note that I said \"preparing\" for change and not \"predicting\" change. Predicting change in a specific way is highly speculative, but planning for the inevitability of change is prudent management. A key question to ask is whether your organizations have the tools and risk-management processes that will allow them to cope with inevitable changes. As fiduciary and investment risk managers, you pay considerable attention to measuring and monitoring the risks your institutions face, and to managing and controlling those risks. You probably also devote considerable time and resources to keeping abreast of the latest advances in finance and risk management. And if you are like most risk managers, you are finding it increasingly difficult to keep up with your colleagues and competitors, because our world--especially those aspects of our lives influenced by science and technology--is changing at an exponential rate. In this regard, one of the biggest risks faced by risk managers is not being sufficiently prepared for the future. As Yogi Berra once said, \"The future ain't what it used to be.\" Yet risk managers face the challenge of ensuring their organizations are sufficiently prepared for whatever events the future holds.\n\nIt's true that we cannot predict the future in a specific way. But if the past is any guide, we can comfortably expect that new technologies will enable us to overcome problems and challenges that today appear to be insurmountable. At the same time, however, innovation will bring new problems and challenges. As risk managers, the key question for us to answer is \"How can we best cope with the dramatic changes and challenges that will inevitably occur?\" Preparation is the easy answer, but foresight and discipline are also needed. At a fundamental level, managers need to make sure that their organizations have in place the risk-management policies, processes, and technologies to properly measure, monitor, and control their risk exposures. Beyond the basics, the next imperative is to do some hard thinking about the future. More specifically, managers need to conduct scenario analysis and scenario planning--and this is one area of risk management that is sometimes overlooked or not taken seriously. Scenario analysis and scenario planning are difficult; they cannot be done on the fly.\n\nFor many financial organizations, scenario analysis starts and ends with quantitative exercises that measure exposures to specific risk factors, such as changes in interest rates or credit spreads. To realize the full benefits of scenario analysis, however, the analysis must move beyond quantitative exercises. A comprehensive scenario analysis should encompass a thoughtful and thorough analysis of potential, but plausible, major changes in the economic, political, and social landscape. What are these changes? How will they affect your business? How will you respond to them if they occur? This analysis is the primary responsibility of risk management.\n\nBut scenario analysis is not the end goal. Something must be done with the results of the analysis. Management must ask itself whether the organization needs to restructure its balance sheet or modify its current risk-management strategy in order to be prepared for scenarios that may unfold in the near future. This strategizing is at the core of effective scenario planning. Some key scenarios you should consider as part of your scenario planning include how forces such as continued globalization, technological advances, and competition from product innovation will affect your particular line of business. For example, globalization makes group management and risk aggregation more challenging, and the increasing power of information technology creates both new opportunities and new risks. Furthermore, new products and new distribution channels are likely to affect your business and risk profile.\n\nThe fiduciary and investment world has not been immune to problems caused by ineffective controls. Well-publicized accounts of late trading and market timing at mutual fund firms, and the related investigations, have touched many businesses including banking, securities, and insurance firms. These types of compliance failures result in sanctions or financial loss and adversely affect the reputation and franchise value of a firm. As a result, we are seeing an increasing focus on enterprise-wide compliance-risk management systems.\n\nThe business lines and activities of financial services firms can cross many legal entities, making it more difficult for a supervisory agency acting alone to determine what went wrong and what processes may need to be improved. In some of the mutual fund cases, the Federal Reserve, as the consolidated supervisor of the relevant banking organization, worked closely with the primary bank regulator and the Securities and Exchange Commission (SEC) to investigate control breakdowns. The agencies exchanged information in order to compare their findings. Findings from SEC examinations of the broker-dealers, investment advisers, and mutual fund distributors were analyzed together with findings from targeted joint reviews of the bank holding companies and the banks conducted by the Federal Reserve and the primary bank regulators. The banking agency reviews were focused on corporate-wide compliance practices and bank lending practices.\n\nMy observation is that the significant deficiencies in mutual fund practices resulted from a combination of factors and a breakdown in controls. First, mutual fund activities were not being effectively overseen by their mutual fund boards. Second, there were strong financial incentives at certain firms to increase the profitability of mutual fund activities, but the legal and reputational risks of these incentives were not appropriately addressed. Third, a lack of adequate employee training resulted in employees deviating from standard procedures, in order to accommodate certain large customers. For example, in some cases, employees granted key customers a routine waiver of redemption fees, which allowed them to engage in market timing at the expense of other fund shareholders. A stronger corporate compliance program would have enabled a local compliance officer to identify these problems and bring them to the attention of higher-level managers within the corporation.\n\nBased on our experience in investigating control breaches in these mutual fund cases, I would like to highlight a few lessons learned. One of the most obvious to me is the need to critically evaluate unusual client relationships that require variances from standard procedures. There is an additional red flag if a high percentage of compensation is derived from a single client. Additionally, organizations should have a formal process for reviewing and approving unique products, customers, and services at the inception of the client relationship. And, finally, it is always a good idea to shine some light on areas historically labeled \"low risk\" to validate that assessment. The low occurrence of loss from an activity should not be the only factor considered when assessing risk.\n\nCompliance-Risk Management Frameworks\nAs these lessons suggest, compliance risk can arise when the organization fails to comply with laws, regulations, or standards of conduct. Fortunately, various models are available to help organizations manage compliance risk more effectively. A number of banking institutions have installed, or are in the process of enhancing, comprehensive, corporate-level compliance-risk management functions. Many organizations are also bolstering their compliance-risk management programs by upgrading their compliance management information systems, which allows for more-integrated and transparent analysis and monitoring. An enterprise-wide approach to compliance-risk management has proven valuable in areas such as Bank Secrecy Act (BSA) and anti-money-laundering (AML) compliance, information security, privacy, transactions with affiliates, and conflicts of interest.\n\nA common trend for both large and small organizations is the transition away from task-oriented compliance programs to process-oriented compliance programs. Process-oriented programs require compliance to be tested and validated on an ongoing basis. In addition, fragmented and duplicative compliance activities are being scrapped for those that enable an understanding of compliance across the organization. This is not to say, however, that local compliance activities in business units are obsolete but rather they should be part of an integrated, global program. This promotes consistency in expectations, documentation, assessments, and reporting.\n\nAn effective enterprise-wide compliance-risk management program is flexible to respond to change, and it is tailored to an organization's corporate strategies, business activities, and external environment. In addition, an effective enterprise-wide compliance-risk management program requires strong board and senior management oversight.\n\nThe board of directors and senior management have different but complementary roles in ensuring the success of an organization's enterprise-wide compliance-risk management program. The board of directors is responsible for establishing a strong compliance culture that makes compliance an integral part of day-to-day operations; the board then entrusts this responsibility to managers and staff at all levels of the organization. A strong compliance culture encourages employees to raise concerns about compliance risks and the need for additional or improved controls.\n\nConclusion\nOur review of industry best practices and an analysis of the experience of other industries suggests that organizations need to supplement their enterprise-wide compliance-risk management systems with strategic and dynamic thinking. To prepare for what may be ahead, organizations should draw not only on past experience but also employ quantitative and qualitative scenario analysis and planning. To quote former Chairman Greenspan, \"The advent of sophisticated risk models has not made people with gray hair, or none, wholly obsolete.\"",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/olson20060410a.htm",
        "title": "Enterprise-Wide Compliance-Risk Management",
        "date": "4/10/2006"
    },
    {
        "content": "April 10, 2006\n\nGovernor Susan Schmidt Bies\n\nAt the America’s Community Bankers Risk Management and Finance Forum, Naples, Florida\n\nThank you for the invitation to speak today. I always find these forums very informative and useful, in part because they keep me apprised of the issues currently on the minds of bankers. And I hope you agree that these conferences are beneficial to the broader dialogue between bankers and regulators. I plan to focus on several issues that I believe are of interest to this group, including commercial real estate, nontraditional residential mortgages, and proposed changes to regulatory capital rules. With these initiatives, supervisors are trying to emphasize our broader goal of improving risk management at banks, while not dictating in which businesses banks should engage or to what extent.\n\nIf you look at their balance sheets and income statements, you will see that community banks are thriving. As Federal Reserve Chairman Bernanke noted recently, capital, earnings, and asset quality are improving for banks of all sizes, but particularly for community banks. Nonperforming assets, net charge-offs, and loan-loss provisions for community banks have been at low levels in recent years. And community banks remain profitable. The continuing strength of this part of the financial sector is also visible in supervisory ratings, with the number of problem community banks at historical lows.\n\nBut of course all of you know that there are still potential risks on the horizon, and supervisors are paid to make sure that bankers manage those risks properly to maintain the safety and soundness of the banking system. So while the banking industry is doing well of late, we believe there are a few recent regulatory proposals to which bankers--including community bankers--should pay attention.\n\nCommercial Real Estate\nIt is perhaps an understatement to say that those gathered here today are aware of the proposed commercial real estate (CRE) guidance recently issued by the U.S. banking agencies. Indeed, we have received hundreds of comment letters on the proposed guidance so far: the comment period ends April 13. I will be honest and say that I have not had a chance to read all of the comments. Naturally, we welcome these comments, since we were looking for extensive feedback when we issued the proposed guidance in the first place.\n\nAt this point, I would like to underscore that the proposed guidance is intended to encompass \"true\" CRE loans. It is not focused on commercial loans for which a bank looks to a borrower's cashflow as the source of repayment and accepts real estate collateral as a secondary source of repayment. That is, the proposed guidance addresses bank lending on commercial real estate projects where repayment is dependent on third party rental income, or the sale, refinancing or permanent financing of the property. These are \"true\" commercial real estate loans for which repayment depends upon the condition and performance of the real estate market.\n\nI would also like to mention up front that the proposed guidance is not intended to cap or restrict banks' participation in the CRE sector, but rather to remind institutions that proper risk management and appropriate capital are essential elements of a sound CRE lending strategy. In fact, many institutions already have both of these elements in place.\n\nCommercial real estate lending played a central role in the banking problems of the late 1980s and early 1990s and has historically been a highly volatile asset class. Past problems in CRE have generally come at times when the broader market encounters difficulties. Therefore, banks should not be surprised by the emphasis of the proposed CRE guidance on the importance of portfolio risk management and concentrations. One reason why supervisors are proposing CRE guidance at this point is that we are seeing high and rising concentrations of CRE loans relative to capital. For certain groups of banks, such as those with assets between $100 million and $1 billion, average CRE concentrations are about 300 percent of total capital. This compares to a concentration level of about 150 percent in the late 1980s and early 1990s for this same bank group.\n\nWhile banks' underwriting standards are now generally stronger than in the 1980s and 1990s, the agencies are proposing the guidance now to reinforce sound portfolio-management principles that a bank should have in place when pursuing a commercial real estate lending strategy. A bank should both be monitoring the performance on an individual loan basis as well as on a collective basis for loans collateralized by similar property types or in the same markets. When a bank targets commercial real estate lending as a primary business activity, it needs to consider that the risk exposure to the performance of its total CRE loan portfolio--the concentration risk--depends upon broader real estate market conditions. For example, if a bank has several borrowers with similar projects that encounter problems--such as longer absorption periods, higher marketing costs, or higher vacancy rates--weaknesses in the broader real estate market can have a cascading effect on the quality of a bank's CRE portfolio as real estate values erode.\n\nSome institutions' strategic- and capital-planning processes may not adequately acknowledge the risks from their CRE concentrations. This is particularly important, since CRE lending in recent years has occurred under fairly benign credit conditions and, naturally, those conditions are unlikely to continue indefinitely. The ability of banks with significant concentrations to weather difficult market conditions will depend heavily on their risk management processes and the level of capitalization. From a risk-management and capital perspective, institutions generally should focus on the emerging conditions in their real estate markets, the potential cumulative impact on their portfolios if conditions deteriorate, and significantly employ other exercises to help identify CRE vulnerabilities. Of course, such exercises should vary according to the size of the organization and the level of the concentration. All of these are key elements of a sound strategy to manage concentrations.\n\nIn evaluating the impact of their CRE concentrations, bankers should also pay attention to geographic factors. Many banks conduct successful CRE lending within a certain geographic area, but problems can arise when banks begin to lend outside their market or \"footprint\" for which they normally have better market intelligence. In recent years, supervisors have observed banks lending outside their established footprint, to maintain a customer relationship, into real estate markets with which they have less experience. The challenge is heightened when the borrower is also venturing into a new market. These practices led to significant losses in prior CRE credit downturns.\n\nI noted that CRE underwriting appears substantially better compared to the late 1980s and early 1990s. However, we have noticed some recent slippage. Therefore, the proposed CRE guidance underscores the existing interagency guidance on real estate lending standards. That is, it offers some reminders about risk-management practices for individual exposures. In order to attract new business and sustain loan volume, banks may occasionally be inclined to make some compromises and concessions to borrowers. As supervisors, we want to ensure that loan-to-value standards and debt-service-coverage ratios are meeting the organization's policies--and that there is not an undue increase in the exceptions to those standards and ratios. We also continue to monitor whether lenders routinely adjust covenants, lengthen maturities, or reduce collateral requirements. To be clear, we have not yet seen underwriting standards fall to unsatisfactory levels on a broad scale, but we are concerned about some of the downward trend in these standards.\n\nFurthermore, no element of the proposed guidance is intended to act as a \"trigger\" or \"hard limit\" for immediate cutback or reversal of CRE lending; rather, it should be viewed as a reminder to institutions that certain risk management standards are vitally important for banks involved in the business. Additionally, if the agencies finalize the proposed guidance, they intend to use the proposed thresholds in the guidance only as a \"first cut\" or \"screen\" to identify institutions that may have heightened CRE concentration risk. The thresholds are intended to serve as benchmarks to identify cases for further review. In some cases, supervisors, after more careful review, may actually find that given the characteristics of its CRE portfolio, an institution has sound risk management and is holding appropriate capital. In general, the proposed guidance is intended to be applied quite flexibly and in a manner consistent with the size and complexity of each organization.\n\nNontraditional Residential Mortgages\nYou are probably also aware that the U.S. banking agencies recently issued proposed supervisory guidance on nontraditional mortgages, for which the comment period ended March 29. We appreciate the extensive feedback from the industry and others. The agencies are now in the process of reviewing the comment letters and deciding on a way forward. While the main focus of the proposed guidance is on banks' ability to adequately identify, measure, monitor, and control the risk associated with these products, the proposed guidance also addresses consumer protection, which is what I would like to focus on today.\n\nAs outlined in our proposed guidance, nontraditional mortgages contain aspects of complexity, which could make it difficult for consumers to understand all the possible consequences of borrowing. In general, bankers need to be especially alert to developing easily understood disclosures as they introduce more innovative and complex products, and as they market them to broader customer segments rather than the original niche customer. The elements relating to consumer protection in the proposed nontraditional mortgage guidance are included because we think addressing these issues is consistent with good risk management.\n\nNontraditional mortgages typically allow borrowers to defer payment of principal and, sometimes, interest. These products include interest-only mortgages and payment option adjustable-rate mortgages (or ARMs), which have been available for many years. They have recently become increasingly popular, often combined with practices such as reduced documentation of income and assets in evaluating applicants' creditworthiness.\n\nAlthough these products can be beneficial for some borrowers, today these products are being offered to a wider spectrum of consumers. Some of these consumers may not understand the associated risks. For example, there could be a substantial increase in the monthly payment, or \"payment shock\" that could occur when the loan's interest rate increases after a low \"teaser-rate\" period and if the market interest rate index rises through the term of the loan. In addition, consumers may not be aware that payments can increase considerably when a loan begins to fully amortize after an interest-only period. They also may not understand the full impact of negative amortization, including the fact that they may be repaying more than they initially borrowed.\n\nThe Board's Truth in Lending regulations require creditors to provide consumers with disclosures about the loan terms, including a schedule of payments. For interest-only and payment option ARMs, the payment schedule shows consumers how their payments will increase to include amortization of the principal. The proposed interagency guidance includes recommended practices that describe how institutions can use their promotional materials to provide information about the features and risks of these products, especially the risk of payment shock. For example, the guidance recommends that institutions inform consumers about the maximum monthly payment they could be required to pay once interest rate caps and negative-amortization caps have been reached. The proposed guidance also lists recommended practices to address other risks. When negative amortization is possible, the guidance suggests that institutions alert consumers about the consequences of increasing principal balances and decreasing home equity. If both reduced- and full-documentation loan programs are offered, the draft guidance advises institutions to inform consumers if they will pay a pricing premium for the reduced-documentation loan. When institutions provide monthly statements with payment options, they are urged to include information that enables borrowers to make well-informed choices--information that explains each payment option and the impact of each choice.\n\nThe draft guidance also urges institutions to ensure that their advertisements, promotional materials, and oral communications are consistent with the product terms and that these communications provide clear, balanced, and timely information to consumers about the risks. This is important so that consumers have the information they need at critical decision times, such as when selecting a loan product or choosing a specific payment option each month. In general, bankers should ensure that borrowers are aware of the range of outcomes with a nontraditional mortgage.\n\nProposed Revisions to Regulatory Capital Rules\nBy now you have probably heard the news that the Federal Reserve Board reviewed and, in an open Board meeting on March 30, approved a draft of the interagency notice of proposed rulemaking (NPR) on the Basel II capital framework, seeking comment on that draft. The draft NPR and some statements made at the public meeting are now available on the Board's web site. The NPR is expected to be issued in the Federal Register once all of the U.S. banking agencies have completed their respective review and approval processes, at which time it will then be \"officially\" out for comment. That should all happen in the next few months.\n\nWe are very pleased that the substantial time spent on the Basel II effort has culminated in this important achievement. We also recognize the significance of the NPR to the industry, Congress, and others who have waited for more detail on the proposed rulemaking. We look forward to receiving comments on the NPR; they will contribute to our assessment of Basel II objectives and help us determine whether the NPR in fact produces a prudent, risk-sensitive regulatory capital regime. In some areas, the agencies are still grappling with the correct approach. For this reason, the NPR contains a number of requests for feedback on specific topics. All of this will help us as we continue to develop the framework. But before commenting further on the NPR and the U.S. Basel II process, I would like to reiterate our rationale for pursuing Basel II.\n\nBasel II Proposals\nThe current Basel I capital framework, adopted nearly twenty years ago, has served us well. For the vast majority of U.S. banks, Basel I continues to provide a useful measure of regulatory capital, while not being particularly burdensome. However, Basel I has become increasingly inadequate for large, internationally active banks that are offering ever more complex and sophisticated products and services. In other words, the current Basel I capital requirements have over the past decade begun to deviate substantially from the risk profiles of large, complex organizations. We need a better capital framework for these large, internationally active banks, and we believe that Basel II is such a framework. And from a financial stability perspective it is in everyone's interest to have regulatory capital measures at large, complex banks more accurately reflect their actual risks.\n\nOne of the major improvements in Basel II is the closer linking of capital requirements and risk. The current Basel I measures are not very risk-sensitive and do not provide bankers, supervisors, or the marketplace with meaningful measures of risk at large, complex organizations. Under Basel I, it is possible for two banks, with dramatically different risk profiles in their commercial loan portfolios, to have the same regulatory capital requirement. And under Basel I, a bank's capital requirement does not reflect deterioration in asset quality. Moreover, the balance-sheet focus of Basel I does not adequately capture risks of certain off-balance-sheet transactions and fee-based activities--for example, the operational risk embedded in many of the services from which many large U.S. institutions generate a growing portion of their revenues. For large, complex banks, the costs of mismeasuring regulatory capital requirements are much higher and, therefore, there is a greater need to amend the rules for these institutions--this need is the impetus for Basel II.\n\nIn addition to making regulatory capital measures more meaningful, Basel II should make the financial system safer by substantially improving risk management at banks. Basel II builds on the existing risk-management approaches of well-managed banks and creates incentives for banks to move toward leading-edge risk-measurement and risk-management practices. By providing a consistent framework for all banks to use, supervisors will be better able to identify banks whose risk management and risk levels differ significantly from other banks. By communicating these differences to banks, management will be able to benchmark their risk assessments, models, and processes in a more detailed and regular manner. We have already seen some progress in risk management at many institutions in the United States and around the globe as a result of preparations for Basel II. The new framework is also much more consistent with the internal capital measures that institutions use to manage their business.\n\nI hope it is clear from the NPR and other statements made by the agencies that we are committed to ongoing, detailed analysis to ensure that U.S. implementation of Basel II achieves its goal: establishing a strong and risk-sensitive base of minimum regulatory capital. First of all, the U.S. agencies included in the NPR a proposed timetable and set of transition safeguards that are more rigorous than those set forth in the 2004 Basel II framework. For instance, the U.S. agencies are proposing three transition periods, during which an individual institution's Basel II minimum required capital will not be permitted to fall below certain percentages of its capital requirement under the general risk-based capital rules.\n\nAt several points during the transition to Basel II, the agencies intend to conduct thorough analyses of each institution's Basel II capital results and Basel II's impact on aggregate minimum required capital in the U.S. banking system. The U.S. regulators are united in their belief that no bank should be permitted to operate under Basel II until it has proven itself ready to do so. In other words, we plan to have very high standards for Basel II qualification. For instance, a bank will be able to move from the parallel run to the first transitional floor period only after its primary supervisor has given it permission to do so--and only after that supervisor has thoroughly evaluated the bank's risk-management methodologies and its ability to calculate minimum regulatory capital using the new framework. Similarly, a bank will only be allowed to move to each successive floor, as well as to the full Basel II minimum capital calculation without floors, upon a finding by the primary supervisor that it is ready, following a rigorous qualification process.\n\nTo reiterate, during and after the transition to Basel II, supervisors plan to rely on ongoing, detailed analyses to continuously evaluate the results of the new framework and ensure prudent levels of capital. And to be quite clear, the Federal Reserve believes that strong capital is critical to the health of our banking system. We also believe that Basel II will help us continue to ensure that U.S. banks maintain capital levels that serve as an appropriate cushion against unexpected losses.\n\nAs we have mentioned before, we will continue to use existing prudential measures to complement Basel II. For example, the current leverage ratio requirement--a ratio of capital to total assets--will remain unchanged for all banks, whether or not they are subject to the Basel II framework. Also, supervisors will continue to enforce existing prompt-corrective-action rules in response to declines in capital. Both the leverage ratio and prompt-corrective-action rules are fully consistent with Basel II.\n\nProposals to Amend Basel I\nOf course, those of you here today have also been following discussions about possible changes--beyond Basel II--to the existing regulatory capital rules. First of all, we expect only one or two dozen banks to move to Basel II in the near term. The vast majority of U.S. banks would be able to continue operating safely under Basel I as amended through the rulemaking process. The Basel I framework has already been amended more than twenty-five times in response to changes in the banking environment and a better understanding of the risks of individual products and services. The agencies believe that now is another appropriate time to amend the Basel I rules.\n\nThese proposals to amend Basel I, also known as \"Basel IA,\" are still at an early stage. Last fall, the U.S. banking agencies issued an advance notice of proposed rulemaking (ANPR) that outlined suggested Basel I changes. In part, these proposed changes are meant to address concerns about the potential adverse competitive effects of Basel II.\n\nWe take concerns about competitive effects seriously. During the Basel I ANPR process, we sought input from the industry and other interested parties. The Federal Reserve has conducted substantial research about the potential effects of Basel II--published on our web site--and plans to continue those efforts. In an effort to mitigate those concerns, regulators have proposed changes to enhance the risk sensitivity of U.S. Basel I rules; we also remain vigilant about identifying potential competitive distortions that might be created with the introduction of Basel II.\n\nWe are also mindful that amendments to Basel I should not be too complex or too burdensome for the multitude of smaller banks to which the revised rules will apply. That is, in Basel I amendments we are trying to find the right balance between added risk sensitivity and low burden. As you are aware, this is not necessarily the easiest balance to find. For example, there may be a desire to tie regulatory capital measures more closely to risk, but only by expanding the use of factors that determine regulatory risk weights. However, if we expand the number of factors that are used--for example, loan-to-value measures, credit scores, and external credit ratings--that will likely increase the burden for calculating regulatory capital. In short, we are faced with a tradeoff. And some of the comment letters received so far address this very point. Naturally, institutions differ with respect to this tradeoff, with some wanting greater risk sensitivity at the cost of greater burden, and some wanting just a little more risk sensitivity with little or no additional burden. Of course, there are also those that would like much more risk sensitivity with no additional burden, but that just does not seem possible in this case. As we develop the NPR for Basel I amendments, the agencies will have to analyze very carefully where we want to end up on the risk-sensitivity-versus-burden tradeoff. And of course we expect to solicit comments on that tradeoff.\n\nAdditionally, we recognize the need for full transparency about the Basel II proposal and proposed Basel I amendments. For that reason, we intend to have overlapping comment periods for the NPRs of both proposed Basel II and the proposed Basel I amendments, so that banks and others can review and comment on both NPRs at approximately the same time. In particular, bankers from potential opt-in institutions and those not planning to move to Basel II can evaluate the potential impact of Basel II in light of the proposed Basel I amendments. In fact, we want all interested parties to compare, contrast, and comment on the two proposals in overlapping timeframes. At this point, we are still reviewing the comments received on the ANPR for amendments to Basel I. The comment period ended in mid-January. The agencies are developing their proposals for Basel I amendments, based on comments received, and hope to have a Basel I NPR by summer.\n\nFinally, I would like to underscore that the U.S. agencies welcome any and all comments on these documents. Accordingly, our proposals could change on the basis of comments we receive or new information we gather. We know that this can be frustrating to some. Still given the breadth and depth of these proposals, it is critical that we consider all viewpoints. To date, comments from bankers--at all sizes of institutions--have been quite helpful. We request that you continue providing feedback on all the regulatory capital proposals we are contemplating.\n\nConclusion\nI hope you found my remarks about recent regulatory initiatives informative. Naturally, bankers may be somewhat concerned about the impact these initiatives--even proposed guidance--could have on their business. We hear your concerns about \"piling on,\" but I think it is helpful to remember that our job as regulators is to ensure that the United States has a safe and sound banking system. In other words, we are in the business of monitoring \"downside risk\" to the financial system, so we must act appropriately when we see possibly excessive risk-taking or inappropriate risk management, controls, or capital. While most U.S. banks today operate in a safe and sound manner, we must always be vigilant of future problems.\n\nI hope it is clear that we are sensitive to the regulatory burden that bankers face; that is one reason why we often issue guidance first as proposals, requesting comments. Indeed, the Federal Reserve is quite responsive to regulatory burden, as evidenced by the recent Senate testimony of Governor Kohn on the subject. Overall, we believe that banks can remain profitable while still adhering to the standards set by banking regulators. And when we propose changes in our policies or guidance, we continue to listen carefully to industry comments and concerns.\n\nThank you.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20060410a.htm",
        "title": "A Risk-Management Perspective on Recent Regulatory Proposals",
        "date": "4/10/2006"
    },
    {
        "content": "April 06, 2006\n\nGovernor Randall S. Kroszner\n\nAt the Conference on the Future of Financial Regulation, London School of Economics, London\n\nI am delighted that my first speeches as a member of the Board of Governors of the Federal Reserve System are in international settings. Earlier this week I spoke at the European Central Bank, and I am honored today to be able to address this group at the London School of Economics (LSE). The global collection of top academics, key policymakers, and leading private-market participants in these audiences represents what I believe is vital in today's world: a strong spirit of international cooperation and understanding.\n\nI am also pleased about the forward-looking nature of the conference on \"The Future of Financial Regulation\" and would like to express special thanks to two of our hosts at the LSE who have had much first-hand experience at policymaking, Sir Howard Davies, formerly of the U.K.'s Financial Services Authority, and Professor Charles Goodhart, formerly a member of the Bank of England's Monetary Policy Committee. LSE certainly seems to have all of the bases covered!\n\nI will focus my remarks on the U.S. experience with removing restrictions on the geographic expansion of banking organizations because I believe that this experience is timely and important for current debates in the European Union (EU). Over the past year or so, the winds of nationalistic protectionism seem to have strengthened in Europe, especially with regard to cross-border mergers. The desire to impede cross-border integration has been expressed in quite a few EU countries and with respect to a number of different industries, including banking. Just a month ago, in commenting on this position in a speech at the LSE on \"The Development of the European Capital Market,\" EU Commissioner McCreevy said, \"Protectionism denies everyone in Europe the economic benefits of market integration--higher growth, and more jobs.\" And later in that speech, he said, \"We have to get rid of the unfavorable and even disabling environment for conducting cross-border transactions. Some supervisors and governments play fair, regrettably others do not.\"1\n\nWhat I want to do in my remarks today is to focus on the economic consequences of removing cross-border barriers in banking to show that Commissioner McCreevy is correct: After the removal of barriers to the geographic expansion of banks, the United States experienced substantial gains in terms of banking efficiency, employment growth, and economic growth.\n\nBefore I begin, I want to emphasize that much of my speech is based on research I conducted with Philip Strahan, of Boston College, before I joined the Federal Reserve Board.2 The views I express here today are my own and do not necessarily reflect those of my colleagues on the Board.\n\nHistorically, banking in the United States has been subject to extensive government regulation covering the prices banks can charge, the activities they can engage in, the risks they can take, the capital they must hold, and the locations in which they can operate. But during the past thirty years, many restrictions on bank prices and activities in the United States have been lifted, including the restrictions on entry and geographic expansion.\n\nAs barriers to banking expansion across borders have come down, the structure of the industry has changed dramatically. Geographic deregulation has spawned a substantial reduction in the number of banking organizations in the United States with, on average, virtually no change in concentration at the local level. In addition, the average number of distinct banking organizations operating in local markets--both urban and rural--is little changed. The total number of banking offices in the United States has risen steadily since the mid-1990s. Moreover, consolidation has produced important benefits for the banking industry, as many banks have become more diversified, less risky, and more efficient.\n\nA growing body of research provides evidence that geographic deregulation has provided substantial benefits to the broader economy as well (Levine, 2004). In the United States, economic performance improved after geographic deregulation, as evidenced by increases in the rate of state-level employment growth and the rate of new business formation. Geographic deregulation may also be associated with improvements in state-level economic stability. Volatility in the growth rates of employment declined, and the link between the health of local banks and growth in the local economy weakened.\n\nIn my judgment, the U.S. experience with geographic deregulation provides some valuable lessons for the European Union, so let me turn to that experience now.\n\nOrigins and Deregulation of Geographic Restrictions in the United States\nRestrictions on the geographic expansion of banks have a long history in the United States. Because the U.S. Constitution prevents states from issuing paper money and from taxing interstate commerce, the states used their power to grant bank charters to generate a substantial part of state revenues (Sylla, Legler, and Wallis, 1987). A state received no charter fees from banks incorporated in other states, so states prohibited out-of-state banks from operating in their territories--hence the prohibition on interstate banking originated in the states themselves and as a fiscal strategy rather than as a matter of optimal policy for banking and consumers. In addition, states restricted the ability of banks to expand geographically within their borders, effectively creating a series of local monopolies from which state governments could extract at least part of the rents. Some state legislatures even passed \"unit banking\" laws, which prevented a bank from having any branches at all.\n\nIn 1927, the Congress passed the McFadden Act, which clarified the authority of the states over national banks' branching within their borders. Although some branching restrictions were removed in the 1930s, most states continued to enforce restrictions into the 1970s. Between 1970 and 1994, however, most states relaxed their restrictions on branching, typically in a two-step process. First, states permitted multibank holding companies to convert subsidiary banks (existing or acquired) into branches. The holding companies could then expand geographically by acquiring banks and converting them into branches of one of their existing bank subsidiaries. Second, states began permitting de novo branching, whereby banks could open new branches anywhere within state borders. Figure 1 describes the timing of intrastate branching deregulation across the states.\n\nIn addition to limiting branching within a state, states prohibited cross-state ownership of banks until the 1980s. Although some banks attempted to circumvent the McFadden Act by building multibank holding companies with operations in many states, the Congress stopped them with the 1956 amendments to the Banking Holding Company Act. Starting in 1978, states began to pass \"reciprocity\" laws in which a state would allow entry by bank holding companies from other states if, in return, bank holding companies from the state were permitted to enter the other states. By 1992, all states but Hawaii had passed such laws. The transition to full interstate banking was completed with passage of the Riegle-Neal Interstate Banking and Branching Efficiency Act of 1994, which effectively permitted banks and holding companies to enter any state (Kroszner and Strahan, 1999).\n\nConsequences of Deregulation of Geographic Restrictions\nDeregulation of restrictions on geographic expansion within the United States has led to a more consolidated, but not a less competitive, banking system--one that is increasingly characterized by better diversified and more-efficient banking organizations that operate across wider geographic areas. Moreover, this system has also has had positive effects on overall economic and employment growth. I believe that better cross-border integration of the European banking system would produce similar benefits in Europe.\n\nStructural Changes\nLet me begin by describing the effect of geographic deregulation on the structure of the banking industry itself. Figure 2 shows that the number of bank and thrift organizations--that is, holding companies and independent institutions--has fallen by more than half since the early 1980s, when states began to dismantle restrictions on geographic expansion. Much of this decline in the number of bank and thrift organizations is due to mergers and acquisitions, although some is also due to failures.3\n\nAlthough consolidation is often a mechanism for eliminating excess capacity from an industry, such does not appear to be the case for the U.S. banking industry. Again as figure 2 illustrates, the number of bank and thrift offices was about the same in 1993 as it had been in 1984, and the number has increased steadily since 1993. Moreover, the rate of de novo bank formation resulting from new charters has been high, on average, since the early 1980s, another indication that the decline in the number of institutions and organizations does not reflect the removal of excess capacity.4\n\nAfter passage of the Riegle-Neal act in 1994, the U.S. banking industry was transformed from a balkanized system, in which institutions operated locally or within a state, to a system that is increasingly integrated nationally. As shown in figure 3, the number of multistate organizations, defined as those with offices in more than one state, more than doubled from 1990 to 2005. Moreover, as figure 3 also illustrates, between 1990 and 2005 the share of offices located in a state other than the one in which the parent organization was headquartered rose substantially; currently, more than 37,000 such bank and thrift offices are in the United States. Clearly, a lot of customers are being served by banking facilities that are located outside the state in which the banking organization's main office is located.5\n\nIt is important to consider whether such dramatic structural changes had an effect on competition, particularly at the local level. Although concentration increased at the national level, a comparable increase in concentration at the local level has not taken place.6 Figure 4 illustrates the local market effects: For both urban and rural markets, the average Herfindahl-Hirschman index (HHI) based on bank and thrift deposits, a commonly used measure of concentration in local areas, remained basically unchanged from 1990 to 2005.7 Patterns in the average number of bank and thrift organizations operating in local markets tell a similar story, as shown in figure 5. The average number of alternatives available to the many banking customers that rely on local institutions has remained remarkably constant in rural markets over the past fifteen years and has been quite stable in urban markets since the mid-1990s. The many customers--primarily households and small businesses--that rely on local banks and thrifts for financial products and services have not been harmed by structural changes in the industry.\n\nThe consolidation of the banking system has involved mergers across local areas as well as within a single area (Pilloff, 2004). Antitrust policy in the United States has also likely contributed to the absence of a change in concentration in local urban and rural markets because mergers that would have significantly increased concentration at the local level were discouraged or were permitted only on the condition of appropriate divestitures.\n\nEffect on Efficiency and Pricing in the Banking Sector\nI would now like to turn to the effect of geographic deregulation on efficiency and pricing in the banking sector. Regulatory changes appear to have led to meaningful improvements in the efficiency of banks, reductions in costs, and reductions in the prices of banking services. Studies show that non-interest costs, wages, and loan losses all declined in the aftermath of branching reform (Jayaratne and Strahan, 1998; Black and Strahan, 2001). These cost reductions led, in turn, to lower prices on loans, although deposit interest rates changed little.\n\nThe mechanism for this improved performance seems to be changes in the market shares of banks after geographic deregulation (Stiroh and Strahan, 2003). Before regulatory reform, well-run banks faced binding constraints on the markets in which they could operate. When these constraints were lifted, however, assets were reallocated toward more-profitable banks as they gained the opportunity to increase market share, largely by acquiring less-profitable banks.\n\nThe consequences of these healthy competitive dynamics are shown in figure 6, which portrays the average share of assets held by banks with above-median profits (or, for short, the \"high-profit share of assets\"). In 1980, before geographic deregulation, the high-profit share of assets in the sixteen unit-banking states (those that did not permit any form of branching) was much less than the high-profit share in the twelve states that have permitted branching since the 1930s or earlier. This difference disappears completely by 1994. By then, the unit-banking states had permitted within-state branching, thus allowing more-profitable banks to dominate the industry.\n\nThe Real Economic Effect of Geographic Deregulation beyond Banking\nThe consequences of a more efficient banking system can go beyond the banking industry to affect the real performance of the economy as a whole. It has been argued that efficient financial systems promote innovations; hence, better finance leads to faster growth (Schumpeter, 1969). This argument has been countered by assertions that the causality is reversed; economies with good growth prospects develop institutions to provide the funds necessary to support those good prospects (Robinson, 1952). In other words, the economy leads and finance follows.\n\nRecent theoretical developments have fleshed out two ways that good financial systems can lead to growth. Financial markets can matter by affecting the volume of savings available for investment and by increasing the productivity or quality of that investment. These theories show that an improvement in financial market efficiency can act as a lubricant to the engine of economic growth, allowing that engine to run faster. Of course other factors, including sound monetary and fiscal policies, remain critical; but it seems increasingly clear that well-functioning financial markets are also a central factor.\n\nEmpirical research provides support for the view that financial market development can play an important role in driving long-run growth. For example, one study finds evidence of a positive correlation between the size and depth of an economy's financial system and its future growth in per capita real income (King and Levine, 1993).\n\nAlthough this evidence is appealing, it cannot rule out the possibility that financial development and growth are simultaneously driven by some common factor, such as good political or legal institutions, that may be difficult to fully hold constant in the empirical analysis.\n\nOther studies attempt to answer this criticism by exploiting cross-industry differences in financial dependence within a country, thereby holding constant those factors specific to a country. These works suggest that in countries with well-developed financial markets, industries that require more external sources of financing for their investment (that is, the \"financially dependent\") tend to grow faster than \"cash cow\" industries (that is, those that can finance investment with internally generated funds).8 Other research that I have been involved with examines the consequences of banking crises across different types of industries. In particular, this work finds that bank crises have a disproportionately negative effect on financially dependent firms in countries with well-developed financial systems: In such systems, the financially dependent firms grow faster in normal times but are hit harder in crisis times, a difference suggesting that the banking system plays a critical role in overall economic performance (Kroszner, Laeven, and Klingebiel, forthcoming).\n\nThe state-by-state deregulation of branching and banking restrictions provides a useful laboratory for investigating the effect of better banking on economic growth. Because states share a common legal system and broadly similar institutional environments, we can investigate the response of the state economy to policy changes that lead to more-efficient finance. A number of studies give a consistent answer: State economic performance measured in a variety of ways improves after the deregulation of geographic restrictions on banking (for example, Kroszner and Strahan, 2006; Stiroh and Strahan, 2003; Black and Strahan, 2001; Jayaratne and Strahan, 1998; and Jayaratne and Strahan, 1996). These results have important implications for the policy debate in Europe and around the globe.\n\nFirst, after controlling for other factors, data on state-level economic performance over the period 1972 to 1992 suggests that state-level branching deregulation spurred faster economic growth (Jayaratne and Strahan, 1996). Second, evidence from 1976 to 1994 shows that state-level employment growth accelerated after the deregulation of intrastate branching as well as after the deregulation of interstate banking (Kroszner and Strahan, 2006).\n\nThird, if more competitive banking really spurs growth, we would expect particularly large benefits among relatively bank-dependent sectors of the economy, such as small firms or entrepreneurs. In work with Strahan, I have found that the rate of new business incorporation increases significantly after branching deregulation (Kroszner and Strahan, 2006; Black and Strahan, 2001). Moreover, the magnitude of the effects of geographic banking reform on entrepreneurial activity are larger than their effects on the overall growth of employment. This differential effect makes sense because bank credit is most important in financing small businesses that do not have access to public securities markets; the effect suggests that the reason that growth accelerates after geographic deregulation is that credit supply to the entrepreneurial sector expands.9\n\nFourth, a more efficient and diversified financial sector might have an important effect on overall economic stability. The expected effect of banking integration on business cycles is theoretically ambiguous, however. On the one hand, shocks to the local economy and local banking system may become less destabilizing when banks are well diversified and operate across many markets. On the other hand, some commentators have raised concerns that negative shocks to the local economy might be destabilizing after integration because, for example, multistate banks can move capital elsewhere. Recent empirical evidence suggests that the former effect dominates, as state-level economic volatility appears to decline with interstate banking deregulation.10\n\nThis finding of better state-level economic stability after geographic deregulation may reflect the fact that state economies became more insulated from shocks to their own banks. In a non-integrated banking system, such as the one we had in the United States before the 1970s, shocks to bank capital lead to reductions in lending, thereby worsening local downturns. In contrast, with integration, a state can import bank capital from other regions when its banks are down, thus continuing to fund projects with a positive net present value.\n\nAccording to this explanation, the correlation of local measures of economic performance or loan availability with the financial capital of local banks ought to weaken with geographic deregulation and integration. Recent evidence seems to support this idea (Kroszner and Strahan, 2006; Morgan, Rime, and Strahan, 2004). Before the advent of geographic deregulation, there was a nearly one-to-one correspondence between state-level loan growth and state-level bank capital growth. This link weakened significantly after interstate deregulation. Similarly, we also observe a weakened correlation between the growth of local employment and that of local bank capital, although the effect is less dramatic than the effect on loan growth. In short, banking integration appears to have salutary effects on business cycles by insulating the local economy from the ups and downs of its local banking system, and vice versa.\n\nLessons for Europe\nAs I mentioned at the beginning of my remarks, I think that the U.S. experience with the change in its banking structure holds potentially valuable lessons for Europe. In my view, if Europeans can prevent nonprudential, noncompetitive, political concerns from impeding cross-border, intra-European bank mergers, then Europe will be likely to enjoy benefits similar to those enjoyed in the United States when interstate banking restrictions were removed. To recap, these benefits are likely to be\n\nBarriers to Greater Banking Integration in Europe\nOf course, some of you may be thinking that I am overlooking significant structural obstacles to cross-border mergers in Europe and that such obstacles may make such integration less beneficial, on net. I have, however, already mentioned what I view as the largest obstacle--political opposition, which makes integration less likely to happen (but not less beneficial when it occurs). I hope that I have given pause to opponents of such integration by pointing out the benefits that the United States has reaped from reducing geographic restrictions on banking.\n\nAnother obstacle is posed by differences in language and culture, which are clearly greater across European countries than they are across U.S. states, and these differences can create nonregulatory barriers. I have no easy answers for reducing such barriers. However, I would point out that a few truly global banks seem to have succeeded in overcoming them.\n\nI hope that my remarks on the U.S. experience have highlighted the potential gains that, in my opinion, would be likely to follow a similar liberalization in Europe. I want to remind you of the words of Commissioner McCreevy, which I cited at the outset: \"Protectionism denies everyone in Europe the economic benefits of market integration--higher growth, and more jobs.\" I hope that the evidence I have provided here will help make liberalization in Europe more likely so that Europeans will not be denied the benefits that we have experienced in the United States.\n\n\n\nReferences\n\nBarth, James R., Gerald Caprio, and Ross Levine (2002). “Bank Regulation and Supervision: What Works Best?” unpublished paper, January 2002.\n\nBlack, Sandra E., and Philip E. Strahan (2001). “The Division of Spoils: Rent Sharing and Discrimination in a Regulated Industry,” American Economic Review, vol. 91, pp. 814-31.\n\nCetorelli, Nicola, and Michele Gambera (2001). “Bank Structure, Financial Dependence and Growth: International Evidence from Industrial Data,” Journal of Finance, vol. 56, pp. 617-48.\n\nJayaratne, Jith, and Philip E. Strahan (1996). “The Finance-Growth Nexus: Evidence from Bank Branch Deregulation,” Quarterly Journal of Economics, vol. 101, pp. 639-70.\n\nJayaratne, Jith, and Philip E. Strahan (1998). “Entry Restrictions, Industry Evolution and Dynamic Efficiency: Evidence from Commercial Banking (332KB PDF),” Journal of Law and Economics, vol. 41, pp. 239-74.\n\nKing, Robert, and Ross Levine (1993). “Finance and Growth: Schumpeter Might Be Right,” Quarterly Journal of Economics, vol. 108, pp. 717-38.\n\nKroszner, Randall S., Luc Laeven, and Daniela Klingebiel (forthcoming). “Banking Crises, Financial Dependence, and Growth,” Journal of Financial Economics.\n\nKroszner, Randall S., and Philip E. Strahan (1999). “What Drives Deregulation? Economics and Politics of the Relaxation of Bank Branching Restrictions,\" Quarterly Journal of Economics, vol. 114, pp. 1437-67.\n\nKroszner, Randall S., and Philip E. Strahan (2006). “Regulation and Deregulation of the U.S. Banking Industry: Causes, Consequences, and Implications for the Future,” unpublished paper.\n\nLevine, Ross (2004). “Finance and Growth: Theory and Evidence,” Working Paper Series 10766. Cambridge, Mass.: National Bureau of Economic Research.\n\nLevine Ross, Norman Loayza, and Thorsten Beck (2000). “Financial Intermediation and Growth: Causality and Causes,” Journal of Monetary Economics, vol. 46, pp. 31-77.\n\nMorgan, Donald P., Bertrand Rime, and Philip E. Strahan (2004). “Bank Integration and State Business Cycles,” Quarterly Journal of Economics, vol. 119, pp. 1555-85.\n\nPilloff, Steven J. (2004). “Bank Merger Activity in the United States, 1994-2003,” Staff Study 176. Washington: Board of Governors of the Federal Reserve System, May.\n\nRajan, Raghuram, and Luigi Zingales (1998). “Financial Dependence and Growth,” American Economic Review, vol. 88, pp. 559-86.\n\nRobinson, Joan (1952). The Rate of Interest and Other Essays. London: Macmillan.\n\nSchumpeter, Joseph (1969). The Theory of Economic Development. Oxford: Oxford University Press.\n\nStiroh, Kevin J., and Philip E. Strahan (2003). “The Competitive Dynamics of Competition: Evidence from U.S. Banking Deregulation,” Journal of Money, Credit, and Banking, vol. 35, pp. 801-828.\n\nSylla, Richard, John Legler, and John Wallis (1987). \"Banks and State Public Finance in the New Republic: The United States, 1790-1860,\" Journal of Economic History, vol. 47, pp. 391-403.\n\nFootnotes\n\n1.  Charlie McCreevy, “The Development of the European Capital Market (943KB PDF),” speech at the London School of Economics, March 9, 2006. Return to text\n\n2.  A summary is in Kroszner and Strahan (2006). Return to text\n\n3.  The term “institution” refers to a separately chartered commercial bank or savings institution. The term “organization” refers to (1) a holding company that owns one or more commercial banks or savings institutions or (2) an independent commercial bank or savings institution that is not part of a holding company. The number of organizations has declined from nearly 18,000 in 1980 to fewer than 8,000 in 2005. The number of institutions has also fallen, from almost 20,000 in 1980 to fewer than 9,000 in 2005. The large reduction in the number of institutions is attributable to bank holding companies’ consolidating multiple bank subsidiaries into a single institution, to mergers and acquisitions of previously unrelated institutions, and to failures. Return to text\n\n4.  According to the Federal Deposit Insurance Corporation, more than 4,000 new bank and savings institution charters have been granted since 1984. Return to text\n\n5.  The increase in interstate banking is even more dramatic if the individual institution, rather than the organization, is the unit of analysis. The number of multistate institutions increased from 99 in 1990 to 520 in 2005. Over the same period, the share of bank and thrift offices located in a state other than that of the institution’s headquarters office rose from 3 percent to 34 percent. The difference in patterns based on organizations and institutions is attributable to the fact that, at the beginning of the period, many multistate bank holding companies had offices in multiple states, but they operated multiple subsidiaries, each of which had branches in only one state. Return to text\n\n6.  In 1990, the ten largest organizations controlled 14 percent of all deposits. Primarily as a result of mergers among very large organizations, the deposit share of the top ten nearly tripled by 2005, to 39 percent. Return to text\n\n7.  Urban local markets are approximated by metropolitan statistical areas as defined by the U.S. Census Bureau, and rural local markets are approximated by nonmetropolitan counties. The HHI equals 10,000 times the sum of squared market shares of each competitor in a market. An alternative way to measure deposit concentration would be to look at commercial banks only. Concentration levels based on commercial bank deposits unambiguously fell between 1990 and 2005, as the average HHI fell in both urban and rural markets. Much of this decline may be due to the conversion of savings institutions to commercial banks. Return to text\n\n8.  Rajan and Zingales (1998) and Cetorelli and Gambera (2001). In addition, Levine, Loayza, and Beck (2000) shows that the exogenous component of banking development is positively related to economic growth. Return to text\n\n9.  We are beginning to obtain cross-country evidence suggesting that opening up financial markets to foreign entry can also create benefits associated with macroeconomic stability (Barth, Caprio, and Levine, 2002). Return to text\n\n10.  Morgan, Rime, and Strahan (2004) and Kroszner and Strahan (2006). In particular, the magnitude of business cycle shocks, estimated as the absolute value of deviations from expected employment growth, became smaller on average after interstate banking reform and the associated integration of the banking system. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20060406a.htm",
        "title": "The Effect of Removing Geographic Restrictions on Banking in the United States: Lessons for Europe",
        "date": "4/6/2006"
    },
    {
        "content": "April 05, 2006\n\nChairman Ben S. Bernanke\n\nAt the Jump$tart Coalition for Personal Financial Literacy and Federal Reserve Board joint news conference, Washington, D.C.\n\nGood morning. We are here today to learn about a report on a topic of vital importance to our economic future: the financial literacy of America’s young people. Increasingly, personal financial security requires the ability to understand and navigate the financial marketplace. For example, buying a home, saving for retirement or for children’s education, or even effectively managing the family budget now requires more financial sophistication than ever before. Financially literate consumers make the financial marketplace work better, and they are better-informed citizens as well. As a former educator and school board member, and as the parent of two young adults, I am personally convinced that improving education is vital to the future of our economy and that promoting financial literacy in particular must be a high priority. I know that those of you here today join me in this conviction.\n\nThe Jump$tart Coalition is a leader among organizations seeking to improve the personal financial literacy of students from kindergarten to the university level. In particular, through its biennial survey of high school seniors--the results of which you will hear about in a few minutes--Jump$tart has brought increased attention to the issue of financial literacy among youth in the United States. Over the ten-year history of this survey, the data gathered have become some of the most useful measures of what young adults understand about finances.\n\nThe Federal Reserve is strongly committed to Jump$tart’s mission to better educate America’s youth about personal finance. On the regional level, many Federal Reserve Bank staff members work closely with the state coalitions to help achieve this mission. In fact, there is at least one specialist in economic education at each of the Reserve Banks and at most of the Branches. Many of these specialists offer training seminars to help educators teach economics and personal finance in their classrooms. The Federal Reserve also supports a variety of programs and initiatives to increase financial literacy. I want to take a moment to describe just two of these initiatives.\n\nThe first is a student competition called The Fed Challenge, a program designed to teach students about monetary policy and the national economy. Among other activities, students in this competition take the roles of Federal Reserve governors and regional Reserve Bank presidents in a mock meeting of the Federal Open Market Committee. Participating Reserve Banks work in partnership with the Board to bring the winning teams to Washington, where the final rounds of competition will be held here in the Board Room. I have had the opportunity on several occasions to serve as a judge in the national finals of The Fed Challenge, and I can attest that the economic knowledge displayed by the students in that competition is remarkable indeed.\n\nThe Board also recently collaborated with many of the Reserve Banks on a redesigned Federal Reserve financial education web portal that brings many of the System’s resources for financial education together in one location. The site, FederalReserveEducation.org, offers easy access to educational resources designed to benefit not only students, but also parents and teachers. We will continue to work to expand the resources and programs devoted to this important public purpose.\n\nI want to take this opportunity to thank the Jump$tart Coalition and their partners for their continued support and commitment to furthering the financial education of our youth. Now, it is with great pleasure that I introduce to you Laura Levine, executive director of the coalition, who will give a report on the status of financial education of youth in this country.",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060405a.htm",
        "title": "Financial Education and the National Jump$tart Coalition Survey",
        "date": "4/5/2006"
    },
    {
        "content": "April 03, 2006\n\nGovernor Randall S. Kroszner\n\nAt the European Central Bank and Federal Reserve Bank of Chicago Joint Conference on Issues Related to Central Counterparty Clearing, Frankfurt, Germany\n\nAs many of you know, I became a member of the Board of Governors of the Federal Reserve System only a month ago. I am delighted to be giving my first speech as a governor at a conference that has resulted from the kind of international cooperation that I see as essential in today's world. The joint sponsorship of this conference by the European Central Bank (ECB) and the Federal Reserve Bank of Chicago represents an extremely fruitful collaboration of researchers, market participants, and policymakers from both sides of the Atlantic. Having been a research consultant at the Chicago Fed for many years and having visited the ECB numerous times since its founding less than eight years ago, I have many friends at both institutions and am pleased to see so many of those friends here today.\n\nIn addition, I am delighted that the topic of this cooperative venture and my maiden speech is central counterparty (CCP) clearing. As an academic, I wrote several papers on clearing arrangements and participated in many conferences such as this one. I am very pleased to be in a room filled with others who share that interest.\n\nIn recent years, public policymakers have demonstrated growing interest and concern about the effectiveness of CCP risk management. In particular, in November 2004 the Committee on Payment and Settlement Systems (CPSS) of the Group of Ten central banks and the International Organization of Securities Commissions (IOSCO) jointly issued comprehensive international standards for CCP risk management.1 I have often cited CCPs for exchange-traded derivatives as a prime example of how market forces can privately regulate financial risk very effectively.2 Indeed, it is hard to find fault with the track record of derivatives CCPs, many of which have managed counterparty risk so effectively that they have never suffered a counterparty default.\n\nBut perhaps it is not unreasonable to ask whether that track record will be maintained. I see that good track record as a result of innovations that over time produced organizational arrangements that have provided market participants with the incentives and capabilities to ensure effective CCP risk management, thereby serving the public interest as well as the interests of market participants. Significant changes to those arrangements could result in less-effective risk management. Furthermore, some CCPs have begun to clear new products, some of which may be less liquid or more complex than exchange-traded derivatives, and thus may pose challenges to traditional risk-management procedures. Finally, more-intense government regulation of CCPs may prove counterproductive if it creates moral hazard or impedes the ability of CCPs to develop new approaches to risk management. As cross-border activity becomes ever more important, regulatory differences across countries may become an increasingly serious impediment to innovation by CCPs.\n\nIn my remarks today, I will begin by reviewing the historical development of CCPs. I do this not for antiquarian interest but because this history illustrates how market forces led to the evolution of organizational and contractual features that have created strong incentives for effective private regulation that addressed both market participants' and public policymakers' concerns about risk control. I will then discuss the possible implications of recent variations on traditional arrangements. Next I will discuss the challenges involved in clearing certain new products, particularly over-the-counter (OTC) derivatives. I will conclude with some views on how government regulation can provide an environment in which private regulation of CCP risk management continues to be effective.\n\nHistorical Development of Futures Clearinghouses\nMy review of the historical development of central counterparties will focus on the CCP for grain futures traded on the Chicago Board of Trade (CBOT). I make no claim that a CCP first arose in the United States. Indeed, a number of coffee and grain exchanges in Europe had some form of CCP in the late nineteenth century, well before any U.S. exchange.3 Rather, I simply am more familiar with developments in Chicago, in large measure because of the time that Jim Moser spent digging through the CBOT's archives while on the staff of the Federal Reserve Bank of Chicago.4 Furthermore, the market forces that drove the evolution of risk controls at the CBOT likely produced a broadly similar evolution on other exchanges.\n\nAn important lesson from the CBOT's experience is that a CCP emerged gradually and slowly as a result of experience and experimentation. Early on, the CBOT recognized the importance of creating incentives for adherence to its rules, including the contractual obligations of counterparties to contracts traded on the exchange. Initially, the primary incentive was the threat that a member that defaulted on its obligations could be barred from the trading floor. No doubt this consequence was a powerful incentive for solvent members to meet their obligations, but an insolvent member might not have assigned significant value to the loss of trading privileges. By 1873, the CBOT recognized the importance of evaluating the solvency of its members and adopted a resolution stipulating that any member whose solvency was questioned must open its financial accounts to inspection and could be expelled if it refused to do so. Around the same time, the exchange introduced initial and variation margin requirements for contracts traded on the exchange and set strict time limits for the posting of margin deposits. Failure to post margin deposits would be considered a default on the member's contracts.\n\nThe next step along the road to addressing private and public concerns about effective risk control was the CBOT's creation of a clearinghouse in 1883. For many years, the clearinghouse was not a true CCP. Rather, as created, it was merely a mechanism to reduce transactions costs by calculating members' net obligations to post margin and to settle contracts. In the event of a member's default, the clearinghouse assumed no responsibility for settling the defaulting member's trades or for covering the losses to other members that exceeded the amount of margin that the defaulting member had posted.\n\nOnly in 1925 did the CBOT form the Board of Trade Clearing Corporation (BOTCC), a true CCP that became the counterparty to all transactions on the exchange. With the creation of BOTCC, members of the exchange were required to purchase shares in the clearinghouse, and only the member-shareholders were permitted to use the facility.5 Members were also required to post their margin deposits with the clearinghouse. In the event of a member's default, the clearinghouse would take responsibility for settling the defaulting member's trades. The clearinghouse would seek to cover any losses incurred in settling the defaulter's obligations by liquidating its margin deposit. But if the losses exceeded the value of the margin, the deficiency would be charged against the clearinghouse's capital, including the capital owned by the non-defaulting members. If the losses were so severe as to deplete the clearinghouse's capital, the members could be required to purchase additional shares.\n\nThis organizational arrangement has been adopted by many other CCPs, both for exchange-traded derivatives and for cash securities transactions. I characterize this structure as a partial integration of the members of the exchange into a single unit because each member is now at least in part financially responsible for the performance of the others' obligations arising from contracts traded on the exchange.6 The mutualization of risk creates incentives for all the exchange's members to support the imposition of risk controls that limit the extent to which the trading activities of any individual member expose all other members to losses from defaults. Moreover, because the members own the clearinghouse, they have the capability to act on their incentives for effective CCP risk management. I see this alignment of incentives for effective risk management with the ability to act on those incentives as the key to the strong historical track record of derivatives CCPs.\n\nWhat is interesting and instructive about the history of these arrangements is that it illustrates how market forces can produce private regulations that address the concerns about safety, soundness, and broader financial stability.\n\nPotential Challenges Raised by Recent Changes to CCP Organization\nDuring the twentieth century, various changes occurred in the historical organizational arrangements that I have characterized as a partial integration of the members of the exchange. And in the twenty-first century, the pace of change seems to be accelerating. Some derivatives exchanges have remained integrated with their CCP, but even in those cases, there now tends to be less integration. Members of the exchange are seldom required to be members of the clearinghouse. Instead, members of the exchange may arrange to clear through other members, which are referred to as \"clearing members.\" When a clearing member agrees to clear for a nonclearing member, it becomes responsible to the clearinghouse for the obligations of the nonclearing member. Only the clearing members are required to buy stock in the clearinghouse or to contribute to a clearing fund that would be used to cover losses from defaults by other clearing members, including defaults on their obligations to perform on positions held by nonclearing members.\n\nIn recent years, an increasing number of exchanges have engaged unaffiliated CCPs to clear their trades. A \"horizontal\" integration of CCPs has replaced the \"vertical\" integration of an exchange and its CCP. Both horizontally integrated CCPs and vertically integrated CCPs have often arranged for insurance policies that limit the potential losses to their clearing members from defaults. Finally, many exchanges have converted from mutual associations of exchange members to for-profit corporations.\n\nClearly some of these changes have important implications for competition among exchanges. But they may also have implications for the effectiveness of risk management, which is the focus of my remarks today. As I have discussed, historically the key to effective risk management has been that the members of the exchange have borne the risk of losses from defaults and have had the capacity to institute risk controls (principally membership standards and margin requirements) that have limited those risks. The question then is whether any of these changes to the organization of CCPs has left those bearing the risks without the capacity to manage those risks.\n\nI would caution against assuming that change is inherently risky. After all, as we have seen, the partial integration model that worked so well for so many years emerged only gradually as a result of experimentation. Moreover, thinking that \"one size fits all\" regarding the organization of financial markets is a mistake. That said, it seems critical that the organization of any CCP, including a CCP that follows the traditional partial-integration model, should conform to a pair of broad principles. First, a CCP's default rules need to be transparent: The party that bears the risk of default (who has \"skin in the game\") must be clear to all. Second, a CCP's governance arrangements must provide those with \"skin in the game\" with substantial influence over the CCP's risk controls.\n\nNew Products\nIn recent years, appreciation of the possible benefits of a well-organized CCP has been growing. CCP arrangements have been introduced in a wide variety of markets that had not previously been served by CCPs. In the United States, the New York Stock Exchange established a clearinghouse in 1892 and transformed it into a true CCP in 1920. But, outside the United States, few securities exchanges established CCPs until late in the twentieth century. Today, a CCP is in place and functioning in nearly all major securities markets. Increasingly often, CCPs for securities clear trades, including trades and repurchase agreements involving government bonds, in the over-the-counter securities markets. Since 1999 the London Clearing House (now LCH.Clearnet) has been clearing growing volumes of some types of OTC derivatives through its SwapClear service.\n\nThe clearing of OTC derivatives is an especially interesting development. Although SwapClear has been gaining traction, it has been met with resistance from some OTC derivatives dealers. Some of them have argued that bilateral credit risk management, which uses many of the same techniques that CCPs use (netting and margin requirements), is highly effective. Moreover, not all OTC derivatives are sufficiently standardized to be cleared. Consequently, some have expressed concerns that CCP clearing of \"vanilla\" products could increase the risks on non-cleared \"exotic\" products by limiting the scope for bilateral netting of vanilla products against exotic products outside the CCP. Another consideration for the most creditworthy dealers may be the potential effect of CCP clearing on mitigating the competitive advantage of their creditworthiness.7\n\nWith regard to systemic risk, the key question about the clearing of OTC derivatives is whether the risk-management techniques that have proved so effective in clearing exchange-traded products will prove equally effective in clearing products that are not as standardized. In particular, the clearing of OTC derivatives tends to entail much less scope for offsetting transactions. As a consequence, if a default occurred, a huge volume of transactions would need to be closed out. The feasibility of a CCP's achieving close-out promptly is clearly a critical issue that deserves careful examination. In that regard, a recent report by leading participants in the OTC derivatives markets expressed concern about the feasibility of close-out procedures in the event of default of a large market participant in stressed market conditions.8 Further experimentation with close-out procedures may be necessary to address that concern.\n\nThe Role of Government\nIn recent years, policymakers have devoted much attention to oversight and regulation of CCPs, with the objective of promoting their soundness and stability. I certainly share that objective, but I would like to call attention to some possible unintended and undesirable consequences of CCP regulation. The first is moral hazard. Policymakers must be very careful to avoid any impression that government oversight comes with a promise of government financial support in the event of a risk-management failure; otherwise, private-market discipline, which has served private and public interests in the stability of CCP arrangements so well for so long, may well be eviscerated.\n\nInstead, government regulation should focus on improving the effectiveness of private-market regulation. In particular, it should enforce the observance of the two critical principles I identified earlier. First, it should ensure that a CCP's risk-management policies and procedures, especially its policies for handling defaults and allocating the burden of losses from defaults, are transparent to market participants. Second, it should ensure that CCP governance arrangements provide the parties who would bear the losses with substantial influence over the CCP's risk-management policies.\n\nMy sense is that policymakers are well aware of the risks that moral hazard poses for financial stability. But I am concerned that a second unintended consequence of regulation has too often gone unrecognized. That is the potential for conflicting regulation (and laws) to impede the evolution of CCP arrangements, especially the potential for economies of scale and scope to be achieved through consolidation. I am always puzzled when I hear the United States held up as the model for the benefits of consolidation of the clearing and settlement infrastructure. We have achieved significant consolidation within the securities markets and within the futures markets. But I am struck by the lack of consolidation of securities and futures CCPs. Perhaps there is no business case for such consolidation. Even if a business case exists, however, I believe consolidation would be difficult to achieve due to the legal and regulatory distinctions in the United States between securities and futures.\n\nLaw and regulation seem also to be placing significant barriers in the way of consolidation of the securities and derivatives clearing and settlement infrastructure in Europe. Most of the fifteen barriers to efficient cross-border clearing and settlement that were identified by the Giovannini Report in 2001, seem to be grounded in law and regulation rather than in the practices of private-market participants.9\n\nPolicymakers in all countries need to examine whether legal and regulatory distinctions are impeding innovation and, if so, whether the distinctions are meaningful and essential for the achievement of public policy objectives. Policymakers must also resist the temptation to place regulation in the service of protectionism. I read with interest and appreciation European Union Commissioner McCreevy's recent speech at the London School of Economics on the development of the European capital markets, in which he decried the signs of a new wave of protectionism in Europe.10 As he noted, \"Protectionism is a proven route to economic stagnation and decline.\"11 This is an important message, indeed.\n\nConclusions\nI find the history of financial markets to be enormously instructive. My reading of the history of CCP clearing is that it teaches us that private-market regulation can be effective for achieving the public policy goal of safety and soundness and broader financial stability. Government regulation and oversight should seek to provide an environment in which private regulation can be most effective. Government regulation should not place unnecessary barriers--domestically or internationally--in the path of the future evolution of private-market regulation. Innovation should be fostered, and regulatory protectionism should be rejected.\n\n\n\nFootnotes\n\n1.  Committee on Payment and Settlement Systems and Technical Committee of the International Organization of Securities Commissions (2004), Recommendations for Central Counterparties (Basel: Bank for International Settlements, November). Return to text\n\n2.  Randall S. Kroszner (1999), \"Can the Financial Markets Privately Regulate Risk? The Development of Derivatives Clearinghouses and Recent Over-the-Counter Innovations,\" Journal of Money, Credit, and Banking, vol. 31 (August), pp. 596-618. See also Randall S. Kroszner (2000), \"Lessons from Financial Crises:  The Role of Clearinghouses,\" Journal of Financial Services Research, vol. 18 (December), pp. 157-71. Return to text\n\n3.  See the discussion on pp. 71-72 of Henry Crosby Emery (1896), Speculation on the Stock and Produce Exchanges of the United States (New York: Columbia University). Return to text\n\n4.  James T. Moser (1998), \"Contracting Innovations and the Evolution of Clearing and Settlement Methods of Futures Exchanges,\" Working Paper (Chicago: Federal Reserve Bank of Chicago). Return to text\n\n5.  Later, a member of the exchange was not required to be a member of the clearinghouse if it could arrange for a clearinghouse member to assume responsibility for the nonmember's obligations to the clearinghouse. Return to text\n\n6.  See Kroszner (1999), p. 603. Return to text\n\n7.  For one account that argues that the introduction of CCP clearing in U.S. futures markets was delayed by financially strong members who were resistant to giving up the advantage of their high credit quality and to implicitly subsidizing weaker, see Craig Pirrong (1997), \"A Positive Theory of Financial Exchange Organization with Normative Implications for Financial Market Regulation,\" Working Paper (St. Louis: Olin School of Business, Washington University). Return to text\n\n8.  Counterpart Risk Management Policy Group II (2005), Toward Greater Financial Stability: A Private Sector Perspective (New York: CRMPG II, July). Return to text\n\n9.  The Giovannini Group (2001), Cross-Border Clearing and Settlement Arrangements in the European Union (Brussels: The European Commission, November). Return to text\n\n10.  Charlie McCreevy (2006), The Development of the European Capital Market (London: London School of Economics, March 9). Return to text\n\n11.  See McCreevy (2006), p. 3. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kroszner20060403a.htm",
        "title": "Central Counterparty Clearing: History, Innovation, and Regulation",
        "date": "4/3/2006"
    },
    {
        "content": "March 31, 2006\n\nVice Chairman Roger W. Ferguson, Jr.\n\nAt the Institute of International Finance Spring 2006 Membership Meeting, Zurich, Switzerland\n\nI am pleased to participate in the panel discussion at this Institute of International Finance Spring 2006 Membership Meeting. As I will make clear, I think meetings of this sort, by contributing to the dialogue between the leaders of financial institutions and policymakers, can play a critical role in increasing mutual understanding and improved decisionmaking by both groups. The financial environment can best be described as \"dynamic.\" Financial innovations have been coming at a rapid pace in recent years; new financial products have been introduced and are expanding rapidly, and new institutions have taken on prominent roles in key financial markets. Financial technologies have improved as well and have the potential to contribute to the efficiency and resilience of financial markets. However, with new products and institutions comes the potential for new risks to financial stability. As a result, we policymakers are likely to be torn. On the one hand, we may want to encourage welfare-improving innovations by limiting the extent of regulation. On the other hand, because of possible systemic concerns, some policymakers may want to regulate innovative instruments and institutions even as they are developing. In my view, policymakers can best balance these goals by expending the effort needed to understand financial innovations as they emerge and by avoiding overregulation that may stifle valuable innovations.\n\nWhen I talk about financial innovations, I have in mind several types of developments. A far-reaching set of innovations is the development and increasing popularity of products for the transfer of credit risk. Prominent among such innovations are credit derivatives, asset-backed securities, and secondary-market trading of syndicated loans. Another important development has been the rapid growth of the hedge fund industry and its expanded role in the financial system. On the retail side, we have seen a proliferation of new lending products in the United States, including home-equity lines of credit, interest-only and even negative-amortization mortgages, and subprime mortgages and consumer loans.\n\nToday, I will discuss briefly the potential benefits and drawbacks associated with new products and institutions and spend most of my remarks on a middle way that regulators might pursue as these new products and institutions emerge.\n\nBenefits and Drawbacks\nFinancial innovations hold the promise of improved efficiency and increased overall economic welfare. For example, new products and markets can open the door to new investment opportunities for a variety of market participants. And improved risk-measurement and risk-management technologies can contribute to an improved allocation of risk as risk is shifted to those more willing and able to bear it.\n\nFinancial innovations also have the potential to boost financial stability. Risk-transfer mechanisms can not only better allocate risk but also reduce its concentration. Improved efficiencies and increased competition may result in substantially lower trading costs and may consequently improve liquidity in many markets. Better liquidity, which is instrumental to faster and more accurate price discovery and therefore to more-informative prices, can also be brought about by an increased presence of new institutions in new or existing markets. The entry of those new institutions into new markets can, so long as the institutions prove resilient, increase the availability of funds to borrowers in times of stress and may thus reduce the likelihood of credit crunches.\n\nAlthough financial innovations have the capacity to improve economic welfare overall, it is natural for policymakers to worry that innovations may have unexpected and undesirable side effects and may even represent new sources of systemic risk. For example, policymakers may be concerned about unexpected price dynamics or problems in infrastructure or operations. Market participants estimate how prices and investment flows are likely to behave for new instruments, but their understanding becomes more detailed and more accurate only as behavior under a variety of economic conditions is observed, and the development of that understanding obviously takes time. Under turbulent conditions, or when new information causes market participants to question their own investment strategies, their behavior may change rapidly, leading to rapid price changes that may seem outsized relative to changes in economic fundamentals. That was briefly the case recently in the market for synthetic collateralized debt obligations. Market participants did not anticipate the sharp decline in implied default correlations that followed the downgrades of Ford and General Motors debt. Prices moved quite a bit for a short time as portfolios were rebalanced, but spillovers to other markets were limited, and market volatility subsequently eased.\n\nProblems with the infrastructure or operations that support an innovation--including the underlying legal documentation and accounting--are also likely to be revealed only over time, as exemplified by the technical difficulties with restructuring clauses in credit default swaps that became apparent a few years ago. In that case, default events and related payoffs sometimes did not occur as expected, and so actual exposures differed from those investors had intended. The result was a change in the value of existing contracts and a period of market adjustment as new restructuring clauses were developed and implemented.\n\nOf course, we should not want to prevent rapid price changes or changes in investment flows, as such changes may be appropriate as new information about fundamentals emerges. And the occurrence of glitches in new markets and institutions need not reflect policy failures or provide evidence that an innovation is undesirable. Preventing all such occurrences would probably require us to stop all innovation. But neither is it desirable that growing pains in one market or at a few institutions spill over so strongly that the financial system as a whole could be destabilized.\n\nA Middle Way in Regulation\nPolicymakers have a range of strategies available for dealing with innovation. At one extreme, in theory we could take a completely hands-off approach, allowing new financial markets and instruments to develop without restrictions and indeed without any scrutiny, trusting private market participants to do everything necessary for stability and efficiency. At the other extreme, policymakers theoretically might be quite heavy-handed, either imposing regulations on virtually every market and instrument to stop any innovations that, in their judgment, could cause harm or, conversely, actively fostering or subsidizing innovations seen as desirable.\n\nObviously, these are extreme positions, and I do not know of any practicing policymaker who seriously wants to pursue either extreme course. Today I wish to argue for a middle ground in which markets are allowed to work and develop and in which policymakers work hard to understand new developments and to help market participants see the need for improvements where appropriate. In my view, regulations should be imposed only when market participants do not have the incentive or the capability to effectively manage the risks created by financial innovation. For example, explicit or implicit subsidies of some institutions could limit market discipline of their risk-taking, leading to a concentration of risk so large that even the most sophisticated institutions would find it next to impossible to manage the risk under stressful circumstances. Or policymakers may be concerned that some potential parties to innovative contracts, especially in the retail arena, are insufficiently knowledgeable to understand or manage the associated risks. I believe such instances are rare. Making a case for early regulatory intervention is particularly difficult when the private parties involved in an innovation are sophisticated because, in many cases, they will be the first to recognize possible problems and will have strong incentives to fix them and also to protect themselves against fraud or unfair dealing.\n\nSo how should policymakers proceed down this middle path? First of all, we need to learn--we need to understand and evaluate the innovations that are taking place in financial markets. This process should include information sharing with other authorities, including those in other nations, in order to benefit from the experiences in other markets and regions. The resulting improved understanding is often enough to prepare policymakers to deal with any breakdowns that do occur and to avoid having the breakdowns turn into systemic problems. The U.S. response to the century date change is an example from a different context that fits into this category. In that case, policymakers worked hard to understand the complex practical issues and to share that knowledge with financial firms. Those firms independently evaluated the risks they faced and took appropriate action to manage them effectively.\n\nImproved understanding may also ease concerns about potential risks. For example, in light of the effects of financial consolidation on the number of firms acting as dealers in the market for dollar interest rate options, the Federal Reserve became concerned about possible risks to the functioning of that market. These concerns included questions about the adequacy of risk management at the remaining dealers and about the possible effects that problems at one of those dealers could have on its counterparties and market liquidity. However, further investigation by Federal Reserve staff suggested that market participants were generally managing their market and counterparty risks effectively and that those hedging risk in the options market would not unduly suffer from a temporary disruption in liquidity. Our wariness about concentration in this market has not disappeared as a result of our improved understanding, but it has diminished. In general, improved knowledge about financial innovations may prevent the imposition of unwarranted restrictions and is surely a precursor to intelligent regulation in the event it is warranted.\n\nA second step for policymakers walking the middle path should be to ensure that market participants have the proper incentives and the information they need to protect themselves from any problems related to new products, markets, or institutions; by so doing, policymakers can perhaps mitigate those problems. Policymakers should insist that regulated firms effectively manage the risks associated with new activities and markets, thereby fostering effective market discipline of risk-taking, including risk-taking by unregulated firms. Such an insistence generally does not require new regulation but rather is an application of existing regulation in a potentially new context. One of the lessons of the difficulties at Long-Term Capital Management (LTCM) was that the hedge fund had been able to achieve very high levels of leverage because some regulated counterparties had not appropriately managed their counterparty risk exposures. Subsequently, both banks and supervisors had to reassess what such management entailed. Clearly, supervisors should strongly encourage institutions to know their risk posture and to be able to control it and react appropriately as circumstances change. Policymakers should insist on similarly high risk-management standards for regulated financial institutions that provide retail products. As a case in point, bank supervisors in the United States recently issued guidance about the management of risks related to home-equity lines of credit. This guidance did not involve new regulation of these instruments but rather reminded institutions offering such products that they have an obligation to manage the resulting risks appropriately.\n\nA pervasive lack of awareness about the risks embedded in new financial products certainly increases the likelihood that users of those products may face difficulties and that those difficulties may become systemic. One way policymakers can help prevent this possibility from happening is by supporting increased transparency and disclosure. Although counterparties in wholesale markets should generally be expected to demand and obtain the information they need to evaluate their risks, policymakers can no doubt help establish high standards. In the case of retail transactions, support for efforts to foster the basic financial literacy of households is a useful complement to efforts to promote appropriate disclosure. The more consumers are equipped to interpret disclosures, the more effective those disclosures are likely to be.\n\nA third feature of the moderate approach I am trying to chart is an active dialogue between policymakers and market participants. In my view, policymakers should serve as a voice for the development of infrastructure and sensible standards and practices. Ideally such steps would be taken by market participants of their own volition, but sometimes informal interventions by policymakers can help foster cooperative efforts by market participants. For example, partly in reaction to the report of the second Counterparty Risk Management Policy Group, the Federal Reserve Bank of New York recently hosted a meeting with representatives of major participants in the credit default swap market, as well as with their domestic and international supervisors, to discuss a range of issues, including market practices with regard to assignments of trades and operational issues associated with confirmation backlogs. The result was an industry commitment to take concrete steps to address issues of concern.\n\nA fourth dimension of my proposed middle path is the ongoing monitoring of key markets and institutions. Policymakers should be aware of any emerging stresses in the financial system, including those related to new instruments and institutions. Indeed, some central banks have created \"financial stability\" staff groups to oversee such monitoring and, in some cases, to publish regular financial stability reports. In the event that such monitoring suggests that the operations of some institutions or markets are under significant strain and, importantly, that the resulting pressures on businesses and households could have a material adverse effect on the real economy, the central bank may want to respond by adjusting the stance of monetary policy.\n\nFinally, financial innovations may on occasion warrant new regulations because financial institutions either cannot or will not manage the associated risks appropriately. Indeed, regulation should be seen as part of the broader \"infrastructure\" that supports both financial stability and innovation, and like other more traditional infrastructure, regulatory regimes have to keep up. For example, developments in financial markets and advances in the ability of banks to measure and manage their risks have increasingly made the existing capital regulation of the largest banks, the 1988 Basel Accord, look antiquated. Basel II is a more flexible framework than Basel I and is intended to better permit capital regulation to keep up with financial market innovations in the future.\n\nTo conclude, I wish to emphasize that policymakers should have a bias toward trusting financial markets to manage the introduction of new products and the development of new institutions smoothly and without undue stress to the financial system. However, we cannot take such an outcome for granted: Financial firms may not consider the effects of their decisions on the stability of other firms or on the broader financial markets, and some may lack the incentives and ability to learn about and manage the risks induced by financial innovations. In such cases, policymakers may need to work with markets and their participants, and on occasion regulate them, to achieve the desired outcomes. However, policymakers should, wherever possible, avoid premature regulation that could stifle innovation. I would note that a significant number of substantial shocks to financial markets have occurred in recent years--including, for example, the difficulties at Long-Term Capital Management and the unexpected and massive fraud at some high-profile companies--and yet the broader effects on the real economy have ultimately been quite small. Our financial markets are flexible and resilient, and they can absorb shocks surprisingly well. As a result, most risks caused by new developments in financial markets should be manageable without heavy-handed regulation. This meeting is a good example of what my middle course suggests we should be doing: working hard to understand innovations and their possible implications. Alertness and knowledge on the part of policymakers would go a long way toward ensuring that our positive recent track record will carry on amid what I am sure will continue to be a rapidly changing financial landscape.",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/ferguson20060331a.htm",
        "title": "Financial Regulation: Seeking the Middle Way",
        "date": "3/31/2006"
    },
    {
        "content": "March 31, 2006\n\nGovernor Susan Schmidt Bies\n\nAt the Banking Institute, Charlotte, North Carolina\n\nThank you for the invitation to speak here at the Banking Institute. I want to discuss with you today some recent and ongoing regulatory issues that are likely of interest to this audience. These issues include efforts to enhance our regulatory capital regime, compliance risk management, and consumer protection.\n\nProposed Revisions to Regulatory Capital Regime\nFirst of all, you probably heard the good news yesterday that the Federal Reserve Board reviewed and in an open Board meeting approved a draft of the interagency notice of proposed rulemaking (NPR) on the Basel II capital framework. The draft NPR was made available on the Board's website as well as some statements made at the public meeting. The final NPR is expected to be issued in the Federal Register once all of the U.S. banking agencies have completed their review and approval processes, at which time it will then be \"officially\" out for comment. We are very pleased that the substantial time spent on this effort has culminated in this agreement among the agencies. We also recognize the significance of this development to the industry, the Congress, and others who have waited for greater specificity on the proposed revisions. We look forward to comments on the NPR; they will be an important contribution to the assessment of Basel II objectives and implementation of the framework. In some areas, the agencies are still grappling with what the correct approach is. For this reason, the NPR contains a number of requests for feedback on specific topics. All of this will help us as we continue to develop the framework. But before commenting further on the NPR and the U.S. Basel II process, I would like to reiterate our rationale for pursuing Basel II.\n\nReasons for Pursuing Basel II\nThe current Basel I capital framework, adopted nearly twenty years ago, has served us well, but has become increasingly inadequate for large, internationally active banks that are offering ever-more complex and sophisticated products and services. We need a revised capital framework for these large, internationally active banks, and we believe that Basel II is such a framework.\n\nOne of the major improvements in Basel II is the closer linking of capital requirements and risk. The current Basel I measures are not very risk-sensitive and do not provide bankers, supervisors, or the marketplace with meaningful measures of risk at large, complex organizations. Under Basel I, it is possible for two banks with dramatically different risk profiles in their commercial loan portfolio to have the same regulatory capital requirement, and a bank's capital requirement does not reflect deterioration in asset quality. In addition, the balance-sheet focus of Basel I does not adequately capture risks of certain off-balance-sheet transactions and fee-based activity--for example, the operational risk embedded in many of the services from which many large U.S. institutions generate a good portion of their revenues.\n\nIn addition to enhancing the meaningfulness of regulatory capital measures, Basel II should make the financial system safer by substantially improving risk management at banks. Basel II builds on the risk-management approaches of well-managed banks and creates incentives for banks to move toward leading risk-measurement and risk-management practices. By providing a consistent framework for all banks to use, supervisors will more readily be able to identify portfolios and banks whose risk management and risk levels are significantly different from the range seen in other banks. By communicating these differences to banks, management will be able to benchmark their risk assessments, models, and processes in a more detailed and regular manner. We have already seen some progress in risk management at many institutions in the United States and around the globe as a result of preparations for Basel II. The new framework is also much more consistent with the internal capital measures that institutions use to manage their business.\n\nBasel II can also provide supervisors with a more conceptually consistent and more transparent framework for assessing the link between risk and capital over time at our most complex institutions; identifying which institutions have deficiencies; and evaluating systemic risk in the banking system through credit cycles. Therefore, Basel II establishes a more coherent relationship between how supervisors assess regulatory capital and how they supervise the banks, enabling examiners to better evaluate whether banks are holding prudent capital levels, given their risk profiles, and to better understand differences among institutions.\n\nAs a central bank and supervisor of banks, bank holdings companies, and financial holding companies, the Federal Reserve is committed to ensuring that the Basel II framework delivers a strong and risk-sensitive base of capital. That is why we support safeguards to ensure strong capital levels during the transition to Basel II, and will remain vigilant in monitoring Basel II's impact on an ongoing basis. This means that during and after the transition to Basel II, supervisors will rely upon ongoing, detailed analysis to continuously evaluate the results of the new framework and ensure prudent levels of capital. To be quite clear, the Federal Reserve believes that strong capital is critical to the health of our banking system and we believe that Basel II will help us continue to ensure that U.S. banks maintain capital levels that serve as an appropriate cushion against risk-taking.\n\nAs we have mentioned before, we will continue to use existing prudential measures to complement Basel II. For example, the current leverage ratio requirement--a ratio of capital to total assets--will remain unchanged for all banks, whether or not they are subject to the Basel II framework. Also, supervisors will continue to enforce existing prompt-corrective-action rules in response to declines in capital. Both the leverage ratio and prompt-corrective-action are fully consistent with Basel II.\n\nBasel II NPR\nI will not try to summarize the NPR here today. We want all of you to read it and come to your own judgments. I would, however, like to highlight a few key points.\n\nAs you know, the U.S. Basel II NPR is based on the 2004 framework issued by the Basel Committee and adheres to the main elements of that framework. But the U.S. agencies, just as their counterparts in other countries, have exercised national discretion and tailored the Basel II framework to fit the U.S. banking system and U.S. financial environment. For example, the U.S. agencies continue to propose that we implement only the advanced approaches of Basel II, namely the advanced internal-ratings-based approach (AIRB) for credit risk and the advanced measurement approaches (AMA) for operational risk.\n\nThe U.S. agencies also included in the NPR a timetable and set of transition safeguards that are more rigorous than those set forth in the 2004 Basel II framework. For instance, the U.S. agencies are proposing three transition floors, below which minimum required capital under Basel II will not be permitted to fall, relative to the general risk-based capital rules. The first transition period would have a floor of 95 percent, the second 90 percent, and the third 85 percent. Part of the justification for implementing more rigorous floors stemmed from the lessons we learned from the fourth quantitative impact study (QIS4) conducted in the United States in 2004. As I have said before, QIS4 was not intended to reflect the ultimate impact of Basel II on U.S. institutions--particularly since it was not based on a complete proposal and bank inputs to QIS4 were not based on fully developed systems or full supervisory guidance. Rather, it was conducted on a \"best-efforts basis\" to provide a snapshot for gauging progress toward implementation of Basel II and to give the U.S. agencies a better sense of how to structure the NPR.\n\nOne of the key areas in the NPR influenced by QIS4 pertains to banks' estimates of loss given default (LGD). QIS4 results showed that, in general, data histories were not long enough to capture weaker parts of the economic cycle, especially for LGDs, which must reflect downturn conditions. As a result, the agencies have provided a supervisory mapping function for those institutions unable to estimate downturn LGDs. The mapping function takes average LGDs and \"stresses\" them to generate an input to the capital calculation that is better suited to the Basel II formulas and produces a more appropriate capital requirement. The Federal Reserve believes this supervisory mapping function is a necessary component of Basel II because it appears difficult for some banks to produce internal estimates of LGD that are sufficient for risk-based capital purposes.\n\nI hope it is clear from the NPR and other statements made by the agencies that we are committed to ongoing, detailed analysis to ensure that U.S. implementation of Basel II achieves a strong and risk-sensitive base of minimum regulatory capital. We need to ensure that the items we identified as incomplete in QIS4 are appropriately addressed, and we also need to ensure that additional areas will not inadvertently lower capital levels. We intend to conduct thorough analysis of each institution's Basel II capital results and the impact on aggregate capital in the U.S. banking system at many stages along the way.\n\nIn addition, the U.S. regulators are united in their belief that no bank should be permitted to operate under Basel II until it has proven itself ready to do so. There will be no \"free pass\" or \"safe harbor\" for any institution, regardless of portfolio composition or business activity. In other words, we plan to have very high standards for Basel II qualification requirements. For instance, a bank will be able to move from the parallel run to live capital calculations with a 95 percent floor only after its primary supervisor has given it permission to do so after having thoroughly evaluated its risk-management methodologies and its ability to calculate minimum regulatory capital using the new framework. Similarly, a bank will need approval to move to each of the other two floor levels. After the third floor period, a bank will be allowed to move to the full Basel II minimum capital calculation without floors upon a finding by the primary supervisor that it is ready, following a rigorous qualification process.\n\nProposed Amendments to Basel I\nBefore I end my remarks about regulatory capital, I would like to offer some thoughts about ongoing efforts to revise existing regulatory capital rules, known as Basel I. First of all, we expect only one or two dozen banks to move to Basel II in the near term. The vast majority of U.S. banks would be able to continue operating safely and profitably under Basel I as amended through the rulemaking process. The Basel I framework has already been amended more than twenty-five times in response to changes in the banking environment and a better understanding of the risks of individual products and services. The agencies believe that now is another appropriate time to amend the Basel I rules.\n\nConcerns have been raised about potential competitive inequities between Basel II banks and Basel I banks. We take these concerns seriously and sought input from the industry and other interested parties in the Basel I ANPR process. In an effort to mitigate those concerns, regulators have proposed changes to enhance the risk sensitivity of U.S. Basel I rules and remain vigilant about potential competitive distortions that might be created by introducing Basel II. We are also mindful that amendments to Basel I should not be too complex or too burdensome for the multitude of smaller banks to which the revised rules will apply.\n\nAdditionally, we recognize the need for full transparency about Basel II proposals and proposed Basel I amendments. For that reason, we expect to have overlapping comment periods for both the Basel II NPR and the proposed Basel I amendments. The intent is to allow banks and others to review both NPRs before both sets of rules are finalized. In that way, bankers from potential opt-in institutions and those not planning to move to Basel II can evaluate the potential impact of Basel II in light of the proposed Basel I amendments. In fact, we want all interested parties to compare, contrast, and comment on the two proposals in overlapping timeframes. At this point, we are still reviewing the comments received on the ANPR for amendments to Basel I. The comment period ended in mid-January. The agencies are developing their proposals for Basel I amendments, based on comments received, and hope to have a Basel I NPR by summer.\n\nFinally, I would like to underscore that both regulatory capital proposals being worked on by the U.S. agencies are just that--proposals. The U.S. agencies welcome any and all comments on these documents. Accordingly, our proposals could change based on comments received or new information gathered by the U.S. agencies. We know that at times this posture can be frustrating to some, but given the breadth and depth of these proposals, it is critical that we consider all viewpoints. This is especially true for the Basel II proposal, which represents a substantial and complex change in bank supervision and regulation. In this respect, I would like to echo the comments made earlier this month by Comptroller John Dugan: if the U.S. agencies see that Basel II is not accurately reflecting risk or is producing unacceptable capital levels, we will seek to make changes. Indeed, we expect to make some adjustments as we move forward, just as changes have been made to Basel I over the years to reflect changes in bank practice and improvements in supervision.\n\nCompliance-Risk Management\nWhile the release of the Basel II NPR is indeed a major step forward, it is of course not the only topic worth addressing here today. Accordingly, I would now like to turn to another area the financial sector and regulators are focused on: compliance-risk management. \"Compliance-risk\" can be defined as the risk of legal or regulatory sanctions, financial loss, or damage to an organization's reputation and franchise value. This type of risk may result when an organization fails to comply with the laws, regulations, or standards or codes of conduct that are applicable to its business activities and functions. The Federal Reserve expects each banking organization to have a compliance culture in place across the whole institution and an infrastructure that can identify and control the compliance risks it faces, along with appropriate rewards and penalties for business managers who oversee the compliance risk.\n\nTo create appropriate compliance-risk controls, organizations must first understand risks across the entire entity. Managers should be expected to evaluate the risks and controls within their scope of authority at least annually. I also emphasize the need for the board of directors and senior management to ensure that staff members throughout their organizations understand the compliance objectives and each member's role in implementing the compliance program.\n\nAn enterprise-wide compliance-risk management program should be dynamic and proactive, meaning it constantly assesses evolving risks when new business lines or activities are added or when existing activities are altered. To avoid having a program that operates on \"autopilot,\" an organization must continuously reassess its risks and controls and train employees to effectively implement those controls.\n\nAn integrated approach to compliance-risk management can be particularly effective for Bank Secrecy Act and anti-money-laundering (BSA/AML) compliance. Often, the identification of a BSA/AML risk or deficiency in one business activity can indicate potential problems or concerns in other activities across the organization. Controlling BSA/AML risk continues to be a primary concern for banking organizations.\n\nWe recognize the commitment that organizations have made to compliance with BSA/AML requirements, and, in return, we continue to work to ensure that obligations in this area are clearly communicated to banking organizations and examiners alike. The Federal Reserve strives to provide clear and comprehensive guidance that directly communicates our expectations to the institutions we supervise, so that institutions do not need to rely on, for example, their own interpretations of public enforcement cases, which are not intended to serve as industry-wide compliance guidance. The Federal Financial Institutions Examination Council (FFIEC) BSA/AML Examination Manual issued last year is one example of our interagency efforts to clearly communicate our expectations.\n\nThe FFIEC BSA/AML Examination Manual reflects a common view of the federal banking agencies and the Treasury Department's Financial Crimes Enforcement Network (FinCEN) with regard to BSA/AML compliance expectations. The agencies universally stress that the purpose of a BSA/AML examination is to assess the overall adequacy of a banking organization's BSA/AML controls, in view of that particular organization's lines of business and customer mix. This is critical to ensuring that resulting controls are risk-based, so that resources are directed appropriately.\n\nWe also are working closely with our Treasury and law enforcement counterparts to disseminate information about perceived money-laundering or terrorist-financing threats. By identifying emerging vulnerabilities, we can better collaborate with banking organizations to develop systems and procedures to combat criminals' abuse of the financial sector. For example, the interagency Money Laundering Threat Assessment (4.1MB PDF) is one step we have taken--with fifteen other U.S. government bureaus, offices and agencies, including law enforcement--to identify significant concerns and communicate them to banking organizations.1\n\nConsumer Protection\nThe Federal Reserve also cares greatly about consumer protection, as should bankers when they are assembling a broad risk-management strategy. Bankers need to be especially alert to developing easily understood disclosures as they introduce more innovative and complex products that can be confusing to consumers. As you may know, the U.S. banking agencies recently issued proposed guidance on nontraditional mortgages. The comment period for this interagency proposal closed on March 29, so we are now in the process of reviewing comments and determining how to proceed.\n\nNontraditional mortgages allow borrowers to defer payment of principal and, sometimes, interest. While the proposed guidance focuses on banks' ability to adequately identify, measure, monitor, and control the risk associated with these products, it also addresses consumer protection. Nontraditional mortgages, including \"interest-only\" mortgages and \"payment-option\" adjustable-rate mortgages, have been available for many years, and are beneficial for some borrowers because of the payment flexibility they offer. Although these products were initially designed for higher-income borrowers, today these products are being offered to a wider spectrum of consumers, including borrowers for whom these types of mortgages may be ill-suited. Moreover, institutions are combining these nontraditional loans with other practices, such as reduced documentation of income and assets in evaluating applicants' creditworthiness. Many borrowers may not fully recognize the risks of nontraditional mortgages, particularly \"payment shock\" when the loan's interest rate increases, or when the consumer is required to make fully amortizing payments. Negative amortization coupled with flattening, or even lower, housing prices could make it difficult for some borrowers to refinance or sell the property to avoid payment shock.\n\nIn addition to ensuring that institutions comply with the Truth in Lending Act and other applicable laws, the draft guidance urges institutions to ensure that their advertisements, promotional materials, and oral communications are consistent with the product terms and that these communications provide clear, balanced, and timely information about the risks. This is important so that consumers have the information they need at critical decision times, such as when selecting a loan product or choosing a specific payment option each month.\n\nThe Board's Truth in Lending regulations require creditors to provide consumers with disclosures about the loan terms, including a schedule of payments. For interest-only and payment option ARMs, the payment schedule shows consumers how their payments will increase to include amortization of the principal. The proposed interagency guidance describes how institutions can use their promotional materials to provide better information about the features and risks of these products, especially the risk of payment shock. For example, the guidance recommends that institutions' promotional materials inform consumers about the maximum monthly payment they could be required to pay once interest-rate caps and negative-amortization caps have been reached. The proposed guidance also lists recommended practices to address other risks. When negative amortization is possible, the guidance suggests that institutions alert consumers about the consequences of increasing principal balances and decreasing home equity. If both reduced-documentation and full-documentation loan programs are offered, the draft guidance advises institutions to inform consumers if they will pay a pricing premium for the reduced-documentation loan. When institutions provide monthly statements with payment options, they are urged to include on the statement information that enables borrowers to make responsible choices, by explaining each payment option and the impact of each choice.\n\nIn addition to the draft nontraditional mortgages, the Federal Reserve Board plans to hold several public hearings this summer on home-equity lending. These hearings are a first step to a broader review of mortgage disclosure rules. One of the issues that will be explored at the hearings is likely to be the adequacy of the existing disclosures for nontraditional mortgages, such as interest-only loans and payment-option ARMs, as well as forty-year mortgages and reverse mortgages.\n\nThe hearings also likely will address issues related to predatory lending and market developments since 2002, when the Board last revised its rules for higher-priced loans under the Home Ownership and Equity Protection Act (HOEPA). Concerns about predatory lending continue to be raised, and the hearings could explore the impact of the HOEPA rule changes on abusive lending practices as well as on the availability of subprime credit.\n\nConclusion\nIn carrying out its role as central bank and banking supervisor, the Federal Reserve must continue to ensure that banking institutions operate in a safe and sound manner with a strong capital base. For large, internationally active U.S. organizations, the Federal Reserve believes that the current regulatory capital regime is insufficient. The Basel II framework, we believe, provides more risk sensitivity and a much better link between capital and risk--especially for complex products, services and processes--promotes advanced risk management practices and improves transparency to supervisors, bankers, and markets about the nature of risk exposures and risk management.\n\nBeyond our work on regulatory capital, we encourage institutions to focus on overall improvements in risk management, of which compliance-risk management is an important element. One key message is to continue to make sure the compliance process reflects the changing product and customer mix of the financial institution. Another is that as institutions provide more complex products with features that are not as familiar to the customer, the organization must also improve the clarity of its communications with customers.\n\nFootnotes\n\n1. U.S. Money Laundering Threat Assessment Released.  Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20060331a.htm",
        "title": "An Update on Regulatory Issues",
        "date": "3/31/2006"
    },
    {
        "content": "March 29, 2006\n\nGovernor Susan Schmidt Bies\n\nAt the OpRisk USA 2006 Conference, New York, New York\n\nI would like to thank the sponsors of OpRisk USA for providing an opportunity for bankers, regulators, consultants, and other interested parties to share their perspectives on operational-risk management.\n\nToday, I will speak about the importance of risk management and its relationship to capital. I will also touch on the broad objectives of effective operational-risk management and offer specific observations on some of the challenges of operational-risk quantification. In addition, I will describe the current status of the Basel II process.\n\nImportance of Risk Management\nOver the past several decades, we have witnessed substantial changes in the U.S. banking industry, particularly at our largest institutions. These very large entities have broad geographic reach, operate in many lines of business, and offer a wide array of complex products and services. The largest institutions have moved away from the traditional banking strategy of holding assets on the balance sheet and have adopted strategies that emphasize redistribution of assets and active management of risks. The risk-management techniques employed by banking organizations continue to improve and adapt to the ever-changing financial landscape.\n\nThe Federal Reserve, in its role as both a bank supervisor and the nation's central bank, has an obvious interest in maintaining the stability of the banking industry and the financial system as a whole. We, along with our counterparts at the other U.S. bank and thrift regulatory agencies, are responsible for ensuring that banking institutions operate in a safe and sound manner and have strong capital levels. But with the advent of very large banking organizations that engage in a wide variety of business activities--some of them quite complex--the Federal Reserve has become even more interested in ensuring that banking organizations understand the risks of these activities.\n\nFor their part, bankers continue to improve the risk-management and risk-measurement processes at their institutions, and regulators have supported these efforts. Banks themselves have created many of the new techniques to improve their risk management and internal economic capital measures in order to be more effective competitors and to control and manage their losses. By more clearly defining risk exposures and identifying the causes of and controls for their losses, bank management can more effectively integrate decisions about risk-taking into their strategic and tactical decisionmaking. Banks that integrate risk measurement into their business-line goals often find that this effort helps them to implement their strategic plans more effectively. Ideally, an institution should use a systematic approach to identify and measure its risk exposures; however, even the best processes for evaluating and measuring risk suffer if flawed data are used. To conduct a credible internal analysis of relevant risks, institutions should identify which risks can generally be quantified and which ones cannot. When risk measurements are based on scarce or incomplete data, or on unproven quantitative tools, institutions might need to use sensitivity analyses, stress tests, or scenario analyses to a greater extent in order to develop meaningful risk measures.\n\nBanks that wish to remain competitive must keep up with the latest developments in risk measurement and management. Bankers must ensure that their models keep up with current practice and continue to capture risks accurately, especially as new activities and new products are introduced. Similarly, the supervisory community needs to keep up with developments in banking and finance. We consider this vitally important because banking is and will remain a highly dynamic industry. Supervisors will have to pay attention to evolving sound practices and ensure that new regulations do not unduly inhibit banks from adopting new banking practices and financial instruments. Our focus on balancing developments in the industry with safe and sound operations at institutions is increasingly important, given the growing complexity, sophistication, and concentration of today's banking system. And with the advent of Basel II, which is intended to update capital rules for large, internationally active U.S. banking organizations, supervisors must become even more involved in understanding emerging sound practices in risk measurement and management.\n\nOne of the most important sound practices for a banking organization is the tying of risk exposures to capital. Banks that use similar risk models can have very different risk exposures. That is why the Basel II approach to capital is so important. Basel II provides a framework in which the risk level banks choose to accept is reflected in their capital. Banks with higher loss exposures will be required to hold more capital than those who have lower risk appetites. This provides a strong relationship with risk management, in that returns earned in riskier business lines will necessarily be higher to cover the cost of the additional capital held.\n\nOperational-Risk Management\nFocusing now on operational risk, one of the most substantial changes in the U.S. banking industry in recent years is the movement of the largest organizations toward fee-based revenue streams. These new activities include securitizing loan portfolios, with the bank retaining responsibility for loan servicing; buying and selling financial instruments for customers; and other business lines that generate revenue by charging customers transaction and account processing fees. These activities generate little balance-sheet exposure, but they present the potential for large losses if the complex systems and financial deals associated with them are not managed in a sound manner.\n\nOperational risks are also becoming more important in the large, complex financial institution as more technology and automated processes are used in all areas of operations. When banks used manual processes, errors were confined to the limited area where the employee worked. But in a modern technology setting, factors such as breakdowns in controls, errors in software code, and processing stream interruptions can have enterprisewide effects on the performance of the organization.\n\nRecent history provides us with ample evidence that operational risk can be significant. Large financial institutions have reported operational losses from breakdowns in operating controls that, in some cases, have exceeded their credit- or market-related losses. In the area of legal risk, for example, many institutions have learned that failing to identify and promptly correct problems can result in losses that significantly exceed management's initial expectations. Over the past decade, large financial institutions have experienced more than 100 operational loss events in excess of $100 million each; some of these individual operational losses, resulting from fraud, rogue trading, and settlements stemming from questionable business practices, have exceeded $1 billion.\n\nAn effective operational-risk management framework, therefore, is essential for identifying and managing operational risks. As you know, analysts at the main rating agencies are placing increasing importance on operational risk when they assess a bank's credit ratings. We believe effective operational-risk management has both quantitative and qualitative components and that reliance on solely quantitative or solely qualitative approaches is no longer appropriate. What remains critically important is how these approaches are combined in the implementation of an effective process for identifying, measuring, managing, and controlling operational risk throughout an organization.\n\nEffective operational-risk measurement tools enable the executive management at the largest banking organizations to make better risk-and-return decisions, thereby enhancing the return on their institution's capital investments. By considering operational risk as part of their assessment of capital requirements and true profitability, corporate decisionmakers can better decide which business lines to invest in or shut down. The organization further benefits when operational-risk measurement is integrated with the management processes of individual business units, because this helps communicate risk-management issues to the business lines. The allocation of operational-risk capital to these units provides them with a financial incentive to reduce the chance of operational losses. Hard numbers, linked to specific risks, also allow business lines to more accurately price their products.\n\nSome banking organizations are already benefiting by factoring operational-risk measurement and management into pricing decisions, strategic planning processes, portfolio management activities, management reporting metrics, and decisions regarding incentive compensation. There are potential longer-term benefits as well, including: a reduction in operational losses as control weaknesses are identified and improved; fewer errors and breaks in customer service, which can lead to higher customer satisfaction and retention; stronger information security and customer data privacy; higher credit ratings, and increased operational efficiency.\n\nOne objective of Basel II is to enhance practices at our largest and most complex banking organizations for identifying operational risk exposures and ensuring that these exposures are appropriately supported by regulatory capital. Importantly, the advanced measurement approach (AMA) for operational risk under Basel II allows banks to use a framework that relies on their own qualifying methodologies for identifying operational-loss events and measuring risk exposure in order to determine their operational-risk regulatory capital requirements. In this way, the AMA gives banks the flexibility to continue developing and incorporating evolving sound practices for operational-risk measurement and management into their AMA frameworks.\n\nChallenges to Operational-Risk Quantification under Basel II\nIn the context of Basel II, it has often been argued that measuring operational risk is much more difficult than measuring market or credit risk; however, any model intended to capture unexpected loss has its challenges. For example, market-risk models can be violated when the price of financial products moves in a way that is outside of the historical norm. Credit-risk models need to consider downturn estimates for loss given default, which can depend on the severity of the economic downturn and the price of collateral. Similarly, operational-risk models need to address potential losses that may not have occurred during the short period that most institutions have been collecting internal operational-loss data. This absence of a robust time series of internal operational-loss data is one factor that makes operational-risk modeling particularly challenging.\n\nTo address the difficulties presented by the very nature of operational risk, the designers of operational-risk measurement frameworks have had to be innovative. For example, we have seen frameworks that use scenario analyses, risk self-assessments, and the judgment of senior business managers in innovative ways. We have also seen creativity in the melding of internal and external loss data to guide thinking about internal loss exposures. Perhaps most significantly, we have seen some truly innovative thinking about ways to integrate operational-risk measurement into the broader framework of operational-risk management.\n\nI would like to offer some specific observations on a couple of key challenges relating to operational-risk quantification. First, with respect to operational-loss data, Basel II banks face the challenge of establishing credible operational-loss databases that they can use in determining their regulatory capital requirement for operational risk. The advanced approaches under Basel II create a link between regulatory capital and risk management. Banks using an AMA for operational risk will be required to adopt more-formal, quantitative risk-measurement and risk-management procedures and processes. For example, Basel II establishes standards for data collection and the systematic use of the information collected. These standards are consistent with broader supervisory expectations that high-quality risk management at large complex organizations depends on credible data--and not just for Basel II. Data are needed for all models and risk measures used in financial services, including credit-scoring models, market-based measures such as KMV, and value-at-risk and other economic capital models. The emphasis in Basel II on improved data standards, therefore, should not be interpreted solely as a requirement to determine regulatory capital standards but rather as a foundation for risk-management practices that will strengthen the value of the banking franchise.\n\nAs I mentioned earlier, regulators view capital from the perspective of ensuring safety and soundness in the financial system. But individual financial institutions generally focus on capital, in particular economic capital, as a means for evaluating the profitability of their activities, defining their risk appetite, and setting risk limits. Although the goals differ, there are important linkages between firms' efforts to quantify operational-risk capital for regulatory capital purposes and for strategic decisionmaking. To the extent the operational-loss data considered in banks' internal economic capital models appropriately reflect the banks' risk exposures, banks should be able to leverage their economic capital data collection efforts to measure their operational-risk exposure under an AMA. This leverage is also consistent with the Basel II objective of better aligning regulatory capital with banks' internal economic capital.\n\nThe second challenge I wanted to touch on is banks' integration of insurance in their processes for quantifying operational risk. As many of you are aware, Basel II contains a provision whereby banks using an advanced measurement approach for operational risk could adjust their calculated operational-risk exposure to reflect reductions due to operational-risk mitigants, such as insurance, subject to certain limitations.\n\nAccording to the Basel II framework, a bank's risk-mitigation calculations must reflect the bank's insurance coverage in a manner that is transparent in its relationship to, and consistent with, the actual likelihood and impact of loss used in the bank's overall determination of its operational-risk capital. To the extent banks want to reduce their operational-risk capital charge through the use of insurance, banks must analyze and demonstrate the relationship between specific losses and the ability to collect from the insurer. At the time of the Loss Data Collection Exercise, banks that incorporated insurance benefits into their operational risk capital calculations appeared to do so through an ex-post adjustment to their capital figure in the aggregate, rather than by embedding the specific effects of insurance into the AMA modeling process itself. We expect operational risk managers to work closely with their insurance managers to make better decisions about insurance coverage. This should include clear communication of the nature of individual loss exposures and consideration of the availability of insurance coverage for particular risks.\n\nWhile work remains for those banks that are building their AMA frameworks, we have seen, and continue to see, significant progress in these AMA development efforts. One indication of this progress can be seen in the results of the Loss Data Collection Exercise. This exercise resulted in the submission of over one million internal operational-loss event observations by participating institutions. As you know, internal loss-event data are a key input for determining an institution's regulatory capital requirement for operational risk. These data have provided the agencies with invaluable insights about the comprehensiveness of data at individual institutions. The agencies have provided feedback to participating institutions that should help them in their continuing AMA development efforts. In addition, the agencies continue to analyze the loss data in an effort to provide the industry with additional insights relating to operational-risk quantification.\n\nProposed Revisions to Regulatory Capital Regime\nI have referred to certain parts of the Basel II framework in my remarks so far, mostly relating to operational risk and the AMA. Now I would like to give a brief update on where we stand with implementing Basel II in the United States, as well as with amending the current Basel I regime. First of all, you may have heard that tomorrow the Federal Reserve Board plans to review a draft of the interagency Basel II notice of proposed rulemaking (NPR) at a public meeting, meaning that a draft NPR will also be made available to the public at that time. The final NPR is expected to be issued in the Federal Register after all of the U.S. banking agencies complete their review and approval processes, meaning it will then be \"officially\" out for comment.\n\nWe are pleased that the agencies have reached agreement on the draft NPR, since, as you know, we have spent substantial time and considerable effort on the document. We also recognize the extent to which the industry, Congress, and others have anticipated the release of this document--and the greater detail it contains about Basel II in the United States. Of course, we look forward to hearing feedback on the NPR. Your comments and those from others will contribute importantly to the assessment of Basel II objectives and its implementation, and will help us as we develop the framework further.\n\nReasons for Pursuing Basel II\nI think it is helpful, as we anticipate release of the NPR, to review the reasons we are developing U.S. proposals for Basel II. The current Basel I capital framework, adopted nearly twenty years ago, has served us well but has become increasingly inadequate for large, internationally active banks offering ever-more complex and sophisticated products and services. We need a revised capital framework for these banks, and we believe that Basel II is such a framework.\n\nOne of the major ways in which Basel II should improve safety and soundness is by more closely linking capital requirements to risk. The current Basel I measures are not very risk sensitive and do not provide meaningful measures to bankers, supervisors, or the marketplace for complex banking organizations. Under Basel I, it is possible for two banks with dramatically different risk profiles to have the same minimum capital requirement, and a bank's capital requirement does not reflect deterioration in asset quality. In addition, the balance-sheet focus of Basel I does not adequately capture risks of certain off-balance-sheet transactions and fee-based activity--for example, the operational risk embedded in the services from which many large U.S. institutions generate a good portion of their revenues.\n\nIn addition to enhancing the meaningfulness of regulatory capital measures, Basel II should make the financial system safer by substantially improving risk management at banks. Basel II builds on the risk-management approaches of well-managed banks and creates incentives for banks to move toward leading risk-measurement and risk-management practices; we have already seen some progress in risk management at many institutions in the United States and around the globe as a result of discussions about and preparations for Basel II. The new framework is also much more consistent with the internal capital measures that institutions use to manage their business.\n\nBasel II can also provide supervisors with a more conceptually consistent and more transparent framework for assessing the linkage of risk and capital over time at our most complex institutions; identifying which institutions have deficiencies; and, ultimately, evaluating systemic risk in the banking system. Therefore, Basel II establishes a more coherent relationship between how supervisors assess regulatory capital and how they supervise the banks, enabling examiners to better evaluate whether banks are holding prudent capital levels, given their risk profiles, and to better understand differences across institutions. Compared with the current framework, Basel II is more able to accommodate new products and transaction types and to provide meaningful capital measures for the risks embedded therein.\n\nAs the central bank and the supervisor of banks, bank holding companies, and financial holding companies, the Federal Reserve is committed to ensuring that the Basel II framework delivers a strong and risk-sensitive base of capital. That is why we support safeguards to ensure strong capital levels during the transition to Basel II, and why we will remain vigilant in monitoring the ongoing impact of Basel II. This means that during and after the transition to Basel II, supervisors will rely on ongoing, detailed analyses to continuously evaluate the results of the new framework and ensure prudent levels of capital. To be quite clear, the Federal Reserve believes that strong capital is fundamentally important to the health of our banking system. We believe Basel II will be a strong contributor to our tradition of ensuring that U.S. banks maintain capital levels that provide an appropriate cushion against risk-taking.\n\nAs we have stated before, we will continue to use existing prudential measures to complement Basel II. For example, the current leverage ratio requirement--a ratio of capital to total assets--will remain unchanged for all banks, whether or not they are subject to the Basel II framework. Also, supervisors will continue to enforce existing prompt-corrective-action-requirements in response to declines in capital. Both the leverage ratio and prompt corrective action are fully consistent with Basel II.\n\nProposed Amendments to Basel I\nBefore I end my remarks about regulatory capital, I would like to offer some thoughts about ongoing efforts to revise existing regulatory capital rules, known as Basel I. First of all, we expect only one or two dozen banks to move to Basel II in the near term. That is, the vast majority of U.S. banks would be able to continue operating safely and soundly under Basel I, as it is amended through the rulemaking process. The Basel I framework already has been amended more than twenty times in response to changes in the banking industry and a better understanding of the risks in individual products and services. The agencies believe that now is another appropriate time to amend the Basel I rules.\n\nConcerns have been raised about potential competitive inequities between Basel II banks and Basel I banks. We take these concerns seriously. In an effort to mitigate those concerns, regulators have proposed changes to enhance the risk sensitivity of U.S. Basel I rules and remain vigilant about potential competitive distortions that might be created by introducing Basel II rules. We are also mindful that amendments to Basel I should not be too complex or too burdensome for the multitude of smaller banks to which the revised rules will apply. Additionally, we recognize the need to have full transparency about Basel II proposals and proposed Basel I amendments. Accordingly, we expect to have overlapping comment periods for both the Basel II NPR and the proposed Basel I amendments. The intent is to allow banks and others to review both NPRs before both sets of rules are finalized. In that way, bankers from potential opt-in institutions and from those not planning to move to Basel II can evaluate the potential impact of Basel II in light of the proposed Basel I amendments. In fact, we want all interested parties to compare, contrast, and comment on the two proposals in overlapping time frames. At this point, we are still reviewing the comments received on the ANPR for amendments to Basel I (the comment period ended in mid-January). The agencies are developing their proposals for Basel I amendments, on the basis of comments received, and hope to have a Basel I NPR this summer.\n\nConclusion\nAs prudent supervisors, we need to ensure that banks have strong capital levels--whether banks operate under our current rules, revisions to our current rules, or Basel II. Our focus will continue to be on ensuring that risk-management processes are appropriate for operations of each institution and that those risk systems operate effectively. Our challenge as regulators is to work with the industry in developing an effective capital framework. We envision Basel II as a significant step toward a more risk-sensitive capital framework. We strongly encourage you to comment on all aspects of the Basel II NPR, so that we have a well-informed basis for further development of the Basel II framework.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20060329a.htm",
        "title": "Sound Capital and Risk Management",
        "date": "3/29/2006"
    },
    {
        "content": "March 20, 2006\n\nChairman Ben S. Bernanke\n\nBefore the Economic Club of New York, New York, New York\n\nI would like to thank the Economic Club of New York for inviting me to speak here this evening. I intend to take the opportunity afforded by an audience of experts on global financial markets to address an intriguing financial phenomenon: the fact that, over the past seven quarters or so, tightening monetary policy has been accompanied by long-term yields that have moved only a little on net. Why have long-term interest rates not risen more, as they have done over previous policy tightening cycles? And what implications does this pattern of long-term interest rates have for monetary policy and the economic outlook? As you will see, in my remarks I will do a better job of raising questions than of answering them. In particular, I will conclude that the implications for monetary policy of the recent behavior of long-term yields are not at all clear-cut. I hope you will agree that these questions are nevertheless worthwhile posing, as they are intertwined with a number of important economic and financial issues. I should say at the outset that the views I will express are my own and are not necessarily shared by my colleagues on the Federal Open Market Committee (FOMC).\n\nThe Federal Reserve's Tightening Cycle\nThe tightening cycle that began at the end of June 2004 is notable in at least four respects. First, its onset was delayed for longer than many observers expected. The FOMC kept policy unusually accommodative for an extended, or should I say for a considerable, period. The goal, as you know, was to help ensure that the economic expansion would be self-sustaining and to protect against a remote risk that the fall in inflation observed during 2003 might culminate in outright deflation--an outcome that could have had potentially serious consequences for the economy and for the efficacy of monetary policy. Indeed, with those concerns in mind, in 2003 the Federal Reserve made explicit for the first time that price stability is a symmetric objective: It is important to avoid inflation that is too low as well as inflation that is too high.\n\nA second way in which the most recent experience has been unusual is the extent to which policy actions have been signaled in advance. Both in the months leading up to the initiation of the tightening cycle and during the cycle itself, the statements issued after each meeting of the FOMC provided qualitative guidance about the likely future path of policy and its dependence on economic events. Providing information about the expected path of policy helped to ensure that long-term interest rates and other asset prices did not build in a projected pace of tightening that was more rapid than the Committee itself anticipated, and the statement's focus on the conditionality of future policy actions emphasized the ongoing need for both policymakers and financial market participants to respond to economic news. In retrospect, the clear communication of policy provided notable benefits, in my view, by increasing the effectiveness of monetary policy while minimizing unnecessary volatility in financial markets.\n\nThird, policy moved gradually, tightening in one-quarter point increments over fourteen successive meetings. Together with expanded communication, this gradual approach served to stabilize policy expectations and damp market volatility. In addition, the measured pace of rate increases gave the Committee time to observe the progress of the economy and to adjust its plans and communications strategy accordingly.1 To be sure, gradualism was possible only because inflation expectations remained contained--testimony to the importance of a central bank's retaining credibility in financial markets and among businesses and households.\n\nA fourth interesting aspect of the latest tightening cycle, which is my principal focus this evening, is the behavior of long-term interest rates. Since June 30, 2004, the overnight interest rate has moved up 3-1/2 percentage points, but the ten-year nominal Treasury yield has only edged higher. At less than 4-3/4 percent, that yield is not much above the target federal funds rate of 4-1/2 percent and, indeed, is about even with yields for maturities of one to three years. In the remainder of my remarks I will speculate on the reasons for and consequences of this historically unusual behavior of long-term rates.\n\nThe Recent Behavior of Longer-Term Yields\nSome discussion of the arithmetic of longer-term yields provides a useful perspective on recent developments in bond markets. The ten-year Treasury yield, for example, can be viewed as a weighted average of the current one-year rate and nine one-year forward rates, with the weights depending on the coupon yield of the security. As I will discuss, each of these forward rates can be split further into (1) a portion equal to the one-year spot rate that market participants currently expect to prevail at the corresponding date in the future, and (2) a portion that reflects additional compensation to the bondholder for the risk of holding longer-dated instruments.\n\nCurrent and near-term forward rates are particularly sensitive to monetary policy actions, which directly affect spot short-term interest rates and strongly influence market expectations of where spot rates are likely to stand in the next year or two. Indeed, as we would expect, the recent tightening of policy has been accompanied by increases in both the current one-year rate and next few years' forward rates. For example, since June 2004, the one-year forward rate for the period two to three years in the future has risen almost 1-1/2 percentage points. As the ten-year yield is about unchanged even as its near-term components have risen appreciably, it follows as a matter of arithmetic that its components representing returns that are more distant in time must have fallen. In fact, the one-year forward rate nine years ahead has declined 1-1/2 percentage points over this tightening cycle. Incidentally, by comparing forward rates implied by yields on nominal Treasuries with those implied by Treasury securities that are indexed for inflation, we can infer that about two-thirds of the overall decline in far-distant nominal forward rates over this tightening cycle has been associated with a drop in real yields, with the remainder reflecting a drop in inflation compensation.\n\nIt is important to note that the marked decline in far-forward interest rates has not been confined to U.S. Treasury securities. The spread in yields between Treasuries and longer-term private securities such as corporate bonds is little changed or is down on net since June 2004, implying that essentially all of the fall in forward rates seen in the Treasury market has occurred in private yields as well. These patterns have also appeared in securities not denominated in dollars. For example, over the same period, longer-term government and swap yields in the United Kingdom and the euro area have moved appreciably lower. Indeed, long-term nominal yields have dropped in a number of countries, often by more than in the United States, and the yield curves in many of these countries are also rather flat or even slightly inverted.\n\nSome Reasons for the Decline in Far-Forward Rates\nWhy have the far-forward rates implied by the term structure of interest rates declined in recent years? Observers have offered two broad (and not mutually exclusive) classes of explanations. One set of explanations holds that bond yields are reacting to current or prospective macroeconomic conditions. Another set focuses on special factors that may have influenced market demands for long-term securities per se, independent of the economic outlook. I will first consider explanations that emphasize possible changes in the net demand for long-term securities and later return to explanations that focus on the link between bond yields and the economic outlook.\n\nAs I have noted, each of the forward interest rates implicit in the term structure can be usefully decomposed into two parts: (1) the spot interest rate that market participants currently expect to prevail at the corresponding date in the future and (2) the additional compensation that investors require for the risk of holding longer-term instruments, known as the term premium. With the economic outlook held constant, changes in the net demand for long-term securities have their largest effect on the term premium. In particular, if the demand for long-dated securities rises relative to the supply, then investors will generally accept less compensation to hold longer-term instruments--that is, the term premium will decline.\n\nTo quantify the importance of the shift in the balance of demand and supply and of the consequent change in the term premium, we can appeal to the research literature on the term structure of interest rates. In modern models of the term structure, yields at each horizon are explained by a small number of factors. In some models, these factors can be explicitly tied to observable economic variables, such as inflation; in other models, the factors represent statistical summaries of the data and have no explicit economic interpretations. These factors, in turn, can be used to estimate term premiums at each point in time, although one should clearly acknowledge that the results can be sensitive to various statistical and modeling assumptions.\n\nAccording to several of the most popular models, a substantial portion of the decline in distant-horizon forward rates over recent quarters can be attributed to a drop in term premiums.2 Using some of these models, we can further divide the term premium into two parts--a premium for bearing real interest rate risk and a premium for bearing inflation risk. Both of these components have trended lower over time as well, according to the standard models, but the decline in the premium since last June 2004 appears to have been associated mainly with a drop in the compensation for bearing real interest rate risk.\n\nAt least four possible explanations have been put forth for why the net demand for long-term issues may have increased, lowering the term premium. First, longer-maturity obligations may be more attractive because of more stable inflation, better-anchored inflation expectations, and a reduction in economic volatility more generally. With the benefit of hindsight, we now recognize that an important change occurred in the U.S. economy (and, indeed, in other major industrial economies as well) sometime in the mid-1980s. Since that time, the volatilities of both real GDP growth and inflation have declined significantly, a phenomenon that economists have dubbed the \"Great Moderation.\" I have argued elsewhere that improved monetary policies, which stabilized inflation and better anchored inflation expectations, are an important reason for this positive development; no doubt, structural changes in the economy such as deregulation, improved inventory control methods, and better risk-sharing in financial markets also contributed.3 Whatever the reason for the fall in macroeconomic volatility, if investors have come to expect this past performance to continue, they might believe that less compensation for risk--and thus a lower term premium--is required to justify holding longer-term bonds. In that regard, it is interesting to observe that long-term forward rates were also low in the 1950s and 1960s. With long-term inflation expectations apparently anchored at low levels and with the prospect of continued economic stability, market participants may believe that it is appropriate to price bonds for an environment like that which prevailed four or five decades ago.\n\nA second possible explanation of the evident decline in the term premium is linked to the increased intervention in currency markets by a number of governments, particularly in Asia. According to this explanation, foreign official institutions, primarily central banks, have invested the bulk of their greatly expanded dollar holdings in U.S. Treasuries and closely substitutable securities, and these demands by the official sector have put downward pressure on yields. This interpretation has some support, including research that I did with two coauthors that found that longer-term yields came under significant downward pressure during episodes of heavy official purchases of dollars in 2004.4 And financial-market participants appear to be especially sensitive to any suggestion that foreign official entities may alter their portfolio preferences.\n\nHowever, these observations speak more to the existence of a short-term impact of large purchases and sales--the result of limits to liquidity in the very short run--than to the perhaps more important question of whether those transactions have a lasting effect on yields. On this latter issue, clear evidence is harder to come by. Several pieces of indirect evidence suggest that the long-term effect of foreign purchases on yields may be moderate. Notably, the global market for dollar-denominated bonds is enormous--perhaps around $25 trillion, including dollar-denominated debt issued by other countries as well as debt issued abroad by U.S. residents. In the long run, therefore, the market should be able to absorb purchases and sales of large absolute magnitude with relatively modest changes in yields. Indeed, long-term yields continued to fall over recent quarters even as foreign official holdings of Treasury securities increased at a slower pace than previously.\n\nThe performance of Treasuries relative to that of other fixed-income instruments also argues against a dominant influence of foreign official portfolio decisions on long-term rates. If foreign official holdings of Treasuries were the source of the decline in their yields, then we would expect to observe increased spreads between yields on Treasury securities and the returns to other types of debt less favored by foreign official holders. But we have not seen a significant widening of private yield spreads relative to Treasuries--quite the contrary--and, as I noted earlier, yields in other industrial economies have fallen as well, in many cases by more than U.S. yields. A reasonable conclusion is that the accumulation of dollar reserves abroad has influenced U.S. yields, but reserve accumulation abroad is not the only, or even the dominant, explanation for their recent behavior.\n\nChanges in the management of and accounting for pension funds are a third possible source of a declining term premium. Reforms proposed in the United States, Europe, and elsewhere are widely expected to encourage pension funds to be more fully funded and to take steps to better match the duration of their assets and liabilities. Together with the increased need of aging populations in the industrial countries to prepare for retirement, these changes may have increased the demand for longer-maturity securities. We have seen little direct evidence to date of sizable pension-fund portfolio shifts toward long-duration bonds, at least in the United States. But judging from anecdotal reports, bond investors might be attaching significant odds to scenarios in which pension funds tilt the composition of their portfolios toward such assets substantially over time.\n\nFourth and finally, as investors' demands for long-duration securities may have increased over the past few years, the supply of such securities seems not to have kept pace. The average maturity of outstanding Treasury debt, for example, has dropped by 1‑1/2 years since its peak in 2001, a trend just now beginning to turn with the Treasury's reissuance of the thirty-year bond. Corporations and households, however, have taken advantage of low long-term rates to lengthen the duration of their debt in recent years, which has compensated to some extent for the reduced duration of available Treasury debt.\n\nLong-Term Yields and Monetary Policy\nWhat does the historically unusual behavior of long-term yields imply for the conduct of monetary policy? The answer, it turns out, depends critically on the source of that behavior. To the extent that the decline in forward rates can be traced to a decline in the term premium, perhaps for one or more of the reasons I have just suggested, the effect is financially stimulative and argues for greater monetary policy restraint, all else being equal. Specifically, if spending depends on long-term interest rates, special factors that lower the spread between short-term and long-term rates will stimulate aggregate demand. Thus, when the term premium declines, a higher short-term rate is required to obtain the long-term rate and the overall mix of financial conditions consistent with maximum sustainable employment and stable prices.\n\nHowever, if the behavior of long-term yields reflects current or prospective economic conditions, the implications for policy may be quite different--indeed, quite the opposite. The simplest case in point is when low or falling long-term yields reflect investor expectations of future economic weakness. Suppose, for example, that investors expect economic activity to slow at some point in the future. If investors expect that weakness to require policy easing in the medium term, they will mark down their projected path of future spot interest rates, lowering far-forward rates and causing the yield curve to flatten or even to invert. Indeed, historically, the slope of the yield curve has tended to decline significantly in advance of recessions.\n\nWhat is the relevance of this scenario for today? Although macroeconomic forecasting is fraught with hazards, I would not interpret the currently very flat yield curve as indicating a significant economic slowdown to come, for several reasons. First, in previous episodes when an inverted yield curve was followed by recession, the level of interest rates was quite high, consistent with considerable financial restraint. This time, both short- and long-term interest rates--in nominal and real terms--are relatively low by historical standards.5 Second, as I have already discussed, to the extent that the flattening or inversion of the yield curve is the result of a smaller term premium, the implications for future economic activity are positive rather than negative.6 Finally, the yield curve is only one of the financial indicators that researchers have found useful in predicting swings in economic activity. Other indicators that have had empirical success in the past, including corporate risk spreads, would seem to be consistent with continuing solid economic growth. In that regard, the fact that actual and implied volatilities of most financial prices remain subdued suggests that market participants do not harbor significant reservations about the economic outlook.\n\nAn alternative perspective holds that the recent behavior of interest rates does not presage an economic slowdown but suggests instead that the level of real interest rates consistent with full employment in the long run--the natural interest rate, if you will--has declined. For example, some observers have pointed to factors that may create a longer-term drag on the growth in household spending, including high energy costs, the likelihood of slower growth in house prices, and a possible reversal of recent declines in saving rates. If these drags on the growth of spending do materialize, then a lower real interest rate will be needed to sustain aggregate demand and keep the economy near full employment. To be consistent with a lower long-term real rate, the short-term policy rate might have to be lower than it would otherwise be as well.\n\nGiven the global nature of the decline in yields, an explanation less centered on the United States might be required. About a year ago, I offered the thesis that a \"global saving glut\"--an excess, at historically normal real interest rates, of desired global saving over desired global investment--was contributing to the decline in interest rates.7 In brief, I argued that this shift reflects the confluence of several forces. On the saving side, the factors include rapid growth in high-saving countries on the Pacific Rim, export-focused economic development strategies that directly or indirectly hold back the growth of domestic demand, and the surge in revenues enjoyed by oil producers. On the investment side, notable factors restraining the global demand for capital include the legacy of the Asian financial crisis of the late 1990s, which led to continuing sluggishness in investment in some of those economies, and the slower growth of the workforce in many industrial countries. So long as these factors persist, global equilibrium interest rates (and, consequently, the neutral policy rate) will be lower than they otherwise would be.\n\nConclusion\nWhat conclusion should we draw? Clearly, bond prices, like other asset prices, incorporate a great deal of information that is potentially very relevant to policymakers. However, the information is not always easy to extract and--as in the current situation--the bottom line for policy appears ambiguous. In particular, to the extent that the recent behavior of long-term rates reflects a declining term premium, the policy rate associated with a given degree of financial stimulus will be higher than usual. But to the extent that long-term rates have been influenced by macroeconomic conditions, including such factors as trends in global saving and investment, the required policy rate will be lower. Given this reality, policymakers are well advised to follow two principles familiar to navigators throughout the ages: First, determine your position frequently. Second, use as many guides or landmarks as are available.\n\nIn the context of monetary policy, these principles suggest that policymakers should monitor bond yields carefully in judging the current state of the economy--but only in tandem with the signals from other important financial variables; direct readings on spending, production, and prices; and a goodly helping of qualitative information. Ultimately, a robust approach to policymaking requires the use of multiple sources of information and multiple methods of analysis, combined with frequent reality checks. By not tying policy to a small set of forecast indicators, we may sacrifice some degree of simplicity, but we are less likely to be misled when a favored variable behaves in an unusual manner.\n\nFootnotes\n\n1.  I spoke about this in more detail in \"Gradualism,\" a speech delivered at an economics luncheon co-sponsored by the Federal Reserve Bank of San Francisco (Seattle Branch) and the University of Washington, Seattle, May 20, 2004.  Return to text\n\n2.  Don H. Kim and Jonathan H. Wright (2005), \"An Arbitrage-Free Three-Factor Term Structure Model and the Recent Behavior of Long-Term Yields and Distant-Horizon Forward Rates,\" Finance and Economics Discussion Series 2005-33 (Washington: Board of Governors of the Federal Reserve System, August). Return to text\n\n3.  Ben S. Bernanke (2004), \"The Great Moderation,\" speech delivered at the meetings of the Eastern Economic Association, Washington, D.C., February 20. Return to text\n\n4.  Ben S. Bernanke, Brian P. Sack, and Vincent R. Reinhart (2004), \"Monetary Policy Alternatives at the Zero Bound: An Empirical Assessment,\" Brookings Papers on Economic Analysis (2), pp. 1-100. Return to text\n\n5.  This is consistent with empirical work in Ben S. Bernanke and Alan S. Blinder (1992), \"The Federal Funds Rate and the Channels of Monetary Transmission,\" American Economic Review, vol. 82 (September), pp. 901-21. In that paper, we found that the level of the federal funds rate was extremely informative about future movements of real macroeconomic variables. Return to text\n\n6.  See the discussion in Jonathan H. Wright (2006), \"The Yield Curve and Predicting Recessions,\" Finance and Economics Discussion Series 2006-7 (Washington, D.C.: Board of Governors of the Federal Reserve System, March). Return to text\n\n7.  Ben S. Bernanke, \"The Global Saving Glut and the U.S. Current Account Deficit,\" Homer Jones Lecture, April 14, 2005. Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060320a.htm",
        "title": "Reflections on the Yield Curve and Monetary Policy",
        "date": "3/20/2006"
    },
    {
        "content": "March 16, 2006\n\nGovernor Donald L. Kohn\n\nAt \"Monetary Policy: A Journey from Theory to Practice,\" a European Central Bank Colloquium held in honor of Otmar Issing, Frankfurt, Germany\n\nI am honored to participate in this tribute to Otmar Issing. I have known and admired Otmar for some time. It has been a source of great pride that he has considered me a worthy intellectual sparing partner over the years. From the hiking trails of Jackson Hole to the restaurants of Frankfurt and at many conferences in between, we have challenged each other to state our assumptions, examine the evidence, and adjust our conclusions accordingly. I have derived enormous benefit from that give-and-take--a sentiment, I am sure, that many others in this room share.\n\nI can think of no better way to celebrate the signal contributions of this leading force in the world of monetary policymaking than to address an issue of great importance to central banks, and one that has drawn considerable public attention and comment of late--namely, the proper role of asset prices in the determination of monetary policy. Otmar and I have debated this issue on many occasions, and these discussions--together with recent research carried out at the European Central Bank, the Bank for International Settlements, and elsewhere--have been both challenging and stimulating. The preparation of this talk has afforded me a welcome opportunity to reexamine my thinking on this subject. So, today, I will review the arguments and the evidence as I see them and draw out the conclusions to which I am currently led.\n\nAt the outset, let me stress that I will be expressing my own opinions, which are not necessarily shared by my colleagues on the Federal Open Market Committee.1\n\nTwo Strategies for Dealing with Market Bubbles\nMost fluctuations in stock prices, real estate values, and other asset prices pose no particular challenge to central banks, as they are just some of the usual factors influencing the outlook for real activity and inflation. But many argue that pronounced booms and busts in asset markets are another matter, especially if actual valuations appear to be misaligned with fundamentals. What should a central bank do when it suspects it faces a major speculative event--one that might be large enough to threaten economic stability when it unwinds? To help frame the discussion, I will focus on two different strategies that have been proposed for dealing with market bubbles.\n\nThe first approach--which I will label the conventional strategy--calls for central banks to focus exclusively on the stability of prices and economic activity over the next several years. Under this policy, a central bank responds to stock prices, home values, and other asset prices only insofar as they have implications for future output and inflation over the medium term. Importantly, the strategy eschews any attempt to influence the speculative component of asset prices, treating any perceived mis-pricing as, rightly or wrongly, an essentially exogenous process. Following this strategy does not imply that policymakers ignore the expected future evolution of speculative activity. If policymakers suspect that a bubble is likely, say, to expand for a time before collapsing, the implications of that possibility for future output and inflation need to be folded into their deliberations. Practically speaking, however, I view our ability to act on such suspicions as limited given how little we know about the dynamics of speculative episodes.\n\nDespite its approach to perceived speculative activity, the conventional strategy does recognize that monetary policy has an important influence on asset prices--indeed, this influence is at the heart of the transmission of policy decisions to real activity and inflation. It occurs through standard arbitrage channels, such as the link between interest rates and the discount factor used to value expected future earnings.\n\nThe second strategy, by comparison, is more activist and attempts to damp speculative activity directly. It was described at length in \"Asset Price Bubbles and Monetary Policy,\" an article published by the ECB last year. I quote from the article: \"This approach amounts to a cautious policy of 'leaning against the wind' of an incipient bubble. The central bank would adopt a somewhat tighter policy stance in the face of an inflating asset market than it would otherwise allow if confronted with a similar macroeconomic outlook under more normal market conditions. . . . It would thus possibly tolerate a certain deviation from its price stability objective in the shorter term in exchange for enhanced prospects of preserving price and economic stability in the future.\"2 I am labeling this second approach extra action, as it calls for steps that would not be taken in ordinary circumstances.3\n\nCompared with the first approach, the extra-action strategy responds to a perceived speculative boom with tighter monetary policy--and thus lower output and inflation in the near term--with the expectation of significantly mitigating the potential fallout from a possible future bursting of the bubble. Thus, the strategy seeks to trade off the near-certainty of worse macroeconomic performance today for the chance of disproportionally better performance in the future, on the theory that the repercussions of a major market correction could be highly nonlinear. But the extra-action proposal is by no means a bold call for central banks to prick market bubbles. As the ECB article stresses, such an attempt would be extremely dangerous given the risk that a concerted effort at stamping out a speculative boom would lead to outsized interest rate hikes and recession. Rather, the extra-action strategy is intended only to provide some limited insurance against the possibility of highly adverse events occurring down the road.\n\nCommon Ground\nI will be talking at length about the differences between the two strategies, but I must stress up front how much they have in common. Both policies aim to achieve the same general objectives of monetary policy, using the same broad analytic framework. Most central banks, I believe, share these basic features of monetary policymaking, notwithstanding important differences in their official mandates and the nature of their economies.\n\nAt the risk of considerable oversimplification, policymakers can be described as seeking to set policy over time so as to minimize the present value of future deviations of output from potential and inflation from a desired level. Of course, we may not be prepared to write down a specific loss function and say, \"There, that's what I'm minimizing.\" But our deliberative processes and our actions seem broadly consistent with that characterization of the general problem. This statement is true whether our institutions have a specific mandate to keep inflation low and stable and output close to potential, as in the United States, or whether our mandate is defined primarily in terms of stabilizing inflation, as in the euro area. Stabilizing output complements maintaining price stability in the medium to long run, and often in the short run as well.\n\nWe also can agree that asset prices play critical and complicated roles in determining real activity and inflation--roles that may be changing over time because an increasing share of wealth is market-determined and easily liquified. Movements in stock prices and real estate values affect household wealth and thus consumer spending. Changes in bond prices, stock prices, and exchange rates imply movements in the cost of capital and relative prices that influence investment and foreign trade; exchange rate movements also directly affect the prices of goods and services. Finally, asset prices can affect the value of collateral and thus the provision of credit, thereby influencing aggregate spending. In cases of sharply falling market valuations, these adverse credit-channel effects may even be exacerbated by the deteriorating health of banks and other financial institutions. In sum, asset prices influence the economy in complex and subtle ways over potentially extended periods of time.\n\nFinally, I think it fair to say that most central banks, faced with only a limited understanding of asset prices and their interactions with the full economy, engage in a form of risk management when dealing with market booms and busts. In part, they do this because any particular policy under consideration is never associated with a single forecast for the future paths of output and inflation but, instead, with a large set of possible scenarios with differing odds of coming to pass. While no one uses formal Bayesian methods to solve this difficult problem, I think most policymakers do engage in at least an informal weighing of the various possibilities and their implications when setting policy.\n\nThree Conditions for Extra Action to Lead to Better Outcomes\nNow let me turn from areas of agreement to more contentious issues, ones that have a strong bearing on the relative merits of the two strategies. As the ECB article notes, extra action is often seen as a type of insurance. And as with any insurance policy, before you buy you have to ask whether the expected benefits outweigh the costs. As I see it, extra action pays only if three tough conditions are met. First, policymakers must be able to identify bubbles in a timely fashion with reasonable confidence. Second, there must be a fairly high probability that a modestly tighter policy will help to check the further expansion of speculative activity. And finally, the expected improvement in future economic performance that would result from a less expansive bubble must be sizable. You may be thinking that, in stating the three conditions, I have slanted the case with such phrases as \"reasonable confidence\" and \"fairly high probability.\" But stick with me, and I hope to persuade you that these probabilistic qualifiers are needed to judge the merits of extra action.\n\nFor the moment, let me set aside the first condition and assume that central banks can distinguish an emerging bubble from improving fundamentals at an early stage. What about the other two conditions? Should we presume that a limited application of restrictive policy would materially restrain the speculative boom and make its eventual unwinding less disruptive for the overall economy?\n\nConsider the U.S. stock market boom of the mid-to-late 1990s. The boom was fueled by a sustained acceleration of productivity and an accompanying rise in corporate profits--fundamental changes that justified a major rise in equity prices. How high those prices should have risen was difficult to judge in real time because no one, not investors or central bankers, could be sure how fast profits would grow in the future. In the event, share prices increased more than was justified by improved fundamentals. But overly optimistic expectations for long-run earnings growth were not being driven by easy money, and I see no reason to believe that an extra 50 or even 100 basis points on the funds rate would have had much of a damping effect on investor beliefs in the potential profitability of emerging technologies. At present, we just do not have any empirical evidence of a link between interest rates and corporate equity valuation errors, as opposed to standard arbitrage effects.\n\nIn general, we have a very poor understanding of the forces driving speculative bubbles and the role played by monetary policy. In fact, we cannot rule out perverse effects.4 Again, consider the U.S. experience of the late 1990s. When the FOMC tightened in 1999 and early 2000, the trajectory of stock prices actually steepened, and equity premiums fell--perhaps because investors became more confident that good macroeconomic performance would be sustained. Since mid-2004, we have seen a marked decline in bond-term premiums, even as the funds rate has risen steadily. These episodes illustrate that risk premiums often move in mysterious ways, and we should not count on the ability of monetary policy to nudge them in the intended direction.\n\nPerhaps housing markets differ from equity and bond markets. For example, homeowners, who may have a less sophisticated understanding of the economy than professional investors, might mistakenly view a one-time rise in home prices--resulting, say, from a decline in interest rates--as evidence of a more persistent upward trend. If so, a monetary easing directed at stabilizing output and inflation might, conceivably, drive up real estate values by more than fundamentals alone would merit. Still, you would expect any mis-pricing from these sources to be reversed over time as interest rates returned to normal. In any event, empirical evidence on this issue is scanty. More broadly, further research into the causal connections, if any, between monetary policy and bubbles would seem to be needed before we would know enough to be able to act on such linkages with much confidence.5\n\nHowever, let us suppose a situation arises in which we are convinced that tighter policy would check the future expansion of an emerging speculative bubble. Even then, with the second condition now met, the third condition might not hold: The expected improvement in future macroeconomic performance from moderating the bubble's expansion may not be enough to more than offset the up-front costs of extra action. To explain this statement, I note again that extra action with near-certainty weakens the economy and reduces inflation before the bubble bursts in exchange for the chance of better macroeconomic performance in the future.\n\nAdmittedly, if the worst-outcome scenario associated with an unchecked bubble is judged sufficiently dire and if the scenario is not seen as too improbable, then a risk-averse policymaker might regard the expected return from extra action insurance as worth its upfront cost. However, our confidence in such an assessment would seem to hinge on believing that the effects of market corrections could be markedly nonlinear. Proponents of extra action often cite an increased risk of severe financial distress as a potential source of such effects. However, without the onset of deflation, how large is this risk? In recent history, the health of the U.S. financial system remained solid after the collapse of the high-tech boom, despite the bankruptcy of dozens of telecom and dot-com firms, the loss of more than $8 trillion in stock market wealth, and stress in the nonfinancial corporate sector. Moreover, the financial sectors of most other developed economies also weathered the worldwide drop in corporate equity values fairly well.\n\nOf course, the nonlinear risks associated with a collapsing bubble may depend on the initial health of the financial system, and under some circumstances we could be worried about the potential for significant financial distress to accompany the bursting of a bubble, should that bubble expand further. Even in such cases, however, I wonder whether good prudential supervision in advance and prompt action to clean up any lingering structural problems afterward would not be better ways to deal with this possibility. Certainly, closer oversight of banking systems during the 1980s, including the United States, would have left many economies in a stronger position during the early 1990s. This lesson has been absorbed by supervisory authorities around the world, as evidenced by our successful efforts to strengthen bank capital and our financial systems.\n\nI do agree that market corrections can have profoundly adverse consequences if they lead to deflation, as illustrated by the United States after the 1929 stock market crash and the more recent experience of Japan. But it does not follow that conventional monetary policy cannot adequately deal with the threat of deflation by expeditiously mopping up after the bubble collapses. In Japan, deflation could probably have been avoided if the initial monetary response to the slump in real estate and stock market values had been more aggressive; in addition, macroeconomic performance would have been better if the government had dealt more promptly with the structural problems of the banking sector.6 As for the Great Depression, the Federal Reserve actually worsened the situation by allowing the money supply to contract sharply in 1930 and 1931, after unwisely attempting to prick the stock market bubble in the first place. Rather than demonstrating the need for preemptive extra action to restrain emerging bubbles, these examples are object lessons concerning the wisdom of central banks' easing promptly and aggressively following market slumps when inflation is already low, so as to head off the threat posed by the zero lower bound. By doing so, policymakers should be able to avoid the severe nonlinear dynamics of deflation.\n\nProponents of extra action often argue that it should reduce the risk of hitting the zero bound, but we should recognize that under some circumstances extra action may actually exacerbate the problem. To see this, suppose that a speculative bubble has emerged and that a central bank, operating under a conventional strategy, has raised interest rates to keep the projected output gap closed and expected inflation at its desired level before the bubble bursts. Now the central bank contemplates taking extra action. In a low-inflation environment, such a step would be a bad idea if, after averaging across all the possible outcomes weighted by their likelihoods, the predicted moderation in the bubble from tighter policy is small. In this case, the expected weakening in real activity after the bubble bursts would be only marginally less severe under extra action, but inflation would be substantially lower because the extra action policy would have generated persistent economic slack beforehand. With inflation having already started out at a low level, such a decline would be extremely dangerous because the zero bound would now be much more likely to constrain monetary policy after the bubble bursts. Under these conditions, extra action would therefore worsen expected economic performance, not improve it.\n\nAnother purported benefit of extra action is that, by raising the cost of capital to firms and households, it helps reduce overinvestment fostered during speculative booms, thereby making it easier for the economy to recover after the bubble collapses. However, we should be careful not to exaggerate the macroeconomic importance of such capital mis-allocation. True, the U.S. high-tech boom led to overinvestment in some sectors, wasting resources and creating lingering difficulties while capital overhangs were eliminated. But it is hard to see much of a cost in terms of diminished aggregate productivity, given the robust growth of output per hour over the past few years.\n\nFurthermore, even if tighter monetary policy would have damped the enthusiasm for dot-com firms in the late 1990s, higher interest rates would have also led to less housing and less business investment outside the high-tech sector, where valuations were not obviously out of line with fundamentals. Thus, mitigating capital misallocation in one sector would have created capital misallocations elsewhere, making the assessment of the net gain from extra action difficult. And, as I have been pointing out, extra action would have idled some capital entirely for a time as economic activity fell short of its level consistent with stable inflation.\n\nNow I would like to return to the first condition, the one I sidestepped a few minutes ago--the question of identifying market bubbles in a timely fashion. The ECB article stressed that such identification is a tricky proposition because not all the fundamental factors driving asset prices are directly observable, as the productivity acceleration and stock market boom of the 1990s illustrate. For this reason, any judgment by a central bank that stocks or homes are overpriced is inherently highly uncertain.\n\nUnder extra action, mistakenly identifying a bubble has significant costs. By acting to mitigate a nonexistent problem, the central bank reduces real economic activity and inflation below their desired levels to no purpose. Admittedly, policymakers, once they recognize their mistake, would presumably want the economy to run hotter for a time to restore the previous rate of inflation and would thereby make up for the initial output losses. But coming to the realization that the original assessment was mistaken and that asset prices were in line with fundamentals is likely to take some time. And the mistaken call would have reduced welfare by needlessly inducing fluctuations in the macroeconomy.\n\nTiming is also an issue. Let us suppose that the evidence is so compelling that policymakers become fairly confident that valuations are excessively rich. Unfortunately, I suspect that this call would often come so late in the day that, given the lags in the monetary transmission mechanism and uncertainty about the duration of bubbles, raising interest rates might actually risk exacerbating instability. The market correction could occur with policy in a tighter position but before extra action had enough time to materially influence speculative activity.\n\nNotwithstanding the controversial aspects of identifying bubbles, policymakers may still want to warn the public about the possibility of asset price misalignments when the evidence merits. Such talk might do some good by prompting investors to stop and rethink their assumptions. And talk by itself should not do much lasting harm even if valuations turn out to be justified--provided, however, that words are not seen as precursors to action under circumstances in which conventional policy would still be the best approach.\n\nTo wrap up this critique, I summarize as follows: If we can identify bubbles quickly and accurately, are reasonably confident that tighter policy would materially check their expansion, and believe that severe market corrections have significant non-linear adverse effects on the economy, then extra action may well be merited. But if even one of these tough conditions is not met, then extra action would be more likely to lead to worse macroeconomic performance over time than that achievable with conventional policies that deal expeditiously with the effects of the unwinding of the bubbles when they occur. For my part, I am dubious that any central banker knows enough about the economy to overcome these hurdles. However, I would not want to rule out the possibility that in some circumstances, or perhaps at some point in the future when our understanding of asset markets and the economy has increased, such a course of action would be appropriate.\n\nAsymmetries and Moral Hazard\nProponents of extra action have their own bones to pick with the conventional strategy, especially as it relates to the alleged asymmetric nature of the policy's response to asset market booms and busts. In particular, the claim is often made that, based on the FOMC's actions over the past twenty years, the Fed actively works to support the economy in an event of a sharp decline in asset markets but does little or nothing to restrain markets when prices are rising, thereby creating moral hazard problems.\n\nThis argument strikes me as a misreading of history. U.S. monetary policy has responded symmetrically to the implications of asset-price movements for actual and projected developments in output and inflation, consistent with its mandate. The most convincing evidence for this statement can be found in the results: Interest rates have been consistent with underlying inflation remaining reasonably stable for some time now, accompanied by relatively mild fluctuations in real activity.7\n\nConventional policy as practiced by the Federal Reserve has not insulated investors from downside risk. Whatever might have once been thought about the existence of a \"Greenspan put,\" stock market investors could not have endured the experience of the last five years in the United States and concluded that they were hedged on the downside by asymmetric monetary policy. Nor, for that matter, should they have concluded that the Federal Reserve does not act on the upside: If asset prices had been more closely aligned with fundamentals in the late 1990s, our policy would almost certainly have been easier, all else equal, because aggregate demand would have been weaker and hence inflation pressures even more muted than they were. The same considerations apply to homeowners: All else being equal, interest rates are higher now than they would be were real estate valuations less lofty; and if real estate prices begin to erode, homeowners should not expect to see all the gains of recent years preserved by monetary policy actions. Our actions will continue to be keyed to macroeconomic stability, not the stability of asset prices themselves.\n\nIronically, one can argue that extra action may pose a more significant risk of moral hazard. It is one thing for policymakers to raise questions about the relationship of asset prices to fundamentals and another for a central bank to take action to influence valuations in the direction of some \"appropriate\" level. How does this strategy play out if a central bank takes extra action and speculative activity continues unabated or even intensifies? Do policymakers raise rates even further above levels consistent with conventional policy and, if so, at what consequences for the economy? And what is the risk that, in taking such steps, a central bank would be seen by investors as taking on partial responsibility for asset prices? If so, would the pressure on central banks to support asset prices in market downturns increase?\n\nConclusion\nMy remarks today have been intended as a salute to Otmar and to all the valuable contributions he has made over the years to my thinking and to that of policymakers around the world. Otmar, you have taught me much about intellectual rigor in support of central bank contributions to economic welfare--and about friendship. Weighing the relative merits of extra action and conventional policy is not easy and requires a nuanced interpretation of the arguments and the evidence, as well as some hard thinking. Otmar and his colleagues have done this in advancing the case for extra action, and by so doing they have given me a good intellectual workout--and Otmar, I thank you for that. I hope my response has provided a good workout in return. I look forward to continuing the debate and especially the friendship for many years to come.\n\nFootnotes\n\n1. David Reifschneider, of the Federal Reserve Board's staff, contributed substantially to the preparation of these remarks. Return to text\n\n2. European Central Bank (2005), \"Asset Price Bubbles and Monetary Policy\" (1.7 MB PDF), Monthly Bulletin (April), p. 58. Return to text\n\n3. The article's label for this strategy--leaning against the wind--has been used for many years to describe the standard behavior of central banks. Given this history, I think that using the term extra action is less confusing. Return to text\n\n4. From a theoretical standpoint, the \"rational bubble\" literature demonstrates how a rise in interest rates might lead rational agents to boost the growth rate of asset prices during a speculative episode. Return to text\n\n5. Recently, ECB staff economists Carsten Detken and Frank Smets have taken a laudable first step at addressing this issue in a paper that establishes some of the basic empirical facts about the correlations among interest rates, money, credit, asset prices, financial distress, and macroeconomic performance. See Carsten Detken and Frank Smets (2004) \"Asset Price Booms and Monetary Policy\" (882 KB PDF), European Central Bank Working Paper Series 364, (Frankfurt: ECB, May).  Return to text\n\n6. See Alan G. Ahearne, Joseph E. Gagnon, Jane Haltmaier, and Steven B. Kamin (2002), \"Preventing Deflation: Lessons from Japan's Experience in the 1990s,\" International Finance Discussion Paper Series 2002-729 (Washington: Board of Governors of the Federal Reserve System, June). Return to text\n\n7. As evidence of asymmetry, observers often cite Claudio E.V. Borio and Philip Lowe (2004), \"Securing Sustainable Price Stability: Should Credit Come Back from the Wilderness?\" Bank of International Settlements Working Paper 157 (Basel: BIS, July). The authors purport to show that the federal funds rate was unusually low during the headwinds period of the early 1990s but not correspondingly high before the onset of the 1990 recession. But their assessment is made in relation to the Taylor rule, which is not a particularly good description of monetary policy during this period of opportunistic disinflation. In the event, inflation in the United States came down steadily over the first half of the 1990s, accompanied by significant economic slack--results that seem to belie the claim that policy was overly easy. Return to text",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/kohn20060316a.htm",
        "title": "Monetary policy and asset prices",
        "date": "3/16/2006"
    },
    {
        "content": "March 13, 2006\n\nGovernor Mark W. Olson\n\nAt the Annual Washington Conference of the Institute of International Bankers, Washington, D.C.\n\nThank you for inviting me to speak to you today. Over the past twenty years, I have attended many of these Institute of International Bankers (IIB) spring conferences in Washington, D.C., either as a speaker or a participant. These meetings continue to address interesting and pertinent issues. The consistently larger number of attendees at a meeting here in Washington, as opposed to a meeting in one of the more dynamic international financial centers like New York or Los Angeles, reflects this group's clear recognition that public policy has a direct impact on the viability of your respective institutions. Appropriately, most of the topics presented over these two days cover legislative and regulatory issues intermixed with critical strategic issues. For my presentation, I will revisit a topic initially raised twenty-four years ago by a former president of the New York Federal Reserve Bank, E. Gerald Corrigan. In 1982, Jerry Corrigan, then the president of the Minneapolis Federal Reserve Bank, wrote an essay entitled \"Are Banks Special?\" for the Bank's annual report. Today I would like to revisit the issue and ponder the question \"Are Banks Still Special?\"\n\nI realize that some of you may be wondering why a focus on the specialness of the domestic U.S. banking industry would be an appropriate topic at a meeting of foreign bank representatives. There are several reasons why the subject is relevant to this group. First, U.S. laws and regulations for foreign banks adhere to the fundamental principle of national treatment. That is, our regulations allow foreign banks to have the same range of product authority as domestic banks. Therefore, any change in policy that affects domestic banks will also potentially affect the range of products foreign banks may offer. Another reason why the issue is likely to be of interest to this audience is that, in any discussion of product authority or regulatory structure, the experiences of other nations are often used as examples. These examples are often presented as policies or practices to be avoided or emulated. But the accuracy of the facts presented to support or oppose other countries' experiences is not always clear. At times, these \"facts\" may be undocumented generalities or even wild speculation.\n\nLet me return to Corrigan's essay. In 1982, the banking industry was facing an identity crisis unlike any it had faced since the Depression era fifty years earlier. Within a period of a few months in the mid-1930s, Congress (1) required the separation of banking from many securities functions by passing the Glass-Steagall Act, (2) provided protection for bank depositors by passing the Federal Deposit Insurance Act, and (3) authorized the creation of a dedicated home mortgage lender by passing the Home Owners' Loan Act. Through these laws, Congress established the legal framework that clearly delineated the competitive relationship for the securities, banking, and thrift industries for close to fifty years. This delineation extended to the marketplace for the banking and securities industries. During this period, there were few issues of controversy between banks and securities firms, the most notable exception being the narrowly focused but heated disagreement as to whether banks should be allowed to underwrite revenue bonds. The Glass-Steagall Act specifically authorized banks to underwrite general obligation bonds, but the act was silent on the issue of revenue bonds. The banking industry viewed that as an oversight; the securities industry viewed it as a purposeful exclusion. This debate went unresolved until 1999, when the Gramm-Leach-Bliley Act specifically authorized revenue bond underwriting by banks. In contrast to the relative peace between banks and securities firms, the competition for deposits between banks and thrifts was quite strong and intensified with the imposition of Regulation Q. Not only did Regulation Q establish ceilings on interest rates for both passbook savings accounts and certificates of deposit (CDs), the regulation gave thrifts an interest rate advantage of initially 50, then 25, basis points for each of their interest-bearing products. The intense marketplace competition between banks and thrifts was matched by the level of competition in the halls of Congress. Bankers aggressively worked to eliminate that quarter-point rate differential, while thrifts worked equally hard to preserve it.\n\nSuch widespread competition was nonexistent between the securities and banking industries. From the 1930s to the late 1970s, there seemed to be a clear distinction between investment and savings or at least between investment dollars and savings dollars. Savings dollars were held in passbook accounts or CDs, and were insured up to certain limits by Federal Deposit Insurance Corporation (FDIC) or Federal Savings and Loan Insurance Corporation (FSLIC) insurance. Investment dollars were in equities or bonds. Banks and thrifts waged a turf war against each other, but neither industry aimed its marketing guns or lobbying efforts at the securities industry. Then along came money market mutual funds, or money funds.\n\nHigh interest rates during the late 1970s and early 1980s had a stunning effect on the flow of deposits into banks and thrifts. Money fund shares issued by investment firms offered market interest rates and a high level of liquidity, through either immediate withdrawal or the ability to effect transactions by using payable-through drafts. By contrast, Regulation Q mandated low interest rates on passbook accounts, specific term restrictions on CDs, and significant penalties for early withdrawal. Money funds offered rates that were at times double the rates on passbook accounts. The banking industry had no products to counter the market appeal. Even though money funds shares are not federally insured, billions of dollars flowed into the accounts. In 1981 alone, $109 billion flowed into money funds. After a tepid and wholly unsuccessful effort by bankers to restrict the growth of money funds, the banking industry began considering a different response. Now that the securities industry had successfully breached the separation between banking and savings by allowing consumers to invest in money funds, the banking industry was forced to examine its fundamental marketplace role. During that period, I was the vice chairman, then the chairman, of the Governmental Relations Committee of the American Bankers Association. At meetings held between 1980 and 1982, as many as 400 bankers representing institutions of all sizes and markets gathered to consider an industry response. Congress's response to the growth of money funds was to pass the Monetary Control Act of 1980 and the Garn-St Germain Act of 1982. Taken together, these laws, along with other provisions, eliminated many of the rate and term restrictions on banks' and thrifts' deposit taking and allowed both types of institutions to offer products that could match the rate and liquidity provisions of money funds. But it was clear to many bankers that a lasting response to marketplace changes would require more than just the removal of Regulation Q. Bankers were seeking answers to such basic questions as \"What is a bank?\" and \"What is the public purpose of banking?\" Jerry Corrigan's essay proved to be a beacon of rationality in that discussion.\n\nAre Banks Special?\nCorrigan identified three characteristics that made banks special:\n\n\n\nSeparation of Banking and Commerce\nThe appropriate separation of banking and commerce is a delicate proposition that has both historical and philosophical underpinnings. The term \"separation of banking and commerce\" can mean different things, depending on the perspective of the person using the term. At the time of Corrigan's essay--the early 1980s--the statutory authority for that separation was largely contained in two laws: the Glass-Steagall Act, which separated commercial banking from certain investment and securities activities, and the Bank Holding Company Act (with amendments), which narrowly limited the activities of corporations that were allowed to own banks. When it passed in 1956, the Bank Holding Company Act was thought to have been in response to one company's aggressive expansion into banking activities. That company, Transamerica Insurance, had expanded into several western states and concerns were raised about the resulting concentration of financial resources. Therefore, the ensuing body of law on the commerce and banking issue concentrated significantly on separating, or was inspired by desire to separate, banking from securities or banking from insurance. From the time of Corrigan's essay until passage of Gramm-Leach-Bliley, the banking industry had expanded substantially into both the securities and insurance agency fields, though significant restrictions still remained.\n\nMuch of the industry's expanded securities authority was gained through the Federal Reserve's approval of securities underwriting for banks, under Section 20 of the Glass-Steagall Act. Under the Federal Reserve's rule, a bank subsidiary was \"not engaged principally\" in securities underwriting if at first 5 percent, then 10 percent, and ultimately 25 percent of its revenue came from underwriting. This expanded authority allowed many banking organizations, though almost exclusively large banks, to significantly expand their domestic securities activities. These same powers were extended to foreign banks operating in the United States.\n\nIn insurance, gains for the banking industry were largely the result of court determinations that annuities could be underwritten and sold by banks. Court determinations also expanded national banks' ability to market insurance products. Consequently, banks of all sizes have expanded their involvement in insurance and annuity activities.\n\nBy 1999, banks and bank holding companies had gained significant additional product authority in securities and insurance, areas that had been perceived to be on the other side of the banking and commerce wall at the time Corrigan wrote his essay. A number of factors persuaded Congress that federal action was appropriate, and the Gramm-Leach-Bliley Act was passed. Congress had learned from the securities and insurance experiences of foreign banks, both the high points and the low points. On the one hand, institutions in some European countries had successfully broadened their securities and insurance activities. On the other hand, there were lingering questions about whether some Pacific rim banks were too closely linked to certain commercial enterprises in the region, thus facilitating the Asian financial crisis of the late 1990s. However, the U.S. banking industry had also developed improved ways to manage risk, and banking supervision had become more effective. In addition, the improved analytical capacity offered by new technology, better coordination among domestic and international bank supervisors, a healthy track record for U.S. banks, and the growth of consolidated home-country supervision across the world helped U.S. banks and their affiliates to further expand their commercial activities.\n\nThe response of Congress was essentially threefold. First, it moved the separation wall between banking and commerce to reflect what had already occurred in the marketplace: Gramm-Leach-Bliley also addressed the broader question of what types of businesses could own or affiliate with banks by allowing companies to own banking, securities, and insurance entities within a structure known as a financial holding company. Second, Congress provided a way for banks to gain new-product authority when the new products were determined to be financial in nature, incidental to banking, or complimentary to existing banking authority. Third, by clearly separating the federally insured entity--either banks or thrifts--from the newer and potentially higher-risk new-product authorities, Gramm-Leach-Bliley reaffirmed as a matter of public policy that banks continue to be regarded as special. But the act offers a clear acknowledgment that the separation of banking and commerce is not a bright line but is instead a negotiated compromise--one that will continue to move as markets change and products are refined. The guiding consideration in this compromise will be the protection of the federal deposit insurance fund.\n\nAre Banks Still Special?\nMuch has changed in the banking landscape since Corrigan wrote his essay twenty-four years ago. Significant increases in international capital flows among bank and nonbank entities, in addition to a broad range of specialized financial instruments mean banks can no longer be considered the only source of transaction accounts. Except for their access to the Federal Reserve discount window, banks are no longer the dominant provider of liquidity for other financial industries. But banks remain the key access point to the dominant wholesale payments network, and they still provide federally insured checking and savings deposits. With the rise of new financial services, products, and techniques, moreover, banks have expanded their role in providing liquidity in more indirect ways, for example, through securitization of loans and backup commitments to securitization vehicles and other capital-markets instruments. Even when banks may not be \"special\" or unique providers in a particular market, banks have proven themselves to be formidable competitors and innovators--which only reinforces banks' importance in the proper functioning of our financial system. In short, the public's trust and confidence in banking continue to be vital to our financial well-being.\n\nBanks provided considerable credit in the aftermath of the September 11 attacks, when financial flows were slowed by operational problems. To be sure, banks were able to provide this credit in part because of the huge injection of liquidity provided by the Federal Reserve. But that is a key role for banks in a crisis: to obtain funds--through the discount window or from open market operations, if necessary--and to channel them to those needing funds, based on an assessment of their creditworthiness. Banks' access to the discount window and the payments system, as well as their ongoing relationships with customers and their credit-evaluation skills, allow them to play this role. During a crisis, those banks that play critical roles in the payments system are especially important. As a result, these banks are expected to be very resilient. Though banks now have a smaller role in transmitting monetary policy, they still help to transmit policy actions by arbitraging between the federal funds market and other money markets.\n\nConclusion\nA strong case can be made that banks continue to be special. And because they are special, we, as regulators, will continue to apply high standards to companies seeking a bank charter. We must also continue to examine and supervise banks for safety and soundness. Likewise, it appears that Congress will take a cautious approach when determining what types of companies may own and affiliate with banks.\n\nBut banks must also compete in the marketplace. Consequently, we can expect over time to see adjustments in both the direct activities of banks and in the line separating banking and commerce. History is, in some sense, about the drawing, re-evaluation, and re-drawing of lines. As a matter of public policy, changes will trail rather than lead the marketplace, and any changes must be informed by a careful study of both the role we want banks to play in our economy and the needs of the marketplace.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/olson20060313a.htm",
        "title": "Are Banks Still Special?",
        "date": "3/13/2006"
    },
    {
        "content": "March 10, 2006\n\nVice Chairman Roger W. Ferguson, Jr.\n\nTo the Financial Stability Forum, International Accounting Standards Board, and International Federation of Accountants Roundtable on Financial Reporting and Auditing, Paris, France (via videoconference on February 16)\n\nIt is a pleasure to join you for this joint Roundtable on Financial Reporting and Auditing. As chairman of the Financial Stability Forum, I would like to express my appreciation to all of you that are participating in this roundtable as distinguished speakers or in other roles. In addition, I appreciate the very good partnership that we have had with the International Accounting Standards Board (IASB) and International Federation of Accountants (IFAC)), and with our host, the World Bank, in preparing for this gathering.\n\nSince our last joint Roundtable in October 2004, much has transpired in the international accounting and auditing arenas. Indeed, 2005 was a very big implementation year for international standards around the world. The topics of this Roundtable--dealing with the experience of implementing IASB and IFAC standards and related regulatory challenges, international convergence and harmonization, issues associated with using fair values in the financial reporting model, and risks and vulnerabilities arising from the financial reporting chain--all reflect the interconnectedness of accounting, auditing, and regulation and the need to consider these issues from the unique perspectives that all of you bring to the table. It is very important that this type of constructive dialogue continues to take place among the many key stakeholders in the financial reporting chain.\n\nThe focus of the roundtable--international financial reporting and auditing--continues to gain increased attention from members of the international regulatory community such as the Financial Stability Forum, the International Organization of Securities Commissions (IOSCO), the International Association of Insurance Supervisors (IAIS), and the Basel Committee as well as from national supervisors, standards setters, banks, investors, analysts, auditors, and many others. As we all know, in recent years, accounting and auditing scandals and other developments have found their way onto the front pages of your daily newspapers in ways rarely seen before. These challenging developments have led to a new resolve on the part of governmental, regulatory, and business leaders that there must be a reinforcement and improvement of standards-setting processes and a return to sound accounting, auditing, and disclosure practices by companies and their auditors.\n\nThe unprecedented problems of a few years ago fueled efforts to help restore investor confidence in U.S. and European capital markets through legislative reforms affecting both the accounting profession and corporate management and directors. For example, these efforts led to, for example, the enactment of the Sarbanes-Oxley Act in the United States which set the stage for a new audit oversight authority--the Public Company Accounting Oversight Board--to regulate, inspect, report on, and in some cases enforce penalties against auditors and led to new requirements for corporate management to maintain strong internal controls and to make periodic public reports on the adequacy of their companies' internal controls. In the European Union, the Eighth Directive and other initiatives were undertaken in recognition of the important role of audit oversight authorities. These initiatives seek to strengthen accounting standards, internal controls, and transparency.\n\nBut long before these legislative initiatives were being considered, steps were taken to establish independent international standards-setting frameworks for both accounting and auditing standards, as reflected by the creation of the IASB in 2001 and the IFAC International Auditing and Assurance Standards Board (or IAASB). Both of these standards-setting processes have been reviewed and enhanced over time and continue to be subject to improvements.\n\nIn his keynote speech earlier today, Michel Prada shared with you IOSCO's initiatives to address international accounting and auditing issues. It is clear that IOSCO has played a very important role in these arenas for many years. Likewise, central banks and banking supervisors have taken steps to enhance their focus on accounting and auditing matters that affect institutions they supervise. During the mid-1990s, the Basel Committee started to devote more resources to developing principles that would help shape and improve bank disclosure, supervisory reporting, and accounting practices. Moreover, the committee recognized that it was also important to support high-quality standards-setting processes. With the formation of the Accounting Task Force by the Basel Committee in 1997, the Basel Committee began to participate more actively in processes for setting accounting and auditing standards. This participation led to the development of enhanced accounting guidance on financial instruments and more comprehensive bank audit guidance. The IAIS has also shown a strong interest in the development of sound international accounting and auditing standards, as evidenced by its participation in the IASB's Insurance Working Group and the recent Group of Thirty (G-30) effort to enhance the transparency of reinsurance activities.\n\nAll of these positive efforts by regulators and supervisors reflect a growing recognition that sound accounting policies and meaningful public disclosure by banking and other financial organizations and by nonbanking companies can improve market discipline. With sufficient, accurate, and relevant information, market participants can better evaluate counterparty risks and adjust the availability and pricing of funds to promote better allocation of financial resources. Thus, more-effective market discipline can, in a sense, \"regulate\" the risk-taking activities of banks and other firms in ways that can complement supervision and regulation of financial institutions and foster stable financial markets.\n\nThe concept of market discipline has assumed greater importance among international banking supervisors with the publication of the new International Capital Framework, called Basel II, which seeks to strengthen the market's ability to aid bank supervisors in regulating capital adequacy. As you know, in addition to pillars on risk-based capital requirements (pillar 1) and risk-based supervision (pillar 2), Basel II includes the very important pillar 3, which addresses disclosure of risks and capital adequacy to enhance market discipline. This approach to capital regulation, with its market-discipline component, signals that sound accounting and disclosure will continue to be important parts of the global bank supervisory approach for many years to come.\n\nIn addition to disclosure and market discipline, supervisors and regulators around the world recognize the need for stronger audit and control standards for financial services firms and other companies. The quality of management information and supervisory and financial reporting is dramatically affected by internal control systems, including internal audit programs and reviews by external auditors. Sound supervisory and financial reporting, good internal controls, and quality audits are becoming more important to financial services regulators because they directly affect regulators' ability to promptly identify institutions in distress and work toward a satisfactory resolution.\n\nThis shared understanding of the importance of sound audit and control practices led to an unprecedented level of cooperation among the international regulatory community--in this case, IOSCO, the Basel Committee, the IAIS, the European Commission (EC), the World Bank, and the Financial Stability Forum, which came to be known as the Monitoring Group. Working together, they were able to develop a common position on reforms. This common position served as a basis for dialogue with the leadership of IFAC about establishing a credible independent oversight authority to help enhance the integrity and objectivity of the IAASB's international audit-standards-setting processes. This led to the creation of the new Public Interest Oversight Board of IFAC, which is now busy assuming its new responsibilities. The Public Interest Oversight Board (PIOB) should, over time, lead to enhanced audit standards and practices around the world. The Financial Stability Forum is pleased to have been a strong supporter of this initiative, and I am happy to see that the PIOB is represented at this roundtable today together with the leaders of IFAC and many Monitoring Group representatives.\n\nThe Financial Stability Forum has taken a close interest in national and international efforts to strengthen confidence in financial reporting frameworks. Stronger and more-convergent national systems bring clear financial stability and efficiency benefits. The forum's efforts have included a focus on the global-standards-setting activities of the IASB on the accounting side and the IAASB on the auditing side. The goal of global standards in both areas will require a high degree of convergence between U.S. and international standards, and efforts toward full commonality are well underway. The forum's interest reflects the strong recognition of its members that the potential financial stability and efficiency benefits are significant: high-quality global accounting standards can lead to enhanced transparency that will improve market discipline and foster stable financial markets. Likewise, external audits performed in accordance with high-quality global audit standards can ensure that financial statements are reliable, transparent, and useful to the marketplace, thus enhancing market confidence.\n\nI am sure that we all want to see, over time, a restoration of public confidence in the quality of financial reporting and auditing, and I hope that dialogue--such as that taking place at this roundtable--and the initiatives of standards setters, can lead to standards that are fully understandable to preparers, users, and auditors; that avoid undue complexity and implementation costs; and that truly contribute to transparency. Let me also stress the need to continue to implement approaches that not only improve international accounting and audit standards, but also encourage, through the implementation and enforcement of these standards, an enhancement in actual international accounting and audit practices.\n\nHowever, like any professional undertaking, developing accounting and auditing standards and practices can be complicated, and gaining a shared understanding of the implications for the financial reporting chain and for financial stability can take time. Therefore, I am pleased to see that the Financial Stability Forum, the IASB, and IFAC conducted this roundtable on important financial reporting and auditing matters and that you are taking the time to contribute to the exchange of valuable information on key issues. I hope that this roundtable will continue to foster constructive dialogue among the many key stakeholders represented here and that this type of dialogue will continue after the roundtable has ended.",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/ferguson20060310a.htm",
        "title": "Remarks",
        "date": "3/10/2006"
    },
    {
        "content": "March 08, 2006\n\nChairman Ben S. Bernanke\n\nAt the Independent Community Bankers of America National Convention and Techworld, Las Vegas, Nevada\n\nGood morning. I am pleased to join you today to discuss matters of mutual interest to the Federal Reserve and community banks; to learn more about your business; and, I hope, to meet many of you in person.\n\nCommunity banks have long played a critical role in the U.S. economy, and this is no less true in the twenty-first century. Today, I will begin by making some observations, based in part on research done at the Federal Reserve and elsewhere, about the health of community banks and their evolving role in our economy. Community banks are generally doing quite well, and I expect that good performance to continue. But community banks also face a changing business environment that presents a number of important long-run challenges. In the second portion of my remarks, I will speak a bit about how the Federal Reserve, as the supervisor of many community banks, is also adjusting to a changing environment, and I will review some of the key financial risks facing community banks.\n\nDevelopments in Community Banking\nBy a wide variety of indicators, the overall performance of community banks in recent years has been quite strong. Average return on equity (ROE), for example, following a decline associated with the 2001 recession, remains solid and indeed has shown a slight upward trend. Return on assets for community banks as a whole demonstrates a similar pattern and has stayed well above traditional benchmarks of strong performance. Net interest margins remain higher than those of the largest banks, and this gap has even widened since 2003. Various measures of loan quality for community banks have been robust, and bank failures have been rare. Equally important, both our on-site examinations and our off-site surveillance system, which uses statistical models to attempt to flag emerging weaknesses at community banks, detect signs of potential problems at very few banks. Consistent with this view, community bank capital ratios remain impressively high, and community banks' ability to attract deposits continues to be a source of strength.\n\nOne strong indicator of the continued health of community banks is the rate at which new banks continue to be created. For example, if we define a community bank as any bank or thrift organization with total real (2002) assets of a billion dollars or less, slightly more than 700 community banks were formed from the beginning of 2000 through 2005, an average of about 120 per year. Clearly, many people remain willing to invest in the future of community banking. The Board has long taken the view that community banks will remain a vigorous and innovative sector of the economy. I think that forecast remains a good one today.\n\nAll of this is good news. But I am sure that many in this audience would agree that community banks also face serious challenges. Expansion of the geographic scope of banking activities, rapid technological change in the production of financial services, the increasing importance of nonbank providers, and evolving patterns of economic growth are among the factors that are changing the banking marketplace. And, while many of these changes have improved the efficiency of our financial system and lowered costs for consumers, it is only realistic to acknowledge that they also present new and sometimes daunting tests for community banks.\n\nIndeed, we have seen major shifts in the structure of the U.S. banking industry in recent decades. Under the same definition of community banks that I used a moment ago, the share of banking industry assets held in community banks has fallen from about 20 percent in 1994 to a little more than 12 percent in 2005. In addition, the number of community banks has dropped from more than 10,000 in 1994 to about 7,200 in 2005. Other definitions of community banks and other structural measures, such as the share of total deposits, also show declines in recent years.\n\nMost of this consolidation is a result of mergers. A recent study by a member of the Federal Reserve Board staff shows that between 1994 and 2003 there were more than 3,500 bank and thrift mergers (Pilloff, 2004). In about 92 percent of these mergers, the target institution had one billion dollars or less in total assets. Although bank merger activity has generally declined since the late 1990s, at least 200 deals were completed in each year from 2000 through 2005.\n\nThe Evolution of Relationship Finance\nThese developments notwithstanding, research by our staff and other economists supports the view that community banks continue to play an important role in the provision of financial services, particularly to small businesses, but also to a wide range of retail customers nationwide. Indeed, conventional wisdom in the research community is that \"the central principle of community banking is 'relationship finance'\" (DeYoung et al., 2004, p. 81). By relationship finance I mean financial services whose value-added depends importantly on the ongoing personal interactions of bankers with their customers, interactions that improve the flow of information and allow for more customized services. Relationship finance strengthens the economy by allowing credit and other financial services to be provided more efficiently.\n\nBut recent research also confirms what many community bankers tell us--that traditional notions of relationship finance are changing, along with the nature of community bank-customer relationships.\n\nThe conventional research paradigm included the idea that small businesses and households tend to be informationally \"opaque\"; that is, information about these potential borrowers can be costly to obtain and hard to quantify. According to this view, the efficient supply of credit to such parties required close interactions to elicit \"soft,\" or qualitative, information, such as the personal characteristics of the borrower or relevant aspects of local markets and opportunities. This paradigm holds that large banks have a comparative advantage lending to those relatively transparent customers from which they can obtain \"hard,\" or quantitative, information, such as standardized accounting data, and community banks have a comparative advantage lending to relatively opaque small businesses and households.\n\nHowever, this division of labor between large and small institutions has begun to blur. Today, practitioners and researchers understand that low-cost information processing, improved credit-scoring, and more sophisticated management techniques are rapidly reducing the effective opacity of many small businesses and households. Credit card lending provides an example of this phenomenon. Technological and financial innovation, including credit scoring, securitization, and economies of scale in data processing, have combined to make credit card lending a hard-information, transactions-driven business, quite different from traditional unsecured personal lending, which relies heavily on personal knowledge and relationships.\n\nSome recent data from the Board's forthcoming Survey of Small Business Finances sheds some light on how the marketplace, and the role of community banks, is changing. The Board conducts this survey every five years. Our most recent data, which are still preliminary and will be released later this year, are for year-end 2003; they are the result of interviews with more than 4,200 small businesses that represent an estimated 6.3 million small businesses in the United States. The surveys show that small businesses are heavy users of financial services. For example, the proportion of these businesses using some type of financial service at a bank or thrift rose from 92 percent in 1998 to 96 percent in 2003. Increases occurred across a broad range of financial services and were especially strong in the area of \"financial management services,\" which includes activities such as check clearing, cash management, letters of credit, and credit card processing.\n\nAccording to the surveys, community banks remain an important provider of these services, albeit in an increasingly competitive marketplace. Among small businesses that reported using a bank or thrift in 2003, about 37 percent used a community bank, down from about 42 percent in 1998. Over the same period, the share of small businesses using a financial service supplied by a nondepository institution rose from 40 percent to 54 percent.\n\nAlthough these surveys show that community banks face increasing competition, including from nondepository providers, they also highlight the importance of one of the traditional strengths of community banks: local presence. For example, in 2003 the median distance between a small business's headquarters and its bank or thrift was three miles, about the same as in 1998. Indeed, part of the success of nondepository institutions may have been due to the fact that the median distance between a small-business customer and its nondepository service provider fell from 83 miles in 1998 to 37 miles in 2003, with most of the change resulting from greater proximity of customers to nondepository loan providers. Being close and convenient is important.\n\nData collected as part of the banking agencies' Community Reinvestment Act (CRA) activities also demonstrate the importance of proximity. As you know, the CRA focuses on banks' lending and services provided within their local communities. From CRA and other data, we can estimate the share of loans to small businesses made by depository institutions located physically within the local market area. These data show that between 1996 (the year we began collecting such data) and 2004, the competition from out-of-market lenders has increased, a result that will not surprise you. However, in value terms, the share of small-business loans made by out-of-market firms did not exceed 18 percent in any year. Small-business owners look overwhelmingly to local lenders for credit.\n\nWe see that, for community banks, the overall picture is complex. In financial terms, community banks remain quite strong, and there is considerable entry into the business. New technologies and management methods have eroded some of the traditional informational benefits of relationship finance, however, and community banks have lost market share to larger banks and to nondepository institutions. But the data also show that many customers want to be served locally; they value proximity and convenience. In my view, the strong relationships and personalized services provided by community banks remain an important reason for their continuing success.\n\nSupervisory Perspectives\nLike community banks, bank supervisors must also adapt to a changing financial and economic environment. I would like to discuss some of the ways in which the Federal Reserve's supervision of community banks has evolved in recent years and also briefly review some of the key financial risks that we see in our examinations.\n\nIn the 1990s, bank supervisors began to take a more proactive, risk-focused approach. Under this approach, examiners focus their on-site reviews on those activities that appear to pose the greatest risks to each individual banking organization, with particular attention to the bank's procedures for evaluating, monitoring, and managing those risks. The objective is to address weaknesses in management and internal controls before, rather than after, financial performance suffers.\n\nIn adapting to change, the Federal Reserve and the other banking agencies have also consistently kept in view the competitive pressures that community banks face, pressures that make the costs of regulation an important concern. Whenever possible, we have streamlined procedures and worked to eliminate unnecessary burden. For example, based on industry feedback and supervisory experience, the Federal Reserve recently modified its Small Bank Holding Company Policy Statement to raise the asset size used to define eligible companies from $150 million to $500 million. These revisions address changes in the industry and in the economy since the initial issuance of the policy statement in 1980. While the bank holding companies (BHCs) affected hold only 6 percent of total BHC assets, this change increases the exempt group to roughly 85 percent of all BHCs, thereby providing some burden relief to many smaller companies. These companies will be exempt from consolidated risk-based capital guidelines and will be allowed to file abbreviated semiannual reports in place of consolidated quarterly financial statements. Under the policy statement, the exemption would not be extended to holding companies with significant nonbank or off-balance-sheet activities or that have material amounts of public debt or equity securities outstanding. Of course, we and the other banking agencies will vigorously enforce prudential capital standards for all deposit-taking institutions, including those owned by the exempt BHCs.\n\nSupervisors have sought to adjust regulatory procedures to account for the needs of community banking organizations in other ways. As you are no doubt aware, in tandem with the review of capital standards for the largest banks, known as Basel II, the federal banking agencies are taking a comprehensive look at additional possible changes to existing regulatory capital guidelines for banks that would not adopt the proposed Basel II revisions. These possible changes to Basel I would seek to increase the risk sensitivity of the framework and to help mitigate any competitive inequities that could result from the implementation of Basel II.\n\nThe recent update to the CRA regulations provides another example in which regulators have taken into account the special features of community banks. Last year, the Federal Reserve and other federal agencies issued final CRA rules that reduced compliance burden by creating a new category of intermediate small banks with assets between $250 million and $1 billion. Banks in this new category now face reduced requirements for data collection and reporting, and they have become eligible for a two-pronged set of CRA tests--a streamlined lending test and a community development test--rather than the three-part CRA criteria that larger banks must meet. These changes are intended to reduce the costs borne by smaller banks and to increase flexibility while still achieving the community development objectives of CRA.\n\nTo target examination resources and to limit the burden of on-site reviews, the Federal Reserve also has increasingly relied on automated off-site monitoring tools. For example, since the late 1990s, the Federal Reserve has supervised many small bank holding companies using an off-site review program. We support this program with a targeted monitoring system that seeks to identify parent company and nonbank issues that may adversely affect affiliated insured depository institutions. This program enables us to limit on-site reviews to those bank holding companies with characteristics that could pose risks to insured depositories. We also use statistical models to monitor the condition of state member banks and quickly address any issues that emerge between regularly scheduled on-site examinations. This year, we substantially updated these models to improve their performance. Thanks in large part to such efforts, examiners today conduct more of their supervisory activities offsite, helping to reduce the burden that is associated with on-site examinations at institutions like yours.\n\nBeyond these changes, the Federal Reserve is participating in an ongoing interagency review of banking regulations pursuant to the Economic Growth and Regulatory Paperwork Reduction Act, known as EGRPRA. This review seeks to identify opportunities to streamline regulatory procedures and requirements when such changes would be consistent with maintaining bank safety and soundness. The Board has also supported various legislative changes that would ease regulatory burden. These include a recently proposed change that would permit supervisors to extend the time between on-site examinations to eighteen months for well-managed and well-capitalized banks with up to $500 million in assets. This change would double the current size threshold and has the potential to allow roughly 1,200 more community institutions to qualify for the extended examination cycle.\n\nIn my remaining time, I would like to discuss some of the key financial and risk-management challenges that we have identified through our supervisory activities.\n\nBanking has always been a business of taking and managing risks, but evolving market and economic conditions affect the types of opportunities available. In recent years, community banks have become more focused on commercial real estate lending, leading to a significant shift in the balance sheet and risk profiles of growing numbers of banks.\n\nIn most local markets, commercial real estate loans have performed well. Our examiners tell us that lending standards are generally sound and are not comparable to the standards that contributed to broad problems in the banking industry two decades ago. In particular, real estate appraisal practices have improved. However, more recently, there have been signs of some easing of underwriting standards. The rapid growth in commercial real estate exposures relative to capital and assets raises the possibility that risk-management practices in community banks may not have kept pace with growing concentrations and may be due for upgrades in oversight, policies, information systems, and stress testing.\n\nIn response to these developments, the federal banking agencies have recently proposed guidance that would focus examiners' attention on those loans that are particularly vulnerable to adverse market conditions--that is, loans dependent primarily on the sale, lease, or refinancing of commercial property as the source of repayment.\n\nI emphasize that, in proposing this guidance, supervisors are not aiming to discourage banks from making sound loans in commercial real estate or in any other loan category. Rather, we are affirming the need for each bank to recognize the risks arising from concentration and to have in place appropriate risk-management practices and capital levels.\n\nAdjusting to changes in the level of short-term interest rates can also pose challenges to community banks. Thus far, the relative stability of community bank net interest margins suggests that they have done a good job of managing their interest rate risk exposure throughout the recent increase in market rates. Importantly, most community banks have effectively controlled the maturity distributions of their assets and made significant improvements over the past decade to their management and measurement of interest rate risk. Certainly, the procedures employed by community banks today are significantly more effective than those typically used as recently as a decade ago. However, we continue to see a small number of institutions with concentrations in longer-term assets. In these cases, our examiners encourage banks to gauge the risks of new yield-enhancing strategies over the intermediate and longer terms.\n\nThe unique funding structure of community banks supports their strong recent performance. For the most part, community banks continue to fund themselves primarily with relatively low cost and stable \"core\" deposits. However, a limited segment of community banks is increasing its reliance on wholesale sources of funding. Greater reliance on these sources places a premium on appropriate measurement and management of liquidity risk. Most community banks manage their liquidity risk positions well, but supervisory reviews suggest that some institutions have room for improvement. With the banking system enjoying a period of relatively high liquidity, now is a good time for all companies to assess the adequacy of their processes for managing liquidity risk.\n\nI emphasize that, on the whole, we do not have broad supervisory concerns with community banks. But it is only prudent to reiterate the importance of sound risk management to the continued success of community banks.\n\nConclusion\nIn closing, I want to return to where I began. In my judgment, well-managed and innovative community banks will continue to play a critical role in the U.S. economy. Community banks provide vital services for their customers and are key contributors to sustained economic growth, both locally and nationally. Indeed, the performance of community banks over the past decade has been very impressive. But neither bankers nor their supervisors should become complacent. Doubtless the future will continue to require both of us to evaluate and respond to changes that are often complex and difficult to understand, much less to predict. It has been my pleasure to be here today, and I look forward to working with you in the coming years to ensure the continued vitality of the U.S. banking and financial system.\n\nThank you.\n\nReferences\n\nDeYoung, Robert, William C. Hunter, and Gregory F. Udell (2004). \"Whither the Community Bank?\" Journal of Financial Services Research, vol. 25 (April/June), pp. 81-84.\n\nPilloff, Steven J. (2004). Bank Merger Activity in the United States, 1994-2003, Staff Study 176. Washington: Board of Governors of the Federal Reserve System, May.",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060308a.htm",
        "title": "Community Banking and Community Bank Supervision",
        "date": "3/8/2006"
    },
    {
        "content": "March 03, 2006\n\nVice Chairman Roger W. Ferguson, Jr.\n\nAt the Howard University Economics Forum, Washington, D.C.\n\nI appreciate the opportunity to speak to you today about the macroeconomic outlook for the U.S. economy. I will discuss both the baseline outlook and some of the risks to that outlook. As always, the views that I will be expressing are my own and do not necessarily represent those of my colleagues at the Federal Reserve.\n\nLast year was, in some respects, a difficult one for the American people and the economy. As you recall all too well, the hurricanes in the fall inflicted a terrible human toll, in terms of both the number of lives taken and the dislocation of so many people. On the economic side, the storms destroyed residential and business capital along with critical infrastructure and also disrupted economic activity, particularly in the energy and petrochemicals industries and at ports on the Gulf Coast. These effects also contributed to the sharp increase in energy prices that occurred last year. Both the aftermath of the storms and the influence of the higher energy prices are reflected in the slower growth of real (that is, inflation adjusted) gross domestic product in the fourth quarter of last year.\n\nEven so, over the course of the year as a whole, real GDP rose a bit more than 3 percent, payroll employment increased significantly, labor productivity posted another solid advance, and the unemployment rate moved down further. Increases in overall consumer prices were boosted by the rise in energy prices, but increases in prices apart from those for food and energy--that is core inflation--remained moderate. In light of the challenges confronting the economy, this was quite a favorable outcome.\n\nMoreover, much of the slowdown in growth last quarter reflected factors that are unlikely to persist. First, as I mentioned, the hurricanes significantly damped economic activity in the Gulf Coast region and led to a sharp increase in energy-related imports. Although the recovery of economic activity in the worst-affected areas will take a long time, many of the affected industries, including those in the energy sector, have made considerable progress toward returning their production to normal levels. Second, much of the weakness in consumer spending came from a sharp drop in motor vehicle sales after the end of last summer's employee pricing programs. Third, defense spending, which can be volatile from quarter to quarter, dropped sharply in the fourth quarter. The unwinding of these factors should provide some support to growth of real activity in the near term.\n\nIndeed, the most recent data suggest that economic activity in 2006 is off to a solid start. Payroll employment expanded briskly in January--the latest month for which figures are available--on top of sizable gains over the preceding two months. Although these increases contain some bounceback from the effects of the hurricanes, they also likely reflect underlying strength in labor demand--an impression that is corroborated by the recent low readings on initial claims for unemployment insurance. In addition, the underlying pace of activity in the industrial sector has been quite robust recently. Real household spending continued to climb in January; although unseasonably warm weather that month left an imprint on the data, the result suggests some underlying strength in this sector. Housing activity has, on balance, been a bit softer recently but still remains at a high level.\n\nOverall, the fundamentals appear sufficient to support continued economic expansion. Underlying productivity growth remains strong, the financial positions of households and businesses remain conducive to spending, and, if we have no further run-up in oil prices, the drag on activity from higher energy prices should diminish over time. And the outlook for activity abroad is quite favorable. In Japan, the expansion appears to be broadening, and signs suggest that the Japanese financial sector may finally be stabilizing. Prospects in Europe are gradually improving, particularly in Germany, after several years of sluggish growth. Many emerging market economies also are doing well, with exports providing a significant boost to activity in these countries. These developments should provide some ongoing support to the U.S. economy.\n\nThe Inflation Outlook\nThe continued surge in energy prices was the dominant factor affecting inflation last year. Rising energy prices contribute to consumer inflation in several ways--by boosting prices for gasoline and other energy goods; by raising the price of non-energy goods and services as firms pass on increased energy costs; and by putting upward pressure on expectations of future inflation. Despite those pressures, core inflation has, as I mentioned, remained contained, a result likely attributable to a range of causes.\n\nThe decline in the economy's energy intensity is one of the factors that has restrained the pass-through of energy prices into core inflation in recent decades. As energy prices started to rise in the 1970s, households responded by purchasing products that were more energy efficient and adjusting their consumption habits in other ways. Businesses responded by designing and purchasing capital goods that were more energy efficient and by redesigning production processes in ways that used less energy. One measure of these changes in energy intensity is the ratio of energy use to real GDP, which has fallen more than half since the mid-1970s.\n\nEconometric evidence suggests, however, that the pass-through of energy prices to core inflation has dropped by more than would be implied by the decline in energy intensity. In particular, we often look at forecasting equations for core inflation that include a term for the price of energy, weighted by a measure of energy intensity. Using data for years preceding 1981, the pass-through of energy prices to core prices is large and statistically significant. In the period since 1981, the evidence of pass-through of energy prices to core inflation is more limited. Because the energy-price term in these models already controls for the decline in energy intensity, this result suggests that other factors also are restraining the pass-through of energy prices to core inflation.\n\nAlthough many factors could have led to these results, a likely explanation is that inflation expectations have become better anchored. In the 1970s, monetary policy unfortunately allowed large increases in energy prices to have a persistent effect on inflation, a policy that undercut the Fed's credibility and caused long-run inflation expectations to be more volatile. Since that time, however, the Federal Reserve has been more aggressive in fighting all sources of inflationary pressures, including energy price changes. This effort appears to have paid off not only in low and stable inflation but also in a reduction in the sensitivity of long-run inflation expectations to energy prices. The reduced sensitivity is evidenced by how little movement has appeared in survey measures in response to the rise in energy prices over the past two years.\n\nThis same tendency can be seen in longer-horizon measures of inflation compensation derived from a comparison of yields on nominal Treasury securities and those on Treasury inflation-protected securities (TIPS), which are indexed to a measure of price change. Specifically, for the period five to ten years ahead, the TIPS-based measure of inflation compensation has remained well anchored in recent quarters. Moreover, econometric evidence suggests that since early 2004, energy prices have had only a modest effect on TIPS-based inflation compensation at relatively longer horizons. Because inflation-indexed securities were not issued in the 1970s and early 1980s, we cannot know for sure how these recent effects differ from those that might have operated earlier, but I believe that the difference would be stark.\n\nAll told, increases in energy prices over the past couple of years probably added about 1/2 percentage point to core inflation in 2005, and the lagged pass-through of past increases in energy prices appears likely to add roughly the same amount this year, provided that energy prices do not rise significantly further.\n\nThe Term Structure of Interest Rates\nAnother development that has received considerable attention recently is the term structure of interest rates--the yield curve. Typically, longer-term interest rates are higher than short-term rates, so a curve plotting yields would rise as maturity lengthens. However, since late last fall, yields on longer maturities have been equal to or less than those at some shorter maturities, creating a flat to inverted yield curve. Going back to the 1950s, a simple picture suggests that the yield curve tends to invert before recessions. In addition, some academic research, along with recent market commentary, suggests that the shape of the yield curve is a strong predictor of future economic growth.\n\nHowever, the Treasury yield curve is now only slightly inverted between one and five years and is roughly flat beyond that. Moreover, yield curves can be flat or inverted either because short-term interest yields are relatively high or because long-term rates are relatively low. Historically, flat or inverted yield curves owing to unusually high short-term rates have tended to be followed by slowdowns, but that has not been the case for those episodes of inverted yield curves owing to relatively low long-term rates. And, in the current situation, the flatness of the term structure results largely from relatively low long-term yields.\n\nIn addition, the relationship between the yield curve and future economic growth may have weakened in recent decades because the decline in inflation, the dropoff in the variance of economic growth since the mid-1980s, and financial innovation may have altered the sensitivity of households and businesses to changes in rates. Indeed, in simple regression models estimated with recent data, a change in the slope of the term structure has a smaller effect on economic growth than when the models are estimated with data taken from a longer sample period. And models using the more recent data anticipate more rapid real GDP growth in coming quarters than do models using the longer sample period.\n\nThe relationship between the yield curve and future growth also appears to depend on the factors that are keeping long-term rates low. Long-term interest rates embody both expectations of future interest rates as well as the amount of compensation demanded by investors for the risk of unanticipated movements in real interest rates and inflation--that is, the term premium. By historical standards, the term premium appears to have come down significantly in recent years and to be quite low. A flat yield curve resulting from low term premiums should have quite different implications for future growth than would a flat yield curve resulting from tight monetary policy. Indeed, an exogenous decline in term premiums would make financial conditions more accommodative and would, other things being equal, be followed by higher growth. To account for these developments, an estimate of the term premium can be added to a simple regression model of the relationship between real GDP growth and the slope of the yield curve. Forecasts of future real GDP growth from this extended model are higher than those from models that include only the slope of the yield curve. And the forecasts from this extended model currently are very close to the private-sector consensus forecast.\n\nThus, we have reasons to believe that the current configuration of the term structure is not signaling an economic downturn. At the same time, the amount of economic stimulus arising from low long-term rates is probably not especially large. Indeed, low long-term rates may be signaling relatively low demand for capital around the world relative to saving, which in turn may reflect a lack of sufficiently attractive investment opportunities. For example, in the United States, the nominal share of business fixed investment in GDP has risen recently only to about its long-run average--a level arguably somewhat lower than might be expected given the low level of long-term interest rates. Because interest rates equilibrate the supply and demand of capital, an excess of desired saving relative to desired investment would tend, all else equal, to hold down long-term rates. In turn, low rates have stimulated activity in areas outside of business investment: Housing has been boosted significantly, and consumer spending may also have received some additional impetus.\n\nSome Risks to the Outlook\nAll told, the U.S. economic expansion appears to be solidly on track. Nevertheless, the outlook for real activity faces a number of significant risks, including the possibility that house prices and construction could retrench sharply and that energy prices could rise significantly further.\n\nHousing construction has been a significant source of strength in this expansion, and consequently, some analysts have suggested that a correction in this sector could take a big bite out of growth. By my reading, the incoming data suggest that the housing market has begun to cool somewhat, but they do not point to a sharper falloff. Sales of both new and existing homes have declined in recent months, although they remain at a high level; and after cutting though volatility likely related to swings in weather, housing starts appear to have softened recently. Moreover, other indicators of activity, along with anecdotal reports, also seem consistent with an easing, but not with a sharp downward correction.\n\nOf course, house prices may become an area of vulnerability. House prices have increased at a remarkable rate during the past several years, and for some fundamentally sound reasons, including low mortgage rates. However, the possibility remains that the recent run-up in prices may be greater than can be justified by the fundamentals and that increases in house prices may moderate or undergo a sharper adjustment. The latest data on house prices--including the figures released this week--provide a hint that a moderation in house prices, and nothing more serious, may now be under way.\n\nThe primary channel through which a deceleration or downturn in housing prices would be likely to affect the economy is the so-called wealth effect. That is, the path of house prices directly affects the value of housing wealth, and changes in wealth influence households' consumption and saving. Estimates from the Federal Reserve Board staff's large econometric model and from various consumption equations suggest that wealth effects are somewhere in the neighborhood of 3-1/2 cents on consumption for every dollar of change in wealth, with roughly half the effect realized within a year.\n\nHowever, these estimates are uncertain, and plausible estimates of the wealth effect range from about 2 cents to 6 cents for every dollar of change in wealth. Moreover, these estimates are obtained from equations that look at changes in total wealth, whereas historically, much of the variation in wealth has reflected movements in equity prices. Although efforts have been made to isolate the effects of changes in different types of wealth, it is difficult to get precise estimates of wealth effects specifically for real estate. And, it is always possible that the effects of housing wealth on consumption might have changed in ways that would be hard to identify using standard econometric modeling techniques. For example, the effects could, perhaps, have increased in the last decade as a wave of financial innovation made it easier and less costly for households to tap accumulated housing equity.\n\nA decline in consumer confidence is another channel through which a correction in house prices could affect the economy. In the current situation, a sizable deceleration in house prices could have an outsized effect on consumer confidence and thereby reduce household spending by more than is implied by conventional estimates of the wealth effect.\n\nAnother possible avenue for gauging the effects of housing prices on the economy is to look at experiences in other countries. House prices have risen markedly in recent years in many industrial countries amid low long-term interest rates, ample liquidity, and steady economic growth. Although movements in real house prices flattened out or turned down in several countries in the first half of 2005, they have since recovered for the most part. Of the countries that have seen recent booms in house prices, Australia has experienced a decline in real house prices over the past year; real prices are down about 3 percent since their peak at the end of 2003. During this period, residential investment contracted, and the growth of real consumption slowed a bit; but the economy continued to grow, in part because investment in other sectors picked up in response to strong global demand.\n\nMore countries have experienced a slowdown in rates of increase in home prices than experienced an outright decline. For example, in the Netherlands--where residential property prices rose particularly rapidly in the late 1990s--the rate of increase slowed significantly beginning in 2000. In 2002 and 2003, the economy experienced a mild recession, with domestic demand contracting as a result of declines in investment (including residential investment) and contracting consumption. But, even so, the economy started to recover in 2004 and 2005, and house prices there have continued to rise in recent years.\n\nIn the United Kingdom, house prices also flattened out last year, and this deceleration was accompanied by a slowdown in real consumption growth. More recently, house prices have started rising again, and consumption growth has also picked up.\n\nThe experiences in Australia and the United Kingdom could be taken as suggesting that adjustments in house prices can be associated, on balance, with continued modest economic growth, while the Dutch experience paints a slightly more pessimistic picture. Having said that, one difficulty in interpreting the foreign evidence is that gauging the direction of causality is difficult--that is, are adjustments in house prices causing a slowdown in real economic growth or is a slowdown in activity causing the adjustment in house prices. On this point, a recent Federal Reserve study of international experience documented the pro-cyclicality of real house prices: House prices have tended to reach a maximum near business cycle peaks, with real GDP growth slowing during the first year or so after house prices peak.1\n\nGiven the limits of what we know about the future path of housing prices and about the implications of any particular house-price scenario for real activity, the Federal Reserve will have to continue monitoring this area closely.\n\nFurther increases in energy prices are another risk to the economic outlook. The spot price of West Texas intermediate crude oil rose from around $30 per barrel in December 2002 to around $65 per barrel in mid-August 2005, shortly before Hurricanes Katrina and Rita made landfall. Since the storms hit, prices have fluctuated widely in response to developments in both domestic and foreign oil markets and recently stood a bit below pre-Katrina levels. Over the same time period, prices of far-dated futures contracts have also risen a substantial amount. As oil prices have pushed higher since late 2003, prices of natural gas also have trended up sharply. And storm-related disruptions to natural gas production as well as weather patterns this winter have caused considerable volatility in natural gas prices.\n\nEconomic theory suggests that energy price hikes of this magnitude should have an important contractionary effect on the economy by reducing the purchasing power of households and holding down business profits outside the energy sector. Persistently higher oil prices also likely reduce labor productivity and potential output over time as firms adjust their production processes to use less energy. As for the reduction in aggregate demand, higher energy prices increase the bill for imported oil and natural gas, which can be viewed as a \"tax\" on U.S. residents by foreign energy producers, thereby holding down aggregate demand. Given the rise in energy prices since 2003, the import \"tax\" has risen more than $150 billion annually. Although they are imprecise, simulations from the Federal Reserve Board staff's large-scale econometric model, which account for these effects, suggest that increases in spot and futures prices of energy from late 2003 to the present subtracted a 1/2 percentage point from real GDP growth in 2004 and more than 1 percentage point in 2005. The model suggests the subtraction this year will be about a 1/2 percentage point.\n\nIn addition to their effect on economic activity, further increases in energy prices also pose a risk to the inflation outlook. Looking ahead, the path of far-dated futures prices for oil indicates that markets are not expecting prices to rise significantly further. However, given strong global demand for energy resources and the ever-present risk of supply disruptions, additional increases in energy prices cannot be ruled out. Such increases would boost the overall inflation rate and might put additional upward pressure on production costs and inflation expectations, which in turn, could create forces that would tend to push core inflation up. If that were to occur, the Fed would need to be particularly vigilant to ensure that inflation remained under control.\n\nWhere Do We Go From Here?\nAs you know, the Federal Reserve seeks to foster price stability and to promote sustainable growth in output, and the members of the Federal Open Market Committee (FOMC) are committed to achieving these objectives. Translating these general economic objectives into operational decisions about monetary policy poses many challenges, and no simple toolkit of economic and financial indicators or economic models can provide reliable guidance at all times. Rather, the FOMC must assess the implications of a wide range of developments and data as well as rely on the best modeling that the economics profession can provide.\n\nIn the current situation, the economic expansion appears to be on track and core inflation has remained moderate. As I indicated, significant risks, if realized, could alter this generally sanguine outlook, and the Federal Reserve will continue to monitor developments closely. Given the considerable uncertainties facing the economy and the outlook for policy, policy decisions in coming months will depend heavily on the implications of incoming economic data for future growth and inflation.\n\nFootnote\n\n1. Alan G. Ahearne, John Ammer, Brian M. Doyle, Linda S. Kole, and Robert F. Martin (2005), \"House Prices and Monetary Policy: A Cross-Country Study,\" International Finance Discussion Papers 2005-841 (Washington: Board of Governors of the Federal Reserve System, September). Return to text",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/ferguson20060303a.htm",
        "title": "Economic Outlook for the United States",
        "date": "3/3/2006"
    },
    {
        "content": "February 24, 2006\n\nVice Chairman Roger W. Ferguson, Jr.\n\nAt the commemoration of Black History Month, The Johns Hopkins University Applied Physics Laboratory, Laurel, Maryland\n\nI am pleased to have the opportunity to be part of the Applied Physics Laboratory's commemoration of Black History Month. Your theme, \"Celebrating Community: A Tribute to Black Technical, Educational, and Social/Civic Institutions,\" aptly highlights a number of the key building blocks that have enabled many African Americans to fulfill their personal dreams. In that regard, I would like to focus my talk today on education--its importance and its ongoing role in economic achievement.\n\nAs members of an organization dedicated to cutting-edge scientific research and development, you undoubtedly deeply appreciate the ongoing need for our nation's workforce to embody advanced levels of training. Investment in human capital--as we economists like to call it--is critical to generating products and services with high economic value. Today, much of that high-value output demands workers with the creativity, cognitive abilities, and skills to interact with challenging technologies. In addition, ongoing innovation requires workers to be flexible and to be willing to view education as a life-long commitment. In short, an educated workforce is a must if our economy is to continue to enjoy significant gains in productivity and living standards.\n\nAt the same time, the link between education and individual economic success is well documented. An investment in education is associated with a higher probability of employment. For African Americans, a college degree can substantially narrow the longstanding gap between their labor market experiences and those of whites. Last year, for example, when the national unemployment rate averaged 5.1 percent, the jobless rate for black adults (25 years and older) with a bachelor's degree or higher was 3.5 percent; for white adults, the jobless rate was 2 percent. For persons with only a high school diploma, both the rates of joblessness and the disparity between the rate for blacks and that for whites were greater: an unemployment rate of 8.5 percent for blacks versus 4 percent for whites.\n\nPerhaps more indicative of the economic value of education, workers with college degrees earn an education premium, and that premium has risen over the past twenty-five years. Most economists have found that an additional year of schooling typically raises an individual's earning power between 8 and 15 percent. Recent studies show that four years of college boost earnings about 65 percent.1\n\nClearly, economic achievement and educational achievement are intertwined. For that reason, education is at the heart of efforts to promote equal opportunity for all Americans. We have made some progress in opening doors to education for African Americans; we must make more.\n\nAs I reflect on the educational attainment of black Americans, I would say that the news is still mixed. The percentage of African Americans aged 25 to 29 who have completed high school or obtained a GED remains on an uptrend. But the improvements slowed over the 1990s, and in 2004 it remained, at close to 89 percent, short of the rate for non-Hispanic white youth, which was just over 93 percent.2\n\nOne important factor in the uptrend in high-school completion has been a corresponding downtrend in the high-school dropout rate for African Americans. Here again, however, the improvement has been slower recently than in the 1970s and 1980s. In 2003, 6.3 percent of black students in grades 10 to 12 left school during the year--down from the over-the-year dropout rate of 9.7 percent in 1981 but little changed from the rate in 1991.3 For white high schoolers, the dropout rate between 2000 and 2001 was 4.1 percent.\n\nEconomists have identified a number of reasons for the racial gap in dropout rates, including lower expected returns to education because of discrimination in the job market and the lower quality of schools attended by blacks. Clearly, raising the quality of our elementary and secondary schools is a longstanding goal, and the potential economic and social payoffs seem likely to be high. Research has shown that improving school conditions in the South from the early to the middle part of the twentieth century contributed significantly to greater school completion rates by African Americans.4 Other findings suggest that the support and encouragement that students receive from their families and communities can also help keep students engaged in school. In recent years, a number of economists, including staff members of the Federal Reserve Bank of Minneapolis, have argued that intensive pre-school programs can help to build important noncognitive skills, such as persistence and motivation and, as a result, have large private and public net benefits.5\n\nOf course our job is not done when our students reach high school graduation--indeed, in today's economy, it is only just beginning. Our goal must be to see that our investments in motivating and educating students in our homes and in the elementary and secondary schools provide the students with the ability to pursue the advanced education and training that today's labor market values so highly.\n\nCertainly, the trends in college-attendance by blacks have been positive in recent years, but compared with the uptrend in high-school completion, progress in college completion among African Americans has unfortunately been relatively slow. The percentage of the black population aged 25 to 35 that has completed four years of college more than doubled between 1970 and 2000, from 6.5 percent to 15 percent. However, completion rates for young white adults, which were already much higher, climbed even more rapidly--to almost 33 percent in 2000.6\n\nThe difference in college completion by race reflects both lower rates of college enrollment and lower rates of graduation by African Americans. Of African Americans aged 18 to 24, the percentage enrolled in college is 10 percentage points lower than the percentage of non-Hispanic whites enrolled.7 The difference in graduation rates for those students who enroll in college is particularly striking. Of those students who enrolled as first-time students at a four-year institution in the 1995-96 academic year, approximately 62 percent of whites had completed a bachelor's degree by 2001, whereas only 43 percent of blacks had done so.8\n\nResearchers offer several potential explanations for the difference in college graduation rates by race. The extent to which family income or borrowing costs significantly constrain the decision to enroll in college has been hotly debated by economists. 9 The common ground in the debate seems to be that needy students who are capable should be helped financially. But money alone is not the answer; students must receive the support, encouragement, and preparation from their teachers, families, and communities that will make the transition from high school to college successful. Research has shown that blacks have a higher rate of college attendance than whites and a similar rate of college completion when the comparison is made across individuals with similar educational achievement in high school.10 This finding provides yet another reason for greater investment in the quality of secondary schools. Such investment may increase not only rates of high-school graduation but also rates of college completion.\n\nAs I noted earlier, the linkage between education and economic opportunity is typically measured by the relationship between education and earnings. Earnings are an important measure of one's success in the labor market, but broader measures of income and ultimately net worth are even more significant yardsticks for gauging the financial health of households. The Federal Reserve's Survey of Consumer Finances, which collects data every three years on the balance sheets of American families, provides comprehensive information on household income, assets, and liabilities.11 Data from the most recent survey show that, from 2001 to 2004, real (that is, inflation-adjusted) family income was little changed for both African American and non-Hispanic white households. And the gap between median incomes for the two groups remained quite large. Specifically, the median income of black households was about $29,000--only 58 percent of the median for non-Hispanic white households. The gap was somewhat narrower among households headed by an individual with a college degree; median income for African American households in this group was close to $54,000--75 percent of the median for non-Hispanic whites. Although black households have gained ground over the past decade, income inequality continues to be a concern. Lower income makes it more difficult for black families to acquire assets and to create wealth.\n\nThe 2004 results show that economic progress for blacks, as measured by real net worth, has been substantial over the fifteen years that the surveys have been conducted. Real median net worth for African-American households in 2004, at $20,400, was more than three and one-half times as great as it was in 1989. That said, the wealth gap between blacks and non-Hispanic whites, whose median real net worth stood at $140,700 in 2004, remains sizable. A substantial part of the wealth gap between black and non-Hispanic white families is associated with their ownership of assets. Although the racial wealth gap is significant at the top of the wealth distribution, a more important difference is that a much greater proportion of African-American families than whites have zero or near-zero real net worth.\n\nThe increase in ownership of nonfinancial assets for black households in recent years occurred primarily in residences, other real estate, and privately held businesses. Because none of these types of assets is owned by a large share of black families, any wealth gains arising from them will not be widely distributed across black families. Nonetheless, blacks continued to make progress in homeownership in 2004. As is the case regardless of race, the home is typically a family's largest and most important asset. Homeownership is one of the cornerstones of wealth creation and is generally associated with a range of socially desirable outcomes, including better schools, less crime, and greater neighborhood stability. For these and other reasons, increasing the rate of homeownership has been a longstanding national priority. Of course, because we are interested not simply in homeownership or the value of homes but in net worth, an important consideration in terms of wealth creation is the amount of equity that families have in their homes--that is the difference between the value of the home and any debt secured by it. Over the most recent survey period, during which property values have risen rapidly, the median value of home equity for African-American homeowners increased an impressive 24 percent.\n\nBusiness ownership, too, remains an important avenue of wealth creation for African Americans. The median net worth of black families with business assets was about $174,000 in 2004, a level more than eight times the median net worth for all black families. Furthermore, the survey results show far less inequality in median net worth and income between black and non-Hispanic white business owners than between black and non-Hispanic white families overall.\n\nAll told, the findings from our most recent survey, along with the other trends that I discussed earlier, highlight noticeable gains in the economic well-being of African Americans. However, they also clearly show that much more needs to be accomplished, and I believe that education is the key to further progress.\n\nGiven the importance of education in today's economy, I was encouraged to see that the Applied Physics Laboratory is committed to a number of programs that engage minority students at all levels of schooling. These programs are aimed at developing a commitment to lifelong learning--beginning with an introduction to math, science, and technology in our primary and secondary schools. In light of the differences in college completion rates I noted earlier, I was glad to see that APL offers internships for talented minority undergraduates in computer science and engineering that give them opportunities to do research and to be mentored by professionals in their fields and that it provides support for graduate study in engineering.\n\nI was also interested to learn that many of your undergraduate interns are students from historically black colleges and universities (HBCUs). To borrow a phrase from Juan Williams, those very special institutions have for decades had as their central mission helping African Americans \"find a way or make one.\"12 And, even though black enrollment at other colleges and universities has risen over time, HBCUs continue to account for more than one-fifth of all bachelor's degrees awarded to African Americans.13 Among those schools and their graduates are many with a longstanding commitment to scientific and technical education, dating from George Washington Carver's tenure at Tuskegee to Julian Earls' work at NASA. In this region, we are fortunate to have a fine group of HBCUs: Howard and Morgan State, both highly regarded across a range of curricula; Bowie State, which specializes in training black students for masters degrees in computer science; Maryland Eastern Shore, with its emphasis on marine and environmental science; and Coppin State, known for its nurturing of students as they build the fundamental skills that allow them to move on to more-advanced work.\n\nAlthough most of my remarks today have centered on the economic value of education, I want to emphasize that a good education is much more that just the classroom-based learning of facts, or even the skill of critical thinking. Formal education is just the starting point for a lifetime of learning and doing. And a truly outstanding education is one that instills in students moral values and ethical behaviors. In striving to encourage our students to do \"well,\" we must not forsake our responsibility to give them a solid grounding in those topics that will help them do \"good.\" You may be surprised to hear that even the economics profession--well-known for its hard-headed assumption of rational actors pursuing their own self interest--has in the past few decades focused on the role of moral and cooperative behaviors in leading to better economic outcomes. The accounting and corporate governance scandals in recent years have revealed how costly such unethical and opportunistic business dealings can be, potentially to all of us.\n\nLet me close by saying that the economy of the United States depends greatly on an educated workforce--one with the skills to tackle new ideas and new technologies, one in which morals and ethics are deeply instilled, and one with a love of learning, exploring, and questioning that lasts a lifetime. The African-American community's commitment to education as a path to equal opportunity dates back at least to Frederick Douglass. We all must resolve to keep that longstanding commitment strong.\n\nFootnotes\n\n1.  Robert Topel, \"The Private and Social Values of Education (891 KB PDF),\" Federal Reserve Bank of Cleveland Conference on Education and Economic Development, November 19, 2004. Return to text\n\n2.  Mary Ann Fox, Brooke A. Connolly, and Thomas D. Snyder, Trends in the Well-Being of American Youth (592 KB PDF), U.S. Department of Education, National Center for Educational Statistics (November 2005). Return to text\n\n3.  Philip Kaufman, Martha Naomi Alt, and Christopher D. Chapman, Dropout Rates in the United States: 2001 (369 KB PDF), U.S. Department of Education, National Center for Educational Statistics (November 2004). Return to text\n\n4.  David Card and Alan Krueger, \"School Quality and Black-White Relative Wage Differentials,\" Quarterly Journal of Economics, 1992. Return to text\n\n5.  James J. Heckman and Pedro Carneiro, \"Human Capital Policy,\" in James J. Heckman and Alan B. Krueger, eds., Inequality in America: What Role for Human Capital Policies? (The MIT Press, 2003), and Rob Grunewald and Arthur Rolnick, \"A Proposal for Achieving High Returns on Early Childhood Development,\" Federal Reserve Bank of Minneapolis, May 2005. Return to text\n\n6.  Yolanda Kodrzycki, \"College Completion Gaps between Blacks and Whites: What Accounts for the Regional Differences,\" New England Economic Review, Federal Reserve Bank of Boston, First Quarter 2004. Return to text\n\n7.  U.S. Census Bureau of the Census, Education and Social Stratification Branch, Current Population Survey Report, Historical Tables, Table A-5a (Excel file). Return to text\n\n8.  U.S. Department of Education, Digest of Education Statistics, Table 311, National Center for Educational Statistics, 2004. Return to text\n\n9.  Refer to, for example, Thomas Kane, \"College Entry by Blacks Since 1970: The Role of College Costs, Family Background, and the Returns to Education,\" Journal of Political Economy, October 1994; J. Bradford DeLong, Claudia Goldin, and Lawrence F. Kartz, \"Sustaining U.S. Economic Growth,\" in Henry J. Aaron, James M. Lindsay, and Pietro S. Nivola, eds., Agenda for the Nation (The Brookings Institution, 2003); Steven V. Cameron and James J. Heckman, \"Can Tuition Policy Combat Rising Wage Inequality,\" in Marvin Kosters (ed.), Financing College Education: Government Policies and Educational Priorities (American Enterprise Institute, 1999).  Return to text\n\n10. U.S. Department of Education, \"Educational Achievement and Black-White Inequality,\" National Center for Education Statistics, July 2001. Return to text\n\n11. Brian K. Bucks., Arthur B. Kennickell, and Kevin B. Moore, \"Recent Changes in U.S. Family Finances: Evidence from the 2001 and 2004 Survey of Consumer Finances (444 KB PDF),\" Federal Reserve Bulletin, February 2006.  Return to text\n\n12.  Juan Williams, I'll Find a Way or Make One: A Tribute to Historically Black Colleges and Universities (HarperCollins, 2004). Return to text\n\n13.  Stephen Provasnik, Linda L. Shafer, and Thomas D. Snyder, Historically Black Colleges and Universities, 1976 to 2001, U.S. Department of Education, National Center for Education Statistics, September 2004. Return to text",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/ferguson20060224a.htm",
        "title": "The Importance of Education",
        "date": "2/24/2006"
    },
    {
        "content": "February 24, 2006\n\nChairman Ben S. Bernanke\n\nAt The Center for Economic Policy Studies and on the occasion of the Seventy-Fifth Anniversary of the Woodrow Wilson School of Public and International Affairs, Princeton University, Princeton, New Jersey\n\nIt is a great pleasure for me to return to Princeton today, to see so many friends and former colleagues, and to help celebrate the seventy-fifth anniversary of the founding of the Woodrow Wilson School of Public and International Affairs. I taught at Princeton for seventeen years--more often than not in Bowl 1, in the deep, dark basement of Robertson Hall--and my wife Anna and I raised our two children here. Like all good New Jerseyans, we will always think of our home address in terms of a Turnpike exit--in our case, Exit 9.\n\nAs you know, the Woodrow Wilson School is named after a renowned Princeton professor of politics and law who, having determined from a stint as the University's president that the institution was essentially ungovernable, decided to try his hand at public service. I do not presume to draw any comparisons between myself and the nation's twenty-eighth President, of course; but besides the Princeton affiliation, we have in common a connection with the Federal Reserve System. President Wilson made the establishment of the Federal Reserve one of his early legislative priorities, signing the Federal Reserve Act into law in December 1913, less than a year after taking office. Wilson helped to negotiate the complex political compromises that finally gave the nation a permanent central bank, following two earlier failed attempts.\n\nTo simplify a complex history, earlier attempts to stabilize the monetary arrangements of the United States had frequently been roiled by perceived conflicts of interest between (on the one hand) the farmers and tradespeople of Main Street America, who believed that they were most advantaged by policies of easy credit, and (on the other hand) the financial barons of Wall Street, who, as creditors and bondholders, preferred \"hard-money,\" low-inflation policies. Recognizing that all parties would be served by a central bank that could help contain the periodic financial crises that afflicted the U.S. economy, Wilson worked with the Congress to develop a structure for the central bank that finely balanced competing interests and concerns. In particular, the Federal Reserve was given a regional structure, with twelve Reserve Banks that were distributed around the country and were empowered to represent sectional interests and to respond to local conditions. Although Wilson understood the political and practical advantages of decentralization, he also resisted some powerful proponents of a completely decentralized system by supporting the creation of a Board of Governors in Washington to oversee and coordinate the activities of the regional Reserve Banks.\n\nThe mandate of the Federal Reserve System has changed since the institution opened its doors in 1914. When the System was founded, its principal legal purpose was to provide \"an elastic currency,\" by which was meant a supply of credit that could fluctuate as needed to meet seasonal and other changes in credit demand. In this regard, the Federal Reserve was an immediate success. The seasonal fluctuations that had characterized short-term interest rates before the founding of the Fed were almost immediately eliminated, removing a source of stress from the banking system and the economy.1 The Federal Reserve today retains important responsibilities for banking and financial stability, but its formal policy objectives have become much broader. Its current mandate, set formally in law in 1977 and reaffirmed in 2000, requires the Federal Reserve to pursue three objectives through its conduct of monetary policy: maximum employment, stable prices, and moderate long-term interest rates.\n\nOne of my goals today is to consider the relationships among the three apparently disparate objectives of monetary policy. In particular, I will argue for what I believe has become the consensus view, that the mandated goals of price stability and maximum employment are almost entirely complementary. Central bankers, economists, and other knowledgeable observers around the world agree that price stability both contributes importantly to the economy's growth and employment prospects in the longer term and moderates the variability of output and employment in the short to medium term.\n\nBut that view did not always command the support that it does today. Notably, during the 1960s and early 1970s, some policymakers appeared to believe that price stability and high employment were substitutes, not complements. Specifically, some influential voices of the time argued that, by accepting higher inflation, policymakers could bring about a permanently lower rate of unemployment.2 As I will discuss a bit later, the demise of the view that higher inflation promotes employment in favor of the modern consensus that low inflation and strong employment are complementary goals resulted from the constructive interplay between academic research and practical policymaking experience, an interplay that significantly improved policy outcomes and economic welfare in the United States. Of course, fostering this sort of interaction between academic analysis and real-world policymaking is a principal objective of the Woodrow Wilson School.\n\nThe Dual Role of Price Stability\nPrice stability plays a dual role in modern central banking: It is both an end and a means of monetary policy.\n\nAs one of the Fed's mandated objectives, price stability itself is an end, or goal, of policy. Fundamentally, price stability preserves the integrity and purchasing power of the nation's money. When prices are stable, people can hold money for transactions and other purposes without having to worry that inflation will eat away at the real value of their money balances. Equally important, stable prices allow people to rely on the dollar as a measure of value when making long-term contracts, engaging in long-term planning, or borrowing or lending for long periods. As economist Martin Feldstein has frequently pointed out, price stability also permits tax laws, accounting rules, and the like to be expressed in dollar terms without being subject to distortions arising from fluctuations in the value of money.3 Economists like to argue that money belongs in the same class as the wheel and the inclined plane among ancient inventions of great social utility. Price stability allows that invention to work with minimal friction.\n\nIn principle, the problem of inflation could be reduced by the practice of indexing dollar payments such as interest and wages to the price level, but people seem to find indexing costly and avoid it when they can. It is interesting and instructive, for example, that the indexation of wages to prices in labor contracts has always been quite limited in the United States; some indexation was used during the high-inflation 1970s but the practice has been substantially reduced since then. Moreover, some countries that adopted indexing during high-inflation periods, such as Brazil and Israel, largely abandoned the practice when inflation receded. Borrowers and lenders likewise seem to prefer to contract in dollar terms, although inflation-indexed financial instruments have gained wider acceptance in recent years. Borrowing and lending in dollar terms, particularly for long periods, requires confidence that the purchasing power of the currency will be stable and predictable. The savings and loan crisis of the 1980s, which cost U.S. taxpayers roughly $150 billion, is an example of the kind of problem that can arise in the absence of price stability. An important source of the S&L crisis was the unexpected inflation of the 1970s, which greatly reduced the real value of mortgage loans made by the S&Ls in an earlier, low-inflation era. These losses effectively de-capitalized the savings and loans, helping to set the stage for the problems that followed.\n\nAlthough price stability is an end of monetary policy, it is also a means by which policy can achieve its other objectives. In the jargon, price stability is both a goal and an intermediate target of policy. As I will discuss, when prices are stable, both economic growth and stability are likely to be enhanced, and long-term interest rates are likely to be moderate. Thus, even a policymaker who places relatively less weight on price stability as a goal in its own right should be careful to maintain price stability as a means of advancing other critical objectives.\n\nLet me elaborate briefly on the relationship between price stability and the other two goals of monetary policy. First, price stability promotes efficiency and long-term growth by providing a monetary and financial environment in which economic decisions can be made and markets can operate without concern about unpredictable fluctuations in the purchasing power of money. As I have already noted, the dollar provides a reasonably secure gauge of real economic values only when inflation is low and stable. High and variable inflation degrades the quality of the signals coming from the price system, as producers and consumers find it difficult to distinguish price changes arising from changes in product supplies and demands from changes arising from general inflation. Because prices constitute a market economy's fundamental means of conveying information, the increased noise associated with high inflation erodes the effectiveness of the market system. High inflation also complicates long-term economic planning, creating incentives for households and firms to shorten their horizons and to spend resources in managing inflation risk rather than focusing on the most productive activities.\n\nResearch is not definitive about the extent to which price stability enhances economic growth. We do not have controlled experiments in macroeconomics, and inflation and growth are both endogenous variables that respond jointly to many factors. Nevertheless, I am confident that the effect is positive and see the international experience as at least consistent with the view that, in combination with other sound policies, the maintenance of price stability has quite significant benefits for efficiency and growth. That view appears to be widely shared among policymakers, as governments around the world have made extensive efforts to bring inflation down over the past two decades or so, with substantial success.\n\nMore recently, the evidence has mounted not only that low and stable inflation is beneficial for growth and employment in the long-term but also that it contributes importantly to greater stability of output and employment in the short to medium term. Specifically, during the past twenty years or so, in the United States and other industrial countries the volatility of both inflation and output have significantly decreased--a phenomenon known to economists as the Great Moderation (Bernanke, 2004). This finding challenges some conventional economic views, according to which greater stability of inflation can be achieved only by allowing greater fluctuations in output and employment. The key to explaining why price stability promotes stability in both output and employment is the realization that, when inflation itself is well-controlled, then the public's expectations of inflation will also be low and stable. In a virtuous circle, stable inflation expectations help the central bank to keep inflation low even as it retains substantial freedom to respond to disturbances to the broader economy.\n\nThis mechanism can be illustrated by comparing the effects of the recent rise in oil prices to the effects of the oil price increases of the 1970s. Thirty years ago, the public's expectations of inflation were not well anchored. With little confidence that the Fed would keep inflation low and stable, the public at that time reacted to the oil price increases by anticipating that inflation would rise still further. A destabilizing wage-price spiral ensued as firms and workers competed to \"keep up\" with inflation. The Fed, attempting to gain control of the deteriorating inflation situation, raised interest rates sharply; however, initially at least, these increases proved insufficient to control inflation or inflation expectations, and they added substantially to the volatility of output and employment. The episode highlights the crucial importance of keeping inflation expectations low and stable, which can be done only if inflation itself is low and stable.\n\nBy contrast, the oil price increases of recent years appear to have had only a limited effect on core inflation (that is, inflation in the prices of goods other than energy and food), nor do they appear to have generated significant macroeconomic volatility. Several factors account for the better performance of the economy in the recent episode, including improvements in energy efficiency and in the overall flexibility and resiliency of the economy. But, the crucial difference from the 1970s, in my view, is that today inflation expectations are low and stable (as shown, for example, by many surveys and a variety of financial indicators). Oil price increases in the past few years, unlike in the 1970s, have not fed through to any great extent into longer-term inflation expectations and core inflation, as the public has shown confidence that any increases in inflation will be temporary and that, in the long run, inflation will remain low. As a result, the Fed has not had to raise interest rates sharply as it did in the 1970s but instead has been able to pursue a policy that is more gradual and predictable. Of course, the relatively benign state of inflation expectations we enjoy today has not come automatically. The anchoring of inflation expectations in a narrow range has been the product of Fed policies that have kept actual inflation low in recent years, clear communication of those policies, and an institutional commitment to price stability.\n\nPrice stability also contributes to the third component of the Fed's mandate, the objective of moderate long-term interest rates. As first pointed out by the economist Irving Fisher, interest rates will tend to move in tandem with changes in expected inflation, as lenders require compensation for the loss in purchasing power of their principal over the period of the loan. When inflation is expected to be low, lenders will require less compensation, and thus interest rates will tend to be low as well. In addition, because price stability and the associated macroeconomic stability reduce the risks of holding long-term bonds and other securities, price stability may also reduce the premiums that lenders charge for bearing risk, lowering the overall level of rates.\n\nThe Origins of the Modern Consensus on Price Stability\nI have briefly laid out the modern consensus that price stability, besides being desirable in itself, tends also to increase economic growth and stability. As I noted earlier, however, this view is quite different from the one that prevailed forty years ago. At that time, the ascendant paradigm was that society faced a long-term tradeoff between price stability and high employment. Implied in this position was a potential conflict between defenders of \"hard money\" and supporters of easy credit that echoed, at least faintly, the political conflicts that Wilson faced in setting up the Federal Reserve. The development of the modern consensus was a fascinating example of the way economic science progresses through the interaction of academic research and policy experience--exactly the kind of activity that the Woodrow Wilson School was designed to promote. Thus I thought I might briefly describe the evolution of that consensus here today.\n\nThe 1960s' idea that greater prosperity could be achieved if only we were willing to accept higher inflation had its origins in an academic study, although the author likely did not intend that outcome. In 1958, A.W. Phillips, using British data, showed that historically inflation had tended to be high in years in which unemployment was low. Similar results were subsequently reported for the United States.4 Phillips did not draw strong policy conclusions from his findings. But that did not stop others from doing so. In the decade following the publication of his paper, his empirical finding was sometimes interpreted (including, for example, by members of the Kennedy and Johnson Administrations) as showing that policymakers could choose (permanently) lower unemployment if they were willing to accept (permanently) higher inflation in exchange. Scholars disagree somewhat about the extent to which policymakers of the time tried actively to take advantage of this supposed tradeoff, but these ideas likely provided part of the intellectual rationale that made the authorities willing to allow inflation to rise throughout the 1960s and in the early 1970s.\n\nThe idea of the permanent tradeoff did not go unchallenged, however. In 1967, economists Milton Friedman and Edmund Phelps independently produced influential critiques of this view. Their key contribution was to observe that, if inflation expectations react to changes in actual inflation in an economically reasonable way, then any tradeoff between inflation and unemployment would be short-lived at best. To illustrate their argument, let us suppose that firms and workers set nominal wages once a year but that, sometime during the year, the prices of firms' output rise unexpectedly as a result of stronger-than-expected demand. The combination of higher prices for their output and fixed nominal wages would raise the profitability of increasing production; thus, assuming that more workers are available at the previously fixed wage, firms would respond to the rise in prices by adding workers. Over a short period, then, higher inflation might bring lower unemployment, consistent with the empirical results found by Phillips.\n\nHowever, this logic applies only during the period in which wages and workers' expectations of inflation are fixed. If inflation were to rise persistently, Friedman and Phelps argued, workers' expectations of inflation would not remain unchanged but would adjust to match the actual rate of inflation. Higher inflation expectations would in turn lead workers to bargain for commensurate raises in nominal wages to preserve the real value of their earnings. With nominal wages rising as well as prices, firms would no longer have an incentive to hire additional workers, and employment would return to its normal level. An attempt to stimulate the economy by choosing a permanently higher level of inflation could thus not succeed, according to this analysis; such an attempt would leave the economy with higher inflation but a level of employment no different than it would have been otherwise. This work was both brilliant and prescient. In particular, among the seminal contributions of the Friedman and Phelps analyses was the identification of the key role of inflation expectations in determining the behavior of the economy, a point that remains central to our thinking today.\n\nMoreover, the performance of the U.S. economy soon bore out the predictions made by Friedman and Phelps. The inflationary policies of the 1960s led not to permanently lower unemployment, as the permanent-tradeoff theory predicted, but instead to persistently higher inflation with no improvement in unemployment. For example, in the 1970s, core inflation averaged 6 percent, compared with 2-1/4 percent in the 1960s, and unemployment in the 1970s averaged 6-1/4 percent, compared with the 4-3/4 percent rate in the 1960s. The volatility of output and (especially) inflation both increased, as the Fed struggled to contain inflation expectations. Other factors, including the aforementioned surge in oil prices, played a role in the deterioration of economic performance in the 1970s. Clearly, though, the theory that a long-run tradeoff exists between inflation and unemployment had sprung a serious leak.\n\nDespite a growing recognition that higher inflation provided no labor-market benefits, there was, until the end of the 1970s, little appetite for taking the actions necessary to reduce inflation. For one thing, economists and policymakers recognized that reversing the rise in inflation expectations that had occurred during the 1970s could take time and that, during the process, the nation could suffer ultimately transitory but still-serious increases in unemployment. Furthermore, at the time, it was widely believed among economists that any stable level of inflation would be as good as another. Although the efficiency costs associated with high inflation were acknowledged, the costs were thought to be associated mostly with changes in the underlying rate of inflation--particularly unexpected changes. In addition, many economists argued that the efficiency costs of inflation were not particularly large.5\n\nMilton Friedman once again was in the vanguard on this issue. In his 1977 Nobel Prize address, Friedman laid out the modern argument--that, because it harms the efficient operation of markets, high inflation is more likely to raise unemployment than to lower it--and he used the experience of the 1970s to illustrate his point.6 Indeed, by the late 1970s, even economists who were not part of Friedman's monetarist circle were beginning to study and acknowledge the costs to the economy associated with high inflation.7\n\nWhen Federal Reserve Board Chairman Paul Volcker embarked on his campaign to break the back of U.S. inflation in October 1979, he drew on this existing work in formulating and defending his program. (Volcker, by the way, was Princeton class of 1949, and he wrote his senior thesis on the Federal Reserve.) In his first testimonies and speeches after becoming Chairman, Volcker emphasized many of the arguments developed by academics for how inflation interfered with the efficient working of the economy. And he drew on Friedman's monetarist approach, both in its advocacy of low and stable inflation and in its prescriptions for policy implementation. In a speech given just after the Federal Open Market Committee announced its adoption of a monetarist-style policy approach in October 1979, Volcker dismissed the notion that lowering inflation meant accepting permanently higher unemployment and suggested instead that the reverse was more likely to be the case.8\n\nUntil this point, academic research (or at least some of it) had paved the way for improved policymaking. After 1979, however, policymakers increasingly began to set the intellectual pace. Volcker's statements from this period in particular are remarkable in the extent to which they anticipate contemporary thinking about the crucial importance of low and stable inflation and inflation expectations. He repeatedly noted, for example, how instability in inflation and inflation expectations were \"jeopardizing the orderly functioning of financial and commodity markets.\"9 Unlike academics, of course, Volcker was in a position to put his views into practice. Under the Volcker-led Federal Reserve, annual core inflation fell from more than 9 percent in 1980 to just below 4 percent in 1987.\n\nAlan Greenspan, who succeeded Volcker as Fed Chairman in 1987, continued to work to stabilize inflation and inflation expectations. Under Greenspan, the Federal Reserve gradually brought core inflation down further, to about 2 percent in recent years. The Greenspan era also saw important steps toward increased transparency at the Federal Reserve, which helped to clarify for the public the Federal Reserve's strong institutional commitment to price stability. In a sense, Chairman Greenspan had the harder sell: As an economist would say, we might expect diminishing marginal returns to inflation reduction. Yet I think subsequent events demonstrate clear benefits from the tenacity of the Fed under Greenspan. Lower inflation has been accompanied by inflation expectations that are not only lower but better anchored, so far as we can tell. Most striking, Greenspan's tenure aligns closely with the Great Moderation, the reduction in economic volatility I mentioned earlier, as well as with a strong revival in U.S. productivity growth--developments that had many sources, no doubt, but that were supported, in my view, by monetary stability. Like Volcker, Greenspan was ahead of academic thinking in recognizing the potential benefits of increased price stability. Indeed, in recent years, academic research on monetary policy has caught up with the policymakers, providing new support for what I have termed the modern consensus, that price stability supports both strong growth and stability in output and employment.\n\nConclusion\nPrice stability plays a dual role in monetary policy. Stable prices are desirable in themselves and thus are an important goal of monetary policy. But stable prices are also a prerequisite to the achievement of the Federal Reserve's other mandated objectives, high employment and moderate long-term interest rates. In particular, low and stable inflation and inflation expectations enhance both economic growth and economic stability.\n\nThe complementarity of price stability with the other goals of monetary policy is now the consensus view among economists and central bankers. That consensus has not been achieved easily, however, but is the product of many years of policy experience, policy leadership, and sustained economic analysis. No doubt we will continue to learn about the economy and economic policy, even as we benefit from the insights of those who went before us. I am sure the Woodrow Wilson School, its faculty, and its students will continue to play an important role in that ongoing process.\n\nReferences\n\nBernanke, Ben S. (2004). \"The Great Moderation,\" speech delivered at the meetings of the Eastern Economic Association, Washington, D.C., February 20.\n\nFeldstein, Martin (1997). \"The Costs and Benefits of Going from Low Inflation to Price Stability,\" in Christina D. Romer and David H. Romer, eds., Reducing Inflation: Motivation and Strategy, University of Chicago Press, 123-56.\n\nFischer, Stanley (1986). Indexing, Inflation, and Economic Policy. MIT Press.\n\n_____ and Franco Modigliani (1978). \"Towards an Understanding of the Real Effects and Costs of Inflation,\" Weltwirtschaftliches Archiv 114, 810-33.\n\nFriedman, Milton (1968). \"The Role of Monetary Policy,\" American Economic Review 58, 1‑17.\n\n_____ (1977). \"Nobel Lecture: Inflation and Unemployment,\" Journal of Political Economy 85, 451‑72.\n\nMiron, Jeffrey A. (1986). \"Financial Panics, the Seasonality of the Nominal Interest Rate, and the Founding of the Fed, \" American Economic Review 76, 125-40.\n\nPhelps, Edmund S. (1968). \"Money-Wage Dynamics and Labor-Market Equilibrium,\" Journal of Political Economy 76, 678-711.\n\nPhillips, A. W. (1958). \"The Relation between Unemployment and the Rate of Change of Money Wages in the United Kingdom, 1861-1957,\" Economica 25, 283-99.\n\nRomer, Christina D., and David H. Romer (2002). \"The Evolution of Economic Understanding and Post-War Stabilization Policy (PDF 439 KB),\" in Federal Reserve Bank of Kansas City, Rethinking Stabilization Policy, 11‑79.\n\nVolcker, Paul A. (1979a). \"A Time of Testing,\" October 9, in Addresses, Essays, and Lectures of Paul A. Volcker, 1979-81, volume 1, Federal Reserve Board.\n\nVolcker, Paul A. (1979b). \"Statement before the Subcommittees on Domestic Monetary Policy and on International Trade, Investment and Monetary Policy of the Committee on Banking, Finance and Urban Affairs,\" U.S. House of Representatives, November 13, 1979, Federal Reserve Bulletin, December 1979, 958-962.\n\nFootnotes\n\n1.  Miron (1986). Return to text\n\n2.  Romer and Romer (2002) provide historical documentation of policymakers' support for the idea of a permanent inflation-unemployment tradeoff. Return to text\n\n3.  Feldstein (1997). Return to text\n\n4.  Phillips' work actually focused on wage inflation rather than price inflation; subsequent work emphasized the latter. Return to text\n\n5.  For example, Nobel-Prize-winning economist James Tobin is famously quoted as saying, \"It takes a heap of Harberger triangles to fill an Okun gap,\" an admittedly jargon-laden way of saying that it was unlikely that the efficiency gains from lower inflation would compensate for the loss in output and employment associated with an aggressive effort to bring inflation down. Quoted in Fischer (1986, p. 3). Return to text\n\n6.  Friedman (1977). Return to text\n\n7.  Fischer and Modigliani (1978). Return to text\n\n8.  Volcker (1979a). Return to text\n\n9.  Volcker (1979b). Return to text",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060224a.htm",
        "title": "The Benefits of Price Stability",
        "date": "2/24/2006"
    },
    {
        "content": "February 23, 2006\n\nVice Chairman Roger W. Ferguson, Jr.\n\nAt the National Association of Insurance Commissioners (NAIC) International Insurance Symposium, Washington, D.C.\n\nI am honored to deliver the keynote address at this important international symposium. Our focus today--on how developments and challenges around the world are reshaping the insurance industry and its regulation in Europe, Latin America, China, and India as well as here in the United States--is certainly warranted. The worldwide integration of economies and financial markets is increasing, and a sound and vibrant insurance and reinsurance industry is needed to sustain global economic growth. Thus, I am glad to see that this symposium has brought together regulators and major market participants from all parts of the globe.\n\nWithin the United States, the National Association of Insurance Commissioners (NAIC) has over the years promoted interstate collaboration on regulatory matters, established and maintained a centralized information system, and supported and improved the state regulation of insurance. Through its work, the NAIC has enhanced the reliability and financial health of the domestic insurance industry.\n\nThe NAIC has also been active in fostering cooperation on supervisory issues in the international insurance arena, through groups such as the International Association of Insurance Supervisors (IAIS), which the NAIC currently chairs; the International Accounting Standards Board (IASB); and the Joint Forum. The NAIC is, of course, also actively involved, through the IAIS, in the work of the Financial Stability Forum (FSF), of which Alessandro Iuppa is a member. The work of the FSF is the main topic I will speak about today. Before I proceed, I want to say that the views I will express are not necessarily shared by my colleagues on the Board of Governors.\n\nThe FSF has taken an interest in a variety of developments affecting the financial sector in recent years. I would like to leave you with three messages based on its work in this area: first, effective collaboration among regulators and other authorities is important; second, enhanced transparency and disclosure are important; third, firms and regulators need to have early and continuous dialogue on regulatory developments.\n\nI will discuss these statements in a minute, but before turning to them I want to give you some background on the creation of the Financial Stability Forum, its composition, and its purpose. As you may know, the FSF was established in 1999, in the wake of the Asian financial crisis, to promote international financial stability. The FSF uniquely brings together national authorities responsible for financial stability in major international financial centers, representatives from international financial institutions, representatives of international regulatory committees, and committees of central bank experts. Its role is to foster cooperation among authorities at a global level in identifying vulnerabilities and strains in the global financial system and developing strategies to reduce systemic risk.\n\nAn important element of the FSF's work is to assess risks to financial stability that might arise from current economic and financial developments, to discuss risk scenarios, and to consider the capacity of the financial sector to absorb shocks. Several years of solid economic growth and a benign financial environment have helped strengthen balance sheets in the financial sector, and this strength seems likely to continue. Nonetheless, a number of risks and potential vulnerabilities are receiving attention, including the implications for growth and inflation of high oil prices; global fiscal and current account imbalances; elevated house prices and household debt; the compression of risk spreads, which has contributed to low long-term yields; and the potential financial fallout from geopolitical events.\n\nI am sure that you yourselves are closely evaluating what these and other matters imply for the insurance industry. Consider, for example, the effect of low long-term interest rates on the insurance sector. Low interest rates may have induced some firms to take on extra exposure through investment in higher-risk assets, including complex or less highly-rated credit products. In some cases, mainly outside the United States, insurers have had difficulty earning sufficient returns to meet the minimum rate they have committed to pay to policyholders. Generally, however, the industry seems to be increasingly focused on better matching the risk characteristics and duration of assets and liabilities. In fact, this better match may be one factor supporting low yields on longer-dated fixed-income assets, at least in some markets.\n\nBut there are other potential explanations for the low long-term rates now prevailing in many countries. These other explanations include an excess of desired global saving over desired global investment spending and a decline in the risk premia demanded by investors, with the latter likely due in part to a marked decline in the volatility of economic activity and inflation. Suffice it to say, we do not have a full understanding of the complex forces that have driven rates down, and the topic is likely to be debated for some time to come. Some observers have expressed concern about the possibility of a steep rise in long-term rates from their currently low levels. Were such a move to occur, the implications for the economy and asset prices would depend on the source of the rate increase. If, for example, desired investment spending were to strengthen--thereby boosting economic growth--the induced rise in longer-term rates need not be a worrisome development. On the other hand, a sharp increase in risk premia, whether because of a rise in perceived risk or a reduced willingness to bear risk, likely would be more problematic. We simply cannot know before the fact whether a potential rise in longer-term interest rates would be disruptive or not.\n\nIn addition to their exposure from unexpected developments in financial markets, insurers face a variety of other significant risks, including the potential liability arising from acts of terrorism and natural disasters. At present there is also much speculation about the likelihood and the possible effects of an Avian flu pandemic. The possibility of a pandemic gives rise to new issues for insurance and reinsurance companies. Pandemics may result in significant losses for parts of the insurance sector from increased sickness and mortality. At the same time, a pandemic could pose operational challenges by causing widespread and extended absenteeism and by disrupting the usual flow of goods and services throughout the world economy. All firms, including insurers, would probably be wise to give serious thought to the question of how they would run their business in the event of an Avian flu pandemic.\n\nGiven that the industry faces these and other risks, the three messages I mentioned earlier take on special significance. I would now like to expound on each of them.\n\nEffective collaboration among financial authorities\nGlobalization is creating new business opportunities, opening new markets, increasing efficiency, decreasing the cost of capital, and enabling well-managed companies to deliver expanded services and greater shareholder value. However, these forces are also prompting financial companies to develop far-flung business operations, which make group management and risk aggregation more challenging. At the same time, these forces are blurring business and sectoral lines and making exposures more interrelated. Such challenges to management exist even in a totally domestic setting, but globalization has increased their difficulty and complexity and has intensified the need for supervisory collaboration, especially between home and host supervisors, within and across sectors.\n\nIn the United States, the nationwide reach of the largest insurers has led to an increasing emphasis on interstate cooperation among insurance supervisors through the NAIC. Now the development of financial conglomerates has rightly prompted increased links with supervisors in other sectors. Meanwhile, globalization has necessitated intensified supervisory cooperation at the international level. In this respect, I am encouraged to see that the NAIC has been meeting twice a year with the European Union since 1999 to address issues affecting transatlantic insurance business. These meetings have led to a Memorandum of Understanding on information exchange and to discussions about the supervision of reinsurance. Similar arrangements for information exchange have also been established with insurance regulators in China, India, Japan, Latin America, and Russia.\n\nI should also like to commend the IAIS for its work over the years in promoting international insurance standards and information exchange. The FSF greatly welcomes the continuing work by the IAIS on encouraging practical cooperation among supervisory authorities and on examining whether there are significant barriers to the exchange of information relating to groups and conglomerates.\n\nRegulators must sustain work in this direction as insurance activity continues to expand internationally, resulting in large, internationally active insurance groups. Because of the formation and activity of such large groups, regulators must have a good picture of the totality of risks that each insurance or reinsurance group is running. Regulators need to confer and to compare national systems so as to identify regulatory best-practices and avoid duplicative regulatory work. It is important also for each regulator to understand and evaluate the major changes in the laws and regulations in the other regulators' countries and the international implications of the changes. Developing strong working relationships through regular, ongoing dialogue also creates better channels for communication when difficulties arise.\n\nFor cross-sectoral international issues, the NAIC makes a valuable contribution to the work of the Joint Forum, which as you know is a group that brings together banking, securities, and insurance regulators from many countries to evaluate and address cross-sectoral regulatory issues. Some key areas of the Joint Forum's work in which the FSF has taken a close interest are the review of the regulatory and market differences across sectors; credit risk transfer; high-level principles regarding outsourcing and business continuity; and the funding of liquidity risk. Many of these issues have the potential to affect financial stability and are of interest not only to the supervisors but to our central bank and finance ministry members.\n\nEnhanced transparency and disclosure\nTransparency and public disclosure are essential for the efficient functioning of markets. Counterparties and investors need to be clear about the risks that firms and the industry are taking in order to manage their own exposures.\n\nThe FSF takes an active interest in this area. Its interest is not just to the narrow question of determining which data items to disclose but in the whole process of information disclosure. The FSF is promoting the development of high standards for accounting and auditing, the establishment of public-interest bodies to oversee the standardsetters' work, and the development and adoption of best-practice disclosure.\n\nLet me elaborate a little on the FSF's interest in the transparency and disclosure practices of the reinsurance sector. After the events of September 11, 2001, and the significant downturn in global equity markets in 2001-02, questions arose whether reinsurance capital might erode to the point that primary insurance capacity would deteriorate. Also at that time anecdotal reports of the industry's growing involvement in credit risk transfer activities, both as investors and as sellers of credit protection, led to concern that such involvement increased the risk that reinsurance difficulties could have wider implications in the financial system. The FSF found that the absence of adequate information at the time made it difficult to assess the knock-on effects of potential difficulties in the sector on primary insurance and on other areas of the financial system.\n\nAs a result, the FSF encouraged and supported efforts by the IAIS and its working groups (Task Force Re and the Reinsurance Transparency Group) to shed light on these issues. The first and second Global Reinsurance Market reports have since been released and have been well received by FSF members and other users of information. Despite limitations arising, in part, from different accounting conventions, the reports provide analysis that enables stakeholders to gain deeper insight into the structure and profile of the reinsurance industry and its links with other sectors of the wider financial system.\n\nForum members have expressed an ongoing interest in further information on the reinsurance industry's systemic links. In this regard, I would also like to welcome the recent work by the G30 on the reinsurance sector and international financial markets. Contained in the G30 report is a chapter on transparency that sets out a framework to improve the disclosure of risk information by reinsurers. With financial markets and product offerings constantly evolving, firms and the industry must release timely, comprehensive, and meaningful information to enable counterparties, investors, and regulators to do their evaluation and analysis.\n\nEarly and continuous dialogue about regulatory developments\nOne area that is now receiving attention in both the private and the official communities is the perception that the financial industry is suffering from regulatory overload. This perception arises partly because of a bunching of regulatory initiatives in recent years. But concern about a more continuous accumulation of regulation over time has also arisen.\n\nOf course, regulations that improve the strength and integrity of the financial system justify an element of regulatory burden. At the same time, the costs and benefits of new regulations need to be carefully weighed against each other, taking full account of financial industry input. Industry commentary has already resulted in significant and beneficial changes to regulations and directives. I encourage firms to continue engaging actively and early with regulatory authorities on the range of international regulatory and legislative developments to ensure that the authorities have the full benefit of your expertise as they set policy.\n\nAll these changes mean challenging times for both industry participants and regulators. Companies must keep up with changing regulations and reporting requirements and must allocate part of their limited time and resources to dealing with them. Regulatory resources are stretched as well, and I understand many regulators are working hard to ensure that the intensity of regulatory implementation efforts does not divert resources from ongoing supervision. Regulators and industry participants will have to work together to meet these challenges.\n\nWe must recognize, however, that regulatory initiatives also reflect a more complex environment. Governments and the public at large understandably want to ensure that safety and soundness are maintained despite complexity. The intricate challenge here is to find the right balance between regulation, on the one hand, and the fostering of industry-led solutions through improved risk management and market practices, on the other.\n\nI should like to say that the FSF fully supports the role of the IAIS as the global standard setter for the insurance and reinsurance industry. We also commend the IAIS on the good work it has been doing in developing principles, standards, and guidance on insurance and reinsurance supervision and in promoting their implementation. This work contributes significantly to improved supervision of the insurance industry, to the development of well-regulated insurance markets, and to global financial stability.\n\nThe NAIC has been active in monitoring discussions and reviewing and commenting on insurance regulatory and supervisory papers. I have no doubt that, given the importance of the coming initiatives, the NAIC will continue to do so.\n\nConclusion\nLadies and gentlemen, it has been my pleasure to set out some thoughts about several issues you will debate at this symposium. I think that you will agree with me on the need for effective supervisory collaboration, for enhanced transparency, for public disclosure, and for a dialogue with industry participants to ensure a well-designed regulatory regime that is effective and proportionate to the risks it addresses.\n\nThe good news is that, in the future, growing global markets will present new and potentially rewarding opportunities for insurance companies. Perhaps the not-so-good news is that the industry is also likely to face a multitude of challenges. The industry has made much progress in recent years. Insurers and reinsurers have revised their underwriting philosophies, developed new models to assess risk, improved the adequacy of their risk profiles, and adjusted their coverage policies. Many regulators have also made significant changes to ensure that their regulations keep pace with developments or, at least, do not fall too far behind. These efforts give me confidence that both firms and regulators will continue to meet the challenges that lie ahead.\n\nThank you, and I wish you all a very fruitful symposium.",
        "position": "Vice Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/ferguson20060223a.htm",
        "title": "Globalization, Insurers, and Regulators: Shared Challenges Call for Collaborative Solutions",
        "date": "2/23/2006"
    },
    {
        "content": "February 06, 2006\n\nChairman Ben S. Bernanke\n\nAt the ceremonial swearing-in by President Bush, Federal Reserve Board of Governors, Washington, D.C.\n\nGood morning.\n\nI would like to begin by thanking President Bush for the confidence he has placed in me and for attending this ceremony. Today marks only the third visit of a President to the Federal Reserve. Franklin D. Roosevelt dedicated this building in 1937 and Gerald R. Ford visited in 1975. Mr. President, you do us a great honor.\n\nMembers of the President's economic team and the heads of the federal financial regulatory agencies have also joined us this morning. I have greatly enjoyed collaborating with many of you during my time in Washington, and I look forward to working with you in the future. Thank you for coming.\n\nI would like to extend a special welcome to members of Congress. The Federal Reserve was created by Congress in 1913 and entrusted with the power, granted originally to the Congress by the U.S. Constitution, to coin money and regulate the value thereof. Accordingly, it is incumbent on the Federal Reserve to report regularly to, and work closely with, the Congress. I look forward to a strong and constructive relationship with members of both the House and Senate.\n\nFormer chairmen Paul Volcker and Alan Greenspan also honor us with their attendance. Their leadership and insight have contributed immeasurably to the strength and stability of our economy. The nation and the world owe a debt of gratitude to these two great Americans.\n\nThat these distinguished guests have chosen to join us today is a testament to the centrality of this institution to the nation's economic life. Our mission, as set forth by the Congress, is a critical one: to preserve price stability, to foster maximum sustainable growth in output and employment, and to promote a stable and efficient financial system that serves all Americans well and fairly. In his remarks in this building in 1937, President Roosevelt described as our purpose \"to gain for all of our people the greatest attainable measure of economic well-being, the largest degree of economic security and stability.\"\n\nAs I contemplate taking up the challenge of leading an institution with such weighty responsibilities, I count, first, on the love and support of my family: my wife Anna, son Joel, and daughter Alyssa, who are here today. I know also that I can rely on the enormous strengths of this institution: excellent leadership on the Board and in the Reserve Banks and unmatched expertise and experience in the staff. Mr. President, as you know, on September 11, 2001, and the days that followed, Vice Chairman Roger W. Ferguson, Jr., who just swore me in, and many members of the Federal Reserve staff--here, in New York and around the country--worked inexhaustibly to ensure the continued functioning and recovery of the American financial system. The dedication and knowledge demonstrated that day by so many people exemplifies why the Federal Reserve as an institution is far more than any single individual.\n\nTo my Board colleagues and to the staff here today, I would like to say thank you for your service to your country and to the world. I am happy to be back among you and look forward to working with you in the days and years ahead. Together I am confident that we will meet whatever challenges the future may bring.\n\nThank you all for coming.",
        "position": "Chairman",
        "href": "https://www.federalreserve.gov/newsevents/speech/bernanke20060206a.htm",
        "title": "Remarks at ceremonial swearing-in by President Bush",
        "date": "2/6/2006"
    },
    {
        "content": "February 02, 2006\n\nGovernor Susan Schmidt Bies\n\nAt the Financial Services Institute, Washington, D.C.\n\nI thank you for the invitation to speak today. My remarks will focus on the continuous challenges banking organizations face as they manage risk. Today, banking organizations have a much wider array of risk-management tools and practices available to them, thanks in large part to innovations in financial products and services, improved technology, and many other developments in the industry. But financial innovation also presents new and different aspects of risk that institutions need to address.\n\n\n\nI will first describe some of the broader risk management issues facing banks. Then, I will describe some guidance supervisors have recently issued to illustrate the importance of these concepts. I know that a number of attorneys who work on banking and financial matters are in the audience, so I will cover not only risks relating directly to lending and trading activities but also those relating to legal and compliance issues.\n\nDimensions of Risk-Management Challenges\n\nCertainly, risk management is not a new challenge. Financial institutions have always faced the task of managing their risk exposures while remaining profitable and competitive. But financial innovation, even when it involves well-established products and exposures that are only slightly altered, continues to present new risk-management challenges. Yet challenges are also opportunities: good risk management is an art that combines the ability to use financial innovation to improve profitability with an understanding of how risk profiles change as a result of that innovation.\n\nThe Federal Reserve, in its role as both a bank supervisor and the nation’s central bank, has an obvious interest in maintaining the stability of the banking industry and the financial system as a whole. We, along with our counterparts at the other U.S. bank and thrift regulatory agencies, are responsible for ensuring that banking institutions operate in a safe and sound manner. But with the advent of very large banking organizations that engage in a wide variety of business activities--some of them quite complex--the Federal Reserve has become even more interested in ensuring that banking organizations understand the risks of these activities as well as their potential impact.\n\nAs organizations grow larger, one of their major challenges can be described as making sure that the “right hand” knows what the “left hand” is doing. In other words, risks must be recognized and managed across the entire organization. In some cases, firms may be practicing good risk management on an exposure-by-exposure basis, but they may not be paying close enough attention to aggregation of exposures across the entire organization. Rapid growth can place considerable pressure on, among other areas, an organization’s management information systems, change-management controls, strategic planning, credit concentrations, and asset-liability management.\n\nOf course, there are other dimensions of risk to consider besides scale. As noted, the banking organizations we see today often operate in many lines of business and offer many types of products. For example, some organizations have increased their size not just by expanding their existing operations, but also by pursuing several new business lines at the same time. Certainly, this strategy of business diversification has its benefits. But the organization must also understand how the various business components interact on a dynamic basis. In other words, diversification does not simply work on its own, but has to be carefully planned and managed.\n\nAnother dimension of risk to be considered is the complexity and sophistication of an organization’s products and services. While an institution may alter its risks by expanding into several business lines, the nature of its products and services also makes a tremendous difference in its risk profile. For example, it matters greatly whether an institution expands its mortgage lending by offering traditional mortgage products with which it has substantial experience, or whether an institution offers new mortgage products for which there is little history. At first glance, the new products may appear to be no more risky than more traditional products; however, when more carefully examined, new products can have a higher risk potential, or at least one that is not fully known. Institutions should also be aware that traditional products packaged in a slightly different way or offered to a new customer segment can substantially alter their risk profile. I point to recent cases of legal and compliance risks that occurred because the institutions involved did not necessarily conduct proper due diligence on their new products.\n\nThe challenge of understanding the potential risks associated with new products in a given business line is heightened when firms attempt to see how those risks intersect with the risks from its other business lines. On the one hand, a firm may be hedging its risks or enhancing diversification by offering new products; on the other hand, the firm may simply be adding to risks it already has. Obviously, knowing the difference is vitally important. Furthermore, an institution has to pay attention to the behavior and performance of its risk mitigants, whose appropriateness and applicability may also vary with changes in the market. The bottom line for today’s banking institutions, particularly the largest and most complex ones, is that they must continue to monitor very carefully the embedded risks of their products and services, pay close attention to subtle changes in business practices that could affect the risks related to a given product, and fully understand how the risks in all their business lines intersect and combine to affect the risk profile of the consolidated entity.\n\nCurrent Perspective on Risk-Management Challenges\n\nCommercial Real Estate\n\nNow that I have described in fairly broad terms the types of risk-management challenges institutions may encounter, I think it would be helpful to provide some concrete examples. As you are likely aware, the U.S. banking agencies recently issued for public comment supervisory guidance on commercial real estate (CRE), which focused particularly on CRE concentrations. Regardless of its size, we believe an institution involved in CRE lending would benefit from a review of the guidance. As you know, CRE played a central role in the banking problems of the late 1980s and early 1990s and has historically been a highly volatile asset class. Over the past dozen years, CRE concentrations have been rising and are now at record levels at many banking organizations. For certain groups of banks--such as those between $1 billion and $10 billion--average CRE concentrations, which can include owner-occupied CRE, are today above 300 percent. This compares to a previous peak of CRE concentrations of 200 percent seen in the late 1980s and early 1990s.\n\nThe agencies’ CRE concentration guidance, which excludes owner-occupied CRE, is intended to reinforce existing risk-management guidelines on CRE, and it offers institutions some suggestions for improving their risk-management practices. The agencies share a concern that some institutions’ risk-management practices are not keeping up with the growth in their CRE exposures. The guidance describes the key risk-management elements for an institution’s CRE lending, because banks, in order to attract new business and sustain loan volume, may be inclined to occasionally make some compromises and concessions to borrowers. As supervisors, we want to ensure that loan-to-value standards and debt-service-coverage ratios are meeting the organization’s policies--and that there is not an increasing number of exceptions to those standards and ratios. We also continue to monitor whether lenders routinely adjust covenants, lengthen maturities, or reduce collateral requirements.\n\nAdditionally, the guidance reemphasizes that institutions should have capital levels appropriate for the risk associated with their CRE concentrations. Some evidence suggests that an institution’s strategic- and capital-planning processes may not adequately acknowledge the risks from its CRE concentrations. By focusing on CRE concentrations, the guidance is also intended to apply to those institutions that might already have excellent underwriting and risk management of their existing CRE exposures but that might not be as vigilant or as rigorous in their management of those exposures on an aggregate portfolio basis. That is, underwriting policies, as well as management and board reporting, should reflect aggregate portfolio risk as CRE loans rise to a significant multiple of capital.\n\nA bank with significant concentrations may need to both strengthen its control environment and hold capital well above regulatory minimums. In certain cases, it may be prudent for an institution to reduce its concentrations. This is particularly important, since CRE lending in recent years has occurred under fairly benign credit conditions and, naturally, those conditions are unlikely to continue indefinitely. From a risk-management and capital perspective, institutions should employ stress tests and other exercises to help identify CRE vulnerabilities, including potential correlations with non-CRE exposures that might move in the same direction during a downturn.\n\nNontraditional Mortgage Products\n\nThe U.S. banking agencies have also issued draft guidance on certain mortgage products. Over the past few years, the agencies have observed an increase in the volume of originations for residential mortgage loans that allow borrowers to defer repayment of principal and, sometimes, interest. These mortgage loans, often referred to as “nontraditional mortgage loans,” include “interest-only” (IO) mortgage loans, in which a borrower pays no loan principal for the first few years of the loan, and “payment-option” adjustable-rate mortgages (option ARMs), in which a borrower has flexible payment options--and which also could result in negative amortization.\n\nIn 2005, option ARMs and IOs were an estimated one-third of total U.S. mortgage originations. By contrast, in 2003, these products were estimated to represent less than 10 percent of total originations. Despite the recent publicity, however, it is estimated that these mortgages still account for less than 20 percent of aggregate domestic mortgage outstandings of $8 trillion. While the credit quality of residential mortgages generally remains strong, the Federal Reserve and other banking supervisors are concerned that current risk-management techniques may not fully address the level of risk in nontraditional mortgages, a risk that would be heightened by a downturn in the housing market.\n\nNontraditional mortgage products have been available for many years; however, these types of mortgages were historically offered to higher-income borrowers only. More recently, these products have been offered to a wider spectrum of consumers, including subprime borrowers who may be less suited for these types of mortgages and may not fully recognize their embedded risks. These borrowers are more likely to experience an unmanageable payment shock at some point during the life of the loan, which means they may be more likely to default on the loan. Further, nontraditional mortgage loans are becoming more prevalent in the subprime market at the same time that risk tolerances in the capital markets have increased. When risk spreads return to more “normal” levels, banks need to be prepared for the resulting impact on liquidity and pricing. Supervisors have also observed that lenders are increasingly combining nontraditional mortgage loans with weaker mitigating controls on credit exposures, such as allowing reduced documentation in evaluating the applicant’s creditworthiness and making simultaneous second-lien mortgages as competition in the mortgage banking industry intensifies. These “risk layering” practices have become more and more prevalent in mortgage originations. Thus, while elements of the product structure may have been used successfully by some banks in the past, the absence of traditional underwriting controls may have unforeseen effects on losses realized in these products.\n\nIn view of these industry trends, the Federal Reserve and the other banking agencies decided to issue the draft guidance on nontraditional mortgage products. The proposed guidance emphasizes that an institution’s risk-management processes should allow it to adequately identify, measure, monitor, and control the risk associated with these products. The guidance reminds lenders of the importance of assessing a borrower’s ability to repay a loan, including monthly payments when amortization begins and interest rates rise. Lenders should recognize that certain nontraditional mortgage loans are untested in a stressed environment; for instance, nontraditional mortgage loans to investors that rely on collateral values could be particularly affected by a housing price decline. Bankers should ensure that borrowers have sufficient information so that they clearly understand, before choosing a product or payment option, the terms and associated risks of these loans, particularly how far monthly payments can rise and that negative amortization can increase the amount owed on the property above what was originally borrowed. These products warrant strong risk-management standards as well as appropriate capital and loan-loss reserves.\n\nCompliance-Risk Management\n\nAnother area the financial sector and the regulatory community are focused on today is compliance-risk management. “Compliance risk” can be defined as the risk of legal or regulatory sanctions, financial loss, or damage to an organization’s reputation and franchise value; this type of risk may result when an organization fails to comply with the laws, regulations, or standards or codes of conduct that are applicable to its business activities and functions. The Federal Reserve expects banking organizations to have in place an infrastructure that is able to identify and control the compliance risks facing their organization. Fortunately, many banking organizations in the United States substantially affected by these risks are ahead of the curve and have invested in enterprise-wide corporate compliance.\n\nTo create appropriate compliance-risk controls, organizations must first understand risks across the entire entity. Managers should be expected to evaluate the risks and controls within their scope of authority at least annually. I also emphasize the need for the board of directors and senior management to ensure that staff members throughout an organization understand the compliance objectives and the role they have in implementing the compliance program. Clear lines of communication and authority help to avoid conflicts of interest.\n\nAn enterprise-wide compliance-risk management program should be dynamic and proactive, meaning it constantly assesses evolving risks when new business lines or activities are added or when existing activities are altered. To avoid having a program that operates on “autopilot,” an organization must continuously reassess its risks and controls and communicate with its business lines.\n\nAn integrated approach to compliance-risk management can be particularly effective for Bank Secrecy Act and anti-money-laundering (BSA/AML) compliance. Often, the identification of a BSA/AML risk or deficiency in one area of business can indicate potential problems or concerns in other activities across the organization. Controlling BSA/AML risk continues to be a primary concern for banking organizations. We recognize the investment and commitment that organizations have made toward compliance with BSA/AML requirements, and, in return, we continue to work to ensure that obligations in this area are clearly understood and communicated to banking organizations and examiners alike. The Federal Financial Institutions Examination Council (FFIEC) BSA/AML Examination Manual issued last year is one example of our interagency efforts.\n\nWe also are working closely with our Treasury and law enforcement counterparts to disseminate information about perceived money-laundering or terrorist-financing threats. By identifying emerging vulnerabilities, we can better collaborate with banking organizations in our efforts to develop systems and procedures to combat the abuse of the financial sector by financial criminals. For example, the interagency Money Laundering Threat Assessment (4.1MB PDF) is one measure we are taking to identify significant concerns and communicate them to banking organizations.\n\nConclusion\n\nOur ongoing supervision of banking organizations indicates that the preponderance of institutions continues to be sound and well managed. This strong performance has occurred concurrently with institutions’ continued efforts to improve their risk-identification and management strategies. That said, there are certain rapidly growing business lines in banking operations that are placing pressures on risk-management systems. In turn, supervisors are increasingly scrutinizing these business lines to ensure that management is fully aware of their risks and has made any necessary risk-management upgrades.\n\nAs institutions continue to offer new products and services, they face the challenge of incorporating the associated risks into their existing risk-management framework. This is no small challenge, especially given the growing size and complexity of many banking organizations. Additionally, as supervisors, we want to ensure that institutions are not only able to identify, measure, and manage their risks, but that they are also developing and maintaining appropriate corporate-governance structures to keep up with their business activities and risk-taking. While most U.S. banking organizations enjoy substantial profitability today, they should remember that continued business success depends on their ability to prepare for unexpected, and potentially much less favorable, events and outcomes.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20060202a.htm",
        "title": "The Continuous Challenges of Risk Management",
        "date": "2/2/2006"
    },
    {
        "content": "January 18, 2006\n\nGovernor Susan Schmidt Bies\n\nBefore the Tech Council of Maryland's Financial Executive Forum, Bethesda, Maryland\n\nI appreciate the opportunity to speak with you today about productivity and the outlook for the U.S. economy. As you know, long-term growth in productivity is critically important to improving the standard of living in any economy. The rate of growth of productivity can significantly affect inflation and economic expansion. But before I comment on productivity, I'd like to begin with a review of recent economic developments.\n\nEconomic Developments\nReal economic activity has continued to expand at a solid pace. Clearly, the tragedy that hurricanes inflicted on New Orleans and surrounding areas of the Gulf Coast will have major implications for the people and the economy in those regions for a long time. However, for the nation as a whole, employment and industrial production indicators were only briefly disrupted by the hurricanes during the late summer and early fall. At the national level, consumer spending has been well maintained, and the fundamentals--such as income growth and household balance sheets--remain supportive. Many news reports and anecdotes suggest that the housing market is cooling and that investors are participating less actively. However, the construction of new homes has remained near recent highs. In the business sector, investment in new equipment continues to expand at a good clip, boosted by robust sales as well as ongoing replacement and upgrading needs. In addition, as I'll discuss in a moment, corporate financial conditions are favorable for investment.\n\nTurning to prices, core inflation has stayed relatively low in recent months despite the run-up in energy costs. For example, the twelve-month change in the price index for personal consumption expenditures excluding food and energy, a widely watched indicator of core inflation, moved down from 2.3 percent in November 2004 to 1.8 percent in November 2005.\n\nAlthough energy prices have receded from the highs last fall, crude oil costs are still well above year-earlier levels. As a result, gasoline prices remain elevated despite a decline of about 70 cents per gallon from the peak recorded in the aftermath of the hurricanes. The prices for home heating oil and natural gas will add to consumers' budget pressures this winter; although spot prices have moved lower in recent weeks, they are still well above year earlier levels.\n\nHigher energy prices have also affected businesses, particularly in those industries with energy-intensive production processes and those that purchase a large share of energy-intensive products, such as industrial chemicals and plastics. There is only limited evidence, most of it anecdotal, of pass-through to consumer prices from the run-up in energy prices. However, we are seeing the effects in the price data for certain energy-intensive categories, such as transportation.\n\nFutures markets currently expect only limited increases in the price of crude oil this year. Nevertheless, tight resource utilization is likely to put pressure on prices. The unemployment rate, at 5 percent in the second half of 2005, was down about 1 1/4 percentage points from its recent peak in early 2003 and at its lowest level in four years. Meanwhile, the factory operating rate--a measure of resource utilization in the manufacturing sector--was 79.6 percent in December, a rate that is approaching its 1972-2004 average of 79.8 percent. Within manufacturing, industries operating at utilization rates above their long-run averages include plastics and rubber products, iron and steel products, machinery, electronic products (excluding computers), and electrical equipment, appliances, and components. And although the overall high technology aggregate is below its long-run average rate of utilization, the operating rate at firms making computers and peripherals is above average, and the rate at manufacturers of communications equipment has risen significantly over the past year. As in the mid- to late-1990s, resilient productivity growth appears to be helping contain the inflationary pressures that might otherwise be expected to accompany a narrowing margin of resource slack. That said, we at the Federal Reserve will remain vigilant for any sign of a deterioration in the inflation outlook.\n\nAs I mentioned earlier, the core inflation rate has stayed relatively low in recent months, as rapid gains in productivity have tended to offset cost increases. Reflecting these developments is a continued rise in corporate profits, which has allowed firms to further bolster their strong balance sheet positions. Corporate balance sheets have improved dramatically over the past couple of years because of surging profits, low interest rates, and a concerted deleveraging, which have combined to reduce debt burdens and increase liquidity in the form of cash assets. Generally speaking, the growth of profits and the related buildup of cash have been broadly distributed across industries. And with the sound corporate financial positions, credit spreads remain narrow, and bank lending terms remain favorable. These beneficial financial conditions, combined with rising utilization rates, bode well for further increases in business capital expenditures. Indeed, capital expenditures for most types of equipment increased significantly during the third quarter, and Census data on orders and shipments suggest that investment continued to expand in the fourth quarter, with much of the gain in spending for information technology--that is, computers, communications equipment, and software.\n\nProductivity and Technology\nLet me now turn from the overall economic outlook to productivity and technology developments. Productivity growth receives a considerable amount of attention from policymakers because its rate is an important determinant of a nation's standard of living. The development of farm machinery in the early 1800s, for example, boosted the productivity of farmers and consequently freed up labor to shift to the industrial sector. More recently, continued increases in industrial productivity have enabled a relative shift of employment into the production of services. Although manufacturing employment has fallen sharply in recent years, both in absolute terms and as a share of total employment, the output of the nation's manufacturers has continued to increase because of impressive productivity gains.\n\nLooking beyond manufacturing to the broader nonfarm business sector, we see that productivity growth has risen significantly over the past decade in the United States. Labor productivity gains accelerated from an average annual increase of 1 1/2 percent over 1973-95 to an average annual increase of 2 1/2 percent over 1995-2001. From the first quarter of 2001 through the third quarter of 2005, labor productivity growth picked up even more--to an annual rate of nearly 3 1/2 percent. Thus, despite a recession, a tech-sector meltdown, a stock market correction, terrorism, and corporate governance scandals, our economy has proven remarkably resilient and productive.\n\nThese productivity gains result from many forces, including business investment that has increased the amount and quality of capital available to the workforce, business process innovations, and the growth of innovative, research-intensive industries such as information technology and biotechnology. Because firms may take a while to absorb a rapid run-up in investment, the productivity payoffs to investment may be drawn out for some time as firms learn more effective ways to use the capital they have acquired. Anecdotal information suggests that some of the recent productivity gains appear to reflect firms making better use of existing capital and improving business processes.\n\nAs I noted, the growing importance of the innovative technology sector has spurred productivity growth. I will focus on developments in the information technology (IT) sector; we at the Federal Reserve know more about IT than other high technology areas because of the availability of a wide variety of data. Moreover, economists better understand the role of IT in the U.S. economy, particularly its influence on productivity growth, because developments in this sector are more easily quantified than developments in other pioneering fields, such as biotechnology.\n\nIn thinking about information technology equipment and productivity, I find a useful starting point to be recalling the role of IT capital accumulation during the last business cycle. Work by Federal Reserve economists suggests that a large chunk of the increase in the rate of productivity growth in the late-1990s was due to the accumulation of IT capital. During that period, many firms invested heavily in IT in an effort to stay on top of the so-called \"technological revolution.\" One consequence of this drive to acquire high-tech equipment appears to have been a massive overhang of IT capital that has only recently been largely worked through. An example of this overhang has been the great amount of underutilized fiber optic cable resulting from the race to build fiber optic networks.\n\nThe accumulation of IT capital boosted productivity growth in many industries. At least some of the capital, however, may have been accumulated without a clear understanding of how to fully utilize the technology to create value for the adopting organizations. As a result, in recent years firms appear to be realizing further productivity gains as they discover new and better methods for using IT.\n\nAs I'm sure many in this audience know better than I do, one way in which firms have started to use IT more effectively is by exploiting synergies among network equipment, computing equipment, and software. These synergies may spur adoption of new technologies and increase the potential for further productivity gains. For example, in the 1990s, the combination of cheap modems, faster semiconductors, and browser software helped to jumpstart the World Wide Web. More recently, firms have been using leading-edge IT products to consolidate their networks, effectively enabling a single communications network to carry data, voice, and video. Besides innovations in hardware, ongoing developments in software have helped firms augment the capabilities of their communications equipment and computer networks. Increasingly, software upgrades are used to roll out new features without the need for heavy investment in new hardware. For example, a business using an Internet-based telephone system, better known as VoIP (voice over internet protocol), may be able to upgrade the features on its phones through a software update rather than buying new phones or installing new cables.\n\nFor many years, companies in the United States have been at the forefront of new technology developments. With demand for IT continually evolving, taking a moment to review the U.S. role in the production of IT equipment is worthwhile. Despite the globalization of high-tech production, the U.S. capacity to produce high-tech products is still increasing. The Federal Reserve Board's estimates of capacity in industries that manufacture high-tech equipment--which includes semiconductors and related electronic components, computers, and communications equipment--increased more than 20 percent between the fourth quarter of 2004 and the fourth quarter of 2005, after rising at an annual average rate of about 8 percent during the previous three years.\n\nSo, what high-tech equipment do we produce in the United States, when every item at the local computer store appears to have been made abroad? A significant portion of the U.S. capacity for high-tech production focuses on leading edge products and on customized products for which close proximity to customers is an advantage. Most desktop computers sold in this country are assembled here, as are many high-end computer servers and storage network devices. Within the category of communications equipment, much of the high-end networking equipment (such as the routers and switches used by telecom service providers) is produced domestically. U.S. factories also produce leading-edge microprocessors, certain flash memory products (used in portable media players, digital cameras, and cell phones), and a variety of semiconductors used in communications equipment. In contrast, laptops tend to be imported in a nearly completed state, and we are largely importers of mobile phones, computer peripherals (monitors, printers, and so forth), consumer communications equipment (such as home routers), and a great variety of semiconductors. But we must keep in mind that, even for products that are produced abroad and imported, U.S. companies continue to perform a significant share of the research and development that those products entail.\n\nBecause the development and the production of IT products play a vital role in the U.S. economy, the Federal Reserve continually watches for new developments in the pace of technological change and tries to gauge the likely influences of these developments on productivity growth. In most cases, data on the pace of technological change are hard to come by, so we carefully examine data on production and price trends. For example, the production of communications equipment dropped sharply during the last recession, and these products accounted for a large share of the capital overhang in high-tech equipment. However, since its trough in 2002, the Federal Reserve Board's industrial production index for communications equipment has risen about 75 percent, with particularly pronounced increases in the production of high-end routers and switches. These devices are needed to support advanced mobile technologies, the upgrading of telecom service provider equipment, and the attempts to solve the so-called \"last-mile\" problem--that is, a low-cost way to bring a fiber-optic speed data pipe down your street and into your home or business.\n\nThe pace of change in communications equipment and computers is, to some degree, related to the pace of change in the components of these products, particularly semiconductors. Even in the semiconductor industry, however, measuring the pace of technological change is difficult and fraught with uncertainty. The difficulty is partly that improvements along one dimension may introduce challenges along other dimensions. For example, as the industry shrinks the features of a semiconductor, such as the microprocessor in your personal computer, the chip can conduct calculations more quickly, but it also produces more heat. Another part of the difficulty is determining exactly when changes occur and how much of any change is due to a particular improvement. Industry data, for example, suggest that the introduction of improvements in calculation speed accelerated in the late 1990s and may have decelerated around the beginning of this decade. However, the exact dates are difficult to pin down, and they tend to vary with the type of semiconductor. In addition, any deceleration in the pace of introducing faster semiconductors may have been offset by innovative efforts to reduce energy consumption or to improve wireless capabilities. With the growing use of mobile, battery-powered communications technologies and computers, energy consumption and connectivity may be just as important as speed.\n\nIn light of the difficulties and uncertainties that are associated with measuring the pace of technological change, I want to take a moment to comment on the risks for the appropriate conduct of monetary policy that are associated with technology and productivity. Because technology feeds into various macroeconomic aggregates--including household and business spending, productivity, and inflation--its implications for the U.S. economy will continue to necessitate careful observation, improved measurement, and study. Members of the Federal Reserve staff, both at the Board and at the Reserve Banks, have contributed significantly along all three of these dimensions by improving measures of high-tech prices and output and by studying the implications of technology on U.S. productivity. A significant slowing in the pace of technological change could have inflationary consequences. Accordingly, monetary policy makers will remain alert, carefully monitoring technological developments that have the potential to mitigate inflationary pressures as well as developments that could raise the risk of overheating.",
        "position": "Governor",
        "href": "https://www.federalreserve.gov/newsevents/speech/bies20060118a.htm",
        "title": "Productivity and Economic Outlook",
        "date": "1/18/2006"
    }
]