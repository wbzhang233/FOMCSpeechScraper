[
    {
        "title": "The U.S. Economic Outlook and the Implications for Monetary Policy",
        "date": "Dec 5, 2016",
        "speaker": "William C. Dudley",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud161205",
        "content": "It is a great pleasure to be here and have the opportunity to talk about the U.S. economic outlook. I want to thank the Association for a Better New York (ABNY) for sponsoring today’s event. This is an excellent venue to speak about what we can do to improve our country’s economic performance. As always, what I have to say reflects my own views and not necessarily those of the Federal Open Market Committee (FOMC) or the Federal Reserve System.1\n\nThe U.S. economy—supported by solid gains in household spending—has expanded at a moderate rate in 2016. Job gains have been sturdy, and we have seen some firming in wage growth as the labor market has continued to tighten. Moreover, as the effects of earlier declines in energy prices have dissipated, the overall inflation rate has begun to move up closer to our 2 percent objective. As a consequence, economic conditions are not far from the Federal Reserve’s dual mandate of maximum sustainable employment and price stability. And, I expect that we will make further progress toward these goals in 2017. So, from a cyclical perspective, the economy is in reasonably good shape.\n\nOver the longer term, however, the U.S. economy faces significant challenges. On the positive side, economic expansions don’t die of old age, and there appear to be few imbalances in the economy that could lead to the current expansion ending. But, in order for this to remain the case, it is important that fiscal policy and monetary policy are well aligned going forward.\n\nIt is also important that the United States retains sufficient fiscal capacity so that fiscal policy can support the economy when the next cyclical downturn does occur. If fiscal policy can play a greater role in promoting macroeconomic stability, it would likely reduce the need for monetary authorities to take extraordinary actions to support economic activity.\n\nThere are other structural issues worth noting. In particular, productivity growth has been anemic over the past few years, while income inequality has increased and income mobility remains low. As a consequence, the gains in living standards generated by the current business expansion have been modest compared to previous expansions, and these gains have not been widely shared. Much more could be done both locally and nationally to increase the economy’s potential to perform better for a broader array of our citizens.\n\nThe Outlook for Growth and Inflation\n\nThe U.S. economy has been expanding at a moderate rate. Growth has averaged about 1.8 percent this year and seems likely to continue at or slightly above this pace in 2017. The main driver of growth in 2016 has been the consumer, as real personal consumption expenditures have increased at a 2.9 percent annual rate during the first three quarters. This solid consumption growth has been supported by sturdy job gains and rising nominal wages. Payroll gains have averaged about 180,000 per month this year. While this is down somewhat from 2015’s monthly pace of nearly 230,000, it is still considerably higher than the 75,000-110,000 monthly pace consistent with the likely long-term growth in the labor force. And wage gains, while still relatively muted, have begun to rise more rapidly as the labor market has continued to tighten.\n\nAnother positive factor for the economy is that household finances generally are in good shape. The household saving rate is 5.9 percent, which is a bit higher than one would expect based on historical relationships between household net worth and disposable income. And, after a long period of deleveraging, household debt has been growing, but very slowly. Over the last four quarters, household debt has risen by 2.4 percent. This slow pace, combined with low borrowing rates and an improving labor market, has pushed down the ratio of household debt service to income close to its lowest level since at least 1980. This suggests that households have the financial capacity to sustain their spending.\n\nIn contrast to consumer spending, many other areas of the economy have been considerably softer. Residential investment, after experiencing strong gains in 2015 and in the first quarter of 2016, fell during the past two quarters. However, increases in single-family housing starts and permits in October suggest that we are likely to see a reversal of this trend in the fourth quarter and into next year.\n\nBusiness fixed investment has also been weak for some time. Part of this weakness reflects the collapse in oil and gas drilling activity following the plunge in crude oil prices during the second half of 2014. This adjustment now appears to be over, as oil and gas prices have recovered somewhat. But, even outside of this area, business fixed investment has been disappointing. Several factors may be at play here—including earlier uncertainty surrounding the presidential election outcome and the fact that capacity utilization rates remain unusually low at this point in the economic business cycle. While the election uncertainty has been resolved, I would expect business fixed investment to only rise slowly in the year ahead.\n\nIn contrast, the trade sector has performed surprisingly well in 2016. This sector had to contend with headwinds created by weak growth in final demand by our major foreign trading partners, as well as the impact of earlier dollar strength on the nation’s export competitiveness. However, I’m not sure that I would take much signal from this performance. The improvement in trade seems to have been driven mainly by weakness in imports, particularly for capital goods, and by some one-off factors, such as the surge in soybean exports last quarter—both of which are unlikely to continue.\n\nWith respect to inflation, we are making progress in pushing toward our 2 percent objective. In particular, headline inflation has risen this year as the earlier declines in energy prices have dropped out of the year-over-year figures. And, core inflation has remained broadly steady, running at 1.7 percent over the past year—as measured by the personal consumption expenditures deflator that excludes food and energy. This stability is noteworthy, because one might have anticipated that lower energy prices and a firmer dollar would have pushed core inflation a bit lower. Also, household inflation expectations—which at times in 2015 appeared to be at risk of becoming unanchored to the downside—have been broadly stable. The University of Michigan long-term inflation expectations measure has generally remained in the 2.5-2.8 percent range of recent years. In addition, the New York Fed’s Survey of Consumer Expectations measure of 3-year median inflation expectations has stabilized in 2016 in a range of 2.5-2.8 percent, after declining modestly over the course of 2014 and 2015.\n\nImplications for Monetary Policy\n\nIf the economy grows at a pace slightly above its sustainable long-term rate, as I expect, the labor market should gradually tighten further, and the resulting pressure on resources should help push inflation toward our 2 percent objective over the next year or two. Assuming the economy stays on this trajectory, I would favor making monetary policy somewhat less accommodative over time by gradually pushing up the level of short-term interest rates.\n\nFollowing this year’s election, we have seen relatively large movements in financial asset prices. In particular, the stock market has firmed, bond yields have risen and the dollar has appreciated. On balance, it appears that financial market conditions have tightened modestly. My personal interpretation of these developments is that market participants now anticipate that fiscal policy will turn more expansionary and that the FOMC will likely respond by tightening monetary policy a bit more quickly than previously anticipated. Assuming this expectation is realized, the recent modest tightening in financial market conditions seems broadly appropriate.\n\nLet me emphasize here that I do not view the recent shift in financial market conditions as one that should prompt great concern. It is important to distinguish between a tightening of financial conditions that is driven by an increase in risk aversion from one that is driven by a greater likelihood of stronger near-term aggregate demand and less downside risk to the growth outlook. We experienced the former at the beginning of 2016, while the latter reflects current expectations of greater fiscal policy stimulus.\n\nObviously, there is still considerable uncertainty about how fiscal policy will evolve over the next few years. At this juncture, it is premature to reach firm conclusions about what will likely occur. As we get greater clarity over the coming year, I will update my assessment of the economic outlook and, with that, my views about the appropriate stance of monetary policy.\n\nMonetary and Fiscal Policy in a Low-Rate Environment\n\nI also would like to highlight the limitations of monetary policy. One important factor contributing to such limits is that the real short-term interest rate consistent over the longer run with a neutral monetary policy stance appears to be very low, and is expected to remain so for some time. This has been helping to keep real inflation-adjusted interest rates lower than they have been historically. In part, this reflects anemic productivity growth as well as the slowing of labor force growth due to the aging of the population. An implication of this environment is that there will likely be less scope than there has been historically for the Federal Reserve to cut interest rates when needed in the face of an economic downturn. All else equal, this means a greater risk for short-term interest rates to be pinned at their effective lower bound and for inflation expectations to become unanchored to the downside. Although forward guidance and large scale asset purchase programs have expanded the ways in which the Federal Reserve can provide additional accommodation, these unconventional monetary policy tools also have their own limits. These limits suggest that fiscal policy may need to play a greater role in stabilizing the economy than has been the case in past decades.\n\nWhat I have in mind here is putting in place stronger, more robust automatic fiscal stabilizers that would provide income support during economic downturns. By reducing fluctuations in disposable incomes, these types of fiscal actions would stabilize aggregate demand, thereby limiting the risk of monetary policy getting pinned at the zero lower bound for an extended period and the need for extraordinary monetary policy measures.\n\nI favor automatic, rather than discretionary, fiscal actions because they would typically go into effect more quickly and would be better anticipated. Expectations matter greatly in affecting economic behavior. For example, if the economy were to weaken, the anticipation that strong fiscal stabilizers would kick in to support incomes should lead workers to be less fearful about losing their jobs, and businesses to be less concerned that demand for their products might fall precipitously. This, in turn, would make workers more confident that they could sustain their spending, and would make businesses more confident that they could keep workers on their payrolls.\n\nWhat type of fiscal stabilizers would be most effective? I would turn first to those that Congress has implemented on a discretionary basis during past economic downturns, such as extensions of unemployment compensation and cuts in payroll taxes. For example, when the unemployment rate climbs, extensions of the duration of eligibility for unemployment compensation could be triggered automatically, helping to stabilize household income. Similarly, when the unemployment rate breaches certain thresholds, payroll tax cuts could be triggered, helping to support the disposable income of workers facing reductions in hours. Payroll tax cuts also have the advantage of skewing more toward low- and moderate-income workers, who typically have a higher propensity to consume out of current income.\n\nObviously, it is up to the incoming Administration and Congress to decide on the appropriate fiscal measures. But, the point that I want to highlight is that robust automatic fiscal stabilizers would complement monetary policy, and take some pressure off of the Federal Reserve to undertake extraordinary measures in situations where there is little scope for cutting short-term interest rates. This approach might also be superior to other proposals—such as raising the Federal Reserve’s inflation objective—that are designed to reduce the risk of monetary policy being trapped at the zero lower bound.\n\nFor such fiscal actions to be sustainable over time, it is important that the United States retain sufficient fiscal capacity. This would also ensure that automatic fiscal stabilizers are viewed as fully credible. On this score, while significant progress has been made in recent years in stabilizing the country’s fiscal situation, circumstances are likely to grow more challenging in the years ahead. There are three areas that I would highlight. First, the earnings that Federal Reserve returns to the U.S. Treasury Department each year are likely to fall in the future, as rising short-term interest rates narrow the gap between what the Federal Reserve pays on its liabilities—primarily currency and bank reserves—and what it earns on its portfolio of Treasuries and agency mortgage-backed securities. In recent years, the Federal Reserve’s remittances to the Treasury have averaged about $90 billion per year, far above the $20-30 billion average prior to the Great Recession. Although the precise trajectory of future reimbursements is highly uncertain, the Congressional Budget Office (CBO) projects that Federal Reserve payments to the Treasury will drop from around 0.6 percent of GDP in fiscal 2016 to 0.3 percent of GDP by 2026.\n\nSecond, the Treasury’s debt-service costs will likely grow as interest rates rise and the amount of outstanding debt held by the public continues to increase. While net outstanding Treasury debt held by the public has risen to $14.2 trillion from $4.8 trillion over the past decade, annual debt-service costs have only increased very modestly. Looking ahead, the CBO’s baseline projection is that debt-service costs will rise from 1.4 percent of GDP in fiscal 2016 to 2.6 percent of GDP in 2026.\n\nThird, the retirement of the baby boom generation will put significant upward pressure on Social Security and Medicare expenditures. Although the trend in Medicare expenditures has flattened out in recent years as healthcare inflation has moderated and utilization growth has slowed, the CBO nevertheless projects that such expenditures will rise from 3.2 percent of GDP in 2016 to about 4.0 percent of GDP in 2026. Consequently, significant pressures on the federal budget are still very much in train. It will be important that fiscal policy be managed so it retains the capacity to be used for macroeconomic stabilization.\n\nStructural Challenges\n\nWhile we have made steady progress over the past year toward our twin goals of maximum sustainable employment and price stability, significant challenges remain for the economy that lay largely beyond the scope of monetary policy. The three I would highlight are the sharp slowdown in labor productivity growth, the increase in income inequality and the low rate of income mobility.\n\nI draw attention to the slowdown in productivity growth for two reasons. First, productivity ultimately drives living standards and how people assess their economic well-being. Second, beyond ensuring a stable macroeconomic and financial environment, monetary policy can do little to improve productivity growth.\n\nAnnual productivity growth has averaged only 0.7 percent during the past five years, near the lowest 5-year pace since the early 1980s. There are a number of competing explanations for the slowdown. These include weak capital investment, a flattening out of educational attainment by new labor market entrants, less new business formation, understatement of quality improvements and, hence, output, and fewer ground-breaking innovations. I suspect that all of these factors play a role.\n\nWhat public policy can do to address this issue is to ensure that the economy operates closer to the frontier of what is achievable. For example, more efforts to improve job skills would prove beneficial—through retraining and apprentice programs, and better alignment of education and training with business needs. Also, encouraging small business start-ups would help—by removing barriers for entrepreneurs to establish new businesses, and by creating more start-up incubator programs. I also suspect that well-targeted infrastructure spending would be advantageous if it addresses logistical bottlenecks and enables people to commute to areas with good job opportunities. For example, in the New York City metropolitan region, mass transit is old, slow and crowded. As a New Jersey resident, I’m still waiting for that second set of rail tunnels into New York City that should have been built years ago.\n\nWhile higher productivity growth could support better living standards, the benefits for low- and middle-income households depend, in part, on how those gains are distributed between business and labor, and on how labor income is distributed across the workforce. Moreover, income distribution shouldn’t be viewed as a static concept. It is important that people are able to move upward in income through their careers, regardless of their starting point.\n\nOver the past several decades, three developments have been discouraging. First, the share of labor income relative to business income has fallen. Since 1970, the labor share of GDP in the United States has declined about 5 percentage points. Similar declines are evident in most developed and emerging market economies. Second, the distribution of labor income has become more skewed. Since 1970, the top 1 percent of households in the income distribution has more than doubled its share of the nation’s income. Third, despite the notion of the American dream, income mobility in the United States remains relatively low compared to many other countries. Also, the potential for income mobility is not evenly distributed. Research has shown that where you are born and raised in the United States still has significant implications for your future prospects.\n\nAs a country, we need to address these issues. With respect to the labor share of income, maintaining the U.S. economy at full employment should help. As the unemployment rate has fallen over the past two years, the labor share of income has moved up almost 1.5 percentage points. Supporting maximum sustainable employment is part of the Fed’s dual mandate, so we can play an important role here.\n\nWith respect to the rising concentration of income and the low level of income mobility, I think considerable improvements are also achievable. Income distribution can be influenced by tax policy and by educational opportunities. Income mobility can be influenced by local policies on education and housing. As Raj Chetty’s pathbreaking work makes clear, income mobility depends on where one is born and lives. This suggests quite strongly that improved access to good education, childcare and affordable housing would help.\n\nAs I noted in a recent speech, an important aspect of a city’s vitality is the ability of its residents to develop the skills necessary to be successful and achieve their dreams.2 A big asset for New York City is its access to affordable education. Each year, tens of thousands of low-to-moderate-income New Yorkers send their children to one of the city’s specialized high schools. When it comes time for college, many of these same students have access to the City University of New York (CUNY) and State University of New York (SUNY) systems. A recent highly regarded study assessed how much of an opportunity children born to less advantaged families in different parts of the United States have of moving up in the world. New York City ranked 10th out of the 50 largest U.S. metro areas.3 This is a better than average performance, but I think New York City should aspire to do even better.\n\nTo conclude, while the near-term outlook for the U.S. economy is reasonably good, there is considerable work to do over the longer term to improve our nation’s productive capacity and foster an environment in which the gains are spread more evenly. Monetary policy is limited in what it can do to address these broader structural issues. However, monetary policy will do its part to ensure the macroeconomic and financial stability that can help in achieving progress in these areas. Moreover, even with respect to macroeconomic stability, monetary policy could use an assist from fiscal policy. It is important that monetary policy and fiscal policy work together and not at cross purposes to be able to bolster the economy when it needs support.\n\nThank you for your kind attention. I would be happy to take a few questions."
    },
    {
        "title": "Remarks on the Panel Financial Regulation Nine Years on from the GFC – Where Do We Stand? at the G30’s 76th Plenary Session",
        "date": "Dec 3, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud161203",
        "content": "Thank you for the opportunity to participate in this panel discussion.  As always, what I have to say reflects my own views and not necessarily those of the Federal Open Market Committee or the Federal Reserve System.1\n\nIn assessing where we are eight years after the financial crisis, I would make three broad observations.  First, we have made considerable progress in bolstering the safety and soundness of the global financial system.  In the U.S., which was the epicenter of the crisis, the risk of a failure of a systemically important financial firm has declined considerably.  Second, although significant progress has been made toward ending “too big to fail,” there is still much more to do.  While the risk of failure of a global systemically important financial institution has diminished, it has not been eliminated.  Without a well-functioning resolution process, the consequences of such a failure could still be catastrophic.  Much headway has been made in the U.S. in developing a Single Point of Entry resolution regime with a layer of total loss-absorbing capacity (TLAC) that would facilitate recapitalization and enable an orderly resolution.  However, significant challenges remain, especially on managing resolution on a cross-border basis.  Third, bank leaders still have much to do to rebuild the trustworthiness of their industry.  Long after the financial crisis, conduct failures have persisted.  We need financial firms to foster cultures that are intolerant of bad conduct and that are attentive to incentive structures that may encourage such behaviors.\n\nSo, let me begin with the good news.  The U.S. financial system is much more resilient now than it was a decade ago on the eve of the financial crisis.  This progress reflects a number of initiatives―some focused on the ability of firms to absorb shocks, and others oriented at reducing structural vulnerabilities within the financial system that spanned the activities of firms and their counterparties. \n\nChanges made to capital and liquidity requirements in the U.S. and globally have made banks more resilient and robust.  The major U.S. banking organizations today have much more capital, much higher-quality capital, and much larger liquidity buffers than prior to the crisis.  On the capital side, we have taken a belt-and-suspenders approach.  The Basel III risk-weighted capital standards framework has been significantly strengthened―a stronger belt―and a supplemental leverage ratio requirement has also been imposed on large banks and bank holding companies to restrain overall leverage―the suspenders.  In addition, and perhaps most importantly, the capital requirements have now become much more forward-looking.  Each year a bank’s capital dividend and share-repurchase plan is assessed against the amount of capital the bank would potentially have under an extremely stressed economic and market environment given its portfolio of assets.  The annual Comprehensive Capital and Analysis Review (CCAR) process has pushed U.S. banks to improve their capital planning processes and forced them, when needed, to retain more earnings and to build up their capital.  As a result, these banks are much less risky now than they were prior to the financial crisis, when their vulnerability was masked by a high, but unsustainable, level of profitability.\n\nOn the liquidity side, there has also been significant improvement.  The Liquidity Coverage Ratio and the Net Stable Funding Ratio requirements have forced banks to hold more high-quality liquid assets and have reduced the scope for maturity transformation.  The ability and willingness of banks to finance illiquid, hard-to-value assets with short-term funding was an important element that contributed to the financial crisis.\n\nIn addition, I believe that supervisory oversight has improved.  There is more attention on horizontal evaluations across the systemically important firms and a greater emphasis on bank governance, risk management, cyber security, and bank data and technology capabilities than in the past.\n\nBut the progress has not just been in terms of enhancing the strength of individual institutions. The system as a whole is also stronger because of a number of important initiatives. Due to time constraints, I cannot be exhaustive today.  But, there are four important structural changes to the financial system that are worth highlighting.  First, the tri-party repo system—which matches institutional investors with bank dealers that need to fund their securities portfolios—has been made much safer.  Before the crisis, each morning the two large clearing banks provided huge amounts of intraday credit to their tri-party repo borrowers when cash invested overnight was returned to lenders.  These temporary extensions of credit have been largely eliminated.  This is an important structural reform because these exposures had proven to be destabilizing during the financial crisis.  In particular, when a major dealer became stressed, the clearing banks were motivated to reassess their large overdraft exposures to the dealer, and that turned out to be an important channel of contagion.\n\nAnother important change has been the shift in over-the-counter derivatives activity from bilateral settlement to central clearing.  Although this shift was already underway at the time of the financial crisis, it has now been underpinned by important changes to the capital and collateral regime, which has accelerated the process by strengthening the incentives to structure and clear trades centrally. \n\nWhen banks settle their trades with central counterparties (CCPs), it reduces risk in the system.  Exposures can be netted, and the counterparty risk is shifted from many individual firms to the single CCP that clears the trades.  Although this shift in clearing activity still has some way to go, the proportion of centrally cleared trades will continue to rise as legacy trades mature.\n\nA third important structural change is the increased focus on the strength and resiliency of CCPs.  Because CCPs are now going to play a more significant role, they must be strong and resilient.  There have been many important initiatives in this regard.  Foremost, perhaps, is the promulgation of the international standards embodied in the Principles for Financial Market Infrastructures (PFMI)—which were developed when I chaired the Committee on Payment and Settlement Systems (CPSS)—and the work currently underway on financial market infrastructure resolution.\n\nA fourth significant structural change has been reform of the U.S. money market mutual fund industry.  The October 14th implementation of floating net asset values (NAVs) for prime institutional money market mutual funds has dramatically reduced their use and has encouraged banks and other borrowers to find sources of funding that are less prone to runs.\n\nThe financial crisis exposed two important flaws of the money market mutual fund industry.  First, the fixed NAV structure created a first-mover advantage that encouraged runs.  That is, if a fund came under stress, investors who quickly withdrew their funds received par value.  Second, although money market mutual fund investments are often collateralized, the funds themselves have no desire to take possession of the collateral in a crisis.  Instead, they tend to head for the exits at the first sign of trouble, which makes the financial system less stable.\n\nMuch progress has been made in making the U.S. financial system safer and less prone to panics.  Still, there is more to do before we can say that we have ended “too big to fail.”  This is work that we absolutely must complete.  In particular, U.S. bank holding companies must do more to make their organizations resolvable under either Title I bankruptcy or Title II resolution.  This requires having clean parent holding company structures, less corporate complexity, and essential service and support operations that are able to continue to operate even when the parent company becomes non-viable. \n\nAlso, while there has been significant progress in terms of developing a workable resolution regime—the Single Point of Entry regime established under Title II of the Dodd-Frank Act—we still have to ensure that this regime can be safely utilized on a cross-border basis.  This requires fully implementing the TLAC framework, and defining more explicitly the expectations and responsibilities of the different supervisory and regulatory oversight bodies that would have to work together internationally.  In particular, this includes how liquidity would be provided to an insolvent firm during the early stages of resolution, so that the firm’s operations could be wound down in an orderly way without destabilizing the global financial system.\n\nFinally, I would like to address the issue of bank culture and conduct.  Although there has been considerable effort here, there is still much more to do.  Banks must restore their trustworthiness.  Only then can they effectively perform their critical financial intermediation role in funneling funds from savers to borrowers, and in helping firms and households manage their finances and risk exposure.\n\nAs I see it, incentives drive behavior, and behavior establishes the social norms that define culture.  This means that bank leaders need to take a close look at the incentives they put in place with respect to compensation and promotion in particular.\n\nIn addition, bank leaders must foster an atmosphere that encourages people to speak up when they witness questionable behavior.  Preet Bharara, the U.S. Attorney for the Southern District of New York, highlighted the importance of getting people to speak up when he spoke at the New York Fed’s conference on bank culture in October.  He argued—and I agree—that big problems can be nipped in the bud when people speak up and senior management responds appropriately.  Therefore, bank leaders ought to recognize and reward those who speak up.  Actions speak louder than words in shaping culture.  Establishing appropriate incentives and fostering an environment in which people feel they can speak up would go a long way toward bolstering bank culture. \n\nI am gratified that many bank leaders and boards of directors are working hard to improve their firms’ cultures, and I very much appreciate the G30’s efforts in this area.  I think most leaders recognize that good culture is fully consistent with strong financial performance, and that it helps enable banks to attract the talent needed to sustain such performance.  Yet, we could make greater progress more quickly if this effort were coordinated across the industry.  I think bank leaders could benefit from sharing best practices and participating in surveys that would allow them to benchmark their strengths and weaknesses versus others on an ongoing basis.\n\nBuilding a financial system that is more resilient and robust, having a bank resolution regime that credibly ends “too big to fail,” and building strong cultures that restore the trustworthiness of the financial industry are all goals we must continue to pursue.  Without success on all three fronts, I don’t think we can say that we have fully addressed the problems made evident by the financial crisis.\n\nThank you for your kind attention.\n\n1 Beverly Hirtle, Antoine Martin, Susan McLaughlin and Joseph Tracy assisted in preparing these remarks."
    },
    {
        "title": "Opportunities for Economic Growth in Puerto Rico",
        "date": "Nov 29, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud161129",
        "content": "I am very pleased to be here and to have this opportunity to speak with representatives of Puerto Rico’s business and academic communities.  In my remarks, I will discuss the lessons learned from municipalities in the United States that have faced crises over the years―such as New York City―and what those outcomes might imply for Puerto Rico as it seeks to restore and sustain economic growth.  I will also touch briefly on the Puerto Rico Small Business Survey, a new survey from the New York Fed that focuses on the business performance, financing needs and borrowing experiences of firms across the Island.  As always, what I have to say reflects my own views and not those of the Federal Open Market Committee or the Federal Reserve System.1\n\nMy meeting with you today is part of the New York Fed’s continuing efforts to understand what is going on at the grassroots level within our Federal Reserve district.  We plan these trips so we can interact with a diverse group of stakeholders.  These meetings help me get a more complete picture of economic conditions on the Island and a fuller understanding of the major issues and concerns.  This is the third time we have visited Puerto Rico in the past five years.\n\nYesterday, I met with Governor-elect Rosselló to discuss the transition into his new role and the key challenges for his upcoming term.  I also spent time with members of El Colaboratorio, a shared workspace organized by the Foundation for Puerto Rico.  I attended a roundtable discussion with local businesses and spoke with former and current members of several of the New York Fed’s advisory councils.\n\nEarlier this morning, I met with the heads of a number of financial institutions and the University of Puerto Rico’s interim president, Celeste Freytes.  Following this visit, I will meet with the leadership from Parallel18, an economic development initiative that attracts early-stage, high-potential startups to Puerto Rico.  Finally, I will join the leadership of the Boys and Girls Clubs of Puerto Rico for a discussion on the organization’s initiatives and a site visit to one of the clubs to learn about workforce development programs and youth services.\n\nAs we all know, the Commonwealth of Puerto Rico has been through a very rough 10 years.  The economy has been in a long slump that could easily be called a depression.  Residents have endured a terrible drought, periodic blackouts and a fiscal crisis.  I don’t need to elaborate on all of the struggles Puerto Rico continues to face.  The Island has been suffering from a vicious cycle.  As employment declines, people move to the mainland to find jobs.  Accordingly, economic activity and tax revenues decline, forcing the government to raise taxes and cut spending, which further reduces employment and prompts still more residents to move.  At this point, the outlook for anyone who has been living here must be bleak, and it may be hard to imagine this situation turning around anytime soon.  And yet, history tells us that a turnaround, though not inevitable, is indeed achievable. \n\nLessons from Past Crises\n\nNew York City is probably the most famous example of a municipality that experienced a major fiscal crisis, recovered and is now thriving.  New York City’s experience is viewed, in retrospect, as a major success, and some, including myself, have pointed to it as a hopeful example of what could come to pass in Puerto Rico.  But, there are important caveats to the New York City story and its applicability to the Commonwealth.\n\nLike Puerto Rico, New York City experienced a number of years of both political and economic pain as a result of its fiscal crisis in the 1970s.  On the political side, New York City residents lost a large measure of their ability to chart their own fiscal direction for several years.  Decisions about tax and spending policies were removed from elected city officials.  The Emergency Financial Control Board was empowered to review all fiscal decisions of local officials and demand changes to the city’s budgets.  Part of the city’s tax base was dedicated to bond repayment—essentially handing it over to the city’s creditors.  For 10 years, the city could not access the debt markets without support from the state.\n\nThe economic pain was also very real.  More than 70,000 city government workers were cut from the payrolls in 1975 and 1976.  City residents faced dramatic service reductions—fire houses closed, schools and police precincts had fewer resources, and crime continued to rise for many years.\n\nIt is also important to note that New York isn’t the only U.S. city that has been through a major fiscal crisis.  Cleveland, Philadelphia, Washington, D.C. and, most recently, Detroit have all struggled to manage their budgets, and all ultimately reached a point when they were unable to pay their bills.  As a result, these cities lost their fiscal autonomy to one degree or another.  In the case of New York City, this turned out relatively well, as the short-run loss of decision-making power led to a strong economic rebound that eventually restored the city’s ability to finance its own choices.\n\nSo, is New York City’s experience really all that instructive for Puerto Rico?  With the advantage of hindsight, it now seems almost obvious that New York City’s fiscal crisis was only going to be a detour.  The city’s economy had many fundamental strengths, so it only needed some assistance to get back on track.  Still, it’s worth pointing out that this was by no means clear at the outset the crisis in the mid-1970s.  Then, many commentators looked at New York City and other large cities as outmoded forms of economic activity, unlikely ever to recover.\n\nWhile we do not have great measures of economic activity for cities, population change is a good indicator of how attractive a place seems to be.  By that measure, New York City at the time was seen to have very poor prospects—the city’s population fell by over 10 percent during the 1970s.  So, New York City’s eventual bright future was far from clear as its fiscal crisis was unfolding.\n\nOf course, not every city that has faced a fiscal crisis has the same track record.  Relative to cities that had similarly grim outlooks, New York City’s rebound has been much stronger.  In Detroit, there are hopeful signs, but it’s too soon to tell how things will turn out.  Both Washington, D.C. and Philadelphia experienced multiple decades of population declines leading up to their crises. These population declines reversed in the 2000s.  Washington, D.C.’s population actually grew over 5 percent between 2000 and 2010, a rate even faster than that of New York City.  Cleveland’s fiscal reforms, on the other hand, were less comprehensive, and the city has never really turned itself around.  Its population in 2010 was less than half the level of 1960.\n\nThe different performances across cities reflect the complex interplay of native endowments, economic conditions and fiscal outcomes.  As in Puerto Rico, the fiscal woes of many of our largest cities have been closely related to long-term economic transformations—essentially, changes in the industrial specialization that occurred in those particular places.  In the case of New York City, the transformation was from manufacturing to services, and it took a generation to complete.  The city’s management of that transition was far from perfect, and the fiscal crisis was the major manifestation of that failure.  The institutions set up to respond to the crisis were crucial to ensuring that fiscal policy didn’t continue to amplify the problems associated with the economic transition.  For a city to turn its circumstances around, it must navigate this economic transformation, identify what will define its comparative advantages going forward, and then build anew around those sources of strength.\n\nThe Road to Recovery\n\nA successful fiscal reform first requires a regime in which local officials recognize and accept the reality of the changing economic situation, and set spending budgets accordingly.  This part is fairly straightforward, because it produces the simple policy dictum that borrowing—in all its forms—must be strictly limited.  Of course, knowing what to do and actually getting it done are different things, especially when politics are involved.  As a result, the record suggests that it can be extremely helpful to have some kind of independent fiscal monitor in place on an ongoing basis.  It’s important that this monitor be insulated from the direct influence of politics, giving it freedom to deliver objective analysis that may at times be very unpopular.  In Washington, D.C., this took the form of the Office of the Chief Financial Officer; in New York City it is the Independent Budget Office, which is modeled after the Congressional Budget Office.  These monitors have proven invaluable in keeping policymakers and the public informed on budgetary matters, and in providing apolitical expertise to assist in keeping budgets on a sustainable course. \n\nThe second major component is a successful completion of the economic transformation, which is a critical factor for reversing―rather than simply halting―a city’s decline, establishing a virtuous cycle of rising growth and strengthening fiscal outcomes.  New York City had the huge benefit of its comparative advantage in finance—an industry that was poised for sustained growth in the post-fiscal-crisis period of the 1980s through the 2000s.  Still, the city weathered a lot of shocks during that period, including the 9/11 terrorist attacks, the financial crisis and Hurricane Sandy.  It’s also worth pointing out that New York City is still prospering, even though employment in its finance sector has stagnated.  This illustrates the need for cities to continually look for new areas of growth, such as health care, technology, and leisure and hospitality services.\n\nThis is why PROMESA (the Puerto Rico Oversight, Management, and Economic Stability Act) emphasizes long-term growth, and why the New York Fed has been focused on identifying impediments to the Commonwealth’s growth.  Since 2012, we have released two major reports and several shorter analyses—most of which appeared in our Liberty Street Economics blog—presenting ideas that, if implemented, might help to make the Island more competitive and reverse its economic decline.  These ideas have ranged from the very narrow and technical to the very broad—from improving the quality of Commonwealth financial reporting to bolstering growth through education and job training to build the Island’s human capital.\n\nSince I’ve identified managing the ongoing economic transformation as a major component of a successful plan for Puerto Rico, let me summarize a few themes on that front.  Most of these feature an important role for the Commonwealth’s government, underscoring the fundamental importance of getting public policy right as a means of restoring economic growth.\n\nFirst, it is important to reduce the cost of doing business on the Island as a way of encouraging business growth and increasing the demand for labor.  We have put forward many ideas to consider here, ranging from ones targeted at hiring—such as giving Puerto Rico more autonomy over setting its minimum wage policy—to more general ideas like streamlining regulation.  One thing that we’ve repeatedly emphasized is the need to find efficiencies in the Island’s energy industry.  Commercial customers on the Island pay roughly 20 cents per kilowatt hour for electricity—nearly double the mainland average.  Island residents and businesses, as you are well aware, are also burdened with exceptionally high water utility bills.  It is also important to think hard about the supply of labor—the willingness of people to work, and the education and skills that they have to bring to the market.  Labor supply is equally important, but less widely discussed than the demand side of the market.  Labor force participation is lower at all ages in Puerto Rico than on the mainland.  This is a crucial impediment to growth, and addressing it requires innovative policy options.  One idea to address this problem would be to extend the Earned Income Tax Credit to Puerto Rico.  In addition, the structure of the Commonwealth’s tax system creates disincentives to work in the formal sector, and suggests that tax reform could produce a growth dividend as well as a more reliable source of revenue.  On top of the low labor force participation is the well known and deeply concerning high level of out-migration.  In addition to the lack of labor demand, it seems likely that quality-of-life concerns such as high crime rates are playing a role in these decisions.  This underscores the importance of basic public service delivery and is another example of the interplay between fiscal and economic outcomes.\n\nAnother important dimension of labor supply involves the skills that workers bring to the market.  Many of these skills are acquired while in school.  Our research indicates that test scores for school-age children on the Island are low, suggesting that there is an opportunity for improvement in the quality of public education.  One clear lesson from recent research in urban and regional economics is that the collective skills of the people living in an area are a critical ingredient for its growth prospects.  Improving the education available to residents can pay large dividends over time.\n\nI know that this list seems long, and none of issues I have raised are easy to address.  Nonetheless, I am confident that Puerto Rico has started on the road to recovery.  In the current environment, it’s hard to see beyond the immediate crisis to a brighter future for Puerto Rico, but that has been said of other places that have come back stronger than ever.  Getting the fiscal situation in order is an important first step.  The factors leading up to the crisis took many years to develop, and history shows that a successful recovery from a crisis also takes time.  It is important to recognize that fact and to stay the course.\n\nBefore I conclude, let me briefly discuss our new survey.  Earlier this month, the New York Fed released the Puerto Rico Small Business Survey, which focuses on the business performance, financing needs and borrowing experiences of firms across the Island.  The survey is intended to fill important knowledge gaps and flag potential economic growth opportunities for the Commonwealth.  It was designed with input from partners here in Puerto Rico, and we are very grateful for their cooperation.\n\nKey findings reveal that firms are persevering through Puerto Rico's economic crisis, though a majority reported declining revenues.  Managing cash flow is a top concern, ranking above concerns about rising business costs.  There is significant demand for credit, with the most common reason being the need to meet operating expenses.  For about half of all small firms, credit needs were under $25,000.  Lastly, about 80 percent of firms do not sell online or export their wares or services.  Firms see this as a growth opportunity, ranking programs in support of increasing sales—to the federal government, via online commerce and through exports—as their top training need.  We hope that the survey will provide useful insights into a key economic sector and help inform policy to support Puerto Rico’s recovery.\n\nThank you for your kind attention.  I would be happy to take a few questions.\n\n1 Jason Bram, Tony Davis, Andrew Haughwout and Joseph Tracy assisted in preparing these remarks."
    },
    {
        "title": "Opening Remarks at the Economic Press Briefing on the Survey of Consumer Expectations",
        "date": "Nov 18, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud161118",
        "content": "Good morning, and welcome to the New York Fed’s Economic Press Briefing.  I am pleased to have the opportunity to speak with you today about our Survey of Consumer Expectations.  This is an important source of information that we use in our analysis of current and future economic conditions to help inform our monetary policy decisions.  As always, what I have to say reflects my own views and not necessarily those of the Federal Open Market Committee or the Federal Reserve System.1\n\nIn pursuing its monetary policy objectives, the Federal Reserve relies on a large number of data sources that provide information about recent developments and the current state of the economy.  Most of these data sources are backward looking.  That is, they provide data about past decisions and outcomes.  In contrast, we know relatively little about the forward-looking subjective expectations and intentions of economic agents, which are a key determinant of current and future outcomes.  This information, though, is essential for formulating policy and evaluating its effectiveness.  The macroeconomic outcomes we observe reflect preferences and constraints, as well as beliefs of individual consumers and firms.  Therefore, a fuller understanding of such outcomes requires reliable data on the beliefs and intentions held by economic agents and how they evolve over time.  In particular, consumer expectations influence a variety of considerations, including spending and saving behavior, wage dynamics, job search, housing demand and the demand for credit.  Given their importance in determining U.S. economic activity, consumer expectations are of particular value in assessing the economic outlook and the related risks.\n\nOver the last decade, we have devoted significant resources to improving the measurement of the economic expectations of households.  This work culminated in the creation of the Survey of Consumer Expectations, or SCE, in 2013.  The expectations data collected in our survey give us direct insight into households’ decision-making and consumers’ financial and economic outlooks.  While we monitor expectations on a number of topics, of particular importance are households’ medium-term inflation expectations.  Having reliable measures of inflation expectations is crucial to assess the effectiveness of monetary policy and central bank communications.  Such an assessment focuses on analyzing the extent to which longer-run inflation expectations remain well-anchored, and on learning more about how agents update and revise their expectations in response to new information. The SCE allows us to carry out this sort of analysis.\n\nBesides their obvious policy relevance, the SCE data are also highly valuable for research.  Our economists have shown that the SCE’s expectations data are informative and predictive of respondents’ current and future behavior.  They have also documented the substantial heterogeneity in the formation and updating of expectations across demographic groups.  To facilitate further research, we make the survey data available in the Center for Microeconomic Data section of our website.  Each month we also illustrate the trends in our expectations series through easy-to-use, interactive charts.  Further, a number of our Liberty Street Economics blog posts describe the data and the analysis we have conducted.  We do this because we want researchers, policymakers, commentators and the general public to understand the data and to follow our updates.\n\nLet me now describe our Survey of Consumer Expectations in more detail.\n\nThe Survey of Consumer Expectations\n\nFollowing extensive design and testing, we launched the Survey of Consumer Expectations in June 2013.  The survey has a number of innovative and unique features, three of which I would like to highlight today.  First and foremost, as I mentioned earlier, it collects households’ subjective expectations and intentions on a wide range of economic outcomes and behaviors.  In addition to short- and medium-term inflation expectations, it collects data on households’ expectations regarding the housing, labor and credit markets, as well as households’ financial situations and a host of other macroeconomic variables of interest.  For example, the survey elicits beliefs about future wage growth, house price appreciation, job search and labor market transitions, households’ spending and income growth, and credit applications.  By collecting data on a broad range of expectations, it is possible to analyze how they interact with each other and jointly influence households’ decisions.  We also study how expectations vary with demographics and other household characteristics.\n\nThe broad scope of the SCE allows us to study many questions relevant to economic policy—such as understanding housing market decisions, disentangling credit demand from supply, analyzing job search behavior and measuring wage growth as people transition between jobs.  In addition, the survey has been used to help answer other policy-relevant questions as they have come up in our discussions.  Two examples are households’ spending response to the decrease in gas prices in early 2014, and households’ early experiences and perceptions of the Affordable Care Act following its enactment.  Some of this analysis relies on the ability of our survey to shed light on whether consumers expect a specific change in economic conditions to be temporary or permanent.\n\nA second important feature of the survey is that—in addition to simple point forecasts—for some variables, such as inflation, we get respondents’ entire forecast distributions by asking them to assign probabilities to various ranges of future outcomes.  These so-called density forecasts in turn allow us to measure households’ uncertainty about future events by looking at the spread in their subjective distributions.  Density forecasts also allow us to monitor the perceived likelihood that respondents assign to specific events, such as deflation or a large increase in earnings.\n\nA third important characteristic of the SCE is that it is a rotating panel.  Each month about 1,300 household heads participate in our survey.  This is a nationally representative sample. Respondents remain on the panel for up to 12 months, with a constant fraction entering and rotating out each month.  This allows us to monitor the expectations of the same respondents over time, which provides important benefits.  First, it gives us greater confidence that month-to-month changes in our headline numbers reflect true changes in beliefs, and not just changes in the sample composition.  Second, observing repeated responses by the same individuals allows us to examine how expectations are formed and revised over time.  Third, by following the same people over time, we are able to study how expectations are related to contemporaneous and future economic decisions and outcomes for these respondents.\n\nToday the SCE team will provide further background about the survey, discuss newly released findings for October and recent trends in some of its main indicators.  The team will then present findings from two special components of the SCE: one that covers households’ credit experiences and expectations, and one that deals with labor market behavior and expectations.  Before I pass this on to my colleagues, I would like to comment briefly on these two special surveys.\n\nNew findings on Credit Access\n\nThe SCE Credit Access Survey supplements the information collected in the Federal Reserve Board’s Senior Loan Officer Opinion Survey on Bank Lending Practices, also known as SLOOS.  The SLOOS is a primary source of information on credit standards and credit demand as perceived and reported by banks.  In contrast, the SCE Credit Access Survey, which has been conducted at a four-month frequency since October 2013, measures experiences in credit markets as perceived by households.  The combination of the two data sources provides a more complete picture of household credit access and demand.  Today, we are releasing the October 2016 findings as well as introducing new data from the survey.\n\nA unique feature of the SCE is its measure of latent credit demand—that is, demand that does not lead to an application for credit.  We measure this latent credit demand as the proportion of households who report that, while needing credit, they did not apply because they expected their application to be rejected.  In addition to current credit experiences, we also elicit households’ expectations of their future credit applications and the likelihood of these applications being approved.  As we will show later, the survey provides useful insights into the extent and variation in met and unmet credit needs across households.  Moreover, our analysis indicates that reported expectations are indeed predictive of credit outcomes reported four months later.\n\nNew Findings on Job Search\n\nWhile the SCE Credit Access Survey allows us to track credit market experiences and expectations, the SCE Labor Market Survey gathers information on the job search behavior and expectations of employed and unemployed respondents.  Aggregate labor market statistics on earnings, employment and labor market transitions are readily available, yet relatively little is known about the expectations and search behaviors of labor market participants and how these change over time.  Our SCE Labor Market Survey collects rich information on respondents’ actual and expected wage offers, as well as their reservation wage, which is the lowest wage or salary they say they would accept for a new job.  We also elicit expectations about future labor market transitions, such as the likelihood of staying with the same employer, switching employers, finding a job or being laid off.\n\nOver time, changes in responses to these questions can shed light on how workers’ expectations evolve, and how these changes relate to observed trends in aggregate earnings and employment dynamics. Our preliminary analyses of the data suggest that these expectations measures are predictive of respondents’ subsequent labor market outcomes. In coming years, as we generate a longer time series for the survey, we will be able to explore whether the expectations we collect can be used to construct leading indicators of the job market.\n\nNow, I'd like to turn things over to our economists, beginning with Olivier Armantier, who will provide further background and specifics about our survey.\n\n1 Olivier Armantier, Giorgio Topa, Joseph Tracy, Wilbert van der Klaauw and Basit Zafar assisted in preparing these remarks."
    },
    {
        "title": "Opening Remarks at the Evolution of Work Conference",
        "date": "Nov 17, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud161117",
        "content": "Good morning.  I would like to welcome you to this conference on the Evolution of Work.  Today, we will explore how the nature of work is evolving, including the role of technology, shifts in employee work arrangements and employer-employee relationships, and the effects of these changes on workforce development strategies.  We are pleased to host this conference with the Board of Governors of the Federal Reserve System and the Freelancers Union.  We welcome all the leaders from across academia, government, industry, labor and the nonprofit sector assembled here to explore the transformation of the workforce, and the impact this has had on communities and the economy more broadly.  As always, what I have to say today reflects my own views and not necessarily those of the Federal Open Market Committee or the Federal Reserve System.1\n\nAs you know, the Federal Reserve monitors the labor market closely, given that maximum sustainable employment is one goal of the Fed’s dual mandate.  The evolving nature of the workplace—and the speed with which it is changing—are therefore important developments that we must continue to monitor.  Traditional nine-to-five jobs are becoming less common, and people are increasingly employed in less traditional work arrangements.  Measuring the creation and destruction of these new jobs is of utmost importance to gain a better assessment of the health of the labor market.  We also need to understand how this evolution is affecting job security and earnings stability.  While these jobs might provide workers with more flexibility, they might also come with increased income volatility and financial vulnerability.  In today’s gathering, we will discuss the measurement challenges we face to better understand this growing segment of the labor market, and we will facilitate a dialogue aimed at developing a workforce aligned with the evolving nature of work. \n\nThe Fed’s efforts to understand labor markets include analyzing the comprehensive data that exist now.  But, in an ever-changing workplace, it is critical that we hear from external stakeholders to understand their views on these changes and to ensure we incorporate them in our assessments.  This is an important part of our discussion.  As technological changes persist, it is inevitable that the nature of our jobs will continue to evolve.  We may not readily recall the days when there was little job flexibility and we did not have today’s technology to assist us in our work.  While technological transformation allows for further growth in productivity and innovation, we must also think through how these changes affect the workforce overall.  We must realign our workforce development strategies with these changes.\n\nToday, we will also focus on what this evolution means for low- and moderate-income households.  One session will explore on-the-ground community development and labor models, with a focus on the implications for low- and moderate-income earners.  This will be an important discussion on how to help workers build the skills necessary to adapt to change—a conversation that should ultimately include workers, their employers, their communities and the public sector, working together.  We all need to share in the responsibility of helping people increase and maintain the skills relevant to contributing valued work.  Again, I wish you a fruitful dialogue, and I commend you for coming together to discuss such important issues and the trends affecting the U.S. workforce.\n\n1Nora Fitzpatrick, Laura Pilossoph, Anika Pratt and Aysegul Sahin assisted in preparing these remarks."
    },
    {
        "title": "Opening Remarks at the GARP Global Risk Forum",
        "date": "Nov 2, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/sti161102",
        "content": "Good Morning. I'm Kevin Stiroh, the head of the Supervision Group at the Federal Reserve Bank of New York. I'd like to extend a warm welcome from the Federal Reserve Bank of New York, which is co-hosting the Global Risk Forum with the Global Association of Risk Professionals. I'd like to thank the many organizers of this meeting—as you can imagine, considerable time and resources went into planning this conference and that is what makes it so valuable for the participants and the risk management profession.\n\nLooking Back at the Global Risk Forum\nWhen I was asked to speak at today's session, I realized that it's been five years since I served on the planning committee that developed the Global Risk Forum program in 2011. I wondered how our risk priorities and risk management issues have evolved over those five years as the world has changed around us.\n\nWith that idea in mind, I thought it would be useful to look at the last five years of this Forum's program in order to identify key themes and common topics. I'd like to discuss three specific issues: systemic risk, governance, and regulatory reform.\n\nFirst, systemic risk has been a central topic of discussion at each Global Risk Forum over the past five years. Given the acute trauma of the financial crisis and its systemic nature, that is not surprising. I did find it interesting to see how the discussions of systemic risk have shifted over these five years. Initially, the discussions were primarily focused on defining systemic risk, including the underlying shocks and propagation channels. During these initial conversations, the concept of macroprudential supervision was introduced as a means of mitigating systemic risk, but the discussion largely centered on defining what macroprudential supervision might entail, and how it differs from more traditional microprudential supervision. More recently, the systemic risk discussions have become increasingly technical and practical in nature—focusing on the details of the resolution regime, monitoring shadow banking and considering implications of shifting activity, and on actual approaches for conducting macroprudential supervision.\n\nSecond, governance has been a frequent topic. Again, this is understandable given its central importance for private sector risk management and the official sector reform agenda. It is interesting that the assortment of discussion topics under the governance agenda item has been remarkably stable over the years. The agendas examined the appropriate roles of the board of directors and senior managers, the degree to which the official sector should prescribe risk management practices and principles, and the need to tailor such expectations depending on an institution's business models and risk profiles. We should also think beyond the traditional governance topics to determine if other key risk types should be covered within these frameworks. I'll return to this latter point.\n\nThe third theme is the changing nature of the discussions around regulatory reform. In 2011 and 2012, much of the agenda focused on the \"what\" and the \"how\" of the reform package—what should be included in the reforms and how would these mechanisms work in practice. Discussion questions were geared toward concerns about the feasibility of these new policies and regulations, and the need for global coordination and cooperation. In recent years, as you know, the global regulatory community has made significant progress on finalizing the post-crisis reforms and the conversation has naturally shifted to the impact of these regulatory reforms; for example, a desire for consistency across firms, business models and jurisdictions and an assessment of factors driving changes in market liquidity.\n\nIn raising these three frequently discussed topics – systemic risk, governance, and regulatory reform—I also wonder what topics have emerged that we are not discussing, but perhaps should be. Since the financial crisis, I think we are all more aware of the importance of actively looking for our blind spots. Gillian Tett, writer for the Financial Times and author of The Silo Effect recently visited the NY Fed, and reminded us to look broadly. In her case, when trying to understand social dynamics, it is important to look not just in the middle of the dance floor, but also at the \"non-dancers\" outside of the spotlight. In our case, it is what other factors might create either idiosyncratic or systemic distress.\n\nSo, with that perspective in mind, I noticed two surprising omissions from prior Global Risk Forum agendas. The first is cybersecurity. Over this time period, there has been an increasing awareness among risk professionals about the constant and present danger that cyber-threats pose to the financial sector. Cyber-threats are a vital concern for the supervisors responsible for the safety and soundness of the financial system and for individual risk managers. In the U.S., the Department of Treasury, our intelligence agencies, financial regulators and supervisors, and risk professionals across the financial sector are working to defend against cyber-attacks and to minimize the potential disruption resulting from a successful attack. U.S. authorities are working to finalize objectives-based supervisory expectations for risk management around cyber threats. I'm encouraged that this year's Forum includes an expert panel on \"Technology Risks\" that will explore the cyber-threat landscape and approaches to mitigate the associated risks.\n\nA second noteworthy omission relates to a specific component of governance. While some governance topics have been remarkably stable, in my view, risks associated with business misconduct at financial institutions have not received adequate attention in these Forums. Through initiatives like the annual conference on \"Reforming Culture in the Financial Services Industry,\" the New York Fed has been challenging the financial industry to consider the many factors that have contributed to recent, widespread instances of misconduct.\n\nIn concert with the industry, I believe that supervisors can help the industry improve governance, culture and behavioral outcomes through supporting the development of effective internal governance regimes, prudent risk management policies, incentives, and strong compliance and control structures, all within a framework of effective oversight from the board of directors. In today's Forum and in the future, I welcome other participants' thoughts on approaches for developing, measuring, and maintaining a strong, ethical culture within the industry.\n\nConclusion\nAs risk professionals, it's important to be forward-looking and to continually ask ourselves: are all risks being captured, measured, and monitored? What is missing? Are we focused on the right areas?\n\nSo, here is another question: When we look back at this conference five years from now, what will we view as a major blind spot? I expect there will be issues that we will be surprised we missed, and part of the purpose of discussions like this is to lower the likelihood and impact of those inevitable blind spots.\n\nThanks for your attention, and I'm looking forward to hearing the discussion throughout the day."
    },
    {
        "title": "Keynote Address at the Institute for International Bankers Annual Seminar on Risk Management and Regulatory Examination/Compliance Issues",
        "date": "Nov 1, 2016",
        "speaker": "F. Christopher Calabia",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/cal161024",
        "content": "I’d like to thank our colleagues at the Institute for International Bankers for inviting me to participate in your seminar today. As representatives of internationally active banks, you are helping to bridge the gaps between regions and continents by promoting trade and investment between our countries. You play an integral role in enabling individuals and enterprises to conduct business far afield from their native lands; in some cases, you may also expand the choices that local customers in our market have for seeking credit and other banking services. You do so by conducting your business far afield from your firm’s own home markets, through local offices here in the United States.\n\nBanks doing business across borders accept a dual responsibility for compliance: they agree to do business within the laws and regulations in the home and host countries.\n\nIn thinking about how international bankers must adapt to the rules and expectations of a host country, I’m reminded of the words of one famous international banker, T.S. Eliot.\n\nNow T.S. Eliot is actually not famous for being a banker. He was an American-born British subject who did indeed work in the international division of a bank in London from 1917-1925. We know him today because T.S. Eliot also wrote essays, plays, and poetry. Today he is recognized as one of the greatest English language poets.\n\nIn response to one interviewer’s question about how he made the transition from writing poetry to writing plays, Eliot explained that he first worked hard to understand the technique of writing for the theater so that he could later forget those techniques. As he put it, Eliot felt “it’s not wise to violate rules until you know how to observe them.”1\n\nI doubt you will ever hear a bank regulator encourage you to violate any rules. I’d like to stress that I’m not suggesting you should ever do so! Still, Eliot’s remarks resonate with me when I think about the challenges some firms face when developing risk management or compliance programs.\n\nIf I might transform his words a bit, we are at the greatest risk of violating rules when we don’t know how to observe them. And in my experience as a banking supervisor, I’ve found that one of the simple reasons that banks – both foreign and domestic – may fall afoul of rules and regulations can be that their managers did not understand or did not take the time to understand the purpose of those rules.\n\nSo I’d like to discuss some of the rules and expectations that we supervisors have for three sets of emerging issues that are of particular importance to foreign banking organizations operating in the United States.\n\nFirst, cybersecurity has become one of the top concerns for all organizations in the public and private sectors that rely on information technology. I’d like to share some of the most common challenges we have seen in branches and agencies and our relevant expectations as supervisors.\n\nNext, as part of the post-crisis regulatory reform in the United States, many branches and agencies are now expected to adopt even stronger controls and risk management processes as part of our enhanced prudential standards. Important standards have come into force this year, so I’ll address the requirements to establish and report to a U.S. Risk Committee as my second topic and some of the requirements related to financial resilience as my third.\n\nMost of my comments will focus on the U.S. operations of foreign banking organizations that maintain less than $50 billion in banking assets in the United States.\n\nOn all of these topics, I’d like to share my sense of the purpose of the underlying rules and regulations so that you may be better able to understand and observe those rules. Any opinions I share are my own and may not reflect those of the Federal Reserve Bank of New York or the Federal Reserve System.2\n\nLet’s start with cybersecurity.\n\nOur world in many ways is smaller, faster, and more accessible than ever before. Ever more sophisticated devices and networks have helped us to overcome the historical divisions between countries and continents created by distance.\n\nBanking has certainly benefited from these advances: the same technology that allows a food blogger to share instantly a picture of a meal she is enjoying half way around the world enables that same person to deposit a paper check without visiting a branch or to pay a bill without speaking to a banker.\n\nYet our dependence on these technologies and infrastructure has exposed weaknesses that malevolent actors have learned to exploit. The motivations for attacks on banks in particular may vary from seeking to steal information or funds to simply creating embarrassment or havoc for the firm or for its customers and counterparties.\n\nBranches and agencies operating abroad face special challenges in managing their cybersecurity in part precisely because they operate remotely from the head office. To simplify my discussion today, I’m going to use the word branch to relate to a firm’s U.S. operations, but please note that my observations are relevant to agencies and often local U.S. subsidiaries of foreign banks as well. I’ll mention two challenges that are unique to the structure of a local branch’s operations, and I’ll conclude this section with a glance at the future.\n\nSome head offices face challenges in overseeing these local systems. Often the local systems may not be familiar to head office auditors. We’ve found that, when head office auditors do visit their branches in our District, they may review solely general IT controls in place in the branch. They may not evaluate U.S. specific systems that the branch maintains or perform vulnerability tests of those systems. We would urge head office to develop capacity to conduct such audits of systems maintained at U.S. branches or, when necessary, to rely on appropriate external auditors who can help evaluate such systems on a head office’s behalf.\n\nConversely, when branch management outsources technology services to third party vendors, too often management has failed to clearly document the monitoring activities that they undertake to ensure that vendors adhere to contractual requirements and that vendors maintain acceptable security and performance levels. We supervisors expect firms to develop such documentation to ensure that all arrangements are well managed and well controlled.\n\nAs supervisors, our goal for U.S. branches is to transform their responses to cybersecurity challenges from ad hoc responses to more clearly defined and articulated processes and standards. While supervisors have already offered some tools to help in this regard – such as the interagency cybersecurity assessment tool3 -- U.S. supervisors are proposing a more enforceable set of standards that go beyond the guidelines we have offered to date.\n\nIn that vein, on October 19, the Federal Reserve, the Office of the Comptroller of the Currency, and the Federal Deposit Insurance Corporation issued a joint advanced notice of proposed rulemaking.4 This notice solicits comments from the industry on ways the agencies might establish enhanced cyber risk standards for the largest and most interconnected firms subject to their supervision. The agencies are contemplating such standards as a way to increase the operational resiliency of large firms and to reduce the potential damage to the financial system in the event of a cyber incident.\n\nAs discussed in the advanced notice, these new standards if approved would apply to larger firms, such as U.S. bank holding companies, savings and loan holding companies, and potentially state and nationally chartered banks and other institutions with more than $50 billion in total consolidated assets. The standards would apply as well to foreign banking organizations with $50 billion or more in total U.S. assets.\n\nThe proposed enhanced standards would set additional supervisory expectations within the current supervisory framework and are grouped into five categories:\n\nTogether, these five categories would constitute the “first tier” of more enforceable expectations applicable to larger firms.\n\nThe advanced notice furthermore suggests that supervisors may propose a second tier of cyber requirements for those firms that have “sector-critical systems” – such as establishing a 2-hour “recovery time objective” to resume business after a cyber event.\n\nThe agencies have requested feedback on these initial proposals by January 17, 2017. I would encourage you to review these standards and offer comments for the agencies’ consideration.\n\nI’d like to turn now to my second and third area of focus and share observations on the enhanced prudential standards that now apply to the U.S. branches and agencies of foreign banking organizations.\n\nAs you know, the Dodd-Frank Wall Street Reform and Consumer Protection Act introduced a host of new standards for capital, liquidity, and governance especially for large U.S. firms, but also for foreign banks active in the United States. Congress enacted this law in response to some of the lessons learned from the recent financial crisis. Our overarching goal is to strengthen resilience and to reduce the risk that a significant disruption or outright failure of a large U.S. or foreign bank might threaten the stability of the financial system or hinder the growth of the broader economy.\n\nFor branches of foreign banking organizations, one of the most interesting lessons learned through the crisis is how much their risk profiles, activities, and strategies have evolved recently.\n\nWhen T.S. Eliot wore pinstripes in London, a bank’s overseas offices might have served largely to facilitate trade and commerce between countries. Internationally active banks supported customers engaged in global commerce by offering letters of credit and other forms of trade finance; by offering international payments or correspondent banking services; or perhaps by facilitating a customer’s access to credit in overseas markets or to foreign exchange.\n\nIn contrast, even just over the last 20 years in the United States, we have witnessed a dramatic progression in the role that some foreign banks play in our markets.\n\nThe enhanced prudential standards applicable to foreign banks are intended to strengthen their resilience and reduce the need to access emergency funding sources, such as the Discount Window or other facilities in the future. Many of those regulations became effective this past July.\n\nRegulations arising from the Dodd Frank Act that are relevant to foreign banks include the requirement for banks with the largest U.S. operations – those with over $50 billion in U.S. assets held outside of a branch structure – to establish intermediate holding companies that will be subject to capital and liquidity requirements similar to those expectations set for domestic holding companies of that size.\n\nToday, I’d like to focus mainly on the requirements imposed on a broader set of foreign banking organizations, including those with smaller U.S. operations that were not required to establish a holding company, which would apply to most of the firms represented at this conference. The requirements relate to governance on the one hand and to financial resilience on the other.\n\nU.S. Risk Committee\n\nThe first requirement I’d like to discuss is for qualifying foreign banking organizations to establish a U.S. Risk Committee. For firms not subject to the intermediate bank holding company requirement, this committee must be established at the board of directors back home.\n\nThis requirement applies to foreign banking organizations with more than $10 billion in global assets if publicly traded, as well as to other foreign banking organizations with total global consolidated assets of $50 billion or more.\n\nIn my view, the intention for this rule is to ensure that a head office plays an active role in overseeing the firm’s activities in the United States and that it has board members and other senior executives with sufficient focus on and understanding of U.S. markets.\n\nFor organizations with less than $50 billion in U.S. assets, the U.S. Risk Committee must conduct two tasks under the new regulation.\n\nAs supervisors, we expect that the home country banks are taking appropriate measures to ensure that U.S. operations implement the policies created by the U.S. Risk Committee. In addition, we expect that the U.S. operations provide sufficient information to the U.S. Risk Committee so that it can carry out its responsibilities.\n\nWith regard to this latter point, one open question remains exactly what it means to provide sufficient information to the U.S. Risk Committee. The final rule doesn’t specify the type, frequency, or quality of information that would satisfy the “sufficiency” requirement outlined above. The Board of Governors has not defined this expectation further.\n\nHowever, our experience supervising other kinds of firms, such as domestic bank holding companies, might lend some insight into what a U.S. Risk Committee might find useful to receive from a bank’s U.S. operations. Again, these represent my views only.\n\nThese requirements are all intended to ensure that large, internationally active firms that conduct banking activities in the United States have a good understanding of those activities. We want senior leaders at head office to review material events and supervisory issues that occur within their U.S. operations so that they can best support their businesses here. That, in turn, should reduce the likelihood of significant disruptions or losses that could affect the U.S. financial system. As always, we expect that head office will be a source of strength to its U.S. offices. The U.S. Risk Committee requirement helps to make that expectation tangible.\n\nAnother way that head office serves as a source of strength to its overseas offices is by possessing adequate financial resources to withstand unexpected losses in its global operations. So I’ll turn now briefly to the requirements related to financial resilience for organizations that maintain less than $50 billion in U.S. assets.\n\nLet’s begin with capital. Under the enhanced prudential standards, firms with U.S. assets below $50 billion must be subject to capital standards at home that are equivalent to those set by the Basel Committee on Banking Supervision.\n\nSuch firms are also required to be subject to one of the most important tools to emerge from the financial crisis, namely stress testing, in which firms imagine how economic conditions might change in the future and evaluate whether they have sufficient capital and liquidity to withstand future potential losses.\n\nThe largest and most systemically important U.S. and recently foreign banking organizations are now subject to the most comprehensive U.S. stress testing requirement, the Comprehensive Capital Analysis and Review (“CCAR”). So today l’d like to address instead requirements that apply to smaller foreign banking organizations.\n\nUnder the enhanced prudential standards, foreign banking organizations with greater than $10 billion in global assets that maintain U.S. operations are required to be subject to a stress test by their home country supervisor and provide it to their supervisor for review.\n\nWe don’t intend to subject foreign banks with less than $50 billion in U.S. assets to a CCAR-like process. Instead, we want the firm to submit only the stress test results, and not the actual stress test. The Federal Reserve would review only whether the firm passed a supervisory stress test and would not comment on the process or methodology. We are still developing a process for how firms will submit their stress test results to us.\n\nIf the bank is not subject to a home country stress test, its U.S. branch/agency network would be subject to asset maintenance requirements, and any U.S. subsidiary banks would be subject to a U.S. stress test.\n\nWith regard to liquidity stress testing, foreign banking organizations with more than $50 billion in global assets will be required to submit the annual results of a global liquidity stress test or of a combined U.S. operations stress test. The test should consider the Basel Committee’s standards that cover scenarios for 30 and 60 days, plus a one year scenario. We are still developing the procedures through which firms will submit this information.\n\nOur goal for capital and liquidity stress testing is to have a better understanding of how well the global parent organization can withstand future potential economic downturns or shocks such that it can continue to serve as a source of strength to its U.S. operations.\n\nI set out to offer some remarks today on emerging issues that the U.S. operations of foreign banking organizations face and how U.S. supervisors expect firms to be best prepared to address them.\n\nWe’ve discussed the most current thinking ranging from how a branch should protect its systems from cyber attacks to how we’ll ensure that the parent organization back home is best able to understand the risks and issues facing its U.S. operations and how well prepared it is to continue to serve as a source of strength to its businesses here throughout the business cycle.\n\nWith that, I’ve come to the end of my prepared remarks.\n\nYet, as T.S. Eliot wrote, “the end is where we start from.”\n\nBanks and supervisors alike have a lot of work ahead of us as we strengthen the resilience of firms and of the broader financial system. To do so well, we need to remain in close dialogue. We supervisors need to understand the challenges that you face. In turn, we’d like to be sure that you understand the purpose of our rules and expectations and so that your firms can best observe those rules.\n\nIn that vein, I’d like to express my thanks to our colleagues at the Institute for International Bankers for hosting this dialogue today."
    },
    {
        "title": "Remarks at The Evolving Structure of the U.S. Treasury Market: Second Annual Conference",
        "date": "Oct 24, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud161024",
        "content": "On behalf of the New York Fed and our co-sponsors, it is my pleasure to welcome you to this second annual conference on the evolving structure of the U.S. Treasury market.  We have the opportunity today to continue the dialogue around the changing nature of the Treasury market, a discussion that grew more urgent following the events of October 15, 2014.1 We have made significant progress since last year’s conference in improving our collective understanding of the Treasury market, and I believe it is important to continue official and private sector collaboration on this topic. Indeed, there is much left to do to ensure the continued integrity and effectiveness of this market, and I look forward to the further progress we will make today.  As always, what I have to say today reflects my own views and not necessarily those of the Federal Open Market Committee or the Federal Reserve System.2\n\nOver the past year, the Joint Member Agencies3 have engaged with market participants and other members of the public to gain more information on the changing structure of the Treasury market. The Treasury’s Request for Information, and comments on various SEC and CFTC proposals, have provided thoughtful perspectives on the evolution of Treasury market structure. We have also benefited from consultation with groups such as the Treasury Markets Practices Group (TMPG), which is made up of market professionals committed to supporting the integrity and efficiency of the Treasury market.\n\nContinued engagement between the official sector and the public on the Treasury market remains necessary to help to manage its evolution, and to best maintain its efficient function and integrity.  Those of you who transact in this market have a crucial perspective on how the Treasury market works and what information is useful for evaluating its function.  Engagement between the official and private sectors can also facilitate the development of timely solutions to important issues in the industry.  Such efforts include the best practices the TMPG has produced for the Treasury, agency debt and agency MBS markets,4 and the Group of 30’s recommendations on banking culture and conduct5―an issue I believe the financial industry must continue to work on.6 In fact, it is this type of collaboration―between a wide array of market participants and official agencies―that helps different perspectives come together to create effective practices.7\n\nIn addition to engagement with the public, the Joint Member Agencies have continued their inter-agency collaboration.  We have been working together to share information and help shape the most effective path forward for public policy.  These actions include the signing of a memorandum of understanding that further facilitates information sharing related to the Treasury market, and again co-hosting this conference. Going forward, the staff of the Joint Member Agencies will continue to work together on effective and comprehensive data collection, monitoring of market activity and liquidity through the Interagency Working Group, development of principles and best practices for Treasury market data transparency, and the regulatory framework for government securities.\n\nA main focus of recent work by the Joint Member Agencies has been to improve the availability of information to both the official sector and the public. Given this emphasis, it is worth spending a few moments discussing the benefits of obtaining timely and robust data on Treasury market transactions. \n\nData are essential to understanding flash events and market liquidity. Attendees at this conference are very familiar with the events of October 15, 2014, when the 10-year Treasury yield experienced a 37-basis-point trading range over the session.  At that time, market participants struggled to identify what had caused that sharp movement.  Similarly, a flash event occurred in the British pound just this month. The pound depreciated nearly 6 percent, and then largely retraced this move within several minutes, seemingly without a major catalyst. Once the domain of equity markets, flash events have happened with increasing frequency, also occurring in the dollar-yen and euro-dollar currency pairs in recent years. \n\nTreasury and foreign exchange markets share a number of common attributes. They are evolving rapidly, they are highly automated in key market segments, and information on trading activity is not widely available.  As a result, it is challenging for the official sector, market participants, and members of the public to effectively analyze these markets, understand the sources and risks of flash events, and evaluate how liquidity is changing.\n\nGiven the role of the Treasury market as the deepest and most liquid fixed income market in the world, it is clear that both the official sector and the public need improved access to transaction-level data, and I am pleased that we are making real progress on this front. Greater transparency into Treasury market activity is necessary in maintaining the market’s many important roles:  as a risk-free benchmark for financial instruments; as a liquid investment and source of safe collateral; as a channel for the implementation of monetary policy; and, of course, as a tool to help finance U.S. government activities.\n\nAs the Treasury market continues to evolve, several complex questions lie ahead. How should data reporting gaps be closed to ensure that as many relevant Treasury transactions as possible are collected?  What is the proper scope and best process for sharing Treasury market data with the public? How can the Joint Member Agencies and private stakeholders best collaborate to ensure Treasury market stability and efficient market function, even as market structure evolves and new technologies emerge?  What considerations need to be made regarding Treasury market clearance practices given the growth in activity by participants who are not central counterparty members?  Will the evolving settlement marketplace introduce new challenges to Treasury market participants?  How must practices and culture evolve as market structure does?  And, how will the interplay of all of these issues affect the future of the Treasury market?\n\nToday’s conference provides an excellent opportunity to discuss these issues. Thank you for coming. I look forward to today’s dialogue and the ongoing engagement between the Joint Member Agencies and the public as we all strive to ensure the continued sound functioning of this essential marketplace.\n\n1 Joint Staff Report: The U.S. Treasury Market on October 15, 2014, U.S. Department of the Treasury, Board of Governors of the Federal Reserve System, Federal Reserve Bank of New York, U.S. Securities and Exchange Commission, U.S. Commodity Futures Trading Commission, July 13, 2015.\n\n2 Ellen Correia Golay, Frank Keane and Nathaniel Wuerffel assisted in preparing these remarks.\n\n3 The Joint Member Agencies are the U.S. Department of the Treasury, the Board of Governors of the Federal Reserve System, the Federal Reserve Bank of New York, the U.S. Securities and Exchange Commission and the U.S. Commodity Futures Trading Commission.\n\n4 Best Practices for Treasury, Agency Debt, and Agency Mortgage-Backed Securities Markets, Treasury Markets Practices Group, Revised February 2016.\n\n5 Banking Conduct and Culture, Group of 30, July 2015.\n\n6 Opening Remarks at Reforming Culture and Behavior in the Financial Services Industry: Expanding the Dialogue, William C. Dudley, October 20, 2016.\n\n7 The Role of Best Practices in Supporting Market Integrity and Effectiveness, Simon Potter, Remarks at the 2016 Primary Dealers Meeting, September 7, 2016."
    },
    {
        "title": "Opening Remarks at Reforming Culture and Behavior in the Financial Services Industry: Expanding the Dialogue",
        "date": "Oct 20, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud161020",
        "content": "Welcome to the Federal Reserve Bank of New York, and to our third conference focused on culture within the financial services industry.  It is encouraging to see all of you in attendance today—in particular, the many non-executive directors, representatives from the investor community, and foreign supervisors who have joined us to help broaden and deepen the dialogue.  Thank you for your participation.  As always, what I have to say today reflects my own views and not necessarily those of the Federal Open Market Committee or the Federal Reserve System.1\n\nThe evidence is pervasive that deep-seated cultural and ethical problems have plagued the financial services industry in recent years.  Bad conduct has occurred in both investment banking and securities market activities as well as in retail banking.2  This has eroded the industry's trustworthiness.\n\nThis erosion impedes the ability of the financial services industry to do its job.  That job is financial intermediation—to facilitate the efficient transfer of resources from savers to borrowers, and to help customers manage the financial risks they face.  Verification—whether through regulation or internal controls—is an expensive substitute for trustworthiness.  Fines for bad behavior drain resources that could be better used to expand access and improve services, but billions of dollars in avoidable penalties are just the start.  The time spent handling a legal crisis is time not spent on more productive pursuits.  Moreover, I worry that, in the long term, an industry that develops a reputation for dubious ethics will not attract the best talent.3 \n\nIn contrast, a trustworthy financial services sector will be more productive and better able to support the economy.  Reliable financial intermediaries can help increase the flow of credit, promote economic growth and make the financial system more stable.  This is why restoring trustworthiness must be the ultimate goal of reforming culture. \n\nThe industry's shared norms—its culture—will not change by mere exhortation to the good, whether from me or from the industry's CEOs.  In my experience, people respond far more to incentives and clear accountability than to statements of virtues and values.  The latter are worthy and necessary, but remain aspirational or even illusory unless they are tied to real consequences.4  What does it mean for a firm to profess to putting the customer first, if employees are compensated and promoted regardless of what's good for customers?  Or, worse, if they are not held to account for activities that can harm customers?  If we focus on nothing else in today's conference, let's explore how best to structure incentives and reinforce accountability to align with core purposes and first principles.\n\nTo put it very simply, incentives drive behavior, and behavior establishes the social norms that drive culture.  If the incentives are wrong and accountability is weak, we will get bad behavior and cultures.  This implies a role for both firms and supervisors.  Firms need to continually assess their incentive regimes so that they are consistent with good conduct and culture.  When they are not consistent, the incentives need to be changed.\n\nSupervisors should also play a role in assessing firms' incentive structures.  For example, they should evaluate:\n\nThe answers to these questions should indicate whether incentives must be changed in order to foster the appropriate behaviors and culture.\n\nThe primary responsibility for reforming culture—and changing incentives—belongs to the industry.  However, the industry does not act alone.  The public sector can play an important role as well.  I'll discuss that issue this morning with my colleagues Norman Chan, chief executive of the Hong Kong Monetary Authority, and Minouche Shafik, deputy governor of the Bank of England.  Later this morning another panel will discuss the ways in which supervision can further contribute to improving bank culture.\n\nLet's also consider ways in which new laws or regulations might help—especially to overcome perennial collective action and first-mover problems that are common across the industry.  Two years ago I proposed solutions to two such obstacles to reforming culture.  First, there should be a database of banker misconduct to combat the problem of \"rolling bad apples.\"5  Second, a baseline assessment of culture is needed in order to measure progress.  I proposed an industry-wide survey, but there may be other good alternatives.  Once again, I invite the industry to take the initiative on these issues, and to look to the public sector for support.\n\nI also hope that we will attend to issues that we may have overlooked in our earlier discussions.  Gillian Tett of the Financial Times argues in her new book, The Silo Effect, that the key to understanding any culture is identifying and explaining \"social silences\"—the issues that are not being discussed.6\n\nWhere do we encounter those issues in reforming the culture of financial services?  One place might be the role of investors in firms.  Are their expectations of returns reasonable and consistent with a trustworthy financial services industry?  Another place might be the boardroom.  Boards should establish and promote a culture based on ethics and respect for law.  Have boards risen to this challenge?  If not, what's needed?  And, going forward, are we thinking hard about how technological change will influence incentives, behavior and culture?  We'll hear these issues discussed in several panels today. \n\nThese issues require thoughtful reflection by the industry.  The need for reflection, though, is not limited to boards, or even to banks.  Regulators and bankers alike can benefit from such a habit.  In that vein, I want to share with you an everyday checklist proposed by Ignazio Angeloni of the European Central Bank.7  He has posed six questions that seek to make responsibility for culture both individual and routine:\n\nI suspect that while seemingly simple, the discipline of asking ourselves these questions on a regular basis and answering them thoughtfully and honestly may be more challenging, thought provoking and effective than one might initially think.\n\nI thank the staff at the New York Fed who put together today's program, and their colleagues who are in the audience and watching today.  The diverse professional training and experience of our staff are among the New York Fed's strengths.  Bank supervisors, risk managers, economists, traders and lawyers contribute insights from their respective professions that lead to better outcomes. \n\nIn the spirit of learning from other disciplines, I am delighted to introduce today's keynote speaker.  Some of you may recall that, two years ago, Sir David Walker opened our first culture conference with the words, \"I am grateful to the philosopher Onora O'Neill for reference to very relevant advice.\"8  Today we have gone to the source. \n\nBaroness O'Neill is professor emeritus of philosophy at the University of Cambridge, former president of the British Academy, and former chair of the United Kingdom's Equality and Human Rights Commission.  She was appointed a life peer and member of the House of Lords in 1999, and, in 2007, was elected an Honorary Fellow of the Royal Society.\n\nToday, Baroness O'Neill serves as a member of the Banking Standards Board.  Her 2002 lectures on trustworthiness—reprinted in the book A Question of Trust—have been tremendously influential among New York Fed staff working on culture.  Baroness O'Neill, the floor is yours.\n\n1 Stephanie Chaly, James Hennessy, Jacqueline McCormack, Thomas Noone and Joseph Tracy assisted in preparing these remarks.\n\n2 William C. Dudley, Ending Too Big to Fail, Remarks at the Global Economic Policy Forum, November 7, 2013.\n\n3 Thomas C. Baxter, Jr., The Rewards of an Ethical Culture, Remarks at the Bank of England, January 20, 2015.\n\n4 Daniel Awrey and David Kershaw, \"Toward a More Ethical Culture in Finance: Regulatory and Governance Strategies,\" in Nicholas Morris and David Vines, eds., Capital Failure: Rebuilding Trust in Financial Services, 284 (2014) (\"While codes of conduct, ethics, and best practice can be drafted and held up as identifying desirable behaviors, the norms these codes purport to reflect may be overpowered by other countervailing influences.\").\n\n5 Mark Egan, Gregor Matvos, and Amit Seru, \"The Market for Financial Advisor Misconduct,\" Working Paper 22050, National Bureau of Economic Research, February 2016.\n\n6 Gillian Tett, The Silo Effect, 45 (2016).\n\n7 Ignazio Angeloni, Ethics in finance: a banking supervisory perspective, Remarks at the Conference on \"The New Financial Regulatory System: Challenges and Consequences for the Financial Sector,\" September 26, 2014.\n\n8 David Walker, Trust and Trustworthiness in Banks and Bankers, Remarks at the Federal Reserve Bank of New York, October 20, 2014."
    },
    {
        "title": "New York City’s Return from the Brink",
        "date": "Oct 19, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud161019",
        "content": "It is a great pleasure to have the opportunity to speak here this evening at the Lotos Club, one of America’s oldest and most distinguished literary clubs. I am, of course, very happy to participate in tonight’s public affairs dinner and to accept the Lotos Award of Distinction.\n\nTonight I would like to take a moment to discuss some of New York City’s recent history and its return from the economic brink.  Aside from being a story of historical interest, I believe there are lessons embedded in New York City’s experience that are relevant to other cities and municipalities that find themselves struggling today. As always, what I have to say reflects my own views and not necessarily the views of the Federal Open Market Committee or the Federal Reserve System.1\n\nGiven the historical relevance of today’s date, it seems appropriate to start by recalling what happened on this day 29 years ago.  In what has come to be known as “Black Monday,” the U.S. equity market suffered one of its steepest one-day drops in history, with the Dow Jones Industrial Average falling 508 points—a decline of more than 20 percent.  This capped one of the worst weeks ever for the U.S. stock market.  Many feared the worst for the U.S. economy—and especially for New York City.  After all, the stock market crash in 1929 was followed by the Great Depression.\n\nAs a young economist working on Wall Street, I certainly was worried—the situation didn’t seem consistent with future job stability.  I went out that evening and bought myself a leather jacket.  If I lost my job, I wanted to leave with something on my back, something tangible.  As it turned out, although the city’s economy and Wall Street did indeed sustain a steep downturn, both were able to recover and to prosper.  Not only was there no depression, the economy avoided falling into recession.  And, importantly to me, I was able to keep both my job and my leather jacket. \n\nIn retrospect, the 1987 crash was just a hiccup in what ultimately turned out to be a remarkable upturn in the city’s performance and prospects.  However, to really understand this story of the city’s resilience, we need to take a look back at one of the darkest periods in New York City’s history: the 1970s. \n\nI remember New York City in the 1970s from my freshman days at Columbia University.  Columbia was a great place to be a young student.  There were engaging professors and I had the opportunity to read the great books of the university’s core contemporary civilization curriculum.  Recognizing the strong literary traditions of the Lotos Club, I would also note that I had the fortune of reading some of the great works of Middle English, including Chaucer.  I also enjoyed being on both the freshman wrestling and golf teams—even if it was an odd pairing and our teams were not very good.  \n\nIn contrast to Columbia, the city seemed much less attractive.  From my perspective as a naïve 17-year-old from suburban Massachusetts, it seemed outright dangerous and dirty.  The danger was driven home immediately.  One of my roommates was robbed his first day during a walk through one of the nearby parks.  He didn’t understand, nor did I at the time, that certain places near Columbia weren’t very safe, even during daylight hours.\n\nWhile you could avoid the crime by being careful where you went in the city, there was no avoiding the pollution.  During this period, New York City still had considerable manufacturing activity, many apartment buildings burned low-grade oil and incinerated trash, and this was before the days of automobile catalytic converters.  This confluence meant that grit was everywhere, and even for the non-fastidious—which I admit I was one—showers were an absolute necessity on a daily basis.  Sometimes it was so bad that you couldn’t see across the Hudson River, which was just a few blocks away! \n\nIn order to understand the situation the city faced in the 1970s, we have to look back a decade earlier.  The 1960s were a time of hope and promise for New York City, with the World’s Fair, the opening of the Verrazano Bridge, and the beginning of construction of the World Trade Center.  The city benefited from a booming stock market and strong overall job growth. \n\nMy memories of New York City during that period were very glowing.  Visiting from Massachusetts, I remember going to the World’s Fair and to a Mets game at the new Shea Stadium—heady stuff for an 11-year-old.  As a boy who planned to be a nuclear physicist when he grew up, I particularly recall the General Electric exhibit at the World’s Fair that highlighted the potential of hydrogen fusion as a future power source.  Although that turned out to be like flying cars—always somewhere just over the horizon—it was exciting to contemplate as an aspiring scientist.  Unbeknownst to me,there were also some ominous signs for the city.  The city’s manufacturing sector was losing jobs at a worrisome pace, population growth had slowed to a crawl, crime was on the rise and racial tensions were growing. \n\nAs the sixties unfolded, there were also a number of major strikes affecting public services, most notably the 1968 teachers’ strike.  Perhaps most importantly, the city’s finances were on a reckless path.  Despite growth in the economy and the tax base, the city was taking on more short-term debt, relying increasingly on state aid and grossly underfunding its pensions.  This is a familiar story where the seeds of future fiscal problems are sown during better economic times.  In effect, a combination of a strong U.S. economy, a bull market on Wall Street and various forms of fiscal gimmickry masked the increasing underlying fiscal stresses that the city was experiencing. \n\nBy 1970, when I arrived, some of this good fortune ended when the U.S economy was hit by recession and a bear market in equities began.  What followed was a massive economic slump in New York City—what would generally be described as a depression.  Some of the numbers are staggering: from 1970 to 1975, New York City saw a 6 percent decline in its population, a 15 percent drop in employment and a 50 percent surge in the murder rate.  The combination of the need to roll over debt from the prior two decades, a shrinking tax base and increased costs of debt—which sometimes had been used to finance operating expenses—caused fiscal problems that forced the city to cut services and raise taxes.  A continued bear market on Wall Street exacerbated the situation.  In effect, New York City looked to be in a vicious cycle with companies shrinking, closing or moving out.  The result was fewer jobs, an exodus of people, declining property values, rising crime rates and eroding tax revenues.  This meant fewer firemen, police and teachers, which led, in turn, to more crime and poorer educational opportunities.  Put simply, it was a horrible dynamic that looked difficult to reverse. \n\nAt the end of 1975, New York City faced perhaps its darkest moment and greatest challenge of the modern era when it ran out of money to pay for basic services.  The city’s persistent deficits caused the financial sector to lose confidence in the city’s ability to repay its debts, and the city lost its ability to borrow.  \n\nYet, this moment—when the outlook appeared bleakest—turned out to be an historic turning point for the city.  The reforms that were subsequently imposed by the state and federal governments through the Emergency Financial Control Board were quite tough in many ways.  There was plenty of pain to go around, and just about every stakeholder―including city officials, lenders, city workers and residents, to name just a few―shared in the sacrifice.\n\nThe city’s extraordinarily large level of short-term debt was refinanced into long-term debt owed to the newly formed Municipal Assistance Corporation (MAC).  To make this debt marketable to the public, MAC was given control over the city’s sales tax.  Revenues from that tax were dedicated to repayment of the MAC debt, and the city could keep only what was left over.\n\nFundamental to the deal was the creation of the Emergency Financial Control Board, which was empowered to monitor proposed city budgets to ensure that they were in balance.  This change alone was very consequential: taxes increased and spending fell sharply over the latter half of the 1970s.  By 1982, the city’s inflation-adjusted wage bill had fallen by almost 30 percent, and real investment in the city’s infrastructure fell by more than three-quarters and didn’t reach its 1974 levels until the early 1990s.\n\nThe city gave up a significant amount of its autonomy in exchange for the relief it received.  Ultimately, however, the reforms that were forced on the city created a responsible and transparent budgeting process that has served the city well to this day. \n\nBy early 1976, the U.S. economy was recovering from a deep recession, and employment in New York City’s financial sector was once again on the rise.  Overall employment in New York City began to recover by 1977, and unemployment began to decline.  By the end of the decade, there were signs of hope, but it wasn’t at all clear yet whether the city was back on solid footing.  While employment did rebound modestly, the city’s population was still declining, crime was still rising and the city was still having trouble borrowing.  It would be 10 years after the initial crisis before the city was able to borrow again on its own in the public markets.\n\nBy the mid-1980s, the resiliency of the city’s economy was becoming more apparent.  City-wide employment barely declined during the wrenching recessions of the early 1980s—the first time in a long while that the city’s economy had out-performed the nation’s.  In the next few years, job growth picked up sharply and real estate markets soared, a clear sign that a growing number of people had regained confidence in the city.  But, even during the booming 1980s, crime rates continued to increase—it wasn’t until 1990 that crime rates peaked and began a long downward trend.\n\nI returned to New York in 1983.  The draw was an employment opportunity in finance.  At the time, the city’s circumstances were improving but crime was still high, and there were still lots of problems caused by years of underinvestment.  For example, I recall that the New York City subway system at the time was a near-wreck, with train speeds sharply reduced in many areas due to poor track conditions and dilapidated infrastructure. \n\nBut, the seeds of improvement were in place.  The financial services industry was beginning a long boom marked by financial market innovation and the rapid development of the nation’s debt capital markets—a boom that ended with the financial crisis.  When I arrived, the build-out of the Battery Park City neighborhood was just starting.  My first year, I lived at Gateway Plaza, which was the first big apartment complex in Battery Park City.  However, it was early days and the nearest grocery store was about half a mile away north on West Street!  \n\nAlthough a major economic downturn throughout the Northeast in the early 1990s took its toll on New York City—especially on employment and home prices—the city’s population continued to grow and its crime rate began to decline.  While the city has had a number of ups and downs since the early 1990s, in tandem with the United States, the general trend has been upwards.  The city withstood a terror attack, financial crisis and natural disaster.  Following the attack on the World Trade Center in 2001, there was uncertainty over whether this would be a permanent setback for the city.  However, the city rebuilt and recovered and Lower Manhattan has evolved into a vibrant commercial, cultural and residential area.  Gateway Plaza is still there, but the Battery Park City neighborhood today is now fully developed and well-established.  There are even several grocery stores!\n\nThe city weathered the recent financial crisis and the Great Recession far better than many would have anticipated.  Although employment in the securities industry has stagnated after the decline due to the Great Recession, the city has found new sources of dynamism, including rapid growth in the technology sector and the further development of tourism.  The number of technology workers in New York City has doubled to about 125,000 in the past decade.  That sector is now is nearly as big as the securities industry.  And New York City had 56 million visitors last year, nearly double the total of 25 years earlier.  \n\nIn 2011, an important milestone was reached: payroll employment surpassed its previous all-time peak, from 1969, and it has grown strongly since then.  Crime rates have fallen significantly across the city since 1990, and real estate values have surged. \n\nOne consistent source of strength for the city has been immigration.  Over this period, New York City benefited from a sizable influx of immigrants, which more than offset out-migration in many neighborhoods.  To illustrate this, let’s take a look at one such neighborhood in Brooklyn, Sunset Park, that was losing residents to the suburbs in the 1960s and 1970s.  As property values fell, new migrants from Puerto Rico, the Dominican Republic and Central America moved in.  By 1990, Hispanics comprised about half of all Sunset Park residents.  These new residents developed a thriving community, which boosted property values.  There was also an influx of Chinese residents, and part of Sunset Park has come to be known as Brooklyn’s Chinatown.  There are many other examples where immigrants revitalized whole neighborhoods: Russian and Ukrainian immigrants in Brighton Beach; Chinese and Korean immigrants in Flushing; and Indian immigrants in Jackson Heights, just to name a few. \n\nSo, has it been all good news for New York City in recent years?  Certainly not.  Many challenges remain.  One by-product of a strong economy and real estate market has been a decline in housing affordability, and an increase in homelessness.  It is estimated that there are over 60,000 homeless people in New York City today—almost double the number from 10 years ago and more than four times as high as in the early 1980s.  Moreover, poverty rates remain high in many parts of the city, and the education system can certainly be improved.  Yet, a strong economy and a sound fiscal position at least provide the means to devote resources to help address these ongoing problems. \n\nThis turnaround of New York City illustrates that reviving a city in trouble is indeed possible. There are some useful lessons from New York City’s experience.  First, it is often darkest before the dawn.  Cities in fiscal distress are like individuals suffering from addiction: they often have to hit bottom and reach out to others for help before they can begin to recover.  New York City hit bottom in 1975, and that created the impetus for reform.  Second, in such situations, cities often have to give up fiscal control for a time.  Accepting a Control Board and operating under its mandates was a painful, but necessary, step toward rebuilding the city’s fiscal health and regaining access to financial markets.  Third, when sound reforms are implemented it still takes a long time to achieve fruitful results, so it is important to stay the course.  Fourth, although a strong economy, good fiscal position and transparency in government will not automatically solve every problem, they provide a strong foundation.\n\nSo, what does the future hold for New York City?  An important aspect of a city’s vitality is the ability of its residents to develop the skills necessary to be successful and to achieve their dreams.  A big asset for the city is its access to affordable education.  Each year, tens of thousands of low-to-moderate-income New Yorkers—many of them immigrants—send their children to one of the city’s specialized high schools.  When it comes time for college, many of these same students have access to universities in the CUNY and SUNY systems.  A recent highly regarded study assessed how much of an opportunity kids born to less advantaged families in different parts of the United States have of moving up in the world.  New York City ranked 10th out of the 50 largest U.S. metro areas.2 The Big Apple should strive to be number one in this ranking by continuing to improve access to quality and affordable education and to job training.  We should all do our part to help make this happen.\n\nAlso, it is important to remember that the measure of a city’s well-being is much broader than how many are employed and the level of economic activity.  As Paul Allen, co-founder of Microsoft, explained:\n\nIn my own philanthropy and business endeavors, I have seen the critical role that the arts play in stimulating creativity and in developing vital communities…the arts have a crucial impact on our economy and are an important catalyst for learning, discovery, and achievement in our country.\n\nThrough the fiscal crisis, into the 1980s and to the present day, the city maintained its status as a center of culture and the arts.  The efforts of the city and the state, starting in the mid-1980s, to redevelop the Theatre District and Times Square area played an important role in revitalizing tourism.  But, of course, the city’s well-deserved reputation for culture and arts goes well beyond Broadway—to the hundreds of Off-Broadway and neighborhood theaters, museums, music venues and organizations like yours.  Supporting this cultural infrastructure is important to maintaining a vibrant economy and a bright future.\n\nThe experience of New York City, which has gone from crisis to vibrancy, shows that the past need not be prologue for the future.  It also shows that a well-managed regime of fiscal control can help begin the process for future recovery.  Other cities and regions are at different points in this journey.  Puerto Rico, for example, which has been in recession for many years and which defaulted on its debt obligations earlier this year, is at a particularly critical juncture.  On the positive side, the fact that Congress has enacted legislation, the Puerto Rico Oversight, Management and Economic Stability Act, or PROMESA, does provide a way forward.  The prospect for good fiscal stewardship has improved and this is an essential first step in the Island’s recovery.  As I have noted elsewhere, there are significant opportunities for action that can leverage the Island’s strengths to foster economic growth.  These include its bilingual adult population, an open economy occupying a central position in the Caribbean, wide experience as host to international corporations and close economic ties to the U.S. mainland.  However, much as New York City took time to recover, so too will it take time for Puerto Rico to recover from its crisis of the last decade.\n\nThe future of Puerto Rico is yet to be determined.  However, the experience of New York City suggests that a turnaround is possible even when underlying conditions appear dire.  New York City is a compelling example of what is possible when you leverage your citizens’ ingenuity and capacity for reinvention.  It shows what can be achieved with the appropriate discipline and effort. \n\nThank you for your kind attention.   \n\n1 Jason Bram, Andrew Haughwout, Joseph Tracy and Luis Uranga assisted in preparing these remarks.\n\n2 Raj Chetty, Nathaniel Hendren, Patrick Kline and Emmanuel Saez. “Where is the Land of Opportunity? The Geography of Intergenerational Mobility in the United States.” Quarterly Journal of Economics, Vol. 129, No. 4, 2014."
    },
    {
        "title": "Remarks at the 40th Annual Central Banking Seminar",
        "date": "Oct 3, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud161003",
        "content": "It’s my pleasure to welcome you to the Federal Reserve Bank of New York to participate in the 40th anniversary of our Central Banking Seminar.  This year we have 140 participants representing 67 different countries and institutions.  The program is designed to provide a broad view of central banking covering the key areas of monetary policy, supervision, payments and financial stability.  This design reflects our belief that a well-rounded perspective can help us to be more effective in our jobs as central bankers.  We also hope this program provides you with an opportunity during your stay to broaden your network of central bank colleagues.  As always, what I have to say reflects my own views and not necessarily those of the Federal Open Market Committee (FOMC) or the Federal Reserve System.1\n\nThe theme of this year’s seminar is the policy challenges we face in an environment of sluggish economic growth and low inflation.  Central bankers will always be confronted by challenges in carrying out their responsibilities, which will change over time.  This puts a premium on our ability to adapt.  Consider, for example, the contrast between the issues faced by the Federal Reserve in 1976—the inaugural year of our seminar—and today.  In 1976, the U.S. economy was recovering from the first oil price shock and the ensuing recession, which had ended in March 1975.  The FOMC faced the problem of stagflation, as both inflation and unemployment were high—inflation was running around 6 percent and the unemployment rate was nearly 8 percent.  The FOMC’s policy rate—the federal funds rate—was about 5 percent.\n\nCompare that to today’s circumstances.  The U.S. economy has entered its eighth year of expansion following the financial crisis and the Great Recession.  In contrast to 1976, inflation has been below the FOMC’s desired level of 2 percent for most of this expansion.  This year, inflation is running at about 1 percent and the unemployment rate is 4.9 percent.  Monetary policy remains accommodative, with the target range for the federal funds rate at 0.25-0.50 percent.  I doubt that a central banker in 1976 would have viewed the current set of circumstances as even remotely plausible, or would have ever contemplated the types of challenges that we now face. \n\nIn my remarks today, I would like to briefly touch on four topics: (1) the efficacy of monetary policy at the zero lower bound, (2) central bank communications, (3) cyber risk, and (4) market liquidity and regulation.  These have particular relevance for central banks today, and are likely to remain relevant in the years ahead.  The speakers that you will hear over the course of our program will elaborate on some of these topics.\n\nThe financial crisis severely impaired the global financial system, and the resulting recession in the United States was the most severe since the Great Depression.  Based in part on the lessons of the Japanese experience, in which falling inflation expectations undermined the effectiveness of monetary policy, the FOMC responded aggressively as the economy weakened.  It began cutting the policy rate from 5.25 percent in September 2007 and reached the effective zero lower bound in December 2008.  From that point on, the FOMC turned to unconventional monetary policy in order to provide additional stimulus to the economy.\n\nAcademic research indicates that recoveries following financial crises have tended to be shallow and protracted.2  One plausible reason for this pattern is that a financial crisis typically results in headwinds that take time to dissipate—for example, the deleveraging of business and household balance sheets or the tightening of credit standards.  Even so, as the expansion failed to gather momentum and persistently underperformed most forecasts, alternative explanations for the sluggish economic growth―such as secular stagnation―began to receive increased attention.  Wednesday’s session on Global Trends and Risks will more fully explore this debate.  The insights from this debate have important implications for the appropriate path for monetary policy and the question of whether monetary policymakers need additional tools or greater support from the fiscal authorities to avoid being trapped at the zero lower bound for interest rates.\n\nBeyond these factors, the tepid pace of the expansion reflects the fact that monetary policy was constrained by the zero lower bound.  Monetary policy affects the economy mainly through its influence on financial market conditions—including long-term interest rates, credit spreads, exchange rates and equity prices.  However, when the policy rate hits zero, conventional policy is constrained in terms of providing additional accommodation―even recognizing that some central banks have pushed short-term rates into negative territory.  To overcome such constraints, the FOMC used forward guidance—promising to keep rates low for longer—and asset purchases to provide additional stimulus to the economy.  Given the initial novelty of unconventional monetary policy tools, central banks did not have a well-developed body of research to draw on to design the programs and calibrate their impact.  While it will take time to build this body of work, research to date varies in terms of the estimated effectiveness of unconventional policy.  Several studies indicate that the FOMC’s first asset purchase program helped to reduce long-term interest rates, while the subsequent programs had smaller though still significant effects on rates.  However, Professor Summers, who is participating in our program, has recently questioned the effectiveness of the Fed’s asset purchase programs when financial markets are well-functioning.3\n\nThere is a related concern given that the federal funds rate is still close to zero at this point in the expansion.  While I’m on record as saying that expansions do not simply die of old age,4 some economists are concerned that the risk of a recession is increasing.  As I indicated earlier, the FOMC was able to reduce the federal funds rate by more than 5 percentage points in an effort to offset the effects of the last recession.  If another recession were to happen in the next few years, it is likely that the FOMC would be unable to respond with a cut of such magnitude.  In this case, the effectiveness of unconventional monetary policy in providing accommodation would again become a central issue, as Chair Yellen discussed in her recent Jackson Hole speech.5 A risk management approach to monetary policy would suggest that the more concerned one is with the effectiveness of these policies at the zero lower bound, the more cautious one would be in the process of removing accommodation.  So, even though we are now slightly off the zero lower bound, an assessment of the effectiveness of unconventional monetary policy has implications with respect to the current stance of monetary policy.\n\nAnother important challenge for central banks is communication with the public.  It is now accepted that the effectiveness of monetary policy is enhanced when the public has a clear understanding of the goals of monetary policy, and the framework and actions that the central bank will use to attain them.  One important example is inflation expectations, which are important to the dynamics of inflation. Effective communication by the central bank can help to anchor inflation expectations at its target, thereby making it easier for the central bank to attain its inflation objective.\n\nThe use of unconventional monetary policy has also heightened the focus on communication.  In the past, communication almost exclusively focused on the likely short-term path for the central bank’s policy rate.  But, once the zero lower bound became a constraint, forward guidance became an important communication tool to surmount the problems caused by that constraint.  In addition, there are now many more dimensions to the stance of monetary policy, including the size and composition of the central bank’s balance sheet.  When communicating about these various dimensions, it is important for the central bank to demonstrate its commitment to a framework rather than to a specific policy path, since the precise path of policy should be data dependent and the future is uncertain.  Good communication can lead to a clear understanding of the Fed’s framework, which will help the public to assess the data along with the Fed and enable it to better predict how the Fed is likely to adjust policy as the economic outlook changes. \n\nAs I indicated earlier, the challenges the FOMC faced in 1976 were very different from today’s.  In addition, advances in information technology also have changed how we do our work.  However, these advances come with the risk of increased exposure to cyber-attacks.  The various motivations for these attacks include causing embarrassment to the central bank, stealing money, and disrupting critical financial services.  The large sums of money potentially at risk mean that we need very strong defenses to lower the probability of success by cyber attackers in order to deter such efforts.  Attacks can have consequences for a central bank’s reputation and the public’s confidence in it, underscoring the importance of a sturdy set of defenses and response actions.  Protecting against cyber-attacks requires not only investment in the right infrastructure, policies and processes, but also vigilance from central bank employees.  Many attacks involve tricking unsuspecting employees into installing malicious code or revealing key access information.  Since this “phishing” plays on human weaknesses, to which we are all prone, protecting against these tactics requires ongoing training and testing, appropriate access controls, and segregation of duties in the context of critical infrastructures.  Central banks also need reliable channels of communication and cooperation with those agencies that track and combat malicious cyber activity.\n\nThe last topic that I would like to touch on is market liquidity and regulation.  Market liquidity is essential to the proper functioning of financial markets.  I think of it as the cost—both in expense and time—of buying or selling an asset for cash.  Market liquidity is like engine oil, which is essential for engine parts to move properly without generating excessive friction and heat.  Market liquidity reflects many factors, including the expense of conducting a transaction, the price of the transaction relative to the midpoint of the bid-ask spread, any impact of the transaction on the market price, and the speed at which the transaction can be completed.\n\nSome argue that market liquidity has been adversely affected by post-crisis regulation.  For example, the higher capital and liquidity requirements for the largest securities dealers may have impaired market liquidity by reducing the profitability of dealer intermediation activities and raising the cost of dealer balance sheets.  But, before considering whether any regulatory changes would be appropriate, it is important to carefully examine the evidence.  New York Fed staff have been conducting an extensive analysis of empirical measures of market liquidity and the effects certain regulations may have had on it.  The work to date finds little evidence—based on traditional liquidity measures—of any meaningful degradation in market liquidity across key asset classes.  However, there are important data gaps that must be filled in order to have a more complete assessment.  For example, the U.S. Treasury market data capture dealer-to-dealer transactions, but not dealer-to-customer transactions that are typically conducted on an over-the-counter basis.  Moreover, we do see other consequences of regulation, such as the rise in the foreign exchange swap basis and the spread between LIBOR and the federal funds rate, as well as the decline in dealer repo financing activity.  Still, it is important to keep in mind that even if further analysis were to identify a regulatory-related decline in market liquidity, any associated cost would have to be weighed against the benefits of these regulations in terms of a safer and more robust financial system.\n\nIn closing, I hope you will find our program both engaging and challenging.  Don’t be shy—our speakers welcome difficult and thought-provoking questions.  Absorb all you can.  Use this opportunity to network with your colleagues.  Hopefully, this experience will stimulate your thinking and help you be successful in your career as a central banker. \n\n1 Elizabeth Mahoney, Jonathan McCarthy, Nick Stanescu and Joseph Tracy assisted in preparing these remarks.\n\n2 For example, see Carmen Reinhart and Kenneth Rogoff. This Time is Different: Eight Centuries of Financial Folly. Princeton University Press, 2009.\n\n3 See Larry Summers' blog.\n\n4 See Remarks at the New York Fed’s Economic Press Briefing on the Household Debt and Credit Report.\n\n5 See The Federal Reserve's Monetary Policy Toolkit: Past, Present, and Future."
    },
    {
        "title": "Is There Room for More Monetary Cooperation?",
        "date": "Sep 30, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/pot160930",
        "content": "It is my pleasure to participate in this final session.  I would like to express my appreciation to our host, the Banque de France, for its gracious hospitality, and to the organizers, the China Finance 40 Forum, the Euro 50 Group and CIGI, and all of the participants for very interesting discussions.  \n\nMy remarks will focus on a question at the very core of this meeting: would individual countries benefit from coordinating their policy stances to jointly achieve their macroeconomic objectives? This naturally leads to another question: have we entered a stage in the history of the international monetary system that requires new institutional and policy solutions to the evergreen issue of how to insulate an economy from external shocks? And if the answer to these questions is yes, or even a partial and qualified yes, what are the next steps for policy design and implementation?   \n\nTo get the ball rolling, let me recapitulate some of the themes underlying the current debate on policy coordination, and highlight a few open issues of particular relevance for our meeting today. It is worth emphasizing that these are my personal views, and need not reflect the views of the Federal Reserve Bank of New York or the Federal Reserve System.\n\nStated in abstract terms, macroeconomic coordination implies an arrangement to give up—at least partially—independence in policymaking to achieve objectives not otherwise in reach.  Traditionally, gains from coordination are deemed to arise from avoiding “beggar-thy-neighbor” externalities that policymaking in one country or region can impart onto the rest of the world. \n\nThe modern theoretical case for coordination was formalized in a number of path-breaking re-visitations of the Keynesian Mundell-Fleming model through the lens of game theory. It is a pity that Richard Cooper was unable to join us today, because his work, along with that of his colleagues since the late 1960s, has posed the scientific basis for an insightful and ambitious research agenda. \n\nIn practice, though, once the Bretton Woods system ended, there were no attempts to revive a global framework of coordinated exchange rate or interest rate policies.1 The general view in the policymaking community was that the potential welfare net gains from cooperation were modest and difficult to achieve, relative to the outcomes attainable within an orderly, yet uncoordinated regime, where central banks and fiscal authorities pursued their domestic objectives.2  \n\nHowever, renewed calls for coordination have recently re-surfaced on a number of occasions.  Why now?  Is this old wine in new bottles, or does the new case for cooperation have its merits?  I will focus on two significant forces that might lead one to reexamine the benefits to cooperation.  The common theme is the restriction on providing monetary accommodation at the zero lower bound (ZLB).  I will also briefly touch on the growing importance of emerging market economies.\n\nFirst, over the last two decades, nearly all advanced economies have witnessed trend declines in their long-run real natural interest rates3 , defined as the hypothetical rates required to balance national saving and investment in a full-employment equilibrium over the longer run.  Lower longer-run natural rates reflect a number of slow moving structural factors, such as the aging of advanced economy populations and lower fertility, which are unlikely to reverse any time soon.  With the success in anchoring inflation expectations at levels consistent with price stability, the low levels of the natural rate going forward imply a likelihood of more visits to the ZLB, or at least market participants placing more weight on such visits even for relatively small demand shocks.\n\nSecond, the global financial crisis was a sufficiently large demand shock that the short-run equilibrium real rate became very negative, and central banks were required to implement a number of unconventional policies to provide accommodation at the ZLB to mimic a highly negative nominal policy rate.  Further, the zero bound has proved very sticky, as headwinds from the aftermath of the global financial crisis, such as consumer uncertainty boosting precautionary saving, tighter lending standards, lower business appetite for risk-taking, and at times fiscal consolidation, have restrained aggregate demand.4\n\nIn these types of environments, exchange rate depreciations may be deemed to be a particularly effective channel of monetary policy transmission as they raise foreign demand for domestic products.  National policymakers may be tempted to deliberately weaken their exchange rates as a countercyclical instrument of demand management.  Further, some of the transmission mechanisms of unconventional monetary policies, such as portfolio rebalancing, can generate strains on financial stability in other parts of the globe. \n\nA question naturally arises whether a coordinated strategy would reduce the risks of exchange rate depreciation that can export disinflation and stagnation, and better promote global financial stability.  Of course, a necessary condition for a positive sum game would be that joint policy can be effective through channels other than the exchange rate and portfolio rebalancing, or perhaps through better balanced combinations.  To be clear, an expenditure-switching component is always associated with monetary accommodation in one country, at the expense of its trading partners.  Likewise, through their impact on the term structure, conventional and unconventional monetary policy both produce some degree of portfolio rebalancing and spillover to financial conditions elsewhere.\n\nThe problem is that in a global ZLB environment this expenditure-switching effect may turn out to be stronger than usual—in fact, strong enough to dominate the positive spillovers of national monetary stimulus inducing an expansion in demand for world output.  The point is that, in a ZLB regime, the negative spillovers abroad may be compounded by the inability of the trading partners to respond to exchange rate shocks and restore cost-competitiveness by easing their policy stances effectively. \n\nBut while press accounts have often focused on negative externalities—alleged currency wars and the like—there is also a question of potential positive externalities—specifically whether, and how, authorities in one country might improve their own prospects by moving in concert with their counterparts in other countries.  Indeed, some have suggested a global policy compact is needed to provide additional countercyclical stimulus to boost demand and make up for the loss of effectiveness of conventional and unconventional national policies. Some have argued that in this context, fiscal coordination could be at least as impactful as monetary, as it provides a different channel for accommodation and can, if applied with some care, also push up longer-run natural rates of interest.  Absent coordination—the argument goes—there is little hope that a country can escape its ZLB sandtrap on its own, if the rest of the world is also simultaneously stuck in a liquidity trap.  Of course, this is just an argument at the moment and we have little concrete evidence that this is empirically true.\n\nAs an illustration of this potential sandtrap, consider the case of a country experiencing an earlier recovery than others from a large financial crisis that had pushed it and its trading partners to the ZLB.  This country will exhibit relatively better fundamentals, and a relatively higher real equilibrium rate, than its trading partners.  Absent coordination, the country would be expected to run a relatively tighter monetary policy.  This tighter stance would trigger net capital inflows as market participants respond to “search for yield” opportunities, leading to a stronger real exchange rate and tighter financial conditions.  Contractionary and disinflationary pressures would then deteriorate the country’s medium-term outlook and offset its initial comparative advantage.  The final outcome could be a return to the ZLB prevailing amongst its trading partners.5 This illustration critically depends on all the countries being relatively equal on structural supply issues.  If the country recovering more quickly has a more flexible supply side entering the financial crisis or successfully implements supply reforms after the crisis, it is less clear the argument holds.  \n\nGoing forward, coordination could also play a role as insurance against future downturns in a global environment of low natural real interest rates.  In such an environment, average policy rates over the long run will be relatively low. Facing cyclical disturbances, national monetary authorities will have little leeway to cut their policy rates and provide effective monetary easing.  But expected coordinated efforts to avoid beggar-thy-neighbor spillovers while supporting global demand through simultaneous easing may reduce the probability of hitting or staying at the ZLB for a prolonged period of time.\n\nThe question of financial stability spillovers has taken on particular prominence with respect to the emerging market economies, or EMEs.  It has been argued, for example, that accommodative unconventional monetary policies in the advanced economies expose the EMEs to excessive and potentially destabilizing fluctuations in capital flows.  And conversely, it has been argued that efforts by some EMEs to resist currency adjustment have posed financial stability risk to other countries, both advanced and emerging, by contributing to a global savings glut.  An often heard case for cooperation is that a more favorable and sustainable global allocation could be achieved if policymakers in both advanced and emerging countries were willing to internalize the externalities they impose on each other, instead of acting unilaterally according to domestic mandates.\n\nBut while these various arguments for increased cooperation are suggestive, they hinge on many theoretical and empirical assumptions that would need to be more rigorously quantified and validated.  And of course, there is the thorny issue of how joint decision-making could be coordinated in practice, given likely difficult and hard to measure tradeoffs among and across various countries’ interests, not to mention all the complexities of real-world policymaking and national accountability. \n\nBut with changed global circumstances, the potential gains from enhanced coordination do appear worth exploring.  Academic economists are starting to address the issues I have discussed, using models that better capture the current international situation. The results from their analysis will be useful in assessing whether there are increased benefits to cooperation, given this new environment of low natural rates and the growing size of the emerging world.\n\nLet me conclude by underscoring the continuing importance of the forms of cooperation that we do see today among central banks, the ones so vitally supported by Claudio and his colleagues at the BIS: the information sharing, the exchange of perspectives based on differing experiences, and different frameworks. \n\nWhile perhaps not as sexy sounding as designing a new architecture for international coordination, this ongoing cooperation has certainly produced numerous benefits, and we are all better off for it.  It merits continued nurturing, including through gatherings such as this.\n\nThank you.\n\n1 The case for limiting competitive devaluations was paramount, for instance, in Europe—leading to the Exchange Rate Mechanism of the European Monetary System and providing the key underpinning of the \"one market, one money\" vision. But post-Bretton Woods attempts at coordination among advanced economies, such as the Plaza Agreement of 1985, were limited in scope and short-lived. And while the episodes of financial turmoil in Mexico and Asia during the 1990s challenged global policymakers’ capacity to coordinate their responses to crises, and led to calls for refocusing outreach to engage the major Emerging Market (EM) economies in new forms of diplomacy, the academic and policy debate kept calling into question the relevance of monetary compacts, arguing that coordination at best achieved little, and at worst led to counterproductive outcomes. For a recent survey see Kahn, Robert B., and Ellen E. Meade (2016), International Aspects of Central Banking: Diplomacy and Coordination, Finance and Economics Discussion Series 2016-062. Washington: Board of Governors of the Federal Reserve System.\n\n2 Exchange rate adjustments, far from being part of the problem, were to a large extent deemed as part of the solution, as they provided insurance against cyclical shocks allowing a country to target appropriate financial conditions and achieve the desired policy stance. One-off episodes of multi-country foreign exchange intervention or simultaneous interest rate cuts by central banks worldwide (as most notably in October 2008 in response to the Great Recession) did not substantially alter this picture.\n\n3 See, for example, Holston, Kathryn, Thomas Laubach, John C. Williams. 2016, Measuring the Natural Rate of Interest: International Trends and Determinants, Federal Reserve Bank of San Francisco Working Paper 2016-11, and Rachel, Lukasz, and Thomas D Smith. 2015. “Secular Drivers of the Global Real Interest Rate,\" Bank of England, Staff Working Paper No. 571, December. \n\n4 Crucially, the distance between inflation-adjusted market interest rates and the equilibrium real rates provides a metric to assess whether monetary policy provides sufficient accommodation or restraint. In many countries nominal policy rates have reached their zero or effective lower bound and have remained there for quite a while. But to the extent that real equilibrium rates remain low and are not expected to move back quickly—if at all—to historical levels, national monetary policies are unable to sufficiently spur domestic demand growth by lowering real borrowing costs below the equilibrium real rates and supporting asset prices.\n\n5 Similar considerations appear in Eggertsson, Gauti, and Lawrence Summers (2016). “Secular Stagnation in the Open Economies: How it Spreads, How It Can Be Cured”, VoxEU, July 22. See also Eggertsson, G., N. Mehrotra, S. Singh, and L. Summers (2016), “A Contagious Malady? Open Economy Dimension of Secular Stagnation,” mimeo, Brown University."
    },
    {
        "title": "The Role of Best Practices in Supporting Market Integrity and Effectiveness",
        "date": "Sep 7, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/pot160907",
        "content": "Welcome to this year's primary dealer meeting, and thank you for joining us today. Primary dealers play several important roles as trading counterparties of the New York Fed in its implementation of monetary policy, as participants in Treasury auctions, and as providers of information on market conditions to support the Desk's monitoring of financial markets.2 These annual meetings provide us an opportunity to strengthen this relationship and communicate our expectations for you.\n\nThe official sector and market participants have a shared interest in ensuring that markets function efficiently, effectively, and fairly in the provision of financial services to all stakeholders, particularly firms and households. To help achieve these goals, the official sector employs a variety of channels, including legal, regulatory, and supervisory. In addition, officials work with market participants to create best practice guidance that complements official-sector initiatives.\n\nToday, I will focus my remarks on how best practice efforts can help support the integrity and effectiveness of financial markets. I will draw upon the New York Fed's experience with the Treasury Market Practices Group (TMPG), the Foreign Exchange Committee (FXC), and the Bank for International Settlements (BIS) Foreign Exchange Working Group (BIS FXWG) in developing and refining best practices for financial markets. The markets covered by these groups are those for U.S. Treasuries, agency debt, agency mortgage-backed securities (MBS), and foreign exchange (FX). Segments of these markets have historically operated under an over-the-counter (OTC) trading model and, as a result, lack a formal central trading venue like an exchange with established rules.3 These markets are of particular interest for central banks globally, because of their important role in the implementation of monetary policy. In light of these considerations, central banks have made extensive efforts to convene market participants to develop best practices that help cultivate fair and effective markets.4\n\nAs a participant in these markets, the Desk has a direct interest in their integrity and effectiveness. I hope my remarks will reinforce why the New York Fed and primary dealers as a group should be interested in continuing to support best practices. Your engagement in the process is essential and your commitment to abide by best practice recommendations is vital to well-functioning markets. I will begin by describing what I mean by best practices, follow with a discussion on central banks' interest in supporting them, and conclude by pointing to some of the challenges in promoting widespread adoption.\n\nBefore continuing, I would like to note that the views I share with you about best practices today are mine and do not necessarily reflect those of the Federal Reserve Bank of New York or the Federal Reserve System. However, remarks on the expectations for primary dealers reflect the policies of the Federal Reserve Bank of New York.\n\nWhat are Best Practices?\n\nBest practices are recommendations intended to encourage market participants to adopt principles, processes, or procedures that affirm or raise standards in their sectors or industries. Many sectors, including manufacturing, agriculture, and education, have successfully employed best practices to bring about positive change.5 In the financial sector, the aim is to design best practices that enhance behaviors or practices that support market integrity and effectiveness. While best practices do not constitute legal or regulatory obligations for market participants—or serve as a substitute for regulatory standards—they can serve as useful guideposts in establishing expectations for responsible market conduct and conventions. Effective best practices—those that are well-designed, consistently applied and upheld by market participants—can produce positive externalities that benefit all industry stakeholders, thereby serving as a public good. If used well, best practices can be adapted quickly as market structures, practices, and participation evolve and thus offer timely guidance to market participants. On occasion, they may also help inform regulation.\n\nBest practices can be broadly categorized into those that are quite narrow, targeting very specific conduct or processes, and those that involve broad principles. Those that are narrow in scope are usually more straightforward to implement, and relate to outcomes that are easier to measure. For example, it is fairly easy to assess adherence to the best practice recommendation to implement a fails charge when a seller fails to deliver a Treasury security, or to use a payment-versus-payment settlement mechanism for FX transactions.\n\nIn contrast, best practices that propose broad principles can be implemented differently across institutions and have outcomes that do not lend themselves to easy measurement. For instance, it is difficult to gauge the extent to which an individual market participant behaves in a manner that supports market liquidity or whether market participants as a group apply this recommendation consistently. Nonetheless, such principle-based best practices can be quite powerful in motivating market participants to think carefully about their internal policies and procedures and consider whether they are consistent with broader principles. Both types of best practices are important.\n\nSuccessful best practices are almost always developed using a collaborative and inclusive approach. Ideally, with respect to financial markets, the process should include a wide range of market participants, including dealers, banks, buy-side firms, and market infrastructures, representing different areas of expertise, including the core business lines, legal, compliance, operations, technology, and custody. A public consultation process may be helpful in further enhancing the development process.6 These types of approaches can bring a wide array of diverse perspectives to bear on the design of best practices, helping to ensure the best practices developed are more effective.\n\nCentral Banks' Role in Supporting Best Practices\n\nCentral banks have an interest in ensuring well-functioning markets—as market participants and for efficient transmission of monetary policy. We also have a stake in enhancing the strength and stability of the financial system as a whole. Moreover, central banks can play an important role in the creation of effective best practices by serving as a convening body for market participants, by signaling the importance of best practice efforts to the market, and by encouraging the inclusion of a diverse set of market segments in the development process.\n\nFirst, particularly where OTC markets predominate, a central bank can bring together market participants to discuss and address issues collectively. There is a rich history of collaboration between the official sector, the academic community, and industry participants to analyze existing issues, identify emerging risks, and develop guidance for financial markets. Over time, the New York Fed has led a number of such efforts. For example, the OTC Derivatives Supervisors Group addressed emerging risks of inadequate infrastructure for the growing credit derivatives markets. The New York Fed and the Board of Governors convened the Alternative Reference Rates Committee, which is currently working to identify a set of alternate reference interest rates for financial products and contracts that are more firmly based on transactions from a robust underlying market and that comply with international standards.7, 8 Similarly, the U.K. official sector worked with a panel of market participants in developing the recommendations in the Fair and Effective Markets Review.\n\nThe New York Fed also sponsors two industry groups that have long-term missions focused on best practices. The TMPG is, as you know, a group of market professionals committed to supporting the integrity and efficiency of the U.S. Treasury, agency debt, and agency MBS markets. To that end, the Group develops guidance to promote market liquidity and transparency, robust control environments, efficient market clearing, and managing large positions with care.9 The New York FXC examines similar issues in the FX market. It has a long history—more than three decades—of publishing documentation to facilitate FX transactions and market functioning and to articulate best practices.10 The New York Fed is also involved in the BIS FXWG to help establish and promote adherence to an FX Global Code, a single set of guiding principles for the FX market. These groups have confronted collective action problems where an individual firm may want to raise concerns, but be unable to drive market-wide improvements.\n\nSecond, when central banks partner with private-sector participants to develop best practices, it can send a powerful signal to the market. For their part, private-sector participants contribute their expertise and have a sense of ownership in the process. By integrating best practices within their own institutions, private-sector participants support market adoption.11 Central banks—with their public orientation and interest in the integrity and effectiveness of markets—can further encourage broad adoption of best practices.12\n\nThird, central bank support can also help broaden the composition of those engaged in designing best practices. The New York Fed strives to have all important segments of the market represented in the membership composition of the groups it sponsors. For example, the TMPG recently added individuals who have experience at a regional broker dealer and a high-frequency trading firm to broaden the views and perspectives on the committee. FXC membership has also evolved significantly since the group's inception. Both committees include institutions from the sell-side, buy-side, as well as infrastructure providers, such as trading venues.\n\nCentral banks, and their counterparts, also encourage international collaboration. Such collaboration has increased over time and has reached a significant milestone through the work to develop an FX Global Code. This work spans sixteen different central banks, covering major financial centers in advanced and emerging market economies, and it includes a set of private-sector market participants. The BIS FXWG formed a Market Participants Group (MPG) which includes representatives across the globe and includes the sell-side, buy-side, and market infrastructure communities. The regional FX committees are also integrally involved in the work.\n\nIssues Addressed by Best Practices\n\nLet me now highlight how, in my experience working with New York Fed-sponsored committees, best practices can play an important role in addressing a range of issues. For example, best practices can identify emerging risks, solve \"first-mover\" problems, challenge long-established practices, improve market function, and assist control functions.\n\nCollaboration between market participants and central banks in groups like the TMPG or the FXC can promote identification of evolving or emerging risks and resulting best practices can publicize those risks, and suggest mitigants for them, to the broader market. For example, the FXC, recognizing the risks associated with the increased use of automated trading, published a set of recommendations in 2006 around \"auto-dealing\" in FX markets. The TMPG provided similar guidance in 2015.13\n\nImportantly, best practices can be a way to resolve first-mover problems. For instance, a sizeable portion of the non-centrally cleared forward-settling agency MBS transactions prior to 2013 remained unmargined, posing counterparty risk in the event one trading party were to default, and potentially systemic risk to the overall financial system. Margining such transactions is a common and sensible way to reduce counterparty exposures and systemic risk. However, no market participant wanted to be the first to begin charging margin out of fear that their counterparties would stop trading with them and move their business to a counterparty that did not require margin. The TMPG addressed this problem with a recommended practice that all market participants exchange two-way variation margin on a regular basis.14, 15\n\nBest practices can also encourage market participants to question long-established trading practices and advocate for change. As an example, in the FX market, concerns were raised about the integrity of FX rate benchmarks, particularly around incentives for potential market malpractice linked to the structure of trading around the benchmark fixings, such as the WM/Reuters 4pm London fix. The WM/Reuters fix is one of the long-standing and dominant benchmarks used in the market. It was structured in a way that created an incentive for dealers to \"trade ahead\" of clients at the fix and also an opportunity for them to influence exchange rates. The FSB's FX benchmark working group produced recommendations that called for a change in methodology around the structure of the fix and changes in firms' trading behavior at the fix. Guidance suggested that firms establish and enforce internal guidelines and procedures for collecting and executing fixing orders, such as separate processes for handling such orders. In addition, the group recommended that transactions conducted at to-be-determined levels be priced in a manner consistent with the risk borne in accepting such transactions through such methods as applying a bid-offer spread or a direct fee. The Global Preamble, published by the FXC in coordination with other FX committees, supported and reinforced the FSB's FX benchmark recommendations.16, 17 Since the release of the recommendations, a number of changes have been made. For example, there is a new structure for certain FX benchmark rates, including the WM/Reuters fix. Many dealers have changed how they provide FX benchmark execution, with a growing emphasis on the use of algorithmic execution and a more transparent pricing structure. Such work is, and should be, continuing.\n\nBest practices have been especially effective in improving financial market functioning. In the period prior to 2006, episodes of persistent failures to deliver securities on settlement days were impacting the smooth functioning of the U.S. Treasury market, posing counterparty credit and systemic risk.18 The introduction of the Treasury fails charge trading practice in May 2009 incentivized sellers of securities to resolve fails promptly and promote efficient market clearing. During the first four months of 2009, Treasury fails averaged a bit over $14.4 billion per day. About a year following implementation of the fails charge, fails declined to only $4.2 billion per day.19\n\nBest practices can serve as a tool by which control functions, such as compliance, audit, and legal, can test their firms' procedures and activities against affirmed standards of best practice. The standards can act as a measuring stick that, along with applicable law, regulations, and supervisory expectations, encourages continuous improvement of front, middle, and back offices. For example, the TMPG has a series of practices directed at managing large positions with care. Implementing them requires compliance functions to be thoughtful in determining how to identify large long or short positions, given the uncertainty of tradable float. It also challenges staff to consider which trading strategies may have adverse implications for market liquidity and whether they warrant policies around them. Further, the TMPG's best practices note that responsibility for establishing and maintaining a rigorous internal control environment within an organization rests with its senior management.\n\nNeither best practices nor laws nor regulations can guarantee a lack of bad behavior. Recent failures illustrate the adverse consequences that behavioral weaknesses can have for those who rely on financial services. It also underscores the importance of adopting a culture that creates an environment in which laws, regulations, and best practices are valued and upheld. To that end, regulators and supervisors in various jurisdictions have heightened their focus on firms' corporate governance programs and emphasized the need for instilling values that promote proper behavior and sound decision-making throughout the firms. For example, the New York Fed has been working to advance industry driven culture reform in the financial services industry over the past few years.20 We are joined by other official institutions and groups across the globe, such as the Group of 30 and the Basel Committee, in pushing for heightened standards of market practice and behavior.21\n\nPromoting Adoption of Best Practices\n\nIdeally, best practice standards should become an integral part of the market, observed consistently and transparently by participants across various segments. However, unlike regulations or supervisory standards, compliance with the best practice standards developed by our committees is not mandatory for the broader market. Furthermore, there may be instances where implementation may vary given the diversity of entities in the market and the different profiles of market engagement. This is particularly relevant for the global FX market, with its range of market participants.22 Central banks play an important role in convening industry stakeholders to develop high standards of behavior that promote fairness and integrity, and in pushing the industry to affirm them. Nonetheless, central banks are not acting as regulators when they sponsor these committees, even when they have relevant regulatory powers.\n\nSince best practice standards that are articulated by the committees that we work with are not, in and of themselves, binding, promoting their adoption is both challenging and important. We work on a number of fronts in this effort. We strive to engage a wide diversity of market participants as we develop best practices. This is to ensure that the practices reflect all relevant segments of the market and to promote buy-in and thus foster their adoption. As I mentioned before, both the TMPG and the FXC are expanding their membership such that a range of market participants feels vested in their missions. In addition, we engage with other sectors of the marketplace, such as trade associations and infrastructure providers, to educate them about best practices that have been developed and facilitate their potential adoption of aspects of them.\n\nWhen central banks operate in the markets ourselves, we expect the best practices to apply. This has two aspects. First, we align our own practices to the best practices, except where it would inhibit our policy functions. As a corollary, we expect those institutions that choose to become our counterparties to comply with the best practices as well, so that when we transact, we do so with the confidence that our counterparties are adhering to these standards of behavior.\n\nFinally, we keep supervisors, regulators and other official sector bodies, inside and outside our four walls, abreast of the work of our committees. In some cases, they may choose to adopt standards first articulated by our committees as part of their frameworks.23\n\nConclusion\n\nMarket participants have a responsibility to themselves, their clients, and the financial markets in which they transact to implement best practices within their own institutions and to encourage their clients and other counterparties to do the same. Broad adoption of best practices can strengthen each market participant's existing controls, help reduce market disruptions, and ultimately foster greater confidence in the broader financial system.\n\nRaising standards of behavior begins with creating a culture in which people are motivated to follow good market practices. The primary responsibility for improving the culture in the financial services industry rests with you, your firms, and every other financial institution participating in global financial markets. Best practices can play a role in building a strong ethical culture that improves the behavior of market participants and strengthens the public's trust in the industry.\n\nCentral banks have an interest in ensuring that financial markets sustain a healthy and robust economy. As active market participants and for the transmission of monetary policy, we support best practices that promote the integrity and effectiveness of the financial markets. We recognize the usefulness of establishing a collaborative process for identifying and affirming standards of good market practice. We also recognize that the success of best practices relies on the contributions and undeniable support of market participants and primary dealers in particular.\n\nAs primary dealers, you play an important role in implementing best practices internally and supporting the integrity and efficiency of markets. The TMPG's and the FXC's best practices are intended not only for dealers or market makers, but for all participants active in the relevant market. Your firms are important first adopters of best practices given your positions as central nodes. We also expect you to be engaged in a process to proactively identify emerging risks and vulnerabilities, and to suggest improvements to best practices as market structure evolves. We highly value your role as responsible counterparties and market participants, and expect your continued engagement and support in maintaining fair and effective markets.\n\nThank you.\n\n1 I would like to thank Radhika Mithal and Michelle Ezer for their excellent assistance in the preparation of these remarks, as well as colleagues in the Federal Reserve System and central bank community for their insightful comments and suggestions.\n\n2 The \"Desk\" refers to the Federal Reserve Bank of New York's trading desk, which implements monetary policy on behalf of the Federal Reserve System, as directed by the Federal Open Market Committee.\n\n3 Exchanges, such as those in the U.S. securities and commodities markets, develop standards that promote efficient and fair trading for activity conducted on the exchange, govern the products traded through the exchange, and set minimum fitness standards applicable to members of the exchange; exchanges thus are able to set rules for their participants that touch on many of the concerns that may be addressed by best practices. That said, even markets that feature a great degree of exchange trading may benefit from a consideration of best practices.\n\n4 The U.K.’s Fair and Effective Markets Review was a critical effort to reinforce confidence in the wholesale fixed income, currency and commodities markets in the wake of serious misconduct seen in recent years. This effort was led by the Bank of England in collaboration with the HM Treasury and the Financial Conduct Authority.\n\n5 Some examples of best practices in these sectors include: the National Education Association, which develops pedagogical standards based on its research; the Best Manufacturing Practices Center of Excellence, which helps businesses identify, research, and promote exceptional manufacturing practices, methods, and procedures; and the Food and Agriculture Organization of the United Nations, which develops and shares \"good agriculture practices\" for a range of commodities.\n\n6 During the development of the first phase of the BIS FX Global Code, a large number of detailed comments and useful suggestions were received from market participants that were integral to the drafting process. In April 2015, the TMPG solicited comments on its proposed best practice updates related to automated trading in TMPG-covered markets and received feedback from firms active in electronic trading and an industry association of proprietary trading firms. This feedback helped refine the proposed best practice updates.\n\n7 The OTC Derivatives Supervisors Group originated in 2005 when the New York Fed hosted a meeting of major OTC derivatives dealers and their domestic and international supervisors. The Alternative Reference Rates Committee was convened by the Federal Reserve in 2014 in a meeting with representatives of major OTC derivatives market participants, their supervisors, and central banks.\n\n8 Outside of New York Fed-sponsored groups is the Group of Thirty (G30). The G30 was established in 1978 and is an international body composed of senior representatives in the private and public sectors and academia. It has a broad mandate and works on various issues relating to international economic and monetary affairs.\n\n9 The current version of TMPG's Best Practices for Treasury, Agency Debt, and Agency Mortgage-Backed Securities Markets is available here.\n\n10 Such documentation includes, but is not limited to, FX master agreements, FX non-deliverable transaction documents, and FX master give-up agreements.\n\n11 Several examples in the FX and fixed income markets illustrate that best practices would not be successful without the undeniable support of market participants. For instance, reduction is settlement risk due to the adoption of payment-versus-payment mechanisms, drop in settlement fails in U.S. Treasury markets after the implementation of the fails charge, and improvements in the derivatives markets infrastructure are just a few examples.\n\n12 The BIS effort to develop an FX Global Code and the Bank of England's leadership of the Fair and Effective Markets Review are two examples of this public-private partnership.\n\n14 The TMPG issued a recommendation to introduce margin requirements for agency MBS transactions in November 2012. It collected data from its members on the progress made towards implementing the recommendation and reported summary statistics to the public, which may have supported wider adoption. The TMPG's elevation of this important issue informed FINRA's amendment of its Margin Requirement rule to establish margin requirements for forward-settling To Be Announced (TBA) transactions.\n\n15 In the FX market, similar concerns around credit risk came to the fore during the global financial crisis, and the FXC adapted its best practices to encourage widespread adoption of credit support annexes in 2010.\n\n16 Significant work has also taken place in regards to FX benchmarks with various regional FX committees working together to produce the Global Preamble to support implementation of the FSB's FX benchmark recommendations. See here.\n\n17 Another example is the market's long-standing use of ICAP's Fed Funds Open (FFO) as a benchmark for various financial transactions. However, ICAP, the FFO publisher, did not consider it to be a financial benchmark and did not calculate it as such, which raised questions about the FFO's suitability to be used in such a manner. The TMPG identified this issue during its work on financial benchmarks and encouraged industry groups to examine possible alternatives to the FFO. This led the Securities Industry and Financial Markets Association (SIFMA) and the Risk Management Association (RMA) to jointly recommend that market participants use the overnight bank funding rate, a rate more consistent with international best practice recommendations. See SIFMA's and RMA's joint announcement here and the TMPG's statement of support here.\n\n18 The following year the TMPG was established and issued its first set of best practices, with several recommendations to improve the clearing and settlement processes to alleviate fails. See here.\n\n19 See \"The Introduction of the TMPG Fails Charge for U.S. Treasury Securities\" for more information.\n\n20 The New York Fed has organized workshops and conferences on reforming culture in the financial services industry. Additionally, several executives have spoken publicly on the topic. See here for more detail.\n\n21 The G30, the Basel Committee, the European Systemic Risk Board, and the FSB have issued papers on culture, governance, and misconduct risk. The Bank of England's Fair and Effective Markets Review called for higher conduct standards for the fixed income markets. The central bank of the Netherlands—De Nederlandsche Bank—has developed new approaches for the supervision of corporate culture and decision-making.\n\n22 The MPG is working with the BIS FXWG to develop market-based adherence mechanisms for the FX Global Code.\n\n23 For instance, FINRA recently amended its Margin Requirement rule to establish margin requirements for forward-settling TBA transactions and noted that its proposal was \"informed by the set of best practices adopted by the Treasury Market Practices Group.\""
    },
    {
        "title": "Remarks at the New York Fed’s Economic Press Briefing on the Regional Economy",
        "date": "Aug 18, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud160818",
        "content": "Good morning and welcome to the New York Fed’s Economic Press Briefing. I am pleased to have this opportunity to speak with the journalists covering our region. You are an important communication channel to the people in our District. This morning I want to focus on economic conditions in our region, giving particular attention to job growth. I also want to discuss our ongoing work on Puerto Rico. As always, what I have to say reflects my own views and not necessarily those of the Federal Open Market Committee or the Federal Reserve System.1\n\nThe New York Fed is deeply committed to serving our region. This commitment manifests itself in several ways. We produce business surveys and local economic indexes to help our constituents track regional economic conditions. My staff writes papers and blog posts on economic issues important to our region. We meet with a number of advisory boards comprised of community and business leaders. And, I regularly visit different parts of our District so that I can meet with local businesses, community development professionals, and the people who live and work here. This on-the-ground intelligence is valuable to me and helps inform my view of the region and the economy, which, in turn, plays an important role in shaping my outlook on policy.\n\nLet me now talk a bit about job growth in our region.\n`\nJob Growth in the Region\n\nNationally, the labor market has added 2.4 million jobs over the past year. And, the strong jobs reports released over the past two months have helped allay concerns that arose earlier this year that job growth was beginning to stall. Indeed, these reports reinforce my view that labor market conditions continue to improve.\n\nTurning to conditions closer to home; job growth in our region has generally been somewhat below the national pace. One important exception is New York City, where job growth has been strong, despite the relatively sluggish performance of the financial services industry. In the past, the City has counted on job growth from Wall Street to fuel economic growth during recoveries and expansions. This time around, however, job gains in the securities industry have been meager. Instead, the City’s job growth has been quite widespread in sectors outside of finance. One that is especially noteworthy is the City’s burgeoning technology sector, which has been creating jobs in industries such as internet publishing, online shopping, and scientific research and development. Growth in these high-paying jobs is picking up much of the slack created by the softness of the securities industry.\n\nIn fact, strong growth in New York City has been helping many of its surrounding areas, including northern New Jersey, Long Island and Fairfield County. Many workers residing in these areas commute to New York City and have access to the City’s vast, diverse and expanding job market. For example, Long Island’s unemployment rate is now below 4 percent, its lowest level in nearly a decade. In Connecticut, Bridgeport has also benefited. When I visited a few months ago, I saw firsthand some of this resurgence. Bridgeport is both the largest and poorest city in Fairfield County, and for decades had seen its population and job base steadily decline. But, since 2000, its population has started to grow again for the first time since the 1940s. Even as other parts of Fairfield County struggle with a number of high-profile corporate relocations, jobs have begun growing and Bridgeport is showing signs of revival.\n\nIn upstate New York, economic growth has generally been modest, with job growth running well below the national pace. This isn’t new, as upstate New York has historically tended to grow much more slowly than the nation. Though growth has been slow for the region as a whole, some places are doing relatively better than others. In particular, job growth has picked up in Buffalo, and the Capital Region continues to see sturdy growth coupled with significant manufacturing job gains.\n\nHowever, not all parts of upstate New York are seeing the same degree of economic progress. There has been little to no growth in the center of the state, and Utica and Binghamton continue to see employment declines. Binghamton stands out as being particularly challenged, as its economy has yet to see any meaningful recovery from the Great Recession. Steep manufacturing job losses have weighed heavily on the area, though these losses appear to be nearing an end. I had the opportunity to visit Binghamton earlier this summer, and I was encouraged by what I saw. The region is making serious efforts to boost economic growth, with Binghamton University playing a key role in fostering local business activity. And, like many places dealing with steep manufacturing job losses, there is a large swath of displaced workers that need assistance returning to the labor market. This has proved challenging and remains a priority.\n\nUpdate on Puerto Rico\n\nOn balance, I think it’s fair to say that the economic news from much of the Second District has been quite positive. An important exception, though, has been the fiscal crisis faced by the Commonwealth of Puerto Rico, home to 3.5 million U.S. citizens. Unfortunately, Puerto Rico continues to struggle under the weight of economic stagnation, employment declines and outmigration. This year, the government has been unable to make its debt payments on schedule and has lost access to the public debt markets. The pressures that culminated in these missed payments have been building for some time. This is an issue that we have been focused on for several years. We have published a number of reports in order to help highlight challenges and opportunities, to inform the discussion around solutions.\n\nI have visited Puerto Rico on several occasions, and had the opportunity to talk to residents and hear about the challenges they face. As an outgrowth of one of these trips—taken in 2011—I commissioned my staff to produce a report on the competitiveness of the Island’s economy. We released this report in 2012, and it served, I believe, a useful purpose in providing technical analysis and identifying some important issues deserving of policy attention. Over the following two years, the Island’s fiscal problems worsened. Consequently, in 2014 we published an update to the initial report―this time focusing on the Island’s public finances―and again offering suggestions for consideration.\n\nIn June, Congress and the Obama Administration took action providing a framework to help address the Commonwealth’s fiscal crisis. Part of that agreement re-emphasized that the Commonwealth’s fiscal sustainability depends crucially on an expanding economy. In an effort to continue to shed light on the important issues, last week we published in our Liberty Street Economics blog a five-part series entitled “Restoring Economic Growth in Puerto Rico.” Some important conclusions emerged from our analysis, and I want to emphasize those today.\n\nFirst and foremost, Puerto Rico’s current level of public debt is unsustainable. Achieving fiscal sustainability requires real reforms and a significant adjustment to the Commonwealth’s debt burden, as Congress and the Administration have recognized. To illustrate this point, our analysis demonstrates that even if growth were to return to between 2 and 2.5 percent per year—an optimistic scenario—debt as a share of GDP would continue to rise under the current fiscal balance and debt structure. This underscores that the current regime is not sustainable. Second, while fiscal reform is definitely needed, restoring economic growth remains a crucial ingredient to achieving a sustainable fiscal regime. Puerto Rico slipped into recession in 2006, and a decade later a recovery has yet to materialize. Any fixed level of debt will grow even more burdensome if the economy is shrinking. Consequently, this long deep recession has exacerbated the Island’s fiscal woes.\n\nMuch of last week’s analysis focused on a key contributor to economic growth—the labor force—so let me point out a few conclusions that I drew from that work. Puerto Rico’s outmigration remains an important concern, and is both a cause and an effect of the economic decline. Although the skills of those who have migrated to the Mainland look much like those who remain, the departure of tens of thousands of residents each year represents a huge drain on Puerto Rico’s potential growth. Among those who remain on the Island, low labor force participation, an aging population and relatively poor preparation for skilled jobs represent significant headwinds to growth.\n\nWhile our analysis seems to paint a somewhat dark picture of economic conditions on the Island, it’s important to remember that there are significant opportunities for action that can leverage the Island’s strengths to foster economic growth. These include its bilingual adult population, an open economy occupying a central position in the Caribbean, wide experience as host to international corporations, and close economic ties to the U.S. mainland. It is in exactly this spirit that we present this analysis—in the hope that it will help policymakers focus on actions that can help to restore growth and prosperity to the Commonwealth. Going forward, we will stay engaged in this effort.\n\nMiddle-Wage Jobs Returning\n\nLet me conclude by returning to the theme of the labor market. Job growth is essential to the vitality of an expansion, but the types of jobs being created are also important. While the labor market has continued to add jobs at a solid pace, many remain concerned about a lack of job opportunities for the middle class. Indeed, growth of middle-wage jobs has been lackluster for the past few decades, with gains occurring disproportionately in higher-wage and lower-wage sectors. This long-term hollowing out of jobs in the middle of the wage distribution has helped fuel rising wage inequality, and has contributed to a growing sense for some that they are being left behind in the current economic expansion. At a previous press briefing, we showed that many middle-wage jobs disappeared during the Great Recession with very few of these jobs returning during the early stages of the recovery. This made it especially difficult for many workers who lost their jobs to rejoin the economy.\n\nToday, our economists will show that the tide has begun to turn. For the first time in quite a while, gains in middle-wage jobs actually outnumber gains in higher- and lower-wage jobs nationwide. These middle-wage jobs include teachers, construction workers, mechanics, administrative support personnel and truck drivers, just to name a few. I believe this is an important development in the economy, because, if it were to continue, it would create more opportunities for workers and their families who have been struggling up to now. Middle-wage jobs are also being created in our region, though such gains have been more evident in some places than others.\n\nI will now ask Jaison Abel to provide more details about job growth in our region.\n\n1 Jaison Abel, Jason Bram, Hunter Clark, Giacomo De Giorgi, Richard Deitz, Jack Gutt, Andrew Haughwout and Joseph Tracy assisted in preparing these remarks."
    },
    {
        "title": "The U.S. Economic Outlook and the Implications for Monetary Policy",
        "date": "Jul 31, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud160731a",
        "content": "It is a pleasure to have the opportunity to speak here in Bali. Today I plan to provide a brief summary of the U.S. economic outlook, incorporating recent U.S. economic data and global and financial market developments. I will then discuss the implications of the outlook for U.S. monetary policy, and explain how international and financial market developments influence my thinking. I will emphasize two key points.\n\nFirst, financial market conditions matter in determining the appropriate stance of monetary policy. Financial conditions affect households’ and firms’ decisions, so that the transmission of U.S. monetary policy to the real economy depends, to a large extent, on how changes in monetary policy help deliver the appropriate financial market conditions to support our objectives of price stability and maximum employment. But financial conditions also evolve in response to domestic and international events. Thus, when I reiterate that U.S. monetary policy is data dependent, that includes not just the information gleaned from important economic releases such as payroll employment and retail sales, but also how financial market conditions react to economic and financial market developments in the global economy.\n\nSecond, external events―such as Brexit―can have effects that go beyond just their impact on global trade. Conversely, what we do in the United States has an impact far beyond our borders, and we need to take that into consideration in how we conduct and communicate monetary policy in the United States. Put simply, monetary policy is a two-way street, and we all need to be cognizant of that.\n\nAs always, what I have to say today reflects my own views and not necessarily those of the Federal Open Market Committee (FOMC) or the Federal Reserve System.1\n\nThe U.S. Economic Outlook\n\nDespite the many twists and turns in the road so far, my baseline outlook for growth and inflation in the United States has not changed much in recent months. After a weak first quarter, real GDP continued to grow sluggishly in the second quarter. On the positive side, consumer spending rose more quickly during the second quarter. On the negative side, business fixed-investment spending continued to disappoint, residential investment was weak and inventory investment was a substantial drag on growth.\n\nLooking forward, I expect economic activity to expand at roughly a 2 percent annualized pace over the next 18 months, appreciably above that of the past three quarters. Over the remainder of this year, the economy should continue to get some lift from consumption and from a fiscal policy stance that remains mildly stimulative. Moreover, I anticipate that residential investment will rebound from its decline in the second quarter. This pace of growth would likely be sufficient to continue to absorb any remaining slack in the labor market and to support inflation moving back to our 2 percent objective.\n\nHowever, any forecast is uncertain and growth could end up higher or lower. Let me turn to an assessment of the balance of risks around my growth outlook. I believe that a sizable pickup in the rate of growth of economic activity relative to that in my forecast seems unlikely for a number of reasons. Although consumer spending strengthened substantially in the second quarter, that stronger pace is unlikely to be sustained going forward. That is because the fundamentals supporting consumption have softened somewhat. Real income growth moderated in the second quarter as the pace of job gains slowed and the inflation rate rose, boosted by somewhat higher energy prices. Even with the surprisingly large addition of 287,000 payroll jobs in June, the pace of improvement in the U.S. labor market appears to have slowed somewhat. For example, over the past three months, payroll gains have averaged about 150,000 per month, as compared to an average of more than 200,000 per month over the course of 2015 and the first quarter of this year. But even 150,000 job gains per month would be consistent with gradually using up any remaining slack present in the U.S. labor market.\n\nAlthough I expect business fixed investment to begin to grow again in the second half, it will likely remain soft as profits have stagnated and election year uncertainty could act as an additional depressant. All else equal, investment tends to be weaker when uncertainty rises because this creates an incentive for businesses to delay decisions until the uncertainty is resolved.\n\nAlso, trade seems likely to exert a net drag on growth. There are two factors at play here: the sluggish growth rate of aggregate demand abroad and the continuing impact of earlier dollar appreciation on U.S. export competitiveness.\n\nWith respect to downside risks to the growth outlook, one that is hard to gauge is the potential fallout from the result of the recent U.K. referendum. It is widely anticipated that U.K. growth will slow as a consequence. Although the direct impact of slower U.K. growth on U.S. trade will almost certainly be very small—total U.S. exports to the U.K. are only about 0.7 percent of U.S. GDP—there are a number of other channels that could amplify the impact of the Brexit decision on U.S. economic activity. These include the potential adverse effects on European economic activity, on the perceived health of the global banking system, and on broader financial market conditions.\n\nTo date, the global financial market fallout from the Brexit vote has been short-lived, and U.S. financial market conditions remain supportive to economic growth. Nevertheless, I believe that the potential aftershocks pose medium-term downside risks to the global economy, and that these risks need to be monitored.\n\nWith respect to inflation, I think the outlook has not changed much recently. Headline inflation, as anticipated, has climbed a bit this year as earlier energy price declines have fallen out of the year-over-year inflation calculations. In contrast, core inflation has been broadly stable, with the core PCE deflator rising by 1.6 percent over the past four quarters, moderately below our 2 percent objective.\n\nThe fact that core inflation has been broadly stable over recent months in the face of the earlier declines in energy and non-energy import prices is notable. It makes me somewhat more confident that overall inflation will return to our 2 percent inflation objective over the medium term as long as the economic growth that I expect actually materializes. If the economy were to grow at the pace I discussed earlier, this would likely translate into sufficient job gains to continue to remove any remaining slack in the labor market—which, by my assessment, is already operating quite close to a level that is consistent with what is achievable on a sustainable basis. This would likely lead to further pressure on labor resources, higher wages and, over time, somewhat higher inflation.\n\nIn contrast, if growth were instead to fall below my forecast, then I would be less confident that inflation would return to our 2 percent objective. In addition, I put some weight on the fact that surveys from both the University of Michigan and the New York Fed indicate that household longer-term inflation expectations have declined somewhat over the past year and are in the lower end of their historical ranges. Nevertheless, I am not unduly worried because the magnitude of these declines has been modest, and because the New York Fed’s three-year-ahead inflation measure has been gradually increasing since January and has reversed much of the decline observed in the second half of 2015. Low longer-term inflation expectations, if allowed to become entrenched, would act as a restraint on actual inflation making it more difficult for us to meet our inflation objective.\n\nIn contrast, I put less weight on the significantly larger declines in market-based measures of inflation compensation over the past two years. From my perspective, these declines seem driven more by changes in bond term premia and the relative liquidity characteristics of nominal versus inflation-protected Treasury securities than by sharply falling inflation expectations.\n\nImplications of the Outlook for U.S. Monetary Policy\n\nIf, as I have indicated, the U.S. growth and inflation outlooks have not changed notably, then why have expectations about U.S. monetary policy shifted so much? Compared to the start of the year, the expected timing of any further Fed interest rate hikes has been pushed back, and the expected upward trajectory of U.S. short-term rates is now much flatter.\n\nThere are several reasons to explain these shifts. First, assessments of the neutral real short-term interest rate have declined as economic growth has consistently fallen short of consensus forecasts. With the U.S. economy having grown at only a 2.1 percent annual rate over the past seven years, it has become harder to sustain the view that the neutral real short-term interest rate is close to, or will soon be close to, its historical level of around 2 percent. Estimates of the neutral real short-term interest rate obtained from many of the DSGE models used within the Federal Reserve System are currently clustered around zero, and this seems reasonable to me. An implication is that at present there is only a small gap between the actual real short-term rate of about -1 percent and the neutral real short-term rate. In other words, U.S. monetary policy is accommodative, but only moderately so.\n\nIn addition, I suspect that many have come to question the view that headwinds from the financial crisis are temporarily depressing the neutral short-term rate, and that the neutral short-term rate will significantly rise in the near-future as these headwinds dissipate. If the headwinds have not dissipated to a meaningful degree in the seven years since the recession ended, then why should one expect them to necessarily diminish quickly over the next couple of years? Evidence is accumulating that some of the headwinds are likely to prove more persistent. For example, the reduced availability of mortgage financing for those with lower credit scores seems likely to continue. Lenders now appreciate that home prices can decline significantly. Thus, they cannot rely as much on the value of the housing collateral in securing their mortgage loans, and consequently now put more weight on the credit histories of the borrowers.\n\nMarket participants may also be taking some signal from the gradual decline in the median long-run federal funds rate projection of FOMC participants shown in the FOMC’s Summary of Economic Projections (SEP). I think this indicates a growing consensus that the neutral real short-term rate will not return anytime soon to its historical norm of 2 percent.\n\nA second reason for the downward adjustment in U.S. interest rate expectations is that U.S. financial market conditions depend, in part, on the stance of U.S. monetary policy relative to monetary policies abroad. If the economic outlook abroad deteriorates and this causes foreign countries to pursue a more accommodative set of monetary policies, then the dollar would likely appreciate―other things equal― reflecting expectations of lower interest rates abroad relative to U.S. interest rates. In this case, the U.S. may need to adjust its own monetary policy path. If the FOMC did not make this adjustment, the stronger dollar could result in an undesired tightening of U.S. financial conditions. The expected forward interest rate paths in Europe and Japan have fallen considerably this year. If the FOMC had followed the median federal funds rate path from the December 2015 SEP projection, then the U.S. dollar would likely have appreciated much more significantly. Instead, the U.S. interest rate path has come down in tandem with the foreign interest rate paths and the dollar has appreciated only modestly.\n\nThis is a crucial point and I want to make sure there is no misunderstanding. The Federal Reserve is not targeting the exchange value of the U.S. dollar. What the FOMC considers are financial conditions broadly defined, because they affect the saving and investment decisions of households and firms. The dollar is but one component of these financial conditions. The level of short- and long-term rates, credit spreads, and equity prices are also important components of the financial conditions that we closely monitor. If international developments shift U.S. financial market conditions―including the dollar―then we need to take this into consideration in our U.S. monetary policy decisions.\n\nThird, I have found that market participants broadly appreciate that the FOMC needs to take a risk management approach in its conduct of monetary policy. There are at least two important aspects of this approach. The first is whether the balance of risks to the outlooks for either economic growth or inflation are skewed to the upside or downside. The second is whether the efficacy of monetary policy itself is asymmetric when monetary policy is at, or close to, the zero lower bound for interest rates. In this situation, it may be easier to implement a tighter monetary policy through raising rates, than it would be to implement a looser policy using unconventional tools. Also, the effects of a policy of raising rates may be more predictable compared to the effects from using unconventional tools.\n\nAs I noted earlier, I think the medium-term risks to the U.S. economic growth outlook are somewhat skewed to the down side. Thus, this needs to be taken into consideration in terms of the appropriate stance for U.S. monetary policy. With respect to the efficacy of monetary policy, given how close we remain to the zero lower bound for interest rates, I also think the risks are asymmetric. Therefore, we need to be a bit more careful about the risk of tightening monetary policy in a manner that proves to be premature, as compared to the alternative risk of being a little late. If we were to realize that we were slightly late, policy can be adjusted by raising short-term interest rates more quickly.2\n\nAll three of these reasons—evidence that U.S. monetary policy is currently only moderately accommodative, the fact that U.S. financial conditions have been influenced by economic and financial market developments abroad, and risk management considerations—argue, at the moment, for caution in raising U.S. short-term interest rates. So, directionally, the movement in investor expectations towards a flatter path for U.S. short-term interest rates seems broadly appropriate.\n\nThat said, to my eye, market expectations derived from futures prices—which price in about one 25 basis point rate hike through the end of 2017—appear to be too complacent. If the incoming information validates my view of the outlook, then I believe that U.S. monetary policy will likely need to move at a faster pace than implied by futures prices towards a more neutral posture as the labor market tightens further and U.S. inflation rises. Moreover, market expectations may be putting insufficient weight on the possibility that the economy could outperform our expectations, that financial conditions could ease, or that the risks to growth from Brexit and other international developments could fade away. If such events were to occur, this might necessitate a faster pace of adjustment.\n\nFor these reasons, I think it is premature to rule out further monetary policy tightening this year. As I said before, it depends on the data, broadly defined, and, as we all know, that is not something one can predict with any accuracy.\n\nIt’s a Two-Way Street\n\nThe U.S. economy plays a large role in the global economy. Most significantly, the U.S. economy represents a sizable share of world GDP—roughly 25 percent at current exchange rates—and the U.S. dollar is the most important international reserve currency. More than 60 percent of central bank reserve assets are denominated in dollars, and that share has been stable to rising in recent years. Most foreign trade is denominated in dollars and most of the foreign currency debt issued by corporations abroad is denominated in dollars. Thus, what happens to the U.S. economy, U.S. financial asset prices and the exchange value of the dollar has important implications for the global economy.\n\nAt the same time, prosperity is very much a two-way street. Developments outside the United States affect our domestic economic outlook through their impact on trade and financial market conditions, and we have to take such developments into consideration in our monetary policy decision-making.\n\nIn many ways, international linkages have become more important over time. This is because international trade has increased rapidly over the past few decades as the world economy has become more developed and globalized—notwithstanding the flattening of the global trade-to-GDP ratio over the past few years. And, global trade interactions have become more complex as supply chains have become more extensive and intricate, often involving many different countries.\n\nFinancial markets across the globe have also become more integrated. Developments in one market now appear to have larger effects on other markets than was generally the case historically. Consider, for example, how European and Japanese central bank quantitative easing activity has helped drive the sharp decline in long-term U.S. Treasury yields this year. Or, in the other direction, consider the global bond market taper tantrum in 2013. In this case, markets reacted to then-Chairman Bernanke’s musing that the Federal Reserve was beginning to evaluate when the time would be right to begin the tapering of the Fed’s asset purchase program. Or, in a similar vein, consider the international financial market reaction to China’s decision to alter its foreign exchange rate regime and how the RMB is managed relative to the dollar versus a broader basket of foreign currencies.\n\nThe growing interdependence can be seen in the increased correlation of market movements both across countries and across asset classes. Periods of “risk on” versus “risk off” trading now occur on a global basis. For example, equity market movements in developed and emerging markets have exhibited a 76 percent positive correlation over the past six months. This is only slightly below the all-time peak of 82 percent in 2009, and significantly above the 57 percent correlation that prevailed from 1995 through 2007.\n\nCorrelations across asset classes have also been increasing. For example, consider the set of assets comprised of the 10-year U.S. Treasury, U.S. equities, international equities, oil, the VIX, a trade-weighted dollar index and the BAA credit spread. We can construct a variable—called a common factor—to capture as much of the overall movement and co-movements for the series in this set. The more closely the series’ movements are tied together, the greater the explanatory content of the common factor. Currently, the derived common factor accounts for 50 percent of the variation in these financial variables, up from 30 percent in early 2014.\n\nOil, in particular, has become more correlated with other assets. Prior to 2008, oil was virtually uncorrelated with equities and Treasuries. Whereas, in 2016, its correlation with these two asset classes has been more than 45 percent.\n\nThese effects are transmitted via many linkages, not just through trade and financial markets. Consider, the many different channels of potential Brexit influence—not only the impact on international trade and global interest rates and currencies, but also on bank equity prices and on political uncertainty.\n\nGiven this interdependency, what should we do about it? Interdependency and linkages do not mean that U.S. monetary policy should subordinate its domestic goals for international ones. The Federal Reserve has a clear domestic-oriented mandate that was set by the U.S. Congress. Instead, I believe that setting U.S. monetary policy to best achieve our domestic mandate will help to support sustainable growth and development abroad. As I see it, there are two key steps that are essential in the design of an appropriate monetary policy strategy.\n\nStep one is to take an expansive view of the global eco-system in which we all operate. We need to take into consideration that our decisions have broad consequences for the global economy and, conversely, that international developments can have significant consequences for the U.S. economic outlook and therefore the appropriate stance of U.S. monetary policy. As part of this, we need to be nimble in incorporating new developments into our monetary policy decision-making.\n\nStep two is to communicate clearly and consistently. That means clarity about the objectives of monetary policy, how the Fed plans to meet those objectives in light of the economic and financial market environment, and how it formulates its responses to unforeseen circumstances that lead to revisions to its economic forecast.\n\nIn my view the Federal Reserve is making progress in both of these areas. With respect to the first step, I believe we do take an expansive view of those factors that might affect the U.S. outlook and we revise our views accordingly. Our speeches, statements, and actions this year illustrate this is how we operate. For example, after the market turbulence at the start of the year, we kept monetary policy on hold at the March FOMC meeting and explicitly referenced “readings on financial and international developments” in the FOMC statement. Similarly, in speeches prior to the Brexit vote and in the FOMC minutes, we raised our concerns about the risks of disorderly outcomes associated with the U.K. referendum. For instance, the June FOMC minutes state: “Most participants noted that the upcoming British referendum on membership in the European Union could generate financial market turbulence that could adversely affect domestic economic performance.”\n\nIn addition we are doing a reasonably good job incorporating the flow of new information into our forecasts. The fact that the federal funds rate projections from the SEP have changed significantly from quarter to quarter indicates that FOMC participants are responsive to new information.\n\nNow, some have expressed an alternative view that the movement in these rate projections is an indication that the FOMC’s reaction function is unstable and unmoored. I do not see it that way at all. The forecasts of FOMC participants with respect to growth and inflation have not changed much this year. What have changed are expectations about the monetary policy stance that would be appropriate in order to achieve those outcomes. It is important to emphasize that these interest rate projections are not commitments. They are point-in-time views of the appropriate interest rate path and are updated as economic circumstances and financial market conditions change and evolve.\n\nWith respect to communication, in recent years the Federal Reserve has shifted towards much greater transparency. This includes quarterly press conferences by the Fed chair following FOMC meetings; publishing growth, inflation and short-term interest rate forecasts of FOMC participants on a quarterly basis; and a concerted effort to lay out the guideposts that the FOMC will look at in assessing progress towards our dual mandate objectives.\n\nOn the communication front, although we have come a long way, I would admit that there is still room for further improvement. For example, the focus of the SEP on the each participant’s modal forecast does not convey how much uncertainty there is about the economic outlook. Similarly, language used in FOMC statements can become stale over time. We tend to make relatively few changes to the statement language each meeting because of the acute market sensitivity to such changes. One could argue that this might not be the best practice to follow, but we should recognize that there would also be significant transition costs if we were to make more extensive revisions to the statement at each meeting.\n\nFor monetary policy to be effective, it is important to have clarity about what the FOMC can be clear and consistent about—its manner of responses to mitigate the potential harmful effects of disturbances and the goals of policy. In contrast, our monetary policy projections and the actions we take cannot be static. If economic circumstances change, then monetary policy needs to change too. Otherwise, we will not be able to achieve our objectives.\n\nThank you for your kind attention. I would be happy to take a few questions.\n\n1Tobias Adrian, Matthew Higgins, Jonathan McCarthy, Paolo Pesenti, Robert Rich, Giorgio Topa, Joseph Tracy and Peter Van Tassel assisted in preparing these remarks.\n\n2Of course, if the economy were to weaken, we still have plenty of tools. Not only could we cut short-term interest rates, but we also could extend the maturity of our Treasury and agency MBS portfolio, purchase additional Treasury and agency mortgage-backed securities and engage in forward guidance with respect to the future path of short-term interest rates."
    },
    {
        "title": "Panel Remarks at Bank Indonesia–Federal Reserve Bank of New York Joint International Seminar",
        "date": "Jul 31, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud160731",
        "content": "In my remarks today, I will focus on why global growth has been so anemic following the financial crisis and what we can do about it.1\n\nI think it is fair to say that the global recovery following the financial crisis has been disappointing. Growth is below pre-crisis levels for the United States, Europe, Japan and China. For example, in the U.S., real GDP growth has averaged only about 2.2 percent per year over the period from 2010 to 2015, below the pre-crisis average of 2.4 percent from 2003 to 2008. This slower growth performance has been accompanied by relatively low inflation rates—persistently below the objectives of the monetary policy authorities in the United States, Europe and Japan.\n\nAs I see it, there are many causes for these persistent shortfalls in growth, and they vary across regions and countries. Let me focus on the reasons for the United States in the post crisis period.\n\nGrowth has been weaker than expected for many reasons. The financial crisis was a searing experience that damaged household and business confidence in a profound way—with a more lasting effect than the typical economic downturn. The housing bust created both a large housing supply overhang and a large number of households that were underwater on their mortgages. Households needed to repair their balance sheets and bring down their debt service burdens to more manageable levels. Higher levels of unemployment further depressed demand.\n\nCredit availability contracted. The tightening of credit standards occurred quite broadly—in housing credit, consumer credit cards and business lending. Some of the credit supply restrictions were imposed by banks that needed to repair their balance sheets, deleveraging by building up capital and working down their bad assets.\n\nFiscal outlays were not high enough to compensate for the contractionary impulse from these first three sources. The crisis led to a significant deterioration in the U.S. federal budget deficit and put pressure on state and local budgets. After the federal budget deficit reached around 10 percent of GDP at the peak, fiscal consolidation occurred at the federal level, as well as at state and local levels, restraining economic activity instead of supporting growth.\n\nProductivity growth has also been quite low. In the period before the crisis (2003 to 2008), U.S. nonfarm productivity growth averaged 2 percent per year. Since 2010, productivity has been closer to 1 percent. While some of this decline can be explained by lower capital spending, the decline is difficult to fully explain.\n\nAnother explanation some have offered is that there is a measurement problem—we are understating output growth and overstating price inflation. This type of issue, long flagged as a hedonic adjustment, seems most credible in particular market segments, such as in healthcare. Alan Blinder has posed this question: Would you rather have 1970 healthcare at 1970 prices or 2016 healthcare at 2016 prices? Based on the measurement of prices, 1970 healthcare is a much better deal. Yet, when offered this choice, almost all choose 2016 healthcare. Why? The reason is that people highly value the advances in healthcare technology, such as in cardiac care, or in the procedures that are practical today, such as hip and knee replacement, that simply weren’t available 50 years ago.\n\nU.S. economic performance has also been held back by developments in the rest of the world. Growth outside of the United States has slowed. In the euro area, comparing the period from 2003 to 2008 to the period from 2010 to 2015, annual real GDP growth slowed from 1.9 percent to 0.8 percent. Japanese growth has remained muted over these periods at 0.1 percent and 1.0 percent respectively. In China, reported growth rates over these two periods slowed from 11.3 percent to 8.3 percent. These developments, along with policy and risk events that have increased demand for U.S. assets, have contributed to significant U.S. dollar appreciation over the last two years. This dollar strength has created competitive challenges for U.S. exporters, especially in manufacturing, and has caused the trade sector to be a drag on U.S. economic activity.\n\nLooking forward, despite the fact that U.S. economic growth has been disappointing, we have actually made considerable progress toward our monetary policy objectives. The United States is quite close to its employment and inflation objectives. On employment, there still seems to be a bit of slack in the U.S. labor market. But, that margin has shrunk greatly over the past few years. Payroll gains have been at least as robust as anticipated.\n\nOn inflation, we are still somewhat below our 2 percent objective. But, even here, I think there has been progress. Despite the strengthening of the dollar and the decline in energy prices, core PCE inflation has been quite stable over the past year—currently running at around 1.6 percent. That is not a tremendous difference from our 2 percent objective.\n\nSo why has the United States done a bit better compared to Europe and Japan in terms of achieving our monetary policy objectives. I think there are a number of contributing factors.\n\nFirst, U.S. household balance sheet repair occurred relatively quickly. This is in part because the housing foreclosure process wiped out many mortgage debts. Part was accomplished by a long period of very slow household debt growth. And part was accomplished by the sharp fall in interest rates that helped cut debt servicing costs.\n\nSecond, in the U.S., the banking system was recapitalized and deleveraging occurred quite quickly following the financial crisis. Capital from the Troubled Asset Relief Program, or TARP, was put into the large banks and soon many of them replaced this TARP capital by raising new equity. The SCAP stress test in 2009, and the annual CCAR stress tests that followed, worked to constrain the rate of bank capital distributions and, thereby, helped to build up bank capital ratios. And, U.S. banks worked hard to clean up their balance sheets. Poor assets were managed down or run off, and underwriting standards were tightened.\n\nThird, the Federal Reserve was particularly aggressive early on in its pursuit of monetary policy accommodation in order to keep inflation expectations well-anchored. This is a necessary condition, in my view, to maintain the efficacy of monetary policy. If inflation expectations were to get unmoored to the downside, then it becomes more difficult to pursue a stimulative monetary policy.\n\nCould things have been done differently in the U.S. in such a way that would have led to better outcomes? Absolutely. With the benefit of hindsight, we could have and should have been even more aggressive on the monetary policy side. While we made progress with some of the innovations on monetary policy that we eventually introduced—such as the open-ended purchase of $85 billion of Treasury and MBS securities per month—it would have been better if we had done this sooner.\n\nThere is a lot as well that is outside the Fed’s purview that could have been done to make the U.S. economy perform better. This includes tax reform, job retraining programs and infrastructure investment.\n\nSo, what can we do to bolster global growth in the future? The first is to undertake the necessary structural reforms to make our economies more efficient. Productivity growth is not preordained. The steps we take as countries to eliminate and lessen bottlenecks and improve our human and physical capital are important. The second is to ensure that our financial systems are well-functioning. Tremendous progress has been made globally in implementing higher capital and liquidity standards post-crisis. But we still see some important banking systems impaired by bad loans, low profitability and inadequate capital.\n\nThe population aging in the developed world does imply that, purely from the productive factor side, less of the global growth contribution is likely to come from Japan, Europe and the United States. And economies like China are likely to continue to slow as their per capita income increases and they make the transition from investment-to consumption-led growth. But this doesn’t imply a bad outcome. There are many other countries with tremendous potential for growth in areas such as Africa, Asia, and South America. Take, for example, the surge in growth that we are seeing out of India.\n\nI am an optimist. To a large degree we do control our destiny. It is up to us to seize those opportunities.\n\n1Linda Goldberg, Jonathan McCarthy and Joseph Tracy assisted in preparing these remarks."
    },
    {
        "title": "The Advantages of Probabilistic Survey Questions",
        "date": "May 19, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/pot160519",
        "content": "Good afternoon.  It’s a pleasure to talk with you about a topic that intrigues me as a former academic researcher, and that’s of significant importance to me in my current role as head of the New York Fed Markets Group and as manager of the System Open Market Account for the Federal Open Market Committee (FOMC): the measurement of policy-relevant expectations through surveys.1\n\nIt is entirely fitting to talk about this topic here in Italy, given the Bank of Italy’s long history of research and experimentation on expectations data. This history includes extensive work on data gathered from surveys using a probabilistic question format—the focus of my talk today. Since 1989, the Bank of Italy’s Survey of Household Income and Wealth has used this question format to ask households about their expected labor or pension earnings in the next year. The findings from these surveys have provided new insights into consumer behavior, such as the impact of subjective earnings uncertainty on precautionary saving.2 The Bank of Italy was also a pioneer in using probabilistic questioning to elicit expectations from businesses in its 1993 Italian Survey of Investment in Manufacturing. Data from this survey have been used to analyze the impact of uncertainty about future product demand on the investment decisions of Italian manufacturing firms.3\n\nAs always, the views expressed are my own and do not necessarily reflect those of the Federal Reserve Bank of New York or of the Federal Reserve System.\n\nWhy Are Expectations Data Useful?\n\nThe importance of compiling high-quality data on the expectations held by economic agents has been increasingly recognized in both academic research and policymaking. Most economic decisions involve uncertainty, and are therefore determined not only by preferences but also by expectations for future outcomes. Typically, neither preferences nor expectations are directly observed—a condition that poses a significant challenge to understanding economic behavior. Although different combinations of preferences and expectations may be consistent with the same observed behavior, they could result in different responses to shocks or to alternative policies and could therefore have different implications for public policy.4 The standard way to address this problem within economic models has been to assume rational expectations—that is, expectations that are consistent with equilibrium behavior in the model. In nearly all cases considered to date, this leads to the conclusion that expectations of economic agents are identical or will converge over time.\n\nWhile this approach has given us invaluable insights in a multitude of models and applications, it faces a major empirical problem: in practice, economic agents hold widely differing views about the future path of economic variables, as I will illustrate throughout these remarks. Thus, recent research has focused on alternative assumptions regarding the nature of expectations held by economic agents. Measuring subjective expectations directly can inform our modeling assumptions on the information sets available to agents, and on the nature of the expectations formation and updating process.5\n\nThere are many Bayesians in the audience today. However, for the benefit of those who are not as familiar with Bayesian terminology, let me quickly define what I mean by “subjective expectations” or beliefs. This term refers to the personal probability distributions that individuals hold over uncertain events. Apart from the laws of probability and rules about how beliefs are updated with the arrival of information, there are no restrictions on these probability distributions, hence the use of the word “subjective.”6 Further, individuals might act according to their subjective beliefs without being able to fully articulate in standard forms their underlying probability distribution. Thus, a need arises to elicit these views in order to understand behavior more fully.\n\nIn monetary policymaking, central bankers have long pointed out the importance of measuring the expectations of financial market participants, households, and firms—especially with regard to inflation and the central bank’s so-called “reaction function” to changes in the economic outlook. For example, measuring market participants’ expectations for the policy rate path sheds light on both the central bank’s effectiveness in communicating its policy intentions and the transmission of anticipated future monetary policy actions into financial conditions. Inflation expectations of households, firms, and other economic agents influence a variety of decisions, including those related to consumption, saving, investment, and the setting of prices and wages, as well as the determination of the real interest rates that influence these decisions. The aggregation of these choices in turn determines realized inflation in equilibrium. Thus, inflation expectations are a crucial determinant of actual inflation and, as such, need to be carefully monitored. Further, the effectiveness of monetary policy and central bank communication relies on longer-run inflation expectations being well-anchored, making their reliable measurement very important for policymakers. More generally, expectations by consumers and businesses about a number of household, firm-level, and economy-wide outcomes are increasingly useful inputs into a variety of forecasting and “now-casting” models.\n\nIn addition to tracking measures of expectations implied by asset prices, central banks today rely increasingly on survey-based measures of subjective expectations. At the New York Fed, we conduct a diverse set of surveys to measure policy-relevant expectations. Today I will talk about the surveys of households and market participants we conduct: the Survey of Consumer Expectations (SCE), the Survey of Primary Dealers (SPD), and the Survey of Market Participants (SMP). These surveys highlight the innovative and cutting-edge research conducted at the New York Fed and represent the culmination of the Bank’s decade-long investment in the collection of better policy-relevant expectations data. As I will now explain, much of this investment has been directed toward introducing probabilistic survey questions, but we have also learned other important lessons about question design and survey mode, particularly regarding the measurement of inflation expectations which I will also discuss. In some cases such as results of the SPD and SMP the underlying probabilistic nature of survey questions is immediately apparent, in the case of the SCE as I will describe the underlying probabilistic question is not so apparent for many of the results.\n\nWhy Use Probabilistic Survey Questions?\n\nA key feature of the New York Fed surveys is their focus on collecting rich quantitative data on subjective expectations. Extending a practice with a longer tradition in the field of psychology and in surveys of professional forecasters, economists, and other financial experts, our surveys rely heavily on a probabilistic question format to elicit the likelihood respondents assign to different future events. This approach builds on a large and growing body of research, led by econometrician Charles Manski, that has demonstrated survey respondents’ willingness and ability to answer questions expressed in this way.7\n\nTraditionally, surveys ask respondents to express their answers in one of a few ways: for example, using a Likert scale (specifying whether an event is \"very likely,\" “likely,” or \"not too likely\"), selecting the outcome they consider most likely (“Do you expect the unemployment rate to be higher or lower than today?”) or giving a point response (“By what percent do you think prices will increase over the next year?”).8 A problem with the Likert scale is that it does not allow comparisons across respondents, because different respondents may interpret the scales differently. A well-known illustration of the problem was provided by Daniel McFadden, who looked at a health status measure that used a Likert scale ranging from “excellent” to “poor.” While 62 percent of Danish men reported their health to be excellent, only 14 percent of French men did so, despite their enjoying two more years of life expectancy. A problem with questions asking for the outcome considered most likely—that is, questions asking for the modal outcome—is the ambiguous information content of the responses. For example, in the case of an event with a binary outcome (where the event either occurs or not), the modal outcome can be assigned a probability that ranges from 50 percent to 100 percent. Finally, a problem with questions asking for a point forecast—“what do you think will be” or “by how much do you expect”—is that it is not clear whether the estimate individuals provide represents a mean, mode, median, or something else. Research also suggests that some respondents provide a point estimate that reflects varying degrees of loss aversion rather than just a pure forecast.\n\nAmong the advantages of questions asking directly for the likelihood or “percent chance” of different outcomes are ease of interpretation, comparability across respondents, and the ability to measure subjective uncertainty. The consistency of a respondent’s answers can be checked using the laws of probability, and importantly, probabilistic questions permit elicitation of the quantitative measures of belief and uncertainty typically required in estimating economic models.\n\nResearchers can use a variety of methods to check the validity and information content of probabilistic subjective expectations data. First, one can look at response rates, reported ease and clarity of the questions used to elicit expectations, the use of focal points in responses, and the internal consistency of responses. Second, one can consider whether reported expectations vary with observable characteristics in a predictable way.9 Third, whenever possible and assuming a stationary environment, one can check for relevance of reported expectations, looking at whether they are meaningfully related to future or past realizations. Fourth, one can ask whether subjective expectations help predict actual behavior. Finally, one can look at whether agents update their reported expectations in sensible ways upon receipt of relevant information.\n\nBased on these criteria, a literature that has grown rapidly since the mid-1990s has demonstrated the validity and information content of responses to probabilistic questions about a broad spectrum of economic concepts. This evidence extends beyond the United States and Europe to developing countries.10Thus, probabilistic questions, with appropriate instructions and visual aid tools, have been successfully used not only in the Survey of Professional Forecasters and the U.S. Health and Retirement Survey (and its sister surveys in Europe and Japan) but also in surveys of fisherman and farmers in India, aspiring migrants in Tonga, poor households in rural Colombia, and junior high school students in Mexico.11 Some of these questions ask respondents to assign probabilities to various ranges of outcome—so-called density forecasts.12 Generally, people across a wide range of countries and walks of life appear to be able to answer probabilistic questions, and their expectations are found to be useful in forming predictions of future economic outcomes.\n\nThe Survey of Consumer Expectations\n\nTurning now to the development and use of expectations surveys at the New York Fed, let me first discuss our Survey of Consumer Expectations (SCE). Over the last ten years, we extensively tested and eventually implemented a new survey of households, with the goal of collecting timely and accurate information at high frequency on U.S. consumers’ expectations and decisions on a broad variety of topics. The SCE, launched in June 2013, was designed to fill gaps in existing data collections about consumer expectations and outcomes, to provide a more integrated data approach, and to take advantage of state-of-the-art survey techniques. The SCE is implemented as a monthly, nationally representative survey of about 1,300 household heads.13 Recently the Bank of Canada implemented its own version of the SCE, fielded at a quarterly frequency.\n\nThe SCE has various components, which are outlined on Slide 2. First, respondents are asked a core monthly module of questions concerning their expectations about various macroeconomic and household-level variables. These questions cover inflation expectations and expectations regarding changes in home prices and the prices of various specific goods and services, such as gasoline, food, rent, medical care, and a college education. The core survey also asks for expectations about unemployment, interest rates, the stock market, credit availability, taxes, and government debt. In addition, respondents are asked to report their expectations about several labor market outcomes, including changes in earnings, the perceived probability of losing their current job or leaving it voluntarily, and the perceived probability of finding a new job. Moreover, respondents are asked about the expected change in their household’s overall income and spending. As I will describe in more detail below, these expectations questions are fielded at various time horizons and with various formats, including both point forecasts as well as density forecasts based on probabilistic questions. The second component of the SCE contains a supplementary “ad hoc” module each month on special topics.14 Finally, SCE respondents also fill out longer surveys each quarter on various topics. These are up to thirty minutes in length and are separate from the monthly survey.15\n\nBefore giving more details about the SCE questions and some background on the research underpinning them, I would like to briefly point out two other important design elements of the SCE. First, responses are collected using an online survey tool. While it has become harder to obtain nationally representative samples with telephone and mail surveys, for which response rates have been falling, the opposite has been true for online surveys. Moreover, some evidence suggests that respondents are more likely to answer financial questions posed in online surveys. In a recent randomized survey experiment conducted by colleagues at the New York Fed and the Dutch central bank,16 the share of respondents reporting their inflation expectations in online interviews slightly exceeded the share reporting their expectations in face-to-face interviews.\n\nIn addition to being more cost effective, online surveys, when fielded at high frequency, provide important flexibility by allowing questions to be added or changed at short notice. It is thus possible to collect information on new economic and financial developments, such as the impact of recent changes in the price of oil. Other attractive features of online surveys are the ease of incorporating graphics and other visual refinements and the ease of conducting randomized interactive experiments to analyze the updating of expectations and the links between expectations and behavior.\n\nA second key design element of the SCE is that it uses a rotating-panel sample design, with approximately 1/12 of respondents rotating in each month, after which they stay in the panel for up to 12 months and then rotate out. In addition to keeping the sample representative over time, this rotational structure allows us to track the changes in individuals’ responses over time. In other words, we can compute changes in expectations over time after differencing out individual idiosyncratic effects. This design contrasts with repeated cross-sectional surveys in which an entirely new sample is drawn each month. The panel structure of our survey, with a more stable sample composition, therefore reduces volatility in summary statistics due to a changing sample and increases the signal-to-noise ratio. Finally, it is possible to use the panel to link short-term expectations to actual realizations, and measure individual-level forecast accuracy and its implications in real time.\n\nDesigning Survey Questions for Household Inflation Expectations\n\nThe launch of the SCE followed an extensive testing phase, dubbed the Household Inflation Expectations Project (HIEP), which was initiated in 2006 to explore the feasibility of implementing a new survey of consumer expectations with a focus on inflation expectations.17 The HIEP had several objectives: to improve existing measures of consumer expectations, to clarify the process of expectations formation and updating, and to study the links between reported expectations and actual behavior in a variety of realms. The HIEP set up a working group—composed of New York Fed and Federal Reserve System economists, academic economists, behavioral psychologists, and survey design experts—that devised, conducted, and analyzed a series of cognitive interviews and experimental surveys to explore various dimensions of the planned new survey.\n\nIn particular, the HIEP analyzed the information content of the inflation expectations questions in the University of Michigan Survey of Consumers, the primary source of information on household inflation expectations at the time; tested alternative wording of potential inflation expectations questions; studied the feasibility of eliciting individual uncertainty about future outcomes for inflation, house prices, and earnings growth; and introduced a panel dimension to the experimental data collection effort in order to study the persistence of inflation expectations and their responsiveness to inflation surprises.\n\nThe HIEP considered different potential time horizons for its inflation expectations questions. For the short-term expectations, we settled on the one-year-ahead horizon—similar to the Michigan survey. We also examined the feasibility of asking for inflation expectations over a longer time horizon. The Michigan survey asks respondents by what percent per year they “expect prices to go up or down on the average, during the next 5 to 10 years”.18 We replicated this question in our experimental surveys, and found that the Michigan question elicits a mixture of interpretations, with some respondents using a 10-year horizon and others thinking about a 5-year horizon.19\n\nFrom the results of our cognitive interviews and experimental surveys, we decided in the SCE to elicit medium-term inflation expectations at the one-year two-year-forward horizon. For example, in this month’s edition of the SCE we ask respondents what they expect the rate of inflation to be “over the 12‐month period between May 2018 and May 2019.” Our testing suggests that respondents understand this format better than that of the 5-10 year Michigan question; consumers are better able to provide their expectations over a specific time period in the future, and there is little value in asking them about a longer time horizon. In addition, we believe that the one-year two-year-forward horizon adopted in the SCE is better suited to measure inflation expectations at the medium-term horizon that matters most for central bankers, since monetary policy is expected to exert its full effect within that time frame.20\n\nWith regard to question wording, it is important to note that the Michigan Survey asks about changes in “prices in general.” The HIEP tested three alternative wordings of a potential inflation expectations question: we presented respondents with questions about the change in “prices in general” (the wording used in the Michigan survey), the change in “prices you pay,” and the “rate of inflation.” Compared with the “rate of inflation,” the phrases “prices in general” and especially “prices you pay” induced respondents to think more about their personal price experiences and to focus on specific price changes for individual items, such as food or gasoline.21 In related research, we have shown that respondents tend to express more extreme inflation expectations when they are prompted to think of specific prices in answering the question rather than overall inflation.22 In the SCE, we therefore ask directly for expectations about the rate of inflation or deflation. 23\n\nProbabilistic Questions on the SCE\n\nAs I noted earlier, a key feature of our survey is its use of probabilistic questions. Following the current practice in the literature for the elicitation of expectations about continuous variables, we tested questions that elicited density forecasts by asking respondents to assign the percent chance that the value of interest would fall within different pre-specified ranges or “bins.” For instance, for inflation, we ask for the subjective probability distribution over a range of possible future inflation outcomes. This is shown in Slide 3.24\n\nFor each respondent, we fit a flexible parametric probability density function (PDF) to their bin probabilities. We can then compute several moments of interest from the fitted density forecast for each individual respondent, including measures of central tendency such as the density mean or median, measures of uncertainty, such as the interquartile range, or IQR, and measures of skewness.25\n\nAs I already noted, there are several important advantages to eliciting a density rather than a simple point forecast. In a world characterized by pervasive uncertainty, density forecasts provide a comprehensive representation of respondents’ views about possible future outcomes for the variables of interest. Importantly, density forecasts enable us to compute the degree of uncertainty associated with each respondent’s estimate. The measurement of forecast uncertainty is important for policymakers in order to assess the extent to which inflation expectations remain well-anchored. In addition, by eliciting density forecasts, we can focus on a specific measure of central tendency and use that for cross-respondent comparison and aggregation purposes. Research indicates that by forcing respondents to consider the likelihood of different outcome ranges, density-based expectations measures, such as the mean, are less prone to outliers than are one-off answers to point expectations, resulting in smaller cross-sectional dispersion.26\n\nEliciting probabilistic beliefs and density forecasts presents a number of challenges: much research is still needed on the best way to elicit such beliefs—for example, with regard to the format of the questions; the number, width, and location of bins; whether to tailor bins to individual respondents; and how to estimate the underlying continuous densities. The elicitation of such beliefs involves many survey design choices, and the extent to which the findings depend on the way in which the information is collected is not yet fully known.\n\nIn line with earlier findings in the literature, our research shows that survey respondents are able and willing to provide density forecasts.27 In our experimental surveys, fielded as part of RAND’s American Life Panel, we observe negligible non- response rates to the bin probability questions. Further, when respondents are asked point forecast questions, some elect to provide a range, in order to express a degree of uncertainty in their forecast. We find that the use of ranges in point forecast questions, as well as the widths of reported ranges, are positively correlated with the number of bins to which respondents assign positive probability in density forecast questions, and with measures of uncertainty based on the fitted density estimates. Respondents also rate the density forecast questions only slightly harder to answer than the corresponding point forecast ones. Finally, respondents’ estimated forecast uncertainty is meaningfully associated with month-to-month revisions in inflation expectations: greater uncertainty is significantly correlated with larger revisions—qualitatively consistent with Bayesian updating.\n\nWhat is the best way to aggregate the information we elicit regarding probabilistic beliefs and density forecasts? For binary outcomes, such as the perceived percent chance of losing one’s job over the next three months, we report the mean probability across respondents. For density forecasts, we can compute several aggregate measures. Slide 4 shows a stylized depiction of how we approach this. As our aggregate measure of expected inflation, we report the median of individual density means. We pick the density mean, which represents an individual’s expectation for the rate of inflation, as our measure of central tendency, and use the median to aggregate across respondents, since the median is less sensitive to outliers. We can also look at medians of various percentiles of the individual distributions.28\n\nWe measure aggregate forecast uncertainty as the median across respondents of the individual IQRs. We also monitor the probability that respondents attach to extreme inflation outcomes by computing, for instance, the median probability of deflation: in other words, the median of the individual subjective probabilities assigned to outcomes with inflation less than zero. Finally, to characterize the variation in inflation expectations across respondents, we compute the dispersion of expected values: the IQR of individual density means. Again, we can look at other percentiles of the distribution across respondents to detect possible clustering and polarization.29 We publish at a monthly frequency many of the aggregate measures of inflation expectations, as well as measures for a broad range of labor market and financial expectations, on our survey website, shown on Slide 5. In addition to providing national figures, we show trends for different demographic subgroups.\n\nCurrent Trends and Additional Research\n\nSo far, we have discussed the elicitation of density forecasts for a single variable of interest. In addressing various policy questions, however, we may be interested in the subjective joint probability distributions for two or more variables. For instance, if we are interested in households’ expectations about real wage growth, we may want to elicit the joint distribution over future outcomes for both inflation and nominal wage growth. Measuring the joint distribution of expectations over income and spending growth may also be very useful.\n\nThis is an area at the frontier of current research, and more work is needed to test alternative ways to elicit joint subjective distributions. At the New York Fed, we have experimented with questions asking for density forecasts for one variable, conditional on ranges of outcomes for the other variable of interest. These conditional probability distributions, together with the outcome probabilities for the conditioning variable, can be used to recover the subjective joint distribution for each respondent. For instance, in the New York Fed’s Survey of Primary Dealers (SPD), we have fielded questions to elicit the joint probability distribution for the size of the Federal Reserve’s balance sheet and the state of the economy.\n\nWe might also be interested in exploring the cross-section of density forecasts across respondents, for example, demographically. As I noted earlier, the SCE is based on a representative sample of U.S. household heads, and the data highlight the rich heterogeneity of expectations held by consumers. For example, Slides 6 and 7 show the recent trends in median three-year-ahead inflation expectations overall and sorted by the education level of respondents. Slide 8 shows trends in inflation uncertainty, also by education. As is common in surveys of inflation expectations, more highly educated respondents typically report lower inflation expectations, and express less uncertainty in their density forecasts. We observe similar patterns when the responses are broken out by income, numeracy, or financial literacy. These differences could reflect heterogeneity in information sets, in individual experiences, or in the way people process information about inflation.30\n\nNext, I’ll share two experiments we conducted during the development phase of the SCE in order to analyze the validity and information content of our inflation expectations questions. In the first experiment, which was financially incentivized,31 survey respondents were first asked for their inflation expectations. They were then asked a series of ten questions offering a choice between two investments, one yielding a nominal return that varies with inflation, the other yielding an inflation-protected payoff with increasingly large amounts over the ten questions. When choosing between the two investments, the respondent seeking to maximize her payoff should switch investments at most once, and if so, should switch from the one yielding a nominal return to the inflation-protected investment sooner, the higher her inflation expectations.32\n\nWe found that the choices in the financially incentivized experiment were indeed meaningfully related to respondents’ inflation expectations, on average. Differences in reported uncertainty were also associated with different choices in the experiment, in the direction predicted by theory. Deviations from expected utility maximization were more likely to occur for respondents with low education, as well as those with low numerical and financial literacy. We were able to repeat the experiment with the same respondents six months later. We found that individual changes in expectations were significantly associated with changes in how early respondents switched between investments, and that not only the direction but also the magnitude of these changes was consistent with a simple expected utility maximization framework. To our knowledge, these results provide the first direct evidence regarding a meaningful link between survey-based inflation expectations and actual behavior.33\n\nIn the second experiment, we studied the formation process of inflation expectations by looking at whether and how survey respondents updated their inflation expectations when they received relevant new information. 34Specifically, we first elicited inflation expectations. We then randomly provided information to a subset of respondents on either past food price inflation, or professional economists’ forecasts of future inflation from the SPF. Finally, we re-elicited respondents’ inflation expectations, and studied whether they were correlated with the information content of the signal they received.35\n\nWe found that respondents, on average, updated their inflation expectations in response to the information provided, and they did so sensibly and in a direction consistent with Bayesian updating—with larger revisions for less informed respondents and for those with greater baseline uncertainty. Further, there was significant heterogeneity both in how fully informed respondents were about objective inflation measures and in their updating behavior. This finding points to the potential importance of allowing for heterogeneous information-processing rules in our economic models. These findings are also consistent with existing sticky-information models of expectations formation, since cross-sectional disagreement falls after the provision of information.36 Results from the experiment also indicate that expectations about changes in the “prices you pay,” like the similar Michigan survey question about “prices in general,” are more responsive to information about food prices than expectations about the rate of inflation. This finding is consistent with our observation that wording based on “prices” causes respondents to focus more on price changes in their own consumption basket and to report expectations that are more correlated with gas and food price changes.37\n\nFinally, we have been exploiting the rotating panel nature of the SCE to better understand the changes in medium-term consumer inflation expectations that has occurred since July 2015. As I mentioned earlier, the panel structure of the SCE enables us to analyze the changes at the individual level, that is, for the same respondent—thus abstracting from possible changes in expectations coming from changes in sample composition. In particular, we looked at the group of respondents who completed the survey both in September 2015 and in January 2016, and compared their three-year-ahead inflation expectations as measured by the individual density means across the two surveys.38 As shown in Slide 9, we found that the entire distribution of medium-term inflation expectations shifted to the left, indicating a widespread decline in expectations. Median expected inflation among these repeat respondents declined by 53 basis points, compared to 39 basis points without controlling for the changing sample composition.\n\nThe May results of the SCE showed an increase of 29 basis points from April in the median of the individual means (see Slide 6) and this can also be examined by holding the sample constant using the panel structure. In this case we find an increase of 27 basis points from April and the distribution shifting to the right, although the statistical significance of the change is more marginal than the decline over the longer period.39 As can be seen in Slide 9 there is a wide dispersion in views about future inflation held by consumers.40 This sort of analysis highlights the value of the panel nature of the SCE in yielding more robust measures of expectations.\n\nLet me turn next to the surveys of market professionals conducted by the New York Fed.\n\nProbabilistic Questions on the Surveys of Primary Dealers and Market Participants\n\nAhead of each meeting of the FOMC, the Trading Desk of the New York Fed asks a group of financial market participants, including both primary dealers and buy-side investors, to provide their views on a range of economic and financial indicators. The Desk has surveyed the primary dealers for over a decade, and since 2011 it has made the questions and results of the Survey of Primary Dealers (SPD) publicly available.41 Beginning in 2014, the Desk also launched the Survey of Market Participants (SMP), which solicits views from active investment decision makers, such as mutual, pension, and hedge fund managers as well as corporate treasurers.\n\nIn both the SPD and the SMP, many of the questions are tailored toward matters that are of policy relevance at the time of the survey. Further, we use the fact that the respondents are market professionals with a good understanding of finance and economics to ask direct and at times technically complex questions. In this regard, the surveys contrast with the SCE, which uses language that is accessible to respondents with no economic or financial training.42\n\nProbability versus Point Forecasts\n\nFor many policy relevant variables, both the SPD and the SMP elicit respondents’ views on the entire distribution of future outcomes, and not only on their modal projections.43 Slide 10 provides a striking example of why this is important. The figure shows the projections for the federal funds rate at the end of 2017 (top panels) and 2018 (bottom panels) at three different times: December 2015, January 2016, and March 2016.\n\nFor each panel, we show the point forecast (red vertical line) averaged across respondents in both the SPD and SMP. This point forecast likely corresponds to the modal path for the federal funds rate at the end of each year.44 The green lines depict the density forecasts, averaged across respondents, and the blue vertical lines show the mean projections implied by these density forecasts, which I will refer to as the “pdf-implied mean” in the remainder of this talk.45 Note that the average modal path is usually close to, but does not match, the average mode of the marginal distributions for the policy rate. To the extent that participants are responding about the mode of the joint distribution, as opposed to the mode of each marginal distribution, this would not be surprising.46\n\nThe striking feature of Slide 10 is the contrast between the steadiness of modal point forecasts and the dramatic changes in density forecasts. Modal forecasts for the policy rate in both year-end 2017 and 2018 barely moved from December to January, and edged slightly down in March. Had we just elicited point forecasts, we would have concluded that survey respondents’ views had not changed very much. In contrast, the pdf-implied mean forecast decreased substantially from December to January, and then increased slightly from January to March. More importantly, density forecasts shifted from being slightly skewed toward higher rates in December to being skewed toward lower rates in January. In fact, density forecasts became bimodal in January, and in March, the “low rate” mode became the dominant one for 2018.\n\nI should stress that the change in the density forecasts from December to January occurred following substantial changes in how the survey questions elicited respondents’ probability-weighted expectations. In December, and in many preceding surveys, both the SPD and SMP asked respondents for the unconditional probabilities that they assigned to various rate outcomes. In January, however, survey respondents were asked to assign probabilities to rate outcomes at the end of 2017 and 2018 conditional on two different scenarios: “returning to the zero lower bound (ZLB) at some point in 2016-2018,” or “not returning to the ZLB at any point during 2016-2018.” Respondents were also asked for their subjective view of the probability of each scenario, a topic that I will discuss later. We changed the survey both because we thought this was a more effective approach to elicit unconditional distributions, and because we were interested in the scenario probabilities. Note that from January to March the survey question did not change, but we added an explicit “negative rate” bin.\n\nIn addition to eliciting density forecasts conditional on various scenarios, the surveys have sometimes asked for joint probability distributions for different variables. This is helpful in assessing how market participants expect the FOMC to respond to various economic scenarios. In June 2013, for instance, the SPD asked for the probability distribution for the size of the Federal Reserve’s securities holdings— which we call the System Open Market Account, or SOMA, portfolio—at the end of 2014, conditional on three mutually exclusive outcomes for the unemployment rate at the end of 2013: less than 7.3 percent, between 7.3 and 7.5 percent, and greater than 7.5 percent.47 For reference, the SOMA portfolio was about $3.2 trillion in June of 2013. Clearly, the idea behind this question is to see how market participants expect the FOMC’s asset purchase programs to evolve in response to new data about the labor market.\n\nSlide 11 shows the three conditional distributions for the projected change in the size of the portfolio relative to the June 2013 level, as well as the unconditional distribution, formed by using the probabilities of the unemployment scenarios. The results show that respondents thought that monetary policy decisions would be strongly dependent on labor market outcomes, a result that is in line with the September 2012 FOMC statement. 48\n\nThe probability of the SOMA portfolio increasing by more than $800 billion to over $4 trillion moved from about 30 percent conditional on unemployment being less than 7.3 percent to about 55 percent conditional on unemployment being greater than 7.5 percent. The probability of the SOMA increasing by more than $1.3 trillion quadrupled between these two scenarios, from about 5 percent to 20 percent. In the end, unemployment was 6.7 percent by the end of 2013, and the year-end 2014 level of the SOMA portfolio reached $4.25 trillion: a $1.1 trillion change relative to June 2013. If we interpolate the bins using a uniform distribution, then this outcome is around the 81st percentile of the distribution conditional on the good labor market outcome and in the 75th percentile of the unconditional distribution.\n\nIf we were to judge on the basis of our survey of primary dealers, the outcome of the asset purchase program was on the high side but not surprisingly so. However, we don’t know how other market participants assessed the implications for the size of asset purchases from the September 2012 FOMC statement, particularly going into the June 2013 FOMC meeting. Many may recall that the June FOMC press conference continued something called the “taper tantrum.” As former Chairman Ben Bernanke writes in his memoir, “What explained markets’ strong reaction and why did it surprise us? In retrospect, I think our view of market expectations was too dependent on our survey of securities dealers.”49 Importantly, it is possible that the size of future asset purchases was seen as smaller by primary dealer respondents than by the marginal investor.50 Following this experience, we introduced the Survey of Market Participants to better capture the diversity of views in the market. In reaction to the taper tantrum, former Governor Jeremy Stein examined the large effects that can be produced if beliefs are dispersed and if ex-ante marginal investors receive information that leads them to strongly revise their views about how much risk to take. The outcome could be that the new marginal investor driving market prices has a very different set of beliefs and risk appetite.\n\nMarket Rates and Probability Forecasts\n\nIt is well understood in theory why the market-implied path of the policy rate usually differs from survey measures of policy expectations.51 Even if the marginal investor’s probability distribution for future outcomes coincided with that of the average survey respondent, she would want additional compensation for bearing risk in states of the world that are unfavorable to her. It is conceivable in the current environment that outcomes in which the federal funds rate would return to the zero lower bound, or more generally remain low, are not good news for the overall portfolio of the marginal investor. Thus, in assessing the price at which she would be willing to trade interest rate derivatives like federal funds futures, she would weight those “negative” states of the world, such as states involving a return to the zero lower bound, more than her subjective probability distribution would imply—and the opposite would hold true for “positive” states of the world. In other words, in this environment the risk premium compensation she requires would pull the market-implied path downward, away from survey expectations.\n\nWhile risk premia, especially at this juncture, are likely very important, looking only at point forecasts may overemphasize their role when these point forecasts coincide with modal forecasts. In particular, when the implied policy rate from federal funds or Eurodollar futures changes, while the point forecasts from surveys do not, it is customary to attribute all the change in the market rates to risk premia.\n\nThe top two panels of Slide 12 compare the market-implied federal funds path for year-end 2016, 2017, and 2018 with the surveyed modal path projections, averaged across respondents.52 It is clear that the modal forecasts are much higher than market-implied rates, especially for 2017 and 2018, when the gap is larger than 100 basis points. Moreover, market-implied rates fell from December to January, while modal forecasts moved little between December and January, and fell slightly in March.\n\nThe bottom left panel of Slide 12 shows the risk premium derived from a standard econometric model.53 These are negative, which is consistent with the gap between market implied rates and survey expectations. However, the premium at short maturities is about as large, if not larger, than that at long maturities—a finding that is inconsistent with the fact that the gap at short maturities is a lot smaller. Moreover, the estimated premium becomes less negative from December to January, implying that market-implied rates should, ceteris paribus, have increased, while in fact they fell.\n\nChanges in the probability distribution are, however, consistent with the changes in market-implied rates. The bottom right hand panel shows the pdf-implied means for the three different horizons, two of which were shown before in Slide 10. These are also higher than market-implied rates, but are closer than the point forecasts, especially in January. Most importantly, they fall significantly from December to January, in line with the change in market rates. Pdf-implied means changed little between January and March. However, as we know from Slide 10, this masks substantial changes in the underlying density forecasts, with the lower rate mode becoming more prominent.\n\nFinance models make a variety of assumptions that allows them to extract risk premia from asset prices. Once that is done, they can obtain the underlying so-called “physical” probability distribution of the marginal investor—that is, the actual probability she assigns to various outcomes. Brodsky et al. propose a complementary approach, called “tilting,” which is, in a way, the reverse of the finance approach.54 It starts from the survey probability distribution and asks: How much do I have to “tilt” the original distribution—that is, how much probability mass do I have to shift around—to obtain the market-implied path as its mean?\n\nThere are many ways to tilt or alter the survey distribution such that its mean is the market rate. Their approach is to require that the tilted distribution is as close as possible to the original survey distribution, where the notion of distance is one that is commonly used in statistics, and is based on a measure called relative entropy or Kullback-Leibler information criterion (KLIC). This relative entropy approach is nothing new. It was introduced to the economic forecasting literature by Robertson, Tallman, and Whiteman (2005), and used more recently by Altavilla, Giacomini, and Costantini (2014), among others.\n\nSo, what do the tilted survey distributions look like, and how large are the discrepancies with the original probability distributions? Slide 13 shows the results for year-end 2018 using two survey dates: December, shown in the left panel, and January, in the right. For each panel, the blue line shows the probability distribution averaged across respondents. The vertical dashed blue and red lines show the survey-implied mean and the market-implied value for the policy rate, respectively. The red line shows the tilted distribution.55 We see that the amount of tilting required for the January survey is very small in comparison to that needed for the December survey. For December, the tilted distributions are very different from the original distributions, with the probabilities in the lowest bin roughly tripling.\n\nHow do we interpret the difference between the original survey distribution and the tilted distribution? If we assume that the marginal investor and the average survey respondent share the same probability distribution, this difference loosely captures the additional premium or discount associated with future interest rate outcomes. Under this interpretation, the cost of insuring against low policy rate outcomes fell from December to January, because less tilting is required in January than in December, a conclusion consistent with the change in estimated model-based risk premia. \n\nAlternatively, the difference might reflect the disparity between the subjective probability distributions of the marginal investor and the average survey respondent—and therefore measure the dispersion in beliefs between the two. Under this interpretation, the marginal investor in the futures market was much more pessimistic—in other words, placed more probability on low rate outcomes—than the average survey respondent in December, while by January the average survey respondent revised her beliefs downward. Note that the marginal investor in the futures market likely changes over time, and this change can be an additional source of fluctuations in the gap between market-implied rates and mean projections. \n\nUnfortunately, we have no direct measure of the beliefs of the marginal investor. But we can obtain evidence on whether the dispersion in beliefs fell from December to March by looking at the dispersion of beliefs among survey respondents. One measure of such dispersion in beliefs is the extent to which the KLIC varies across respondents. A high (low) KLIC means that the respondent’s forecast distribution requires substantial (little) tilting to deliver the market-implied rate. The left panel of Slide 14 shows, in the bars outlined in light blue, the KLIC distribution across respondents for the 2018 projections in December, and in the dark blue bars, the distribution for the 2018 projections in January. It is clear that dispersion in beliefs fell dramatically between the two surveys. The right panel shows that dispersion increased slightly in March but remains below the December levels. \n\nBelief Heterogeneity\n\nSlide 14 already illustrates that forecasts are very different across survey respondents. I’ll now provide further evidence of how pervasive and important heterogeneity in beliefs is.\n\nRecall that in both the recent March and January surveys, respondents were asked about their subjective probability of “returning to the ZLB at some point in 2016-2018.” Slide 15 plots the distribution across respondents of this probability. It shows that respondents belonged to three groups, roughly speaking: a few who believed this probability to be low (less than 15 percent of respondents), a majority who thought the probability was about one in four (between 20 and 35 percent of respondents), and a sizable minority who assessed this probability as fairly large (more than 40 percent of respondents).\n\nThis heterogeneity in the probability assigned to a return to the ZLB translates of course into heterogeneity in the probability distribution for the federal funds rate. The left panel of Slide 16 shows the smoothed density forecasts for the year-end 2017 federal funds rate for all March 2016 survey respondents. The density shown in the bottom left panel of Slide 10 is simply the average of the densities shown in Slide 16. Two features stand out. First, respondents display very different views on future policy rates, consistent with the evidence in Slide 15. Second, the bimodality of the distribution shown earlier in Slide 10 is not simply the result of aggregation, but is a feature of many individual density forecasts. In fact, the majority of respondents appear to have a “low rate” mode, which mostly lies in the 0 to ¼ percent range, and a “higher rate” mode, which varies substantially across participants and ranges from about 1 to 3 percent.\n\nThe right panel of Slide 16 shows the cross-sectional distribution of the modal path projections. The cross-sectional distribution of point forecasts gives the misleading impression that there is broad agreement among respondents on the policy rate at year-end 2017, when in fact this is not the case, as we can see from the left panel. This misleading impression arises from the fact that when interpreting the modal projections one often assumes that the density is unimodal—an assumption that is clearly violated here.\n\nConclusion\n\nIn conclusion, I would like to re-emphasize three points. Survey respondents can answer probabilistic questions. Probabilistic questions produce measures of subjective expectations that are superior to point forecasts or most likely outcomes. Heterogeneity of beliefs is pervasive and can have important implications for policymakers. Thank you for your attention.\n\nSlides\n\nReferences\n\nAdrian, Tobias, Richard K. Crump, and Emanuel Moench. 2013. \"Pricing the Term Structure with Linear Regressions.\" Journal of Financial Economics 110, no. 1: 110-38.\n\nAltavilla, Carlo, Raffaella Giacomini, and Riccardo Costantini. 2014. \"Bond Returns and Market Expectations.\" Journal of Financial Econometrics 12, no. 4: 708-29.\n\nArmantier, Olivier, Wändi Bruine de Bruin, Simon Potter, Giorgio Topa, Wilbert van der Klaauw, and Basit Zafar. 2013. “Measuring Inflation Expectations.” Annual Review of Economics 5: 273-301.\n\nArmantier, Olivier, Wändi Bruine de Bruin, Giorgio Topa, Wilbert van der Klaauw, and Basit Zafar. 2015. “Inflation Expectations and Behavior: Do Survey Respondents Act on Their Beliefs?” International Economic Review 56, no. 2 (May): 505-36.\n\nArmantier, Olivier, Wilbert van der Klaauw, Scott Nelson, Giorgio Topa, and Basit Zafar. Forthcoming. “The Price Is Right: Updating of Inflation Expectations in a Randomized Price Information Experiment.” Review of Economics and Statistics.\n\nArmantier, Olivier, Wilbert van der Klaauw, Giorgio Topa, and Basit Zafar. 2016. “Who Is Driving the Recent Decline in Consumer Inflation Expectations?” Federal Reserve Bank of New York Liberty Street Economics (blog), January 25.\n\nAttanasio, Orazio. 2009. “Expectations and Perceptions in Developing Countries: Their Measurement and Their Use.\" American Economic Review Papers and Proceedings 99, no. 2: 87-92.\n\nAttanasio, Orazio, and Katja Kaufmann. 2009. “Educational Choices, Subjective Expectations, and Credit Constraints.” NBER Working Paper no. 15087, July.\n\nAttanasio, Orazio, Costas Meghir, and Marcos Vera-Hernández. 2005. “Elicitation, Validation, and Use of Probability Distributions of Future Income in Developing Countries.” Paper prepared for the 2005 Econometric Society Meeting.\n\nBernanke, Ben S. 2015. The Courage to Act: A Memoir of a Crisis and Its Aftermath. New York: W. W. Norton and Company.\n\nBrodsky, Bonni, Marco Del Negro, Joseph Fiorica, Eric LeSueur, Ari Morse, and Anthony Rodrigues. 2016a. “How Do Survey- and Market-Based Expectations of the Policy Rate Differ?” Federal Reserve Bank of New York Liberty Street Economics (blog), April 7.\n\nBrodsky, Bonni, Marco Del Negro, Joseph Fiorica, Eric LeSueur, Ari Morse, and Anthony Rodrigues. 2016b. “Reconciling Survey- and Market-Based Expectations for the Policy Rate.” Federal Reserve Bank of New York Liberty Street Economics (blog), April 8.\n\nBruine de Bruin, Wändi, Wilbert van der Klaauw, and Giorgio Topa. 2011a. “Expectations of Inflation: The Biasing Effect of Thoughts about Specific Prices.” Journal of Economic Psychology 32, no. 5 (October): 834-45.\n\nBruine de Bruin, Wändi, Charles F. Manski, Giorgio Topa, and Wilbert van der Klaauw. 2011b. “Measuring Consumer Uncertainty about Future Inflation.”Journal of Applied Econometrics 26, no. 3 (April-May): 454-78.\n\nBruine de Bruin, Wändi, Wilbert van der Klaauw, Maarten van Rooij, Federica Teppa, Klaas de Vos. 2016. ”Measuring Expectations of Inflation: Effects of Survey Mode, Wording, and Opportunities to Revise.” De Nederlandsche Bank working paper no. 506.\n\nBruine de Bruin, Wändi, Wilbert van der Klaauw, Giorgio Topa, Julie S. Downs, Baruch Fischhoff, and Olivier Armantier. 2012. \"The Effect of Question Wording on Consumers’ Reported Inflation Expectations.\" Journal of Economic Psychology 33, no. 4: 749-57.\n\nCarroll, Christopher. 2003. “Macroeconomic Expectations of Households and Professional Forecasters.” Quarterly Journal of Economics 118, no. 1: 269-98.\n\nCorreia-Golay, Ellen, Steven Friedman, and Michael McMorrow. 2013. \"Understanding the New York Fed's Survey of Primary Dealers.\" Federal Reserve Bank of New York Current Issues in Economics and Finance 19, no. 6: 1-8.\n\nCrump, Richard, Emanuel Moench, William O'Boyle, Matthew Raskin, Carlo Rosa, and Lisa Stowe. 2014. “Survey Measures of Expectations for the Policy Rate.” Federal Reserve Bank of New York Liberty Street Economics (blog), December 5.\n\nDelavande, Adeline, Jinkook Lee, and Joanne K. Yoong. 2012. Harmonization of Cross-National Studies of Aging to the Health and Retirement Study: Expectations. Santa Monica, Calif.: RAND Corporation.\n\nDelavande, Adeline, Xavier Giné, and David McKenzie. 2011a. “Measuring Subjective Expectations in Developing Countries: A Critical Review and New Evidence.” Journal of Development Economics 94, no. 2 (March): 151-63.\n\nDelavande, Adeline, Xavier Giné, and David McKenzie. 2011b. “Eliciting Probabilistic Expectations with Visual Aids in Developing Countries: How Sensitive Are Answers to Variations in Elicitation Design?” Journal of Applied Econometrics 26, no. 3 (April-May): 479-97.\n\nEngelberg, Joseph, Charles Manski, and Jared Williams. 2009. “Comparing the Point Predictions and Subjective Probability Distributions of Professional Forecasters.” Journal of Business and Economic Statistics 27, no. 1: 30-41.\n\nGiné, Xavier, and Stefan Klonner. 2007. “Technology Adoption with Uncertain Profits: The Case of Fibre Boats in South India.” Mimeo, World Bank.\n\nGuiso, Luigi, Tullio Jappelli, and Daniele Terlizzese. 1994. \"Earnings Uncertainty and Precautionary Saving.\" In Albert Ando, Luigi Guiso, and Ignazio Visco, eds, Saving and the Accumulation of Wealth: Essays on Italian Household and Government Saving Behavior. Cambridge: Cambridge University Press.\n\nGuiso, Luigi, and Giuseppe Parigi. 1999. “Investment and Demand Uncertainty.” Quarterly Journal of Economics 114, no. 1 (February): 185-227.\n\nHurd, Michael. 2009. “Subjective Probabilities in Household Surveys.” Annual Review of Economics 1 (September): 543-64.\n\nLeiser, David, and Shelly Drori. 2005. “Naıve Understanding of Inﬂation.” Journal of Socio-Economics 34, no. 2: 179-98.\n\nLikert, Rensis. 1932. “A Technique for the Measurement of Attitudes.” Archives of Psychology 22, no. 140: 1-55.\nMahajan, Aprajit, Alessandro Tarozzi, Joanne Yoong, and Brian Blackburn. 2008. “Bednets, Information, and Malaria in Orissa.” Mimeo, Stanford University.\n\nMankiw, Gregory, and Ricardo Reis. 2002. “Sticky Information Versus Sticky Prices: A Proposal to Replace the New Keynesian Phillips Curve.” Quarterly Journal of Economics 117, no. 4: 1295-328.\n\nManski, Charles. 2002. “Identification of Decision Rules in Experiments on Simple Games of Proposal and Response.” European Economic Review 46, no. 4-5: 880-91.\n\nManski, Charles. 2004. “Measuring Expectations.”Econometrica 72, no. 5 (September): 1329-76.\n\nMcKenzie, David, John Gibson, and Steven Stillman. 2013. “A Land of Milk and Honey with Streets Paved with Gold: Do Emigrants Have Over-Optimistic Expectations about Incomes Abroad?” Journal of Development Economics 102: 116-27.\n\nPotter, Simon. 2011. “Improving Survey Measures of Inflation Expectations.” March 30, 2011, speech at the Forecasters Club of New York.\n\nPotter, Simon. 2012. “Improving the Measurement of Inflation Expectations.” June 7, 2012, speech at the Barclays 16th Global Inflation-Linked Conference, New York.\n\nRobertson, John C., Ellis W. Tallman, and Charles H. Whiteman. 2005. \"Forecasting Using Relative Entropy.\" Journal of Money, Credit, and Banking 37, no. 3: 383-401.\n\nStein, Jeremy C. 2013. \"Yield-Oriented Investors and the Monetary Transmission Mechanism.\" Proceedings of the symposium Banking, Liquidity, and Monetary Policy.\n\nSvenson, Ola, and Goran Nilsson. 1986. “Mental Economics: Subjective Representations of Factors Related to Expected Inflation.” Journal of Economic Psychology 7: 327-49.\n\nVan der Klaauw, Wilbert, Wändi Bruine de Bruin, Giorgio Topa, Simon Potter, and Michael Bryan. 2008. “Rethinking the Measurement of Household Inflation Expectations: Preliminary Findings.” Federal Reserve Bank of New York Staff Reports, no. 359, December.\n\nWilliamson, Maureen R., and Alexander J. Wearing. 1996. “Lay People’s Cognitive Models of the Economy.” Journal of Economic Psychology 17, no. 1 (February):3-38.\n\n1 I would like to thank Marco Del Negro, Giorgio Topa, and Wilbert van der Klaauw for their excellent assistance in the preparation of these remarks, Luis Armona, Daniele Caratelli and Joseph Fiorica for able research assistance, and colleagues in the Federal Reserve System for their insightful comments and suggestions.\n\n2 Guiso, Jappelli, and Terlizzese (1994).\n\n3 Guiso and Parigi (1999).\n\n4 Manski (2002).\n\n5 Data on subjective expectations can also be used to improve the efficiency of model parameter estimates, refine posterior estimates of agents’ unobserved types, and help researchers estimate latent variables in macroeconomic models.\n\n6 Of course it is also possible that survey respondents have beliefs that do not satisfy these conditions. I will discuss some evidence on this.\n\n7 See Manski (2004), Hurd (2009), Attanasio (2009), and Delavande, Giné, and McKenzie (2011a, 2011b) for recent overviews.\n\n8 Likert (1932).\n\n9 For instance, researchers can consider whether mortality expectations vary in expected ways with age, education, and risky behaviors.\n\n10 See Delavande, Giné, and McKenzie (2011a) for a comprehensive review.\n\n11 See Mahajan et al. (2008), Giné and Klonner (2007), McKenzie, Gibson, and Stillman (2013), Attanasio, Meghir, and Vera-Hernández (2005), Attanasio and Kaufmann (2009), and Delavande, Lee, and Yoong (2012).\n\n12 More precisely, respondents are asked to assign probabilities to a set of mutually exclusive and exhaustive ranges of continuous outcomes.\n\n13 The SCE survey instrument is fielded by the Demand Institute, a nonprofit organization jointly operated by the Conference Board and Nielsen.\n\n14 Three such modules are repeated every four months, leaving three “floating” supplements per year on topics that are determined as the need arises. The three repeating supplements cover credit access, job search and retirement, and spending. Topics addressed so far in the “floating” supplement include the Affordable Care Act, savings from lower gas prices, student loans, family leave, and use of insurance products. Together, the core monthly module and the monthly supplement take about fifteen minutes.\n\n15 Most of these surveys are repeated at a yearly frequency. The SCE currently contains quarterly surveys on the housing market; the labor market; informal work participation; and consumption, saving, and assets. A subset of these surveys is wholly or partially designed by other Federal Reserve Banks.\n\n16 Bruine de Bruin et al. (2016).\n\n17 See van der Klaauw et al. (2008) and Armantier et al. (2013) for overviews.\n\n18 The Michigan survey’s precise question format is as follows: First, respondents receive the question “What about the outlook for prices over the next 5 to 10 years? Do you think prices in general will be higher, about the same, or lower, 5 to 10 years from now?” Those who respond “stay the same” are then asked whether they mean that prices will go up at the same rate as now, or that prices in general will not go up during the next 5 to 10 years. Those who indicate that they mean prices will go up at the same rate are then given the same follow-up questions as those who answer that they believe prices will be higher 5 to 10 years from now. Respondents who answer that they expect prices to be higher [lower] 5 to 10 years from now receive the question “By about what percent per year do you expect prices to go up [down] on the average, during the next 5 to 10 years?” Only respondents who give a response over 5 percent are then asked the clarifying follow-up question “Would that be [x] percent per year, or is that the total for prices over the next 5 to 10 years?” Respondents who answer “total” are then asked for a “per year” amount.\n\n19 Further, the clarifying question used in the Michigan survey that asks whether respondents meant their response to reflect price changes per year or over the entire time period induced significant revisions. The follow-up question is administered only to respondents who give expectations over 5 percent, thus failing to correct misinterpretations among those who gave lower responses. In our experimental surveys, we instead administered the follow-up to everyone. We found that if we administered the follow-up question only to those giving responses over 5 percent, then the median long-term expectation would be 4.3 percent instead of 3.7 percent in our test sample. Thus, we have reasons to believe that the 5 percent follow-up question in the Michigan Survey leads to a systematic overestimation of actual average responses.\n\n20 In contrast, for the Survey of Primary Dealers discussed later, we added a question in 2007 that directly elicited uncertainty over CPI inflation from 5 to 10 years ahead. See Potter (2011, 2012).\n\n21 In analyzing individual changes in expectations (from one month to the next) expressed by our SCE respondents, we find that changes in inflation expectations are little correlated with changes in expectations about future gasoline price changes.\n\n22 Bruine de Bruin et al. (2011a) carried out two studies to see whether individuals who think about specific price changes in forming their inflation expectations report more extreme and dispersed expectations because they focus on more extreme specific price changes. In the first study, the researchers show that those who are asked to report any or the largest individual price change tend to recall more extreme ones than those who are asked to report the average price change, and subsequently report more extreme inflation expectations. In the second study, the researchers show that among those who are asked about inflation expectations without first being asked about individual price changes, about half nonetheless think about individual items. Those who do so then report more extreme and dispersed inflation expectations.\n\n23 In our open-ended cognitive interviews and experimental surveys, we found that respondents generally are familiar with the term “inflation” and have a good understanding of the concept of inflation (Bruine de Bruin et al. 2011a, 2012). Other studies have found that members of the general public are familiar with the term “inflation” and have a basic understanding of what it means (Leiser and Drori 2005; Svenson and Nilsson 1986; Williamson and Wearing 1996).\n\n24 An advantage of eliciting density forecasts in this way—as opposed, for example, to eliciting values of the cumulative distribution function—is that answers are less likely to violate the laws of probability, such as monotonicity of the distribution function. A simple instruction and visual tool showing a running total as probabilities are assigned to bins leads to a negligible number of cases in which probabilities do not add up to 100 percent.\n\n25 Following the approach described in Engelberg, Manski, and Williams (2009), we fit a generalized beta distribution to the bin responses of each individual respondent whenever the respondent assigns positive probability to three or more bins. When respondents only assign positive probability to one or two bins, we fit a uniform or a triangular distribution, respectively.\n\n26 Delavande et al. (2011b).\n\n27 See Bruine de Bruin et al. (2011b).\n\n28 By tracking the same individuals over time, we can in principle study the extent to which respondents update their expectations in response to new information. For instance, if respondents receive information that deviates from their priors, their density forecasts could initially exhibit greater uncertainty and even become bimodal until they converge to a new value for their central tendency.\n\n29 For example, in the immediate aftermath of the financial crisis, we detected a certain amount of polarization in the distribution of inflation expectations, with some consumers expecting relatively high inflation, and some expecting deflation.\n\n30 The observed heterogeneity of expectations is important for policy purposes, since we find that agents with different characteristics act on their inflation beliefs and update their expectations in distinct ways.\n\n31 Armantier et al. (2015).\n\n32 This is an agent’s optimal choice under expected payoff maximization. We also show that the respondent should switch sooner to the inflation-protected investment, the higher her risk aversion and the more uncertainty she expresses in her inflation expectations.\n\n33 This financially incentivized experiment provides a joint test of our question’s success in measuring a respondent’s inflation expectations, and of whether consumers act on their expectations according to theory.\n\n34 Armantier et al. (forthcoming).\n\n35 Before providing the information, we also asked respondents for their expectations about the information provided in each treatment (e.g., what they thought past food price inflation had been), in order to control for respondents’ priors about the information that they would receive.\n\n36 See, for instance, Mankiw and Reis (2002) or Carroll (2003).\n\n37 In our research, we have also been studying the extent to which subjective expectations in the SCE are associated with individual outcomes. For instance, for a given individual, the perceived probability of making large purchases over the next 4 months (home appliances, electronics, furniture, home repairs, improvements or renovations, autos or other vehicles) is highly correlated with actual purchases of those items 4 months later. Household spending growth expectations (over the next 12 months) are also significantly associated with self-reported actual spending growth 12 months later. In the labor market realm, the perceived probability of finding a job over the next 3 months accurately predicts actual transitions from unemployment to employment, in those 3 months. And the expected arrival rate of job offers is predictive of actual job offers over a 4-month horizon. So the evidence suggests that the subjective expectations we measure are indeed informative of future events, with individuals more likely to experience an event typically reporting a higher expectation of the event occurring.\n\n38 See Armantier et al. (2016).\n\n39 Using a standard test of the difference between two distributions, the change from September to January is significant at the 1 percent level. The change from April to May has a “p-value” of 16 percent.\n\n40 This dispersion implies that random samples of small size (less than one thousand) will exhibit variability even if the underlying population distribution of inflation expectations remains constant.\n\n41 Correia-Golay, Friedman, and McMorrow (2013) provide an introduction to the SPD.\n\n42 Many surveys of market participants do not use their technical sophistication to the extent necessary to produce robust measurement of expectations. For example, currently many surveys ask market participants to identify the FOMC meeting at which the next increase in the fed funds target range will most likely occur. This question implicitly assumes that the next rate change will be an increase; moreover, eliciting expectations about the most likely meeting when the number of future meetings is large is particularly uninformative. For example, there are five FOMC meetings remaining this year, so the “most likely” meeting could be one with 21 percent probability. In the SPD and SMP, we are careful to elicit the probabilities of the full range of possible outcomes. For example, the current mean probability that the next change in the fed funds target will be an increase is around two-thirds.\n\n43 This is the case for some of the variables of interest, such as the level of the policy rate or, in the past, the projected level of the balance sheet.\n\n44 As noted earlier, questions that elicit a “most likely” value, or mode, can be difficult to interpret. This is particularly true of questions about the path of a variable, for example, the federal funds rate. If respondents are asked for their modal path, then technically, they should give the mode of the joint distribution. For example, the “mode” for policy rate projections could be the mode across all paths that the federal funds rate could follow from 2016 to 2018. It is an open question whether survey participants respond in this way, or respond with the mode of each marginal distribution.\n\n45 A number of explanations are in order. The survey asks for probabilities (marginal distributions) associated with bins for the federal funds rate. The green line is the density implied by these probabilities, assuming a uniform distribution within each bin, and assuming that the end bins are truncated (since this is the policy rate, truncation of the lower open bin is quite natural; less so for the higher open bin, but very little probability is placed there by the average distribution). For visual reasons, the density forecast is plotted by connecting the mid-points of each bin. The mean is that implied by this density forecast.\n\n46 In addition, even if participants were reporting the mode of the marginal distribution, the mode of the average distribution does not necessarily match the average of the modes, as is well understood. Of course, there is always the possibility that participants may not provide consistent answers when asked about the modal forecast and the entire distribution.\n\n47 The specific question was: “Please indicate the percent chance you attach to the dollar level of the SOMA portfolio falling within the following ranges at year-end 2014 for each of three hypothetical unemployment rate scenarios.” See the June 2013 SPD survey.\n\n48 “If the outlook for the labor market does not improve substantially, the Committee will continue its purchases of agency mortgage-backed securities, undertake additional asset purchases, and employ its other policy tools as appropriate until such improvement is achieved in a context of price stability.” \n\n49 Bernanke (2015, 549).\n\n50 To quote Bernanke again, “In effect our PhD economists surveyed their PhD economists. It was a little like looking in a mirror. It didn’t tell us what rank-and-file traders were thinking”.\n\n51 See e.g. Crump et al. (2014), and more recently Brodsky et al. (2016a).\n\n52 Market-implied rates were derived from futures prices at the same time as responses were received from the surveys.\n\n53 Adrian, Crump, and Moench (2013).\n\n54 Brodsky et al. (2016b).\n\n55 Note that this distribution is constructed by tilting the average of respondents' probability distributions. Alternatively, we could have tilted the probability distribution for each respondent and then averaged across these tilted distributions. While the two approaches do not, in general, deliver the same answer, in this case they almost do."
    },
    {
        "title": "Opening Remarks at the Economic Press Briefing on the U.S. Economy in a Snapshot",
        "date": "May 19, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud160519",
        "content": "Welcome to our Economic Press Briefing.  Today we are going to introduce you to a data product that we have been making available to the public, U.S. Economy in a Snapshot.  My role is to put this product in the broader context of the Federal Open Market Committee’s (FOMC) communication strategy about the economy and policy.  My colleague Robert Rich will then walk you through the structure of the product and discuss excerpts from the May release.  As always, what I have to say reflects my own views and not necessarily those of the Federal Open Market Committee or the Federal Reserve System.1\n\nConventional U.S. monetary policy is conducted by targeting the level of the federal funds rate—an overnight interest rate on bank reserves.  Few participants in our economy have any direct interaction with this interest rate.  How, then, is controlling this interest rate such an important part of setting monetary policy and steering an 18 trillion dollar economy toward the Federal Reserve’s dual mandate objectives of maximum sustainable employment and price stability?\n\nThis is the remarkable aspect of monetary policy.  Just as a pebble dropped into a lake disturbs the water not just at the point of impact but ripples outward far from the origin, changes in the federal funds rate influence the prices of financial assets more broadly and this, in turn, affects the broader economy.  An important insight of modern monetary theory is that this transmission of monetary policy to the broader economy through its many channels works best when the central bank is transparent about its goals, policy strategy and approach to implementing its strategy.  In this case, market participants and households both understand and can anticipate actions by the central bank.  By doing so, the transmission channels of monetary policy are enhanced.  This places considerable importance on effective central bank communication.\n\nRecognizing this, the FOMC has undertaken many initiatives over the years to improve its communications with the public.  These include providing clarification in terms of the goals of monetary policy, discussions of the FOMC’s broad policy strategy, as well as FOMC participants’ ongoing explanations of their views on monetary policy.  As part of this effort, I provide a comprehensive update on my outlook for the economy and monetary policy several times during the year.  Once a year we publish our staff’s judgmental economic forecast.  We have also provided in the Liberty Street Economics blog discussions of an internal staff model-based economic forecast.  In addition, we now regularly release our real-time forecast of “current quarter” real GDP growth.  All of these help the public to understand our assessment of economic conditions through the lenses of these different types of forecasts.\n\nWhen asked about the trajectory for the monetary policy stance, I always point out that it is data dependent.  The FOMC calibrates the stance of monetary policy to best achieve our twin objectives of price stability and maximum sustainable employment, taking into account our forecast for how the economy is evolving.  This forecast reflects the ongoing flow of the data.  Data releases that are close to our expectations have little additional impact on the forecast, while data releases that deviate significantly from our expectations can lead to more significant revisions of the forecast.  It is, therefore, important for market participants and households to be able to follow the data along with the FOMC and to understand how we are likely to interpret and react to incoming data.\n\nWhile market analysts readily have access to economic and financial data, this access is not widely available to everyone.  To help fill this information gap, for the past year we have been making available on our website a monthly publication called U.S. Economy in a Snapshot.  This publication contains a set of key data charts as well as commentary by our staff economists.  The purpose is to provide interested readers easy access to clear and concise presentations of important economic information.  The associated commentary provides some pertinent observations one might draw from the data.\n\nOur goal is to provide information that helps households and businesses follow the data along with the Fed.  If people know more about how the economy is performing and how that is likely to influence our actions, this should make it easier for us to achieve our twin objectives. \n\nI will now turn it over to Rob to discuss the Snapshot in greater detail.\n\n1 Jonathan McCarthy, Paolo Pesenti, Robert Rich and Joseph Tracy assisted in preparing these remarks."
    },
    {
        "title": "Market Structure and Liquidity in the U.S. Treasury and Agency Mortgage-Backed Security (MBS) Markets",
        "date": "May 17, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/wue160517",
        "content": "Good morning, it is a pleasure to speak before the Mortgage Bankers Association today on the topic of market structure and liquidity. Please note that my remarks reflect my personal views on these topics and not necessarily the views of the Federal Reserve Bank of New York or the Federal Reserve System.1\n\nFixed income market liquidity has garnered substantial attention from market participants, the media, and policymakers in recent years. There has been a notable amount of market commentary suggesting deterioration in liquidity, yet it has been difficult to find compelling evidence for this in many traditional indicators used to measure liquidity. Why might this be the case? Market liquidity is central to the functioning of financial markets, and a sustained examination of this topic should remain a priority for private market participants as well as the public sector.\n\nToday I'd like to focus on the interaction of liquidity and market structure in the Treasury and agency MBS markets as offering some important insight into this question. We should expect liquidity conditions to be dynamic, changing as the structure of markets change, including the instruments, participants, practices and policies that impact trading across financial markets. The Treasury and agency MBS markets in particular have each been undergoing significant structural changes that have implications for the way that we measure and interpret liquidity conditions now and in the future. Indeed, market structure changes may help explain why the market narrative on liquidity at times appears to diverge from some of the traditional measures.\n\nWhat are the linkages between liquidity and market structure in the Treasury and agency MBS markets?\n\nThe Treasury and agency MBS markets represent the largest, most liquid government securities markets in the world. The Treasury market comprises over $13 trillion in marketable securities, while the amount of outstanding agency MBS is nearing $6 trillion. About $500 billion in Treasuries trade hands every day, and agency MBS volumes represent another $200 billion in daily trading.2\n\nThis immense liquidity is important for the health of financial markets and the economy. Liquidity in the Treasury market is essential to the market's vital role in global financial markets, lowering financing costs to the U.S. government, contributing to the market's use as a risk free benchmark, and allowing Treasury securities to serve as an important store of value and means of managing risk for investors. In the mortgage market, a well-functioning secondary market allows mortgage market participants to manage their risk, and helps lower the cost of home ownership. For the Federal Reserve, these markets are important because of their role in the implementation of monetary policy. We conduct open market operations that rely on and interact with liquidity in these markets, like the large scale asset purchases and reinvestments. We also analyze these markets to help understand monetary policy expectations, and how financial conditions and structure are changing.\n\nHow did these markets become so liquid and vital? Were their structures determined by historical accident, or was it by clever design? A number of scholars and analysts have studied these markets in depth, including the evolution of their instruments, participants, practices and policies—all elements of what I will define as being \"market structure.\"3 I won't be able to do justice to their work in this discussion. But suffice it to say that historical evolution, innovation, and important private- and public-sector policy choices along the way have shaped these markets in ways that help them serve the public and private sectors very well.\n\nOn the policy side, decades ago Treasury developed the practice of prioritizing a 'regular and predictable' approach to holding auctions, a change which helped develop secondary market liquidity across the curve of maturities, and is widely credited with reducing uncertainty and minimizing government borrowing costs.4\n\nThe Treasury market has been undergoing rapid change in the last decade, as explored in detail in a Joint Staff Report on the events of October 15, 2014, when the Treasury market experienced what is sometimes called a \"flash rally.\"5 For example, the interdealer market has evolved into high speed electronic central limit order books, or CLOBs, that play a key role in Treasury market price discovery. As it has in equities, futures and foreign exchange markets, the shift towards electronic and automated trading has reduced the costs of trading and enhanced many dimensions of liquidity by enabling a greater number of market participants to trade in a more efficient manner.6\n\nThe agency MBS market traces its roots back to 1970 when the first Ginnie Mae mortgage-backed security was issued. By pooling several loans into a tradable security, and guaranteeing the timely receipt of principal and interest, Ginnie Mae created a tradable mortgage instrument that was attractive to a wide variety of global investors. Soon afterwards, the development of the To-Be-Announced (TBA) market allowed for fungible forward-trading of what is today millions of otherwise less liquid individual securities. Roughly 90 percent of the of agency MBS trading occurs in the TBA market, making it the second most actively traded fixed income market behind Treasuries in the United States. The combination of the agency guarantee and the TBA market structure has been estimated to lower mortgage rates by as much as 50 basis points and this benefit can be higher during periods of stress.7\n\nFrictions and risks in these markets can also highlight the importance of market structure. For example, when interest rates fell close to zero around the time of the financial crisis, the Treasury and agency MBS markets experienced heightened levels of settlement fails. In response, the Treasury Market Practices Group (TMPG)—a private sector group of market professionals dedicated to the integrity and efficiency of the Treasury, agency debt and agency MBS markets—responded by recommending a charge for fails in the Treasury market and later the agency debt and MBS markets.8 These charges led to a sharp reduction of fails and improvement in market functioning.9 A few years later, the TMPG took steps to improve market functioning by recommending margining for forward-settling agency MBS transactions.10 The TMPG's efforts have removed obstacles that would otherwise undermine market functioning, and helped identify practices that have improved the integrity and liquidity of these markets.\n\nWhether policy choices, or the innovation of practices and technology, these developments provide us with insights into the relationship between liquidity outcomes and characteristics of market structure. For example, it appears that liquidity is generally enhanced where trading:\n\n• is concentrated in a limited number of instruments with a high degree of standardization;\n• involves a large and diverse group of participants;\n• uses common and efficient trading practices, frequently that are electronic; and\n• is guided by private- and public-sector policies that contribute to the integrity and efficiency of markets.\n\nWhere these conditions exist, liquidity tends to be higher on average: trading is faster and less costly, depth is greater and prices are more efficient. When they do not exist, liquidity tends to be lower: trading is slower and more expensive, with less depth and efficiency.\n\nWhy do market structure changes make understanding liquidity in these markets challenging?\n\nLet me start by laying out a definition of liquidity.\n\nLiquidity can be a fairly abstract concept and there are numerous ways to define and measure it. The definition I prefer is this:\n\nMarket liquidity is the cost of quickly converting a desired quantity of an asset into cash (or cash into an asset) at an efficient price.\n\nThe definition incorporates four components. First, cost. What is the transaction cost of executing a trade? Second is timeliness. How quickly can you get in or out of a position? Third is depth. What quantity can be bought or sold at a given price? And finally, efficiency. Are trades conducted at efficient prices?\n\nPrice efficiency is not usually included in the definition of liquidity, but the concept seems quite important, especially in a world of high-speed electronic markets and a higher incidence of \"flash\" events. Low transaction cost and quick execution may mean little if the price at which a transaction is conducted is at a level that defies common sense or is obviously artificial.11 A number of flash events in different asset classes provide recent examples of this concern, including the equity markets in May of 2010, the Treasury market in October of 2014, and foreign exchange markets in March of 2015. In each of these cases, there was no new information that could have changed the value of these assets so dramatically, especially in such a rapid fashion, but nevertheless the stocks of some healthy companies traded briefly at pennies, major foreign exchange pairs whipsawed, and the prices of the world's preeminent risk-free assets experienced historic moves, including an unprecedented 16 basis-point round-trip in the 10-year Treasury security that spanned just a few minutes amidst the fourth-largest intraday trading range in close to two decades. During each of the events, trading volume surged, market depth fell, and realized volatility spiked, as market participants seeking to manage their exposures and liquidity risk amplified price movements.12 Despite these signs of illiquidity, in flash events bid-ask spreads often remain narrow, and price movements can be fairly continuous.13\n\nFlash events thus might be characterized as a modern form of illiquidity in which transactions in highly automated markets may still take place quickly—sometimes on the order of microseconds—and even at narrow bid-offer spreads, and yet broader liquidity conditions deteriorate significantly, with buyers and sellers no longer transacting at efficient prices. Notably, while traditional market makers report that days like October 15, 2014 are highly unusual, many firms employing high-speed automated trading report that such days do not necessarily require them to adjust their algorithms, and they may even see the events as relatively slow-moving. These divergent perspectives and the increased incidence of flash events across markets might foreshadow an increase in market liquidity \"jump\" risk—the risk of a sudden and large jump in prices or volatility—a concern for which there is some evidence.14 More work must obviously be done to better understand the financial stability risks associated with flash events.\n\nGiven the multidimensional nature of liquidity that I've just described, quantifying liquidity can be challenging, but there are a number of measures market participants use to monitor dimensions of market liquidity over time. In the Treasury market, analysts frequently look at measures like bid-ask spreads to discern the cost of trading, which is often supplemented with other measures, like order book depth and price impact measures, that evaluate how trade sizes and price changes are related. To gauge market efficiency, market participants often use measures like yield curve fitting errors, which measure the dispersion of traded yields around a smoothed yield curve. Overall, measures of liquidity in the Treasury market appear to paint a somewhat mixed, albeit largely positive, picture of market liquidity, with bid-ask spreads stable and price impact well within historical ranges. Some measures, including yield curve fitting errors, show some deterioration in recent years, but with levels near longer-run averages (Exhibits 1-3).15\n\nFor mortgage-backed securities, some cost metrics like quoted bid-ask spreads are less useful because they come from dealer-to-customer platforms and capture only the likely, or \"indicative,\" price at which a transaction might occur. As a result, analysts focus on measures of activity such as trading volume, trade size and turnover rate in addition to derived bid-ask spreads based on actual transaction prices and other indicators, such as price impact. Like the Treasury market, most measures of liquidity conditions have declined somewhat in recent years, but levels are at or near long run averages. For example, average trading volume and average trade size have each declined from the high levels witnessed just before and after the crisis, but are consistent with levels seen in the early part of last decade, while measures of bid-ask spreads and price impact suggest that liquidity conditions are relatively stable (Exhibits 4-6).16\n\nDespite this relatively benign backdrop, market participants have consistently described liquidity conditions as increasingly challenged, with many quick to point out that traditional measures of liquidity may not be indicative of actual trading conditions. Traditional liquidity metrics only capture trades that have taken place, but do not capture the immediacy with which participants were able to trade, or if they were able to trade at all. Furthermore, firms report having had to change their market making operations in response to liquidity conditions, with many pointing out that they can no longer trade in the size that they have in the past without impacting market prices.\n\nSo if liquidity conditions seem to have changed, why is it not obvious in traditional measures?\n\nChanging market structure provides at least some insight to this question. Structural changes mean that our interpretation of some liquidity measures must adapt, and that we may also need to search for new ways to measure liquidity. In other words, both the interpretation and measurement of liquidity must evolve as market structure evolves. To illustrate this point, I'd like to describe three important ongoing changes in fixed income market structure that help us reconcile aspects of the narrative around liquidity and what we see in the data. These include: (1) electronic and automated trading; (2) bank regulation and risk management; and (3) evolving public sector ownership.\n\nElectronic and Automated Trading\n\nI mentioned earlier that bid-ask spreads in Treasury markets have been low and stable in recent years, which most would interpret as a positive indicator for liquidity. But it is no longer clear that we can take a strong signal about liquidity from bid-ask spreads in the interdealer Treasury market. The growth of high-speed automated market making in interdealer Treasury markets has dampened variation in this measure as firms employing these strategies tend to provide narrow bid-ask spreads, even during volatile times and \"flash\" events. However, high speed market makers often respond to instances of elevated volatility by rapidly reducing the quantity they are willing to buy or sell at a given price, resulting in lower market depth, suggesting that measures that incorporate depth and its volatility may prove more important in highly automated markets. Some refer to this aspect of liquidity as creating a \"liquidity illusion\" or \"phantom liquidity,\" as it can disappear suddenly. On October 15, 2014, as noted in the Joint Staff Report, bid-ask spreads moved very little throughout the day, but market depth was volatile and dropped to very low levels just before the round-trip in prices (Exhibits 7 and 8).\n\nIt's also the case that bid-ask spreads are very much dependent upon the market in which they are measured. The interdealer market, which trades using CLOBs, may not reflect the bid-ask spreads borne by participants in request-for-quote (RFQ) markets like the dealer-to-customer Treasury and MBS markets. In dealer-to-customer environments, which represent about half of the Treasury market and an even greater share for MBS, we have relatively poor insight into actual bid-ask spreads, because as I mentioned, observed spreads are indicative.17 In MBS, one can use data from TRACE to construct estimates of effective bid-ask spread measures based on actual trades.\n\nBut in Treasuries it is particularly difficult to measure market liquidity in the dealer-to-customer market, because of the lack of nearly any publicly or commercially available trade data. Treasury market data collected after the Joint Staff Report suggests that the liquidity experienced in dealer-to-client markets may be different than that in highly automated markets. For example, during the most volatile period of October 15, 2014, the non-response rate to client trade inquiries rose to over 30 percent from its normal level of around 5 percent for the 10-year security (Exhibit 9). The spread between prices in the dealer-to-client and interdealer markets also widened during the volatility (Exhibit 10). More data would be necessary to understand these dynamics, suggesting that data collection and public reporting of the kind that is done in the MBS market may have value in the Treasury market as well.18\n\nBank Regulation and Risk Management\n\nThe second trend I'd like to highlight is adaptation to the post-crisis regulatory environment and ongoing risk management changes. These trends have been widely discussed by market participants, the media, and regulators, with frequent references to the shrinking amount of dealer balance sheet allocated to capital markets business lines that rely on trading inventories and repo, both of which have declined (Exhibits 11 and 12).\n\nHow do these trends relate to market liquidity? Many market participants have pointed to recent changes in fixed income relative value relationships, particularly those measuring the relative pricing of cash securities versus synthetic products, or derivatives, which receive different balance sheet treatment under current regulation. Since the middle of last year, swap spreads, or the difference between swap rates and Treasury yields, have declined and even gone negative, or dipped further into negative territory, for longer tenor rates (Exhibit 13). Over the same time frame, relative value relationships between cash Treasury securities and Treasury futures have experienced similar volatility.\n\nA common theme among these anomalies is the influence of the rise in the cost of funding cash as compared to derivatives instruments. Market participants have pointed to constraints on dealers' repo financing of Treasuries as contributing to these trends, as such constraints increase the cost of establishing and funding positions in cash securities relative to swaps and futures. In the agency MBS market, dollar roll implied financing rates for securities currently being produced (\"production coupons\") have increased to multi-year highs, and in some cases exceed agency MBS term repo. Similar to the dynamics affecting the Treasury market, MBS participants cite constraints related to dealer balance sheets as driving implied financing rates above MBS repo rates.\n\nWhile these developments are difficult to directly tie to overall market liquidity, changes in funding costs and funding market liquidity almost certainly will have an effect on secondary market liquidity in cash securities. Dealer balance sheets play a central role in the functioning of funding markets, which in turn are important to enable intermediation and arbitrage in financial markets more broadly.19 Importantly, while many market participants have pointed to regulation as driving these changes, dealer balance sheet adjustments have also been influenced by private decisions to manage risk more prudently after the financial crisis.20\n\nOf course, a thoughtful examination of these recent developments would weigh their costs and benefits, including the cost of potentially lower liquidity and the benefit of greater financial stability from the regulatory and risk management changes. Some increase in the price of liquidity or in market liquidity \"jump\" risk may be worth reducing the likelihood of the collapse of a major financial institution, which could set off sharp dislocations across markets. A cost-benefit analysis of this kind is complex, because it is unlikely that we have reached a new equilibrium: not all changes in the regulatory landscape have yet been implemented, and market participants are already adapting and finding new ways of providing liquidity. Moreover, we do not yet have robust ways of measuring some aspects of the costs, including the risk or implications associated with a potential increase in flash events or liquidity \"jump\" risk. It is clear that any analysis will require a deep understanding of the specific ways in which these structural changes affect liquidity and financial stability risk.\n\nPublic Sector Ownership\n\nThe third development I'd like to discuss is the evolution of public sector ownership in these markets and the possible implications for liquidity. Following the large scale asset purchase programs implemented during the global financial crisis and in the years after, the Fed's System Open Market Account (SOMA) portfolio now holds nearly $2.5 trillion of Treasuries and over $1.7 trillion of agency MBS.\n\nGiven their large size, Fed purchases or holdings have the potential to increase or decrease liquidity in these markets, and market functioning considerations have informed the implementation of purchase operations. As one example, implementation of the purchases in Treasuries took into account relative value, and our purchases provided reliable demand for less liquid off-the-run securities.21 These factors may have helped suppress price inefficiencies in the cash market, as measured by yield curve errors. Thus some deterioration in this metric after we ceased secondary market purchases might be expected.\n\nWith respect to agency MBS, the Fed is now the largest single holder, owning roughly 30 percent of the outstanding stock. The vast majority of purchases during the purchase programs were made in newly-issued MBS traded in the TBA market, as are all of the reinvestment related purchases we conduct today. Purchasing in the TBA market is both an efficient way to execute such sizeable transactions, and also helps avoid liquidity strains that could emerge if purchases were made in less liquid coupons.22\n\nBecause Fed purchases or holdings have the potential to create scarcity in the assets being purchased, in both the Treasury and agency MBS markets we employ operations designed to help support the liquidity of these markets. These include lending our Treasury holdings and conducting dollar rolls to facilitate the settlement of our agency MBS purchases.\n\nContemporaneous with the Fed increasing its agency MBS holdings, Fannie Mae and Freddie Mac were mandated by the FHFA to wind down their retained portfolios, a move that reduced their portfolio management and hedging activities in the market. Commercial banks, the second largest owner of MBS behind the Fed, have also reduced these activities in response new capital requirements and accounting rules, which have contributed to an increase in their allocation of holdings to passive held-to-maturity (HTM) accounts. As a result, the ownership base in the MBS market has shifted meaningfully since the crisis from those who hedge the negative convexity inherent in MBS to those that do not. How this affects liquidity is difficult to determine though there are a few hypotheses. Some analysts speculate that the narrowing of swap spreads is at least partially related to this, because active hedgers frequently use swaps as a means to hedge interest rate risk of their portfolio. Others believe this phenomenon has driven average trading volumes lower, all else equal. Lastly, the shift in the ownership base from those who hedge duration, to those who do not, likely mutes the impact of negative convexity events witnessed during times of heightened interest rate volatility, whereby hedgers need to sell in a declining market and purchase in a rising market.23\n\nLooking ahead, Some Current Public Sector Initiatives\n\nLastly, I'd like to cast attention on some forward looking public policy initiatives at the heart of the debate around market structure and liquidity in the MBS and Treasury markets.\n\nIn Treasuries, the steps identified in the Joint Staff Report on October 15, 2014 represent the most comprehensive evaluation of the market’s structure in a quarter century. As part of the follow up from that work, the Treasury Department earlier this year issued a request for information (RFI) on the evolving structure of the Treasury market, recognizing the importance of improved official sector access to data across all areas of market activity.24 The information from the October 15 report and the RFI provide important insights for the policies that will help shape the future structure of the Treasury market. For example, a consensus has already emerged for a greater need for official sector access to transaction data, and just yesterday the Treasury and the SEC announced that they are working together to explore means of collecting cash market transaction information, working with FINRA.\n\nIn MBS, the TBA market is about to go through its greatest structural change since it was introduced nearly 50 years ago, with the Common Securitization Platform (CSP), a new infrastructure that will harmonize Fannie Mae and Freddie Mac's single-family mortgage securitization activities, and enable them to eventually issue a Single Security.25 The Single Security TBA contract, recently dubbed Uniform MBS (UMBS), will allow either Fannie Mae or Freddie Mac securities to be delivered into a single contract.\n\nRoughly eight years after the start of the financial crisis, continuing steps to change the structure of the mortgage market seems important, lest this market's depth and liquidity be diminished over time. For many years, Freddie Mac securities have been less liquid than their Fannie Mae peers, and market participants have suggested that there are sizeable taxpayer costs associated with maintaining separate securitization platforms for both entities.26 Commentary on the planned Single Security has highlighted the lessons I mentioned earlier around market structure and liquidity: in order to improve liquidity, the initiative must help create a deep pool of standardized instruments, continue to attract a large and diverse group of investors, employ common securitization and trading mechanisms, and be guided by policy that is informed and led by the collaboration of the public and private sectors.\n\nThese ongoing initiatives and others build on a long history of dynamic change in the structure and liquidity of Treasury and agency MBS markets and will continue to shape their evolution in the years to come. In the process, we should be mindful to regularly reassess our interpretation and measurement of liquidity so that we understand and continue to support this critical attribute of the Treasury and agency MBS markets.\n\nExhibits\n\n1 George Eckerd and Brian Greene assisted in preparing these remarks, along with contributions from Tobias Adrian, Alain Chaboud, Michael Fleming, Frank Keane, Michael McMorrow, Brett Rose and Ernst Schaumburg, helpful comments from a number of others, and assistance with data from Romen Mookerjee and Rich Podjasek.\n\n2 Treasury market trading volumes can be challenging to estimate due to the lack of comprehensive data on activity across dealer-to-client and interdealer markets, see Fleming, Keane, Schaumburg (2016) Primary Dealer Participation in the Secondary U.S. Treasury Market, Liberty Street Economics Blog, February 9. MBS volumes are obtained from transaction data reported by members of the Financial Industry Regulatory Authority (FINRA) to the Trade Reporting and Compliance Engine (TRACE).\n\n3 See for example: Fleming, Mizrach, Nguyen (2009) “The Microstructure of a U.S. Treasury ECN: The BrokerTec Platform”; Grossman and Miller (1988) “Liquidity and Market Structure”; O’Hara (2015) “High Frequency Market Microstructure”; Garbade, Keane, Logan, Stokes, and Wolgemuth (2010) “The Introduction of the TMPG Fails Charge for U.S. Treasury Securities.”\n\n4 See Garbade (2007) “The Emergence of “Regular and Predictable” as a Treasury Debt Management Strategy,” FRBNY Economic Policy Review, for background and analysis of the evolution of Treasury auction practices.\n\n5 See Joint Staff Report: The U.S. Treasury Market on October 15, 2014 (JSR). Though the JSR does not specify, the contributors to the report from the Federal Reserve System included: Alain Chaboud, Dobrislav Dobrev, Michael Fleming, Frank Keane, Michael McMorrow, Suraj Prasanna, Ernst Schaumburg and Nathaniel Wuerffel, with assistance from Nashrah Ahmed, Joseph Fiorica and Ron Yang. The report was prepared in close collaboration with staff at the Department of the Treasury, the U.S. Securities and Exchange Commission, and the Commodity Futures Trading Commission.\n\n6 See Mizrach, Bruce and Christopher J Neely(2006) “The Transition to Electronic Communications Networks in the Secondary Treasury Market,” Federal Reserve Bank of St. Louis Review for a discussion of the transition to electronic trading and its effects across markets.\n\n7 One study by Federal Reserve staff found that the TBA mechanism reduces borrowing costs by 9 to 12 basis points, on average, with the benefit increasing substantially during periods of stress to as many as 65 basis points; see Vickery and Wright (2013) TBA Trading and Liquidity in the Agency MBS Market, FRBNY Economic Policy Review. Another study estimated that the GSE guarantee reduced borrowing costs by 32-33 basis points. See Passmore (2005) “The GSE Implicit Subsidy and Value of Government Ambiguity”\n\n8 See the TMPG best practices page for the latest information on a wide range of market structure issues handled by the group.\n\n9 See Fleming (2012) Failure is No Longer a (Free) Option for Agency Debt and Mortgage-Backed Securities, Liberty Street Economics Blog, for further discussion of TMPG’s fails charge recommendation in the mortgage market.\n\n10 See Margining in Agency MBS Trading, Treasury Market Practices Group, November 2012, for further information.\n\n11 Though definitions vary, an efficient price can generally be thought of as one that reflects the consensus valuation of an asset, incorporating all publicly available information. Research on market efficiency (see for example Abreu and Brunnermeier (2003) “Bubbles and Crashes” and Fama (1965) “The Behavior of Stock Market Prices” for perspectives on market efficiency) often explores the relative inefficiency, or mispricing, of assets within or across markets, not unlike the arbitrage opportunities that might exist along a yield curve. It would appear that in flash events, relative mispricing across instruments or markets may be little changed, and yet the market as a whole may become inefficient, no longer correctly reflecting available information. These conditions might help explain why, for example, firms employing high-speed arbitrage strategies reported that their algorithms operated within normal tolerances on October 15, 2014.\n\n12 For example, as noted in the JSR, short interest rate and volatility (or “gamma”) positions likely amplified the moves on October 15, 2014 as market participants sought to dynamically hedge their exposures.\n\n13 See Schaumburg and Yang (2015) Liquidity During Flash Events, Liberty Street Economics Blog, August 18, for further analysis of flash events in the Treasury, foreign exchange, and equity markets.\n\n14 See Adrian, Fleming, Vogt (October 2015) “A Note on Measuring Liquidity Jumps” and Adrian, Stackman, Fleming, and Vogt (2015) Has Liquidity Risk in the Treasury and Equity Markets Increased?, Liberty Street Economics Blog, October 6, for discussion of the potential rise in liquidity risk.\n\n15 See Adrian, Fleming, Stackman, Vogt (2015) Has U.S. Treasury Market Liquidity Deteriorated?, Liberty Street Economics Blog, August 17, the first of a series of posts on Treasury market liquidity.\n\n16 See Podjasek, Molloy, Fleming, Fuster (2016) Has MBS Market Liquidity Deteriorated?, Liberty Street Economics Blog, February 8, for background on the structure and liquidity of the agency MBS market.\n\n17 See Fleming, Keane, Schaumburg (2016) Primary Dealer Participation in the Secondary U.S. Treasury Market, Liberty Street Economics Blog, February 12, for a breakdown of volumes in the Treasury market by segment.\n\n18 Another byproduct of increased automated trading is smaller average trade sizes in the interdealer broker and futures markets. When trading on CLOBs, participants tend to break large orders into smaller-sized trades. In the interdealer broker and futures Treasury markets, average trade size has fallen markedly over the past 10-years, coinciding with the growth of automated trading. While some associate smaller average trade sizes with lower liquidity, given these structural changes it is not clear that the interpretation is so straightforward. It is possible that small trades can be executed even more easily in highly automated markets, whereas it may be more difficult to execute large trades.\n\n19 See Adrian, Begalle, Copeland, Martin (2011) Repo and Securities Lending, Federal Reserve Staff Report, and Copeland, Davis, LeSueur, Martin (2012) Mapping and Sizing the U.S. Repo Market, Liberty Street Economics Blog,for a discussion of the repo market's structure and size.\n\n20 See Adrian, Fleming, Goldberg, Lewis, Natalucci, Wu (2013) Dealer Balance Sheet Capacity and Market Liquidity during the 2013 Selloff in Fixed-Income Markets, Liberty Street Economics Blog, for analysis on the drivers of dealer positioning and risk metrics.\n\n21 See Gagnon, Raskin, Remache, Sack (2010) “Large-Scale Asset Purchases by the Federal Reserve: Did They Work?” FRBNY Staff Report, which provides information on the implementation and effects of the Fed’s asset purchase programs.\n\n22 See remarks by Simon Potter The Implementation of Current Asset Purchases, March 2013, for further details on the effects of asset purchases on the MBS market, including market functioning considerations.\n\n23 See Malz, Schaumberg, Shimonov, Strzodka (2014) Convexity Event Risks in a Rising Interest Rate Environment, Liberty Street Economics Blog, for analysis on reduced MBS hedging needs\n\n24 See U.S. Department of Treasury, Notice Seeking Public Comment on the Evolution of the Treasury Market Structure, January 2016, for details on the RFI.\n\n25 See the Common Securitization Platform by the FHFA for details on the reform.\n\n26 See for example The Mortgage Banker Associations response to the FHFA's request and Charting the Course to a Single Security by Lorie Goodman and Lewis Ranieri for information regarding the Single Security."
    },
    {
        "title": "Panel Remarks at the 7th High-Level Conference on the International Monetary System",
        "date": "May 10, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud160510",
        "content": "It is a great pleasure to have the opportunity to participate in this panel. To me, the question posed for discussion seems relatively straightforward: Will more reserve currencies strengthen the international monetary system?  My answer can be summed up in one word: “yes.”  I believe that a reserve currency—or at least an enduring and important one—has to have a number of demonstrated attributes.  These include that the reserve currency country has a strong and credible institutional structure; that its policymakers follow a sustainable fiscal regime as well as a monetary policy that keeps inflation low and stable over time; and that the currency is readily convertible and serves as a stable store of value.  These are good attributes for any country—reserve currency or not. The greater the number of countries that have such attributes, the more stable and sound the global financial system is likely to be.  Also, such a regime is desirable because it provides an environment that should encourage international trade and an efficient allocation of resources, which, in turn should lift productivity and living standards.\n\nAs always, what I have to say today reflects my own view and not necessarily those of the Federal Open Market Committee or the Federal Reserve System.1\n\nAlthough a reserve currency can formally be defined as any currency that is part of a foreign country’s foreign exchange reserve holdings, I do not think this is what we have in mind when we talk about a reserve currency.\n\nWhile the status of being a reserve currency can be conferred, the stature of being a reserve currency must be earned.  As I see it, a successful reserve currency needs to satisfy a number of conditions.  First, the currency needs to be fully convertible—with no risk of restrictions that could limit the ability to convert such holdings into cash denominated in the investors’ domestic currency.  Second, the reserve currency country needs to have deep, liquid and transparent capital markets so that the sovereign debt instruments and other high-quality debt obligations can be bought and sold at narrow bid-ask spreads and easily converted to and from cash.  Third, the assets in the country must be governed by a well-understood, consistent and enforceable legal regime.  This is to ensure that investors’ rights will be well protected, regardless of short-term political developments or changes in the governing regime.  Fourth, the financial system in the country must be strong and stable to ensure that liquidity will be available even during times of stress.  Fifth, the risk of default on the country’s sovereign debt must be viewed as negligible, to ensure both that the currency is a stable store of value and that continuous, deep liquid domestic markets will be available regardless of the economic circumstances.  Sixth, the country must have a credible central bank that is insulated from political interference and committed to keeping inflation low and stable—a necessary condition for a reserve currency to be a reliable store of value.\n\nI think that the U.S. dollar satisfies these general criteria.  That is why over 60 percent of foreign exchange reserve holdings are denominated in dollars—a share that has been quite stable in recent years.  The dollar’s share has remained steady despite the presence of a number of currencies that broadly satisfy the attributes that I have just discussed.\n\nBut should the U.S. be concerned if more countries satisfy the attributes needed to be successful reserve currencies?  Would it be problematic for the United States if the U.S. dollar share of foreign exchange holdings slipped over time because of progress made elsewhere?\n\nI think the answer to both of these questions is “no.”  It would seem positive to me if a decline in the U.S. market share reflected the following developments: a greater number of stable and effective central banks and fiscal authorities, more investors who are reassured about countries’ institutional arrangements and financial stability, and deeper and more liquid capital markets outside the United States.  These attributes are a prerequisite for a vibrant international financial system and are supportive to global trade.  A regime with many reserve currencies would imply a well-governed international economy.  This should facilitate global investment, resulting in rising productivity growth and improving standards of living.  In other words, I don’t see this as a zero-sum game.  If other countries’ currencies emerge to gain stature as reserve currencies, it is not obvious to me that the United States loses.  This is assuming, of course, that the emergence of alternatives is being driven by their progress, rather than by the U.S. doing a poorer job in satisfying the attributes that I discussed earlier.\n\nHaving more countries achieve the stature of having a reserve currency may also be desirable for other reasons.  First, with more reserve currencies available, portfolio diversification opportunities are enhanced. This is desirable because, all else equal, it would allow investors to move further out on the risk/return frontier.  Second, more currencies are likely to be close substitutes, which could dampen currency volatility.  With many viable reserve currencies available, no particular one would necessarily have to bear the bulk of any adjustment.\n\nThe dollar’s dominant reserve currency status has sometimes been referred to as the United States’ “exorbitant privilege,” implying that the U.S. benefits extraordinarily from this privileged status.  I’d argue that the situation is much more nuanced.  Yes, this status does allow the U.S. to benefit from seigniorage.  More than half of all U.S. currency outstanding is held abroad.  But, there are also costs of being the dominant reserve currency.  For example, this can lead to shifts in the valuation of the dollar that are due primarily to developments abroad that affect risk appetites and international capital flows.  In such cases, the dollar’s valuation can be pushed to levels inconsistent with U.S. economic fundamentals.\n\nFor the United States, I believe that the most important goal must be to keep our own house in order.  If we do this, then I expect that the U.S. dollar will earn the right to remain the most important reserve currency in the world.\n\nThe United States has a number of advantages in sustaining the dominant reserve currency status of the U.S. dollar.  First, there is a first-mover advantage.  As the leading reserve currency in the world, there is no strong incentive for countries to move to other currencies as long as the dollar continues to have the attributes I discussed earlier.  The history of reserve currency usage is characterized by considerable inertia.  The U.S. dollar emerged as the leading reserve currency quite a bit after it became the world’s largest economy.  Typically, the loss of dominant reserve currency status requires either substantial economic decline or political instability that motivates foreign counterparties to shift to a new reserve currency. Second, the U.S. has the deepest and most liquid capital markets in the world.  This is important in making U.S. Treasuries and agency mortgage-backed securities attractive holdings as part of countries’ foreign exchange reserve portfolios.  Third, the U.S. has an independent central bank—the Federal Reserve—which has been successful in keeping inflation low and stable.  As a result, the dollar has been a reliable store of value in recent years.\n\nAnother point in the United States’ favor is the considerable steps taken to shore up areas of weakness revealed by the financial crisis.  In particular, the safety and soundness of our financial system has been enhanced. Capital and liquidity requirements for U.S. banks have been raised significantly.  Stress tests have been imposed to enhance the viability of large systemic institutions even in very adverse economic and financial environments.  Reforms have been enacted to facilitate the recovery and resolution of troubled institutions.  This includes living will requirements for the major systemically-important financial institutions, and the development of an improved resolution regime to ensure that institutions that do get into difficulty can be resolved without disrupting the stability of the financial system.\n\nIn conclusion, I welcome other countries’ progress toward achieving the preconditions necessary for their currencies to attain the stature of a reserve currency.  However, we should not act as if this is sufficient to achieve a well-functioning global financial system.  In particular, the current regime is inefficient in a number of important ways.  Countries have found it necessary to self-insure against the risk of large capital flow reversals.  This has led to a very sharp rise in aggregate foreign exchange reserve holdings.  This form of self-insurance is very expensive—especially when the return on the foreign exchange reserve portfolio is less than the cost of the domestic liabilities that fund these holdings.  As I have said in the past, I encourage more work to examine whether there are other more efficient regimes that, for example, would economize on required foreign exchange reserve buffers.  In this regard, I think expanding the capacity of the IMF’s resources and working to further de-stigmatize drawing on the IMF’s liquidity facilities could be worthwhile steps in this direction.\n\nThank you for your kind attention.\n\n1 Linda Goldberg and Joseph Tracy assisted in preparing these remarks."
    },
    {
        "title": "Implementing Monetary Policy Post-Crisis: What Have We Learned? What Do We Need to Know?",
        "date": "May 4, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/pot160504",
        "content": "As most of you know, the Federal Reserve System is engaged in an extended effort to evaluate potential long-run monetary policy implementation frameworks. 2 There are many reasons to take a close look at the issues now. For example, the Federal Reserve has new tools, like the authority to pay interest on reserves and reverse repurchase agreements with an expanded set of counterparties, which were not available pre-crisis. 3 The money markets in which the Trading Desk of the Federal Reserve Bank of New York (the Desk) operates have different dynamics than before, in part because of our large-scale asset purchases. We want to take stock of these changes and ask ourselves what the best way of implementing monetary policy is today and in future environments in a manner that is consistent with the Federal Reserve’s normalization principles. 4\n\nTo make good choices about the monetary policy implementation framework going forward, it is, of course, useful to learn from the past. Our task is to assess the strength and weaknesses of existing and previous frameworks, particularly through the lens of what happened during the recent global financial crisis, when central banks were called on to innovate in a number of ways. In addition, the varied experience of other central banks prior to the crisis can also provide valuable insights. What have they learned about what worked well, or did not work so well, with their frameworks?\n\nThe current framework in the United States looks very different than the pre-crisis framework. One way in which it is different is that excess reserves are much higher than they used to be. This shift is due, in large part, to the large-scale asset purchases that the System undertook, starting in December 2008. But, if you look closely, you’d see that excess reserves actually started expanding earlier and control of the federal funds rate deteriorated. 5 So what happened? \n\nIn reviewing the events of the second half of 2007 and most of 2008, it is clear that the Federal Reserve faced a tension between the need to provide liquidity to ease money market strains and the need to keep control of the interest rate target within its earlier framework. Demand for dollar liquidity increased in many money markets and some of that demand came from banks headquartered outside of the United States. A by-product of the Federal Reserve’s efforts to provide liquidity to the financial system is that the supply of reserves, which can only be held by banks, increased. Reducing money market strains may require injecting more reserves than banks want to hold at the target rate, creating an excess that can lead to a decrease in interbank money market rates. This tension is a potential feature of implementation frameworks that rely on scarcity of reserves. If reserves become less scarce, and if you cannot pay interest on reserves, you will lose control of the interest rate that you are trying to steer. Francesco Papadia’s presentation later today will illustrate some of the flexibility that the European Central Bank (ECB) had in its framework in 2007 relative the Fed.\n\nIt might be difficult to recall, but for the period prior to mid-December 2008, the Desk was directed to hit a point target for the federal funds rate significantly above zero. The Federal Reserve did not have authority to pay interest on reserves until October 2008, so maintaining a scarcity of reserves was the only method available to control the federal funds rate. If the Desk had allowed reserves to increase above predicted demand at the target rate, the traded rate would have fallen well below the target rate set by the Federal Open Market Committee (FOMC).\n\nTo maintain scarcity and thus control, the Desk had to “sterilize” operations aimed at easing strains in money markets; that is, it had to offset any injection of reserves by removing an approximately equivalent amount from the system. One possibility would have been for the Fed to borrow from its traditional counterparties, the primary dealers. But dealers are usually borrowers in our money markets, not lenders, meaning they had limited capacity for these kinds of transactions, especially during the crisis. Knowing that fact, we designed the System’s securities portfolio to accommodate some amount of sterilization. The portfolio was composed of a substantial stock of short-dated Treasury securities, so that, as these securities matured, the supply of reserves would shrink. Draining reserves in that way takes time, however, and the amount of liquidity that would have been needed to ease the strains in our financial markets far exceeded the amount that could be supplied while still keeping control of the federal funds rate. 6 Another way to drain reserves is by selling assets. The Desk did some of that in response to the crisis, but, for a variety of reasons, including operational constraints and concerns that sales of securities could have counterproductive effects, the amounts were small. Other tools that can be used to drain reserves, which were developed later, are a Term Deposit Facility and term reverse repos with extended counterparties. 7\n\nProviding liquidity to financial markets at the onset of the crisis was important to ensure that the stance of monetary policy would be transmitted to the real economy. One simple measure of the stance of policy is that the average of trading in the overnight interbank market is very close to the FOMC’s target rate. However, this simple measure can provide a misleading picture. The experience of the crisis suggests that transmission of the appropriate stance can be impaired even when temporary open market operations keep the effective fed funds rate at the target. Recall that in this period the fed funds and Eurodollar markets exhibited a bifurcated structure with many foreign banks paying considerably more to borrow than U.S. banks. As Marvin Goodfriend will discuss, part of this was due to appropriate market pricing of credit risk, but other aspects reflected segmentation in the money markets that was exacerbated by a pronounced lack of dollar liquidity. Some of this decline in money market functioning was not reflected in the effective fed funds rate as it was measured at that time. 8\n\nThe situation today is very different for a number of reasons. The Federal Reserve has the authority to pay interest on reserves. Many countries that run a “corridor” system of monetary policy implementation have had this authority for a long time and could transition from reserves scarcity to a state of more plentiful supply of liquidity while maintaining adequate control of their interest rate, as I noted was the experience for the European Central Bank.\n\nIn addition, the Desk has augmented an old tool, reverse repurchase agreements, or RRPs for short, that allow an expanded set of counterparties to invest with the Federal Reserve either at a fixed interest rate or up to certain quantity.9 With the advent of interest on reserves, RRP, and the Term Deposit Facility mentioned previously, policymakers have more tools for influencing market interest rates, and sterilization is no longer necessary to control rates with large amounts of excess reserves.\n\nPerhaps the most important development is the successful liftoff of the federal funds rate in December. Although we don’t know yet whether the interest rate control exhibited to date with the new tools will continue at different settings of the target range, so far, things have gone as well as could have been expected.\n\nWhile other countries have operated a “floor” system before, no country with as complex a financial system as the United States, with its wide variety of bank and nonbank institutions, had ever attempted to raise interest rates with such a large supply of excess reserves. The success of the new tools so far has been important for instilling confidence that policymakers can control overnight interest rates at levels appropriate to meet their goals. It also means that, in future, the Federal Reserve will not have to face the same technical constraint that leads to a conflict between easing strains in broad money markets and controlling the policy rate whether it uses a floor or corridor system. This could make the Federal Reserve System more agile and better able in future circumstances to provide the elastic currency that has been one of our stated purposes since the Federal Reserve Act became law more than a century ago.\n\nUntil a few months ago, many conversations about the long-run framework for monetary policy implementation considered at least two options; on the one hand, we had the old system based on reserve scarcity inducing competition for reserves, and, on the other hand, we had a theory about how competition in a range of money markets would pull market rates up in line with the interest rates administered by the Federal Reserve. The old system certainly had flaws, many of which were exposed during the crisis, but it had worked reasonably well. And a system that works reasonably well is often more appealing than one that exists only in theory. The successful liftoff has brought theory into practice, and the results have been good so far.\n\nI am not claiming that the current framework is perfect, or that it is necessarily better than the old framework augmented with the new tools used to directly absorb excess reserves, but it is useful to have flexibility in how to implement monetary policy. We have now seen in practice that changes in administered rates can effectively move market rates—both because effective competition helps bring administered and market rates closer together and because the mere existence of administered-rate facilities, like the RRPs I mentioned earlier, can affect competition between private borrowers and investors even when these facilities see little usage. As we learn more about the practical aspects of using these mechanisms to influence money market rates in the United States, a much wider range of potential operating frameworks becomes viable. It will now be important to make sure that all the costs and benefits of different approaches are considered carefully to get the most effective framework for monetary policy implementation.\n\nIn particular, there are important trade-offs that may need to be considered in a framework that can flexibly provide liquidity for monetary policy implementation and broader transmission. It is possible, for example, that financial market participants may anticipate that the central bank will make liquidity plentiful during times of stress and that this could reduce their incentives to manage their liquidity conservatively. For banks, new Basel III liquidity regulation, such as the liquidity coverage ratio and the net stable funding ratio, partially mitigates this concern. Nevertheless, these regulations do not apply to all participants in U.S. money markets. Paul Tucker’s presentation will discuss a number of the broader costs and benefits of such potential flexibility for the role of central banks in democratic societies.\n\nAnother important aspect of the future framework is that it should be as transparent as possible. Market participants can make better decisions if they have a clear understanding of the way the central bank operates and can anticipate its actions in a wide range of circumstances. Indeed, it is well understood that in the case of coordination failures, a commitment to maintaining liquid markets can prevent illiquidity from occurring, in some case without even having to intervene. For example, during the crisis, some money market participants did “hoard” liquidity and refuse to lend to other institutions because they were concerned that they might need the liquidity for themselves. This behavior led to a situation where liquidity was in short supply because all market participants expected it to be. 10 A central bank can help create conditions where market participants contribute to market liquidity because they believe that market will remain liquid.\n\nThere are many other considerations and trade-offs, but let me wrap up by saying that to make sure we learn the right lessons from the experience of the last few years, we surely benefit from drawing on a wide range of perspectives, including those of academics, current and former central bankers, and market participants —people like you, who “live and breathe” this somewhat technical topic. It is also valuable to share the findings more broadly to enhance discourse on the subject.\n\nI hope that you will be frank in your assessment of the things that worked well or did not work well in the Federal Reserve’s framework, and perhaps in other systems. There is much to learn from a robust discussion of our different experiences. These lessons will be an input into our thinking as we examine the long-run framework."
    },
    {
        "title": "Toward a New Paradigm for Resiliency and Security",
        "date": "May 3, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dzi160503",
        "content": "Good morning. I’d like to thank SIFMA for the opportunity to participate in today’s program. Speaking before an audience of industry leaders on a matter of such fundamental import for the resiliency and security of our financial system represents both a rare privilege and a tremendous responsibility.\n\nIt is an equal honor, and I confess a bit humbling, to follow Secretary Chertoff, upon whose remarks I hope to build with a targeted application for financial market infrastructures. My perspective today will not be that of a Federal Reserve policy maker, or of a supervisor, but decidedly that of a practitioner.\n\nI had the privilege of hearing General Michael Hayden, former head of the NSA and CIA and a member of the Chertoff Group, address a payments symposium hosted by the Federal Reserve Bank of Chicago in late 2014. In that venue, consistent with Secretary Chertoff’s remarks today, General Hayden spoke of the tectonic shifts affecting the global landscape, and their prospective impact on cyber, terror, and geopolitical threats.\n\nHis basic message, in two parts: keep your seat belts on, it promises to be a volatile century (yes, century); and when it comes to our critical financial market infrastructure being subject to attack, it is not a matter of “if” but “when”.\n\nAs the operator of wholesale services for the Federal Reserve Banks, arguably the foundational element of our nation’s financial market infrastructure, this was a sobering message on which to reflect.\n\nI have titled my remarks today “Toward a New Paradigm for Resiliency and Security”, and intend to address, at a conceptual level, how global realities and an understanding of the escalating threat landscape have transformed our historical approach to these critical subjects. My remarks reflect my own views and do not necessarily represent those of the Federal Reserve Bank of New York or the Federal Reserve System.\n\nThe Nature of the Federal Reserve’s Wholesale Services\n\nAs context, and to underscore the stakes, please indulge me in a brief reflection on the nature of the Federal Reserve’s wholesale services, consisting of the Fedwire Funds Service, the Fedwire Securities Service, and the National Settlement Service.\n\nI like to frame the wholesale service suite as the “franchise” when it comes to our nation’s financial market infrastructure. It is a bold assertion, but not an unreasonable one, reflecting at least four considerations:\n\nTransactional Value\n\nIn 2015 we processed in excess of $1 quadrillion in Funds, Securities, and National Settlement transactions. That is a one followed by 15 zeros. For those like me for whom a quadrillion is a difficult number to conceptualize, it reduces to the Gross Domestic Product of the United States effectively flowing through our pipes every four days. It is not an exaggeration to suggest that the wholesale services represent the central conduit of liquidity, indeed the circulatory system, for our nation’s economy and financial system.\n\nInterconnectedness\n\nIn 2012, as I am sure you will recall, the Financial Stability Oversight Council, under Title VIII of the Dodd-Frank Act, designated eight financial market utilities, including the Clearing House as operator of CHIPS, CLS Bank, the Depository Trust Company, the Chicago Mercantile Exchange, ICE Clear Credit, and the Options Clearing Corporation, as systemically important.\n\nThe wholesale services operated by the Reserve Banks were not formally so designated, but the Board of Governors, acting in its oversight capacity, committed at the time to hold us to “as high or higher a standard” as it holds these private sector utilities; I assure you, we are experiencing, appropriately, that “or higher” side of the spectrum.\n\nFor our purposes today, I will merely note the existence of critical dependencies of many of these systemic market infrastructures on the availability of the wholesale services in their daily operational life cycles to fund, defund, or settle positions derived from transactions in other markets. The inverse is not necessarily true, placing the wholesale services at the base of the pyramid on which other systemically important infrastructures, and indeed our financial system, ultimately rest.\n\nRole as Central Securities Depository and Fiscal Agent\n\nAs Central Securities Depository for over $70 trillion in par value of Fedwire-eligible securities, the Fedwire Securities Service functions as central repository for the largest, deepest, and most liquid pool of collateral in the world.\n\nMoreover, in support of the fiscal agent responsibilities of the Reserve Banks, the Fedwire Securities Service facilitates the issuance, maintenance, and redemption of all Fedwire-eligible securities, performing an indispensable role in financing the operations of the U.S. government and those of other issuers.\n\nSupport for Monetary Policy Execution\n\nAny one of these elements would likely qualify the wholesale services as “systemic”; in the aggregate they represent a staggering portfolio on which the execution of both our nation’s fiscal and monetary policies absolutely depend. A wholesale service outage, or even a meaningful disruption that impairs public confidence, risks a significant shock to the United States that would have profound, and potentially unpredictable, consequences, for which the only appropriate policy response is “failure is not an option”.\n\nHistorical Approach to Resiliency and Security\n\nConsistent with industry best practice, our historical posture to ensure the resiliency and integrity of the wholesale services has principally reflected a post-9/11 construct focused on geographic dispersion of infrastructure and human capital. Like you, we have invested considerable resources to ensure operational redundancy through geographic dispersion of data centers and operating sites, real time data replication, and split operations. These investments have yielded significant resiliency dividends, and deserve to be heralded.\n\nWhile geographic dispersion of infrastructure and human capital remains an indispensable prerequisite for responding to physical threats, and is likely sufficient for most contingency scenarios we face, it no longer suffices as the central organizing paradigm for resiliency in the wake of the escalating cyber threat. Global realities compel a paradigm shift in how we contemplate the resiliency and security of systemically important infrastructure. To borrow the vernacular of our supervisory colleagues, we must prepare for “extreme but plausible” events.\n\nConsider, for example, a cyber-breach of perimeter security resulting in the insertion of pernicious malware, a severe data corruption in which confidence in account balances is compromised, or even an application failure that propagates itself almost instantaneously across primary, secondary, and tertiary operating sites. An unfortunate byproduct of instantaneous data replication, such a scenario risks rendering a systemic infrastructure functionally inoperable.\n\nAggravating the cyber challenge, we must recognize, in contrast to traditional resiliency scenarios, the likelihood of facing an adversary that can anticipate and adapt to our contingency response in real time. Moreover, the nature of the challenge is asymmetric: we must defend across an extended front; the adversary need only find a single point of entry or vulnerability. These dimensions add a dynamic to resiliency planning we have not previously contemplated.\n\nToward a New Resiliency and Security Paradigm\n\nIn recognition of these escalating threats, the Committee on Payment and Market Infrastructures (CPMI) and the Board of the International Organization of Securities Commissions (IOSCO) have recently published a consultative report providing guidance on cyber resilience for financial market infrastructures. The guidance is designed to supplement CPMI-IOSCO’s Principles for Financial Market Infrastructures, and is unequivocal in its expectation that FMIs establish a two hour resumption objective for critical operations in the event of disruption, even in the case of extreme events, regardless of their nature (cyber or physical).\n\nFor most infrastructures, I will suggest, this expectation remains aspirational, with some parallel to the supervisory guidance issued following 9/11 that mandated geographic dispersion of infrastructure and human capital. Just as the industry responded to the prior physical challenge, so will it respond to the current cyber charge, I have every confidence. I would be remiss, especially in recognition of our SIFMA venue, not to herald cross-industry collaboration as an indispensable means to identify the design of alternative solutions to accelerate recovery, contemplate their cost-effective deployment, and strengthen not merely the resiliency and security of individual components but the system as a whole.\n\nBeyond our respective efforts to enhance perimeter security, isolate critical applications, rotate more nimbly across data centers, guard against insider threats, and bolster detection and readiness, I will suggest that the central question for systemic market infrastructures to consider in response to the present cyber challenge relates to third site capacity.\n\nHistorically, third site solutions have been integrated into a data replication scheme and scaled to restore only critical functionality in traditional doomsday scenarios in which primary and secondary data centers are lost. But the iron triangles on which market infrastructures have relied may be suffering from corrosion in recognition of the escalating cyber threat. Prospectively, I will suggest that market infrastructures need to contemplate technologically diverse, off-network third site solutions, representing an impregnable firebreak, and a platform for recovery, if the core of an application suite or data set becomes corrupted. A technical point of nomenclature: perhaps one day we will refer to these solutions as “third level” rather than “third site”, recognizing that technology increasingly is liberating us from “physical” limitations (such as data centers) to consider “metaphysical” alternatives (such as cloud or hosted solutions).\n\nThe prospect of a technologically diverse third level of resiliency raises several important questions:\n\nFinancial market infrastructures will likely respond differently to these questions, and just as surely they will come up with a range of technical solutions to respond to CPMI-IOSCO’s clarion call, reflecting their unique circumstances and their respective assessments of the threat. In fact, it may be preferable for market infrastructures to develop alternative solutions for responding to the cyber challenge, lest a monolithic solution result in an unintended concentration risk or an unhealthy “groupthink”. I do not intend to suggest a prescription to be applied universally across market infrastructures; I do intend to inspire a necessary reflection on an issue of fundamental import.\n\nIt is worth noting that the CPMI-IOSCO cyber guidance also exhorts financial market infrastructures to develop contingency plans for events in which the two hour resumption objective is not met. We are therefore compelled to consider not merely “star wars” resiliency but also “stone age” contingency.\n\nLike many of our peers, this charge has already encouraged my colleagues in the Wholesale Product Office and across the Federal Reserve System to consider our remedial actions to mitigate customer and market impacts in the event of a wholesale service disruption from which we cannot recover on a same day basis, our best efforts to ensure resiliency notwithstanding.\n\nThis work proceeds across multiple fronts, including analyzing and parsing our transaction flow to identify systemic activity, exploring alternative constructs to process that activity via other channels and service providers, and (later this year) conducting table top exercises with systemically important customers and interfacing financial market infrastructures to test our hypotheses and procedures.\n\nBut let us not delude ourselves: no matter how mature our framework for responding to protracted outage scenarios, no matter how sound our procedures, no matter how tested our protocol, we never want to rely or rest on these measures from a contingency perspective. The lesson for any market infrastructure, especially one at the epicenter of the financial system, is to so invest in resiliency and security that we never find ourselves in this position.\n\nA Concluding Analogy and Aspiration\n\nAs a former Army officer, I cannot resist concluding with a military analogy. In responding to the present challenge, let us be sure not to construct an inflexible Maginot line whose rigidities are easily subverted by a creative and nimble adversary. Let us instead develop a coherent and integrated system that relies upon the classical elements of defense, but none of them exclusively: perimeter security to keep the adversary outside of the environment; defense in depth to safeguard our most critical assets; sophisticated intelligence to understand the adversary’s tactics; robust surveillance to monitor for intrusion and ensure environmental integrity; rapid response to fend off attack; effective collaboration with allies to enhance collective security; and, occupying the central position in my remarks today, a strategic reserve to respond deftly in the event of loss.\n\nAn important supplemental point not to be overlooked: not only do these measures enhance security and resilience, they also represent extremely effective deterrents in raising the costs upon and marginalizing the effectiveness of our cyber adversaries.\n\nMake no mistake: as it relates to the wholesale services of the Federal Reserve Banks, we aspire not merely to a commercial standard of resiliency, or even a supervisory standard, but something approaching national security grade. We proceed on this trajectory from a position of strength, reflecting a record of experience that has endured the most severe terror attack in our nation’s history, a financial crisis of historic proportion, and extreme weather events such as Super Storm Sandy, among other noteworthy examples. But in this sphere, either intentionally we are progressing, or inevitably we are regressing: there is no idleness.\n\nI trust I have made myself unequivocally clear on the choice we make; the stakes compel it.\n\nThank you for your generous attention today. I’d be pleased to respond to a few questions."
    },
    {
        "title": "Market and Funding Liquidity: An Overview",
        "date": "May 1, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud160501",
        "content": "It is a pleasure to have the opportunity to participate in the Federal Reserve Bank of Atlanta’s 21st Annual Financial Markets Conference. This year’s topic, focusing on the connection between liquidity and financial stability, is an excellent one. As the first speaker, I thought it would be useful to lay out what I see as some of the key issues. I may raise more questions than provide answers, but, hopefully, my remarks will help to stimulate our discussion and debate over the next two days.\n\nAs I see it, in the context of our financial system there are two distinct types of liquidity. The first is what is typically referred to as market liquidity. I would define market liquidity as the cost—both in expense and time—of buying or selling an asset for cash. Market liquidity reflects a number of factors, including any direct transaction expense, such as brokerage costs; the price the transaction is executed at relative to the midpoint of the bid-ask spread; how much, if at all, the transaction moves the market price; and the immediacy or speediness with which the transaction can be completed.\n\nWith respect to market liquidity, there are a number of important open questions. Has market liquidity declined for some asset classes? If so, what are the causes? In particular, has the increase in the regulatory requirements imposed on systemically-important securities dealers adversely impacted market liquidity? If this is the case, are there adjustments that could be made that might improve market liquidity without impairing financial stability?\n\nThe second important type of liquidity is funding liquidity. By funding liquidity, I mean the ability of a financial entity to raise cash by borrowing on either an unsecured or a secured basis. Although market and funding liquidity are often treated as distinct, they can be closely related. This is especially the case during a financial crisis. If funding liquidity declines because of market stress, for example, this may cause intermediaries to become less willing to provide market liquidity. Declines in market liquidity, in turn, may further impair funding liquidity, creating a negative feedback dynamic.\n\nCapital and liquidity requirements play a role in preventing such a self-reinforcing negative dynamic. The central bank may also have an important role to play as the lender-of-last-resort. When and how the central bank intervenes―and what the appropriate preconditions should be to maintain market discipline and mitigate moral hazard―are important open questions.\n\nAs always what I have to say today represents my own views and not necessarily the views of the Federal Open Market Committee or the Federal Reserve System.1\n\nTurning first to the issue of market liquidity, the evidence that market liquidity has diminished is mixed. Some market liquidity metrics have not changed much in recent years. For the U.S. Treasury market, quoted bid-ask spreads in the inter-dealer market have remained stable since 2010 at their pre-crisis levels (Exhibit 1). And, order book depth, while lower now than in 2012 and 2013, does not appear low by historical standards (Exhibit 2).2 For the corporate bond market, some evidence even suggests improving market liquidity. For example, excluding the financial crisis period, the realized bid-ask spread calculated from TRACE data has been trending down since the early 2000s (Exhibit 3).3\n\nBut these metrics don’t tell the whole story. In particular, we have significant data gaps that make it difficult to comprehensively measure market liquidity. For example, in the U.S. Treasury market we have poor visibility into dealer-to-customer transactions. In this market segment, business is conducted bilaterally on an over-the-counter (OTC) basis—typically by phone or through electronic trading platforms like Tradeweb and Bloomberg that are based on the traditional request-for-quote model. The U.S. Treasury has issued a detailed request for information seeking public comment on the many factors that are affecting the structure and liquidity of the U.S. Treasury market, and is seeking comments on the relevant data gaps.4 This is an important step that will help clarify what further work is needed in this area.\n\nEven in the corporate bond market where we have detailed TRACE transaction data, this still tells an incomplete story. TRACE data show what trades have taken place, not what trades have potentially been foregone because of any possible diminution of market liquidity. Gaining a better understanding of this phenomenon is being actively researched by my staff.\n\nIn contrast to the market liquidity metrics such as bid-ask spreads, indirect evidence on market liquidity suggests that there has been a decline. Asset managers report that large blocks of securities are now harder to buy and sell without generating significant movements in prices. Also, average trade sizes have decreased (Exhibit 4) and there is evidence of a growing bifurcation in the corporate bond market, with greater activity and liquidity in those bonds that were issued more recently, larger in size or brought to market by larger issuers.\n\nAlso, capital and liquidity requirements for the largest securities dealers—which have been raised significantly since the financial crisis—might have adversely impacted market liquidity. These regulatory changes likely have affected the profitability of dealer intermediation activities and consequently the provision of market liquidity.\n\nFor the Treasury market, the most important change in regulatory requirements appears to be the supplementary leverage ratio (SLR). The SLR limits the leverage of dealer balance sheets. In the SLR, all assets are treated the same in terms of required regulatory capital. Consequently, the SLR is much tougher on low-risk assets as compared to the risk-based capital measures that it complements. This makes the economics of repo-financing much less attractive. Not surprisingly, we observe significant declines in the quantity of repo financing by dealers (Exhibit 5).\n\nFor the corporate bond market, the most relevant regulatory changes appear to be the increase in the Basel risk-weighted capital ratio, the Comprehensive Capital and Analysis Review (CCAR) stress tests and the Volcker Rule—which prohibits proprietary trading for a number of asset classes such as corporate bonds, albeit not U.S. Treasuries. We do observe significant declines in recent years in securities dealers’ holdings of corporate bonds (Exhibit 6).\n\nIt would be surprising if these declines in repo and corporate bond holdings did not have some consequence for market liquidity. However, we do not observe marked declines in dealer holdings of Treasuries, even as overall dealer assets have stagnated (Exhibit 7). The stagnation in dealer assets might indicate that inventory risk has shifted from dealers to asset managers and other buy-side participants, reflecting a shift in market-making from a “principal” to an “agency” model.\n\nWhile increases in regulatory requirements have undoubtedly played a role, I think that other non-regulatory factors are also important. Liquidity provision is affected by changes in market structure, how trades are executed, and by competition from those operating outside of the regulatory boundaries that have been established for systemically-important financial institutions. Similarly, the demand for liquidity is also evolving with the growth of large asset managers, mutual funds, exchange traded funds (ETFs) and principal trading firms (PTFs)—which are sometimes referred to as high-frequency trading firms. In the U.S. Treasury market, for example, high-frequency trading firms have seized a large proportion of the inter-dealer market in recent years (Exhibit 8). In the credit market, a rapidly rising share of corporate bonds outstanding is now held by mutual funds (Exhibit 9). Because mutual funds are not natural providers of intraday liquidity,5 their rapid growth may have contributed to the decline in secondary market trading activity. Fixed-income market liquidity may also be declining for cyclical reasons. Dealer inventories have historically been sensitive to changes in the size of credit risk-premia and term-premia (Exhibit 10). Finally, the effects of unconventional monetary policy on rate expectations and corporate bond issuance may also be affecting market liquidity. This is an area in need of further study.\n\nTo reiterate, the evidence that market liquidity has diminished is mixed. But even if one agrees with the position that market liquidity has diminished, concluding that tougher regulatory requirements are the most important factor would be premature at this stage. The available evidence and common sense suggest that the changes in the regulatory regime are likely important, but we need to do considerably more work before we reach a definitive conclusion on their relative contribution.\n\nFurthermore, even if additional evidence supported the conclusion that market liquidity has declined due to the increase in the regulatory burden on the major securities firms, this decline would still need to be weighed against the benefits of a more resilient and robust financial system. During the last cycle, market liquidity was plentiful prior to the financial crisis. But, as the financial system became stressed and several large systemic firms teetered on the brink of failure, market liquidity quickly dried up. A somewhat higher cost for the provision of market liquidity during the more benign stages of a financial cycle might be worthwhile if it were accompanied by less volatility and stress when the cycle inevitably turned down. In fact, even if market liquidity costs are now going to be higher on average, this might be a small price to pay for a much more stable financial sector.\n\nAlso, it is important to recognize that market liquidity conditions a few years from now may be very different from the situation today. If the cost of providing liquidity services has increased, this creates strong incentives for innovators to find better ways to match buyers and sellers.\n\nIn my view, this innovative response is already underway. For example, in both the Treasury and corporate bond markets, firms are being established to more efficiently match buyers and sellers of less liquid bonds in order to reduce search costs and the need to carry inventory. Furthermore, execution algorithms have been developed that divide large trades into smaller sizes, thus reducing the costs of moving large blocks of securities. Market liquidity conditions could very well improve over time as innovation occurs in response to this new regime.\n\nLet me now turn to the topic of funding liquidity. I think this issue deserves as much attention as market liquidity. After all, during the financial crisis the demise of several major financial institutions resulted from their inability to finance their long-dated illiquid assets. The deterioration in funding liquidity also increased the degree of systemic stress as firms were forced to sell assets, which depressed their prices, undermining the solvency of firms throughout the financial system.\n\nAs I noted earlier, while funding liquidity can be considered as its own distinct subject, it does have a direct link back to market liquidity. If firms that trade and invest in securities cannot obtain funding liquidity, this will likely lead to less transactions volume, thinner markets and less overall market liquidity. It is also likely to lead to less pricing efficiency resulting in wider and more persistent disparities in the prices of similar assets. Without access to funding, leveraged players, such as hedge funds, will find it more difficult to react to transient dislocations in market prices by engaging in arbitrage—that is, buying the cheaper and selling the more expensive asset. Without a reliable source of short-term funding, market functioning can become impaired, which can further disrupt funding and market liquidity.\n\nThis can be seen by looking at the relationship between the volume of dealer-funded repo backed by U.S. Treasury securities versus the size of Treasury bid-ask spreads (Exhibit 11).6 During times of financial crisis—such as in 2008—sharp deleveraging of dealers’ repo books coincides with an increase in Treasury bid-ask spreads. However, during non-crisis periods, this correlation is quite low. This relationship between funding and market liquidity is also evident when we examine the relationship between the dispersion of Treasury yields relative to a fitted Treasury yield curve and Treasury bid-ask spreads (Exhibit 12). Large dispersions of Treasury yields from what is implied by a fitted yield curve may indicate funding liquidity strains, as market participants find it more difficult to arbitrage relative price distortions.7 Again, we see a strong correlation between the sizes of the yield curve errors and of bid-ask spreads during a financial crisis, but not during non-crisis periods.\n\nThe drop in the availability of dealer-funded repo financing also appears to have led to less efficient arbitrage between closely related asset pairs, such as on-the-run and off-the run Treasuries, corporate bonds and credit default swaps, and Treasuries versus interest rate swaps. The price differences or yield spreads between the more capital intensive and less capital intensive assets have widened.\n\nFor example, we can see this in the difference between the fixed-rate paid on a 10-year interest rate swap and the secondary market yield on a 10-year Treasury note (Exhibit 13). Whereas historically this spread has been positive—reflecting the fact that an interest rate swap has more credit risk than a Treasury security—the spread has been negative since September 2015. Market participants suggest that this shift has occurred because the higher capital requirements have made it unattractive for dealers to arbitrage away this dislocation. Similarly, dealers apparently are unwilling to provide the funding and the balance sheet capacity necessary for their leveraged clients to arbitrage away this dislocation. The degree to which this shift in the yield spread between Treasury and interest rate swap yields is due to the differential capital requirements imposed on cash versus derivative instruments warrants more investigation.\n\nJust as market and funding liquidity are interlinked, so are funding liquidity and capital requirements. All else equal, stronger capital requirements reduce firms’ vulnerability to runs. This suggests that funding liquidity risk can be addressed in many different ways. First, capital requirements can be bolstered to reduce the risk that a firm will become insolvent in particularly bad states of the world. Second, improved reporting and transparency can reduce the degree of uncertainty about whether a firm is solvent, thus reducing the risk that investors might wrongly perceive that a firm is insolvent, when, in fact, it still has adequate capital. Third, liquidity requirements can be increased to reduce the risk that the firm will have to engage in a fire sale of assets that might deplete its capital. Also, a liquidity buffer gives the firm’s management time to respond to bad events without having to resort immediately to selling assets. Fourth, a lender-of-last-resort can provide a liquidity backstop that makes funding liquidity more resilient. Consider the case when a counterparty is uncertain about how other potential lenders view a firm’s solvency. If there is no lender-of-last-resort backstop, this may cause firms to be unwilling to lend, not because they perceive the counterparty as insolvent, but, instead, because they are worried about what others think. That is, I may be unwilling to lend to you, not because I perceive you as insolvent, but because I am uncertain about how others view your solvency. If I do lend and others don’t share my view, will I be able to get my money back? With a lender-of-last-resort added to the picture, I no longer need to know what others think. As long as the lender-of-last-resort is available, if I am correct in my assessment that the firm is solvent, then I can be assured that I will be paid back.\n\nThis issue was particularly relevant during the financial crisis when counterparties even pulled back from firms that were widely viewed as solvent. The Federal Reserve stepped in and created funding backstops for a wide array of assets, such as commercial paper and asset-backed securities. This bolstered the confidence of counterparties in one another and restored market and funding liquidity. Of course, this type of lending is only feasible to an institution that is deemed to be solvent with sufficient collateral available to pledge to the lender-of-last-resort so that it can safely secure its loan.\n\nIn practice, all of these methods are in place to address funding liquidity risk. Capital requirements have been increased; there is much better reporting about how firms will do under significant stress tests; liquidity requirements—such as the liquidity coverage ratio—provide a buffer that allows management more time to respond to problems; and, if necessary, the central bank can act as the lender-of-last-resort to support market confidence and funding liquidity.\n\nSo, should more be done to support funding liquidity? In my view, an important issue is to identify and address gaps in the lender-of-last-resort function. In the U.S., some significant gaps remain. For example, the Federal Reserve has a very limited ability to provide funding to a securities firm, even on a fully collateralized basis. The Discount Window is only available to depository institutions, and Section 23A of the Federal Reserve Act severely constrains the ability of a depository institution to pass Discount Window funding along to its securities affiliate. Although the Federal Reserve may be able lend to a securities affiliate under its emergency lending authority under Section 13(3) of the Federal Reserve Act, this is not a standing facility—it can only be established if “unusual and exigent” circumstances exist and the program or facility must have broad-based eligibility, among other requirements.8\n\nIn the U.S., given the restrictions on central bank lending, if a securities firm were to lose access to funding the remaining options available would be finding a means of replenishing the firm’s capital, selling assets, or selling all or part of the securities firm’s operations. While this might be manageable in the case of a firm-specific idiosyncratic shock, it might prove more difficult if a common shock was broadly hitting the securities industry. In this circumstance, the failure of one firm could increase the stress on other firms that were facing similar difficulties. If all of the requirements for Section 13(3) are met, the central bank could provide liquidity support. However, since this is not a certainty, it is worth considering possible alternatives. Now that all major securities firms in the U.S. are part of bank holding companies and are subject to enhanced prudential standards as well as capital and liquidity stress tests, providing these firms with access to the Discount Window might be worth exploring. To me, this is a more reasonable proposition now than it was prior to the crisis when the major dealers weren’t subject to those safeguards.\n\nMore generally, I am happy to report that the Committee on the Global Financial System—one of the Bank for International Settlements’ central bank groups—is engaged in a project to determine what lender-of-last-resort gaps currently exist, focusing, in particular, on those that may create vulnerability in terms of financial stability. One area that I anticipate will receive considerable attention is whether there are any gaps with respect to the activities of globally systemic firms that operate on a cross-border basis.\n\nAnother issue that still needs greater attention is the appropriate role for the home- versus host-country supervisor. The regulatory and supervisory responses for large, systemically-important firms that operate on a cross-border basis need to be closely coordinated, especially during times of stress. In particular, expectations about who will be the lender-of-last-resort need to be well understood in both the home and host countries.\n\nWe have made considerable progress in recent years in enhancing the safety and soundness of the U.S. financial system. But there is more that we need to do. In particular, we need to study and understand how market liquidity and funding liquidity conditions are changing and what factors are driving that evolution. Conferences like this one go a long way in that regard. They serve as an important forum for discussing ideas and improving our knowledge and understanding. This will provide a solid foundation for any policy actions.\n\nThank you for your kind attention. I would be happy to take a few questions.\n\n1 Tobias Adrian, Nina Boyarchenko, Michael Fleming, Catherine Kung, Ernst Schaumburg, Joseph Tracy, Erik Vogt and Nathaniel Wuerffel assisted in preparing these remarks.\n\n2 See Adrian, Fleming, Stackman, Vogt (2015) Has U.S. Treasury Market Liquidity Deteriorated?, Liberty Street Economics Blog, August 17, for further analysis of Treasury market liquidity.\n\n3 See Adrian, Fleming, Vogt, Wojtowicz (2016) Corporate Bond Market Liquidity Redux: More Price-Based Evidence, Liberty Street Economics Blog, February 9, for further analysis of corporate bond market liquidity.\n\n4 See U.S. Department of Treasury, Notice Seeking Public Comment on the Evolution of the Treasury Market Structure, January 2016.\n\n5 Mizrach, B. (2015). Analysis of Corporate Bond Liquidity. FINRA Office of the Chief Economist.\n\n6 Brunnermeier and Pedersen (2009) “Market Liquidity and Funding Liquidity” Review of Financial Studies 22(6) pp. 2201-2238 provide a theory that features strong correlation between market and funding liquidity during crises.\n\n7 Hu, Pan, Wang (2013) “Noise as Information for Illiquidity” Journal of Finance 68(6), pp.2341-2382 provide further evidence on the relationship between yield curve fitting errors and alternative funding liquidity metrics. \n\n8 A program or facility has broad-based eligibility only if it is designed to provide liquidity to an identifiable market or sector of the financial system.  The program or facility will not be considered to have a broad-based eligibility if fewer than five persons or entities would be eligible to participate, or if it is designed to assist a company avoid bankruptcy, resolution or insolvency (including by removing assets from its balance sheet) or aid a failing financial company.  See Regulation A, 12 CFR 201.4(d).  In addition to the “unusual and exigent” circumstances and broad-based eligibility requirements, other conditions for making Section 13(3) emergency credit include obtaining the affirmative vote of a requisite number of Federal Reserve Governors and the prior approval of the U.S. Treasury Secretary, the emergency credit is not extended to an insolvent borrower, the lending Federal Reserve Bank is secured to its satisfaction, evidence is obtained regarding the unavailability of adequate credit accommodation from other banking institutions under prevailing circumstances to borrowers of the emergency credit, penalty interest rate is set for the emergency credit, and meeting disclosure/reporting requirements.  See Section 13(3) of the Federal Reserve Act (12 USC 343) and Regulation A (12 CFR 201.4(d))."
    },
    {
        "title": "Opening Remarks at the Transatlantic Economy: Convergence or Divergence Conference",
        "date": "Apr 18, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud160418",
        "content": "It is my pleasure to welcome you to today’s conference.  Over the past decade, the New York Fed has been working together with the European Commission and the Centre for Economic Policy Research in sponsoring a number of conferences on interdependencies and policy challenges on both sides of the Atlantic Ocean.  Today, as in the past, we have a great program with many distinguished speakers from academia, and the private and public sectors.  In particular, I would highlight the keynote speech following lunch by Pierre Moscovici, European Commissioner for Economic and Financial Affairs, Taxation and Customs.\n\nAs the two largest economies in the world, the United States and the European Union have a shared stake in the vitality of the global economy. Without growth in both our regions, the global economy will not prosper and we will not be able to provide the conditions to enable our citizens to fulfill their aspirations.  Both the United States and Europe suffered large social and economic costs from the financial crisis and the deep recession that followed.  We continue to take steps to minimize the risks that we will ever find ourselves in that situation again.  In many respects, the United States and Europe face similar challenges and have similar goals.\n\nToday I want to focus on some of the challenges.\n\nOn the regulatory side, there is considerable good news.  In particular, substantial progress has been made in strengthening the global banking system. Capital and liquidity standards for internationally active banks have been raised significantly.  This should reduce the risk of failure for these banks. Also, the financial system is being made more resilient and robust.  For example, the global set of principles for financial market infrastructures has been updated and strengthened. \n\nMoreover, some steps have been taken to better deal with the failure of systemically important financial firms on a cross-border basis.  Key attributes for resolution regimes have been promulgated by the Financial Stability Board.  Efforts are underway to improve how we can work together in a coordinated way on a global basis. \n\nBut more needs to be done.  The impediments to an orderly cross-border resolution still need to be fully identified and dismantled.  This is necessary to solve the so-called “too big to fail” problem. Also, cross-border regulatory cooperation needs to be further enhanced.  This includes greater exchange of confidential supervisory information so that national regulators can be fully informed about the conditions of the banks that operate within their borders.  It also requires efforts to ensure a level playing field across jurisdictions so that the focus is on promoting safety and soundness rather than on trying to protect, favor, or shield national champions.\n\nOn the macroeconomic side, the news for the United States is mostly favorable. Labor market conditions have significantly improved over the past few years and unemployment is now around 5 percent.  The household sector is much less leveraged, the housing sector is recovering, the banking system is much healthier, and the corporate sector is highly profitable, even as business fixed investment spending remains quite weak.  Although inflation has fallen short of our objective, I am confident that inflation will return to our 2 percent objective over the next few years as the labor market tightens further and the transitory factors that have held inflation down dissipate.  After years at the effective lower bound for short-term interest rates, economic conditions have finally warranted the start of U.S. monetary policy normalization.  But these monetary policy adjustments are likely to be gradual and cautious, as we continue to face significant uncertainties and the headwinds to growth from the financial crisis have not fully abated.\n\nFor Europe, the economic outlook has started to improve.  The European Union is experiencing a cyclical recovery and the unemployment rate has been steadily declining over the past two years.  The end of fiscal tightening appears to be a key factor in this recovery.  Risks of low inflation, however, remain a concern for the European Central Bank and have motivated their aggressive use of unconventional monetary policy instruments.\n\nToday’s conference touches on a range of interesting questions concerning both regions:  \n\nI would like to thank the participants for addressing these and other questions during today’s three panels, and I look forward to insightful discussion over the course of the day.\n\nIt is now my pleasure to introduce the European Union Ambassador to the United States, His Excellency David O’Sullivan.   The ambassador is extraordinarily well-versed to speak on the issues of interdependence and the policy challenges we face.   Welcome, Ambassador O’ Sullivan."
    },
    {
        "title": "Opening Remarks at the Community Bankers’ Conference",
        "date": "Apr 11, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/sti160408",
        "content": "Introduction\n\nGood morning. I'm Kevin Stiroh, the head of the Supervision Group here at the Federal Reserve Bank of New York. I would like to welcome all of you to today's Community Bankers' Conference, which I am sure will be a productive and informative day.\n\nWe have representatives from over 50 community and regional banks from across New York, northern New Jersey, southern Connecticut, and the Commonwealth of Puerto Rico in attendance. In addition, my colleagues from the New York Fed and the Board of Governors will offer some supervisory perspective. I'm also pleased to welcome fellow supervisors from the states of New York, New Jersey, and Connecticut, plus federal colleagues from the FDIC, the Consumer Financial Protection Bureau, and our neighboring Reserve Banks in Cleveland and Philadelphia. This may be a record in terms of attendance at this annual event.\n\nBy way of introduction, I took over my current role as head of the Supervision Group at the New York Fed in October of 2015, after spending time in our Research, Markets and Integrated Policy Analysis Groups. I also held other positions in Supervision earlier in my tenure with the Bank.\n\nI'm an economist by training and I spent part of my career studying the performance and function of financial institutions in the U.S. There is a long and distinguished academic literature that documents the unique role of community banks and the critical impact they can have on our economy. As an economist, I'll look to the data and that literature to make three points and then draw supervisory implications.\n\nMy comments today are my own and do not necessarily represent those of the Federal Reserve Bank of New York or the Federal Reserve System.\n\nThe Role of Community Banks\n\nFirst, community banks are different from large banks, and not just in terms of their size. Community banks engage in a different form of banking that is more \"high touch\" and more reliant on \"soft information\" such as lenders' subjective judgment and knowledge of local markets, and relationships with borrowers. There is considerable academic research that documents this difference in lending strategy. As specific examples, smaller banks tend to make loans over shorter distances and interact with borrowers in more personal ways than larger banks with more in-person contact rather than phone calls or email. Both findings are consistent with the idea that smaller banks leverage softer information in important ways that creates a competitive niche.\n\nLooking at balance sheet composition also reveals important differences. At the end of 2015, for example, banks with less than $10 billion in assets held over 40 percent of loans to small businesses (defined as loans below $1 million in size), but only 19 percent of total banking system assets. By contrast, regional banks with between $10 billion and $50 billion in assets held about 10 percent of both loans to small business and banking system assets. On the liability side, there is a greater focus on core deposits that are sourced from local savers.\n\nTaken together, these differences paint a picture of local-facing banks that rely on local-sourced funds to support local lending to small businesses.\n\nSecond, this focus on small business lending supports local economic growth. Small businesses are vital to the health of local economies, employing about half of the U.S. labor force and creating nearly two-thirds of net new private sector jobs in the U.S. each year, according to the U.S. Small Business Administration. The academic literature makes this link: more small banks lead to more small business lending and a higher survival probability for those businesses. This link is also seen internationally as countries with larger community banking sectors tend to grow faster.\n\nThird, community banks are less likely to create systemic risk for the broader national economy or financial markets than large banks. Community banks don't have the scale, the complex business models or the balance sheet composition to create the types of fire-sale externalities or interconnectedness features that are typically associated with systemic risk or contagion among financial institutions.\n\nI should note that there is some tension here. Community banks are essential for economic growth at the local level, yet are not considered \"systemically\" important for the financial system or economy as a whole. This apparent contradiction can be resolved by focusing on the appropriate unit of analysis—a community bank can be crucially important for local consumers and businesses, but less so on a national level or the financial system as a whole.\n\nImplications for Supervision\n\nThe implication of these observations is that community banks require prudential regulation and supervision, but in a form that is different from larger institutions. This is not a new or novel insight, but one that is receiving greater attention among policymakers. \"Tailoring\" of expectations is now an important part of the supervisory vocabulary. Both Federal Reserve Chair Yellen and Governor Tarullo have spoken recently about the need to tailor the scope of supervision to the activities and risks of specific banks. This can include things like limiting exam length, increasing off-site supervisory activities, or enhancing our analytical tools and processes.\n\nAll of this is designed to ensure that we achieve our safety and soundness, consumer protection, and economic growth objectives in the most efficient manner in terms of the burden placed on firms, our own supervisory resources, and the impact on critical financial intermediation.\n\nAs one example of the need to focus on prudential concerns, commercial real estate (CRE) continues to be an area of focus from the supervisory perspective as evidenced in a recent interagency statement, SR 15-17, that reminded banks of existing guidance on prudent risk management for CRE. As articulated in earlier guidance, financial institutions should maintain underwriting discipline and exercise prudent risk management practices in their CRE lending activity. There is evidence, for example, that higher concentrations of CRE are positively correlated with the probability of failure for community banks. Chris Calabia, who is responsible for the supervision of community, regional, and small foreign banking organizations and consumer compliance in the Second District, will talk more about our current focus on commercial real estate and what we're seeing in terms of performance and changing concentrations later in the program.\n\nI hope you will find this conference informative and I look forward to meeting many of you later in the day and hearing more about your views on these important topics.\n\nThank you for your attention."
    },
    {
        "title": "Economic Opportunity and Income Mobility",
        "date": "Apr 11, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud160411",
        "content": "Good morning. I am very pleased to speak at the 6th Annual Community Development Conference.  I would like to give a special thanks to the Association for Neighborhood and Housing Development for organizing this conference about issues of importance to the community development industry and to the future of New York City.\n\nAt the Federal Reserve Bank of New York, we develop a wide variety of research and data products to gain a deeper understanding of regional economic conditions.  We track the health of household balance sheets at the state and local level using data from the New York Fed’s Consumer Credit Panel.  We also conduct an annual poll of small businesses to understand their credit needs and credit availability.  My meeting with you today is part of our continuing efforts to understand what is going on at the grassroots level of our economy and to share insights from the New York Fed.\n\nThe Federal Reserve has dual objectives, set by Congress, with respect to monetary policy—maximum sustainable employment and price stability.  I would like to focus my remarks today on the first of these objectives. As always, what I have to say today reflects my own views and not necessarily those of the Federal Open Market Committee or the Federal Reserve System.1\n\nThe Federal Reserve’s employment objective can be viewed both narrowly and broadly. Narrowly, we focus on the degree of slack or tightness in the labor market.  This leads to discussion of many measures of the labor market such as the unemployment rate, the participation rate and the growth of wages. Broadly, the economy’s potential growth rate depends on effectively investing in and taking advantage of all of the resources in the economy—in particular, we need to achieve the full potential of the human capital of all Americans. For the United States to reach its maximum economic potential, all Americans must have the opportunity to reach their potential.\n\nThe United States has always prided itself on being “the land of opportunity.” Parents hope that their children can achieve more than they did. Over the course of our history, immigrants have journeyed to America in search of a better life, a chance to live the “American Dream.”\n\nWhat defines the American Dream? President Reagan thought that one element of the American dream is “the opportunity to grow, create wealth, and build a better life for those who follow,” while President Obama has described it as follows: “A child’s course in life should be determined not by the zip code she’s born in, but by the strength of her work ethic and the scope of her dreams.”  One’s destination in life should not depend on where the journey begins.\n\nEqual opportunity does not imply equal outcomes—some people may work harder, be more fortunate in terms of their disposition and endowments, or just be luckier in how their lives evolve.  But it does require that income mobility—in particular, upward mobility—be widely evident and remain part of the fabric of the nation.  \n\nIt is important to keep in mind the distinction between income mobility and income inequality. Income mobility is a dynamic concept—the degree to which individuals or families can move up or down in the income distribution over time.  Income inequality is a static concept—how unequal are individual or family incomes at a particular point in time.\n\nRecognizing that rising income inequality in the United States is an important issue, my focus today will be mainly on income mobility.  I don’t think the issue of income mobility receives the attention it deserves.  It is a foundational element for a well-functioning democratic society and provides evidence about the ability of an economy to provide opportunities for its citizens.\n\nA standard approach to measuring income mobility is to compare where a child is in the income distribution relative to where the parents were in the income distribution. A striking result is that intergenerational income mobility in the U.S.—a child’s chances of moving up in the income distribution relative to her parents—has remained stable over the past half century. However, because of the increase in income inequality, the consequences of being born to low-versus high-income parents are larger today than in the past.  One explanation for the lack of a correlation between income inequality and income mobility in the U.S. is that income inequality is strongly influenced by the top one percent of the income distribution, but there is little correlation between income mobility and this top income percentile.2\n\nWhile income mobility in the United States has been relatively unchanged, it remains well below several other nations. According to Stanford economist Raj Chetty, the probability of moving from the bottom quintile to the top quintile is 7.5 percent in the United States, as compared to 11.7 percent in Denmark and 13.5 percent in Canada—two countries with relatively high levels of intergenerational mobility.3 So effectively the chance of achieving the American Dream is not the highest for children born in America.\n\nImportantly, Chetty finds even more significant variation in intergenerational mobility across different metro and rural areas within the United States.4  While some regions in the U.S. match the highest mobility levels among developed countries, others persistently offer less mobility than in most other developed countries. Relative income mobility is highest in the Mountain West and the rural Midwest and lowest in the Southeast. Now these differences could be caused by the experience of living in different neighborhoods, but they may also simply reflect differences in the people living in these neighborhoods. In an effort to distinguish between these two possible explanations, Chetty and fellow economists study children from families who changed their neighborhood during childhood and provide evidence that much of the variation in upward mobility reflects the experience of living in particular neighborhoods. Their findings imply that upward mobility depends strongly on where you grow up as a child.\n\nThis research indicates that it is the places themselves, rather than the families who live there, that affect the outcomes for children. An important question is what specific features of neighborhoods are associated with greater upward mobility. Chetty and co-authors find that higher upward mobility is associated with areas that are less residentially segregated by race or income, have lower income inequality (i.e. a bigger middle class), higher quality schools, 5 stronger social networks, higher community involvement and stable family structures. \n\nThe finding that intergenerational mobility is primarily a local problem has two broad policy implications. The first is the need for place-based policies to invest more in at-risk communities to improve their childhood environments. The second is to facilitate families to be able to move into areas with better childhood environments.\n\nWith respect to the first, I would initially like to talk about the importance of schools―and in particular, strong public education.   Consistent with the findings by Chetty, there exists a large literature that has established the importance of high-quality education as a key determinant of economic mobility.  While there are differences of opinions on the most relevant measures of school quality, there is ample evidence that children who attend better schools on average end up with better outcomes later in life—such as higher levels of educational attainment and higher earnings. It is also well known that there exist large disparities in the quality of education by geography and socioeconomic background. Compared to children born to low-income parents, those born to higher-income families can better afford to move and live in areas with high-quality schools.\n\nThe sorting of families by income and their desire for better educational opportunities leads to enclaves of higher-quality and higher-spending schools that are separated from areas of economically less-advantaged populations and lower-quality schools. To a degree, this residential segregation has been perpetuated by our system of school financing that relies heavily on local property tax funding.  Local financing of public schools leads to a bundling of two distinct choices—residential choice and school choice—and increases the degree of socioeconomic segregation across school districts.\n\nWe can promote greater income mobility by unbundling the residential and school choices. This can be done in one of two ways. The first is to work to equalize school quality across location, while the second is to allow parents more choice of schools from a given location. Along the lines of the first approach, New York Fed research has shown that school finance reforms that seek to equalize funding across school districts by reducing the role of local property taxes can go a long way in decreasing residential segregation and equalizing quality of education.6 The key mechanism is that such reforms make underperforming neighborhoods more attractive, thus reducing socioeconomic segregation and leading to consequent gains in peer quality in previously-challenged enclaves. This in turn reduces disparities in school quality stemming from reduced socioeconomic segregation.\n\nOn the second approach, there exists research indicating that school vouchers that enable students to move to private schools or better-quality public schools can lead to improvements in public school quality and student performance by increasing competition among schools.7 It is important to note, though, that not all voucher programs are created equally, and the design of the program matters in how vouchers impact school quality.  Charter schools have provided another source of school choice in education.  More research into evaluating the effectiveness of voucher programs and charter schools at raising student achievement would be desirable in terms of informing public policy choices.8\n\nThe importance of education is not limited to elementary and secondary levels.  In fact, there exists convincing evidence from the child development literature that suggests that educational investments early in childhood have especially large rates of returns in terms of lifetime earnings and other outcomes. Take, for example, the Perry Preschool program for low-income African-American children, a two-year preschool program experiment in the mid-1960s. Recent research shows that this program had very large beneficial long-term effects, with participants experiencing higher rates of high school graduation, higher salaries, higher homeownership, more stable family structures, fewer arrests and lower welfare dependence. The large positive economic externalities from these investments in children imply that such programs more than pay for themselves over the long term. Estimates indicate that the aggregate benefits over the life of a child exceeded by eightfold the program’s per-pupil cost.9 Almost equally-large gains were reported for children who enrolled in Head Start, a federally-funded nationwide preschool program for low-income children.10  \n\nRelated literature on neighborhood effects has found that labor market outcomes for adults are also significantly affected by the type of neighborhoods where people reside.  The role of labor market networks and referrals has been amply-demonstrated as affecting the likelihood of finding a job, the quality of matches between employers and workers, and the earnings associated with these jobs. Several studies show that many of these networks have a significant local neighborhood component. Further, workers who live in neighborhoods with higher-quality networks are more likely to move to jobs with higher wages.11\n\nLet me now turn to policies aimed at improving upward income mobility through increased residential mobility.  There is some striking evidence that shows providing more opportunities for families to move to wealthier areas may improve upward mobility while also being cost-effective.12 In the mid-1990s, as part of a lottery-based housing mobility experiment sponsored by HUD—called the Moving to Opportunity (MTO) experiment—families in high-poverty housing projects were given housing vouchers to move to lower-poverty mixed-income neighborhoods.  While this project appears to have had little impact on the economic outcomes of the adults and older children, the program appears to have had remarkable long-term impacts on children who moved to such neighborhoods at a young age, with the impacts increasing by the number of childhood years they ended up living in the better environment.\n\nAnalysis of the data from the experiment shows that those who moved to lower-poverty areas when they were 12 or younger were found to earn about 30 percent more, on average, in their early- to mid-twenties compared to those who did not move, were 27 percent more likely to have entered college, and 30 percent less likely to be a single parent.  The MTO experiment again illustrates the importance of neighborhood effects on child development early in childhood. It also demonstrates that grants to move families to lower-poverty neighborhoods may reduce the intergenerational persistence of poverty.\n\nOf course, mobility of the sort that Chetty and other researchers have emphasized interacts in a very important way with the availability of affordable housing.  Making high quality neighborhoods accessible to families from all socioeconomic backgrounds is a major challenge for public policy.   Of course, illegal racial and other discrimination has played a role in preventing access to high-quality neighborhoods.  However, part of the challenge arises from the fact that in a free-market economy, the quality of the environment is reflected in the price of land—and thus housing—in that place.  This means that the neighborhoods with the best schools, the best access to jobs, shopping, recreation and other amenities will be the most desirable and will have the highest cost of land and housing.  Simply put, families seeking the upward mobility that better neighborhoods can promote may well find that housing in these neighborhoods is unaffordable.\n\nSome kinds of public policies may exacerbate, rather than lessen, the tendency of the housing market to price some families out of good neighborhoods.  Zoning and other land use regulations are sometimes enacted to artificially reduce the supply of housing, thus driving its price up even further. Take, for example, minimum lot size zoning which specifies a minimum amount of land per housing unit. When this regulation is binding, it reduces the number of housing units that can be built in a given area, and thus introduces artificial scarcity into the housing market, thereby raising prices.\n\nZoning can also be used to promote affordable housing—to include, rather than exclude low-and moderate-income families by setting aside space for those families in exchange for the right to develop market-rate units.  This is a strategy that New York and other large cities have pursued for many years, and in these cities it is easy to point to units that were developed under the program. Nonetheless, these programs are often controversial—some economists and others argue by increasing the cost of development they restrict supply.  Thus, while some percentage of new units developed will be affordable, fewer new units will be developed with uncertain effects on affordability overall.\n\nIt’s perhaps informative to contrast this kind of program with one that has historically been the main mechanism by which the supply of affordable housing has increased. The program I have in mind here isn’t a housing program, but, in my view, it has extremely important effects on the housing market. I’m talking about intra-urban transportation, which can serve to dramatically increase the locations available for development, and thus promote the availability of affordable housing.13 I believe that safe, reliable, affordable and efficient transportation to job locations should be a crucial element in an effective housing policy. Such a program indisputably increases the supply of sites available for development, which is a key way to get the benefits of lower prices to a broader set of families.  Indeed, it is clear that cities’ ability to grow and attract new residents requires them to either increase density—essentially meaning taller buildings—or to expand outwards by increasing transportation access.\n\nAccess to affordable credit is yet another pillar of a policy program that promotes housing affordability.  We at the Federal Reserve have long worked to ensure that credit flows equitably and that financial services are available to all U.S. citizens.  The Fed’s research on these subjects ranges from the pioneering study of redlining by a team of Boston Fed economists, to a very recent New York Fed study on “banking deserts” that appeared in our Liberty Street Economics blog just last month, as well as ongoing community credit work.  We understand the importance of credit in allowing communities to grow, and the importance of the Community Reinvestment Act in requiring financial institutions to make investments in their communities.  But, as my remarks here have indicated, credit is only one factor—and perhaps not the most important one—that affects the affordability of housing in our cities.\n\nI’ve addressed the importance of geographic mobility in supporting income mobility from the perspective of providing parents options for better neighborhoods in which to raise their children. Geographic mobility is also important in terms of individuals receiving the highest return to their human capital. Local economic shocks can depress employment and wages in some labor markets. This creates a strong incentive for individuals to move to an alternative labor market in order to more fully utilize their skills. Financial frictions to geographic mobility can reduce this movement of individuals across labor markets leading to less-efficient outcomes for the economy.  Income mobility can be enhanced through policies that attempt to limit these financial frictions. This argues for avoiding, where possible, policies that create “in-place” subsidies where the household can access the benefits only while remaining in a specific location.\n\nThe Federal Reserve has the twin objectives of maximum sustainable employment and price stability. Achieving the first of our objectives requires that every individual has the opportunity to achieve her full potential in life. Where you happen to be born should not determine your chance of living the American Dream. Research is highlighting possible key determinants of economic opportunity and income mobility. More research is necessary to inform future policy choices in this critical area.\n\nReferences\n\nAngrist, Joshua, Sarah Cohodes, Susan Dynarski, Parag Pathak and Christopher Walters. 2016. “Stand and Deliver: Effects of Boston’s Charter High Schools on College Preparation, Entry, and Choice,” Journal of Labor Economics, 34(2): 275-318.\n\nAslund, Olof, Per-Anders Edin, Peter Fredriksson and Hans Gronqvist. 2011. “Peers, Neighborhoods, and Student Achievement: Evidence From a Placement Policy,” American Economic Journal Applied Economics, 3(2): 67-95.\n\nBaum-Snow, Nathaniel. 2007. “Did Highways Cause Suburbanization?” Quarterly Journal of Economics, 122(2): 775-805.\n\nBayer, Patrick, Steven L. Ross and Giorgio Topa. 2008. “Place of Work and Place of Residence: Informal Hiring Networks and Labor Market Outcomes,” Journal of Political Economy, 116(6): 1150-1196.\n\nChakrabarti, Rajashri and Joydeep Roy. 2015. \"Housing Markets and Residential Segregation: Impacts of the Michigan School Finance Reform on Inter- and Intra-district Sorting,\" Journal of Public Economics, 122: 110-132.\n\nChakrabarti, Rajashri. 2008.\"Can Increasing Private School Participation and Monetary Loss in a Voucher Program Affect Public School Performance? Evidence from Milwaukee,\" Journal of Public Economics, 92(5-6), 1371-1393. \n\nChakrabarti, Rajashri. 2013. \"Impact of Voucher Design on Public School Performance: Evidence from Florida and Milwaukee Voucher Programs,\" B.E. Journal of Economic Analysis and Policy: Contributions, 14(1): 349-394.\n\nChetty, Raj, Nathaniel Hendren, Patrick Kline, Emmanuel Saez and Nicholas Turner. 2014. “Is the United States Still a Land of Opportunity? Recent Trends in Intergenerational Mobility,” American Economic Review: Papers and Proceedings, 104(5): 141-147.\n\nChetty, Raj and Nathaniel Hendren. 2015. “The Impacts of Neighborhoods on Intergenerational Mobility: Childhood Exposure Effects and County-Level Estimates,” Harvard University and NBER.\n\nChetty, Raj, Nathaniel Hendren, Patrick Kline and Emmanuel Saez. 2014. “Where is the Land of Opportunity? The Geography of Intergenerational Mobility in the United States,” Quarterly Journal of Economics, 129(4): 1553-1623.\n\nChetty, Raj, Nathaniel Hendren, and Lawrence F. Katz.  2015. “The Effects of Exposure to Better Neighborhoods on Children: New Evidence from the Moving to Opportunity Experiment,” Harvard University and NBER.\n\nDeming, David. 2009. “Early Childhood Intervention and Life-Cycle Skill Development: Evidence from Head Start,” American Economic Journal Applied Economics, 1:3, 111-134.\n\nDeming, David. 2011. “Better Schools,  Less Crime?” Quarterly Journal of Economics, 126(4): 2063-2215.\n\nDeming, David, Justine Hastings, Thomas Kane and Douglas Staiger. 2011. “School Choice, School Quality and Postsecondary Attainment,” NBER Working paper No. 17438.\n\nDobbie, Will and Roland Fryer. 2011. Are High-Quality Schools Enough to Increase Achievement Among the Poor?  Evidence from the Harlem Children’s Zone,” American Economic Journal: Applied Economics, 3(3): 158-187.\n\nDobbie, Will and Roland Fryer. 2015.\"The Medium-Term Impacts of High-Achieving Charter Schools,” Journal of Political Economy, 123(5): 985-1037.  \n\nHeckman, James J.  2006. “Skill Formation and the Economics of Investing in Disadvantaged Children,” Science 312, 1900.\n\nHellerstein, Judith K., Melissa McInerney and David Neumark. 2011. “Neighbors and Co-Workers: The Importance of Residential Labor Market Networks,” Journal of Labor Economics, 29(4): 659-695.\n\nHoxby, Caroline.  2003. School Choice and School Productivity (Or, Could School Choice be the Tide that Lifts all Boats?),\" The Economics of School Choice, University of Chicago Press, Chicago. \n\nJackson, Kenneth T.  Crabgrass Frontier: The Suburbanization of the United States. Oxford University Press, 1987.\n\nPiketty, Thomas and Emmanuel Saez. 2003. “Income Inequality in the United States, 1913– 1998,” Quarterly Journal of Economics, 118 (1): 1–41.\n\nSchmutte, Ian M. 2015. “Job Referral Networks and the Determination of Earnings in Local Labor Markets,” Journal of Labor Economics, 33(1): 1-32.\n\n1 Rajashri Chakrabarti, Andrew Haughwout, Giorgio Topa, Joseph Tracy and Wilbert van der Klaauw assisted in preparing these remarks.\n\n2 Piketty and Saez (2003) and Chetty, et al. (2014).\n\n3 Chetty, et al.  (2014).\n\n4 Chetty and Hendren (2015), Chetty, et al. (2014).\n\n5 Measured by income-adjusted test scores, drop-out rates, average class size and per-pupil spending.\n\n6 Chakrabarti and Roy (2015).\n\n7 Hoxby (2003), Chakrabarti (2008) and Chakrabarti (2013).\n\n8 Dobbie and Fryer (2011), Dobbie and Fryer (2015), Angrist, et al. (2016).\n\n9 Heckman (2006)\n\n10 Deming (2009).\n\n11 See Bayer et al. (2008), Schmutte (2015), Hellerstein et al. (2011), Aslund et al. (2011).\n\n12 Chetty et al. (2015)\n\n13 See Baum-Snow (2007) and Jackson (1987)."
    },
    {
        "title": "Community Banking in an Ever Changing World",
        "date": "Apr 11, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/cal160408",
        "content": "I am delighted to welcome you back to the Federal Reserve Bank of New York’s annual conference on community banking. For 2016, our staff chose “Community Banking in an Ever Changing World” as our theme. Each year, the amount of change we face seems only to increase, and that seems particularly true for community and regional banks since the end of the financial crisis. As we’ll discuss throughout the day, we’ve seen how the national and local economies have adapted and evolved as the economic recovery continues. We’ve experienced the onset of new regulations stemming from the Dodd-Frank Act and a new regulator—the Consumer Financial Protection Bureau. We’ve witnessed a renewed discussion on the culture within banks and the emphasis on setting the right “tone at the top” and filtering it down to all employees within a bank.\n\nWhen it comes to changes like these, we may think of change as something that we observe. The poster we designed for our conference illustrates that form of change. This photograph of the historic Trinity Church, just a few blocks from the Federal Reserve, was drawn from an internal photography contest we held for staff in the Regional, Community, Foreign Institution Supervision and Consumer Compliance Function last year. Trinity Church was founded in the late seventeenth century, though the steeple in the foreground dates back to the 1840s and made Trinity Church a dominant building in the area then. Yet by the early 1900s, the first “skyscrapers” arose around the church, such as the Trinity and US Realty Buildings—22 stories high—visible just behind the steeple in the photo. Of course the photo also captures the modernistic tower of the new One World Trade Center rocketing considerably higher to become the tallest building in the western hemisphere.\n\nSo certainly change can be something that we observe. But as community bankers, you are leaders in your communities. You are not merely observers of change. You can be critical agents of change as well. Usually it takes many to change the world. In rare instances, significant change can begin with just one person. And few people exemplify how much a single person can change the course of history than a former parishioner of Trinity Church, Alexander Hamilton—the Revolutionary War hero and first Secretary of the Treasury.\n\nIf you haven’t seen the astonishing show on Broadway yet, I’m afraid it may be easier to get your hands on a gold bar downstairs than on tickets to “Hamilton.” This unlikely musical tells the unlikely story of an orphan from the West Indies who became our “ten dollar Founding Father without a father.” Lin Manuel-Miranda, the composer and lyricist for the production, and Ron Chernow, the author of the biography that inspired the musical, breathe incredible depth and dimension into Hamilton and the other Founding Fathers (and Mothers) whom we otherwise know largely from marble statues and oil portraits.\n\nHamilton comes to life as someone who, as the hip hop influenced lyrics proclaim, “got a lot farther by working a lot harder, by being a lot smarter, by being a self-starter.” Through his passion, intelligence, determination, and sometimes sheer stubbornness, Hamilton became a courageous leader in the Revolutionary War and then the outspoken champion for the Constitution. His ideas transformed our young country from a “nation of states” into a single nation. As the musical unforgettably relates as well, he argued vigorously in favor of establishing a “national” or central bank at the heart of his design for our financial system. So we at the Federal Reserve can trace our origins in part to Hamilton’s vision for the U.S. economy.\n\nFew may change the course of history and the destiny of our country—and perhaps the world—as dramatically as Hamilton did. But as community bankers, you know that you can change the world in other important ways, especially for members of your communities.\n\nWhen you underwrite a young couple’s first mortgage, you are changing the world for them by helping to establish their first family home. When you extend the first line of credit to a budding entrepreneur, you’re changing the world not just for her, but also for others in the community she’ll hire. You are often far closer to your neighbors and regions than any other financial service providers. You consequently serve businesses and consumers whom other firms might otherwise overlook.\n\nSo rather than feeling like you are an observer of change, please remember that as a community or regional banker, you create change. To paraphrase one of the most upbeat songs from the musical, don’t throw away your shot. Be constructive agents of change for your communities.\n\nIn my remarks this morning, I’d like to address three themes for your consideration as you take stock of your own firm’s recent performance and prepare yourselves to catalyze change in the future.\n\nBefore continuing, I would like to remind you that I’m speaking for myself today. The views and musical preferences I express may not reflect those of the Federal Reserve Bank of New York or of the Federal Reserve System.1\n\nI. Recent Performance of Community & Regional Banks in the Second Federal Reserve District\n\nAsset Quality\nFirst, let’s talk about how community banks (banks with up to $10 billion in assets) and regional banks (banks with up to $50 billion in assets) have performed over the past year. The good news is that Second District bank managers in 2015 carried on the hard but necessary task of working out troubled assets—some of which predate the financial crisis—and have been writing them down or selling holdings such as defaulted real estate properties to third parties. In total, nonperforming assets in Second District community banks fell by 20 percent over the course of 2015, with similar declines in many of our regional banks, a favorable development.\n\nAt the same time, community and regional banks have sought new opportunities for lending. Asset expansion has been fortified in particular by pronounced increases in commercial real estate lending, which has outpaced the growth both in core funding sources as well as in capital levels.\n\nConsequently, many firms are increasingly dependent on noncore sources of funding, such as brokered deposits, which have historically been less stable sources of funding than core deposits. Similarly, many firms are experiencing reductions in their regulatory capital ratios given that assets, and especially commercial real estate-related lending, have increased at a much faster pace than capital. We have taken note of this trend because larger concentrations of commercial real estate lending relative to capital have been associated with a stunning increase in bank failures statistically. I’ll come back to this matter in my second theme.\n\nI should note that one part of our District is experiencing the opposite trend. Bank holding companies in Puerto Rico have been increasing their core deposits while decreasing their brokered deposits, and generally capital has grown faster than risk-weighted assets. These developments seem appropriate given the conditions in Puerto Rico today.\n\nEarnings\nThe sustained strengthening of asset quality in many firms in the Second District has kept earnings positive and generally stable, yet thin given the pressure of low interest rates on new loans. Moreover, competition for new lending may be adding to the pressure on rates and loosening lending standards: these trends should attract your attention as much as they do ours.\n\nLiquidity\nWhat’s funding the growth in lending? Second District community and regional banks have continued to rely largely on core deposits to fuel the increase in their assets over the past year. Still, core deposits growth has not kept pace with asset growth in our District. Banks in 2015 instead increased markedly their dependence on noncore funding, particularly brokered deposits.\n\nThis relatively sudden increase in noncore funding is a red flag for all. I’ll repeat the findings of an independent study2 that I had shared at last year’s conference, namely that community banks across the United States that were more dependent on core deposits—which tend to be “sticky” and more favorably priced—fared better during the financial crisis than did community banks that were more dependent on wholesale funding sources and were consequently more likely to fail.\n\nCapital\nThis brings us to the firms’ solvency. While capital ratios for Second District community and regional banks remain above regulatory requirements, the strong growth in lending has outpaced capital formation overall. From the first quarter to the final quarter of 2015, total assets when risk-weighted under U.S. capital guidelines grew by 13.6 percent. In contrast, the three measures of capital—Common Equity Tier 1 (CET1), Tier 1, and Total Capital—grew by only about 9.2 percent each.\n\nTo put this into context, the average Tier 1 risk-based capital ratio for community banks has fallen from 15.3 percent at the end of 2014 to 14.3 percent at the end of last year—a one hundred basis point drop. For mainland regional banks, the decline has been greater, as the Tier 1 risk-based capital ratio fell from 13.7 percent to 12.0 percent over the same time period. Since capital serves as both a source for future growth as well as the last line of defense against unexpected losses, lower capital ratios can imply curtailed opportunities for future growth—and potentially less protection against the unexpected. When we encounter a firm with declining capital ratios, we seek to understand why the ratio is falling and whether the existing stock of capital suffices for the firm’s risk profile.\n\nII. Commercial Real Estate Concentrations (CRE) and Supervisory Guidance\n\nIn that regard, changes in the risk profiles of some community and regional banks in our District are worthy of closer management and supervisory attention. This change may be most evident precisely in the significant growth some firms pursued in commercial real estate lending last year, which brings me to the second theme for my remarks today.\n\nThe renewed zeal for commercial real estate lending has not been matched with similar growth in capital. For example, community banks grew their total volume of CRE lending by nearly 26 percent over the last four quarters and by just over 61 percent over the last eight quarters. Regional banks were within nearly the same range, increasing their total volume of CRE lending by almost 34 percent over the past four quarters and by almost 51 percent over the prior eight quarters. Recall that the three key measures of capital have grown by only about 9.2 percent over the past four quarters.\n\nAs a result, aggregate CRE loans are now equivalent to 367 percent of Total Risk-Based Capital for community banks and 451 percent for regional banks in our District.\n\nConcentrations at these levels demand heightened management and supervisory attention because of the greater risk of failure. Research conducted in 2013 jointly by staff from the Federal Reserve and the Office of the Comptroller of the Currency found that firms with high concentrations of CRE lending tended to fail at vastly greater rates during the financial crisis compared to firms with lower concentrations.3\n\nIn particular, the researchers studied the failure rates of banks that exceeded certain thresholds for CRE lending concentration as defined in interagency guidance4 that the Federal Reserve, the OCC, and the FDIC issued just prior to the financial crisis in 2006. Under the interagency guidance, when banks meet or exceed one of two thresholds for CRE-related lending, supervisors set heightened expectations for those firms’ risk management and capital. The heightened expectations apply in either of the following situations:\n\nThe researchers found that 23 percent of all U.S. banks that exceeded both thresholds set out in the 2006 interagency guidance failed during the financial crisis. In contrast, only 0.5 percent of all other U.S. banks that did not meet either of the thresholds failed.\n\nMeeting just one of the two criteria was likewise associated with significantly higher rates of failure during the crisis: 21 percent of firms failed that had a CRE concentration over 300 percent and met the 50 percent growth rate over 36 months; 13 percent of firms failed that had a CLD concentration over 100 percent during the crisis.\n\nAs suggested in the aggregate statistics I cited, some community and regional banks in the Second District (and in other parts of the country) now already meet the supervisory threshold for CRE concentrations above 300 percent, and some of those firms furthermore meet the 50 percent growth measure over 36 months. Likewise, some firms already exceed the 100 percent threshold for CLD lending. I want to stress that these thresholds are not limits. Instead, when a firm approaches or breaches these thresholds, we look more closely at its risk management practices to manage exposures to this more cyclical asset class.\n\nConsequently, I urge you to refer to the guidance in SR 07-01 5 and supplemental guidance issued in SR 15-17 to evaluate where your firm stands with regard to either the 300 percent threshold for CRE lending concentrations or the 100 percent threshold for CLD lending concentrations. If your bank meets either threshold—or if it is trending upward toward those thresholds—please apply the guidance to ensure that you have established prudent board and management oversight over these concentrations. This includes the following considerations, among many others:\n\nI’ve highlighted only a handful of the steps that the two SR Letters suggest. For firms that are trending toward or have already exceeded either the CRE or the CLD threshold, you’ll be better prepared to answer our questions when you adhere to the expectations set out in the SR Letters. More importantly, you’ll create a better understanding of the significance of your concentrations and a better risk management infrastructure to administer them responsibly.\n\nIII. Other Risk Management Issues\n\nOn that note, our supervisory programs are designed to give us broad insight into the prudence and judgment of a firms’ senior management and board of directors. So I’d like to turn to the final theme for my remarks and highlight some of the key issues that our examiners identified in 2015 as requiring the attention of senior management and appropriate oversight by the board of directors.\n\nThese four sets of issues—establishing better internal frameworks to support compliance with the Bank Secrecy Act and anti-money laundering requirements; strengthening information security measures; enhancing the credit risk management infrastructure; and overhauling processes and systems used to manage compliance with consumer protection rules—resulted in the issuance of large numbers of “matters requiring attention” (MRAs) or matters requiring immediate attention (MRIAs) for community and regional banks in the District over the course of 2015.\n\nWhile each of these issues represents distinct areas of focus, one theme that ties many of them together is resources. Community banks—and some regional banks—may have relatively small numbers of staff and smaller sets of resources available to them compared to larger firms. Consequently, rather than seeking to manage all processes internally, firms may decide to outsource activities to help reduce the costs of compliance. As outsourcing has evolved beyond core bank processing and information technology, smaller firms and quite a few larger ones increasingly are outsourcing other internal activities, such as internal audit, accounting, loan review or loan servicing, or asset and wealth management.\n\nWhile a firm may decide to rely on a service provider to conduct some activities, its senior management remains responsible for ensuring that those activities are conducted in a safe and sound manner and in compliance with laws and regulations. The board of directors, in turn, must assure itself that management has developed an appropriate vendor management program. The Federal Reserve issued guidance in 2013 outlining the key risks involved when employing a service provider, what responsibilities the board and senior management have, and a broad overview of a framework and processes to manage the resulting risks. For those firms that are experiencing challenges with outsourced services—or may be contemplating hiring an external service provider in the future—this guidance is intended to help reduce the risk of supervisory actions, monetary losses, reputational damage, and litigation.6\n\nConclusion\n\nAs I noted at the outset of my remarks, we have no shortage of change in the world today.\n\nAs the economy continues to recover, some community and regional banks in our District have emphasized commercial real estate as a key part of their lending strategies, yet this emphasis is leading to an increasing dependence on noncore sources of funds and declining capital ratios.\n\nThat growth in CRE lending has furthermore increased the concentration of exposures to this more cyclical asset class. Given the elevated risk of bank failures associated with high concentrations of commercial real estate related lending, we set heightened supervisory expectations for ensuring adequate risk management and capital to offset those risks.\n\nFinally, other risk management issues that require attention relate to building an appropriate infrastructure to support compliance with the Bank Secrecy Act and anti-money laundering expectations; strengthening information security; improving the credit risk management infrastructure; and thinking more strategically about the overhaul of processes and systems that support compliance with consumer protection issues or other priorities.\n\nWith so much change and so many issues, it’s easy to get lost in the details. So please remember Hamilton’s challenge to you: don’t throw away your shot. Be an agent of positive change in your communities. We need you to continue to safeguard individuals’ hard-earned savings and to serve as intermediaries of credit, the lifeblood of the modern economy, to businesses and consumers in the region. When you do so prudently and in a manner that is sustainable financially and that is fair to consumers, you can better fulfill the community banker’s role as a leader in an ever-changing world.\n\n1 Ralph Acevedo, Deborah Arndell, Jacqueline Fenton, Bettyann Griffith, Jordan Light, Jonathan McLeman, and John Walsh commented on and provided input for these remarks.\n\n2 Financial Institutions: Causes and Consequences of Recent Bank Failures, United States Government Accountability Office, January 2013.\n\n3 An Analysis of the Impact of the Commercial Real Estate Concentration Guidance, Keith Friend, Harry Glenos, and Joseph B. Nichols, Office of the Comptroller of the Currency and Board of Governors of the Federal Reserve System, Washington, D.C., April 2013.\n\n4 Interagency Guidance on Concentrations in Commercial Real Estate, Board of Governors of the Federal Reserve System, (SR 07-01) Washington, D.C., January 4, 2007.\n\n5 Interagency Statement on Prudent Risk Management for Commercial Real Estate Lending, Board of Governors of the Federal Reserve System, (SR 15-17), Washington, D.C., December 18, 2015.\n\n6 Guidance on Managing Outsourcing Risk, Board of Governors of the Federal Reserve System, (SR 13-19, CA 13-12), Washington, D.C., December 5, 2013."
    },
    {
        "title": "The Theory and Practice of Supervision",
        "date": "Apr 11, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/sti160411",
        "content": "Good afternoon. I'm Kevin Stiroh, the head of the Supervision Group at the Federal Reserve Bank of New York. I'd like to thank SIFMA for the opportunity to speak today about the theory and practice of supervision.\n\nBy way of background, I took over my current role in October of 2015, after spending time in the New York Fed's Research, Markets and Integrated Policy Analysis Groups, as well as earlier positions within Supervision. I am an economist by training and I spent much of my career studying the performance and function of financial institutions in the U.S., the critical role of bank capital, and the potential link with financial stability.\n\nI'd like to cover three topics today related to the theory and practice of supervision. First, what is supervision? Second, why is it necessary? Third, what are some of the emerging issues in the execution of supervision?\n\nBefore beginning, let me emphasize that my comments today are my own and do not necessarily represent those of the Federal Reserve Bank of New York or the Federal Reserve System.\n\nWhat is Supervision?\n\nThe Federal Reserve has many responsibilities, most closely linked to the execution of U.S. monetary policy and the regulation and supervision of the U.S. financial system. In fact, the official title of the Federal Reserve Act includes the phrase \"to establish a more effective supervision of banking in the United States,\" so supervision has been a core responsibility of the Federal Reserve from the beginning.\n\nBut what does \"a more effective supervision of banking\" mean? At the highest level, the Federal Reserve's mission for supervision and regulation is to promote a safe, sound, and stable banking and financial system that supports the growth and stability of the U.S. economy and a fair and transparent consumer financial services market. That's a lot, so it is useful to drill down a little; I can do that by describing what we do on an ongoing basis at the Federal Reserve Bank of New York.\n\nAt the New York Fed, we operate under delegated authority to supervise the financial institutions in the Second Federal Reserve District, which encompasses New York, parts of New Jersey and Connecticut, the Commonwealth of Puerto Rico, and the U.S. Virgin Islands. This area includes enormous diversity in institutions from state member banks with total assets of less than $100 million operating in a local community to bank holding companies with trillions of dollars of assets operating on a global scale.\n\nOur staff assesses the safety and soundness of domestic banking institutions and operations of foreign banking organizations in the district through onsite evaluations and offsite financial analysis and surveillance. One of our fundamental responsibilities is to ensure that each institution has in place the appropriate risk identification and risk management processes that are necessary for prudent banking. We also analyze issues and industry developments to identify emerging risks and to contribute to the development of domestic and international supervisory policy. I'll return to a discussion of emerging risks later in my talk.\n\nThe Supervision Group at the New York Fed works with our colleagues around the Federal Reserve System to develop and implement the System's supervisory programs. This includes working through governance structures like the Large Institution Supervision Coordinating Committee (LISCC) that oversees supervision for the largest, most systemically important institutions in the U.S. We are also involved in similar System-wide committees for other types of institutions such as community banks, regional banks, foreign banks, and other large banking institutions, as well as consumer protection. I'll speak mostly today about our supervisory responsibilities for the largest firms, but will note that we recently hosted a conference at the New York Fed focusing on the role of community banks.\n\nFor the largest firms, we actively participate in System-wide horizontal examinations such as the Comprehensive Capital Analysis and Review, or CCAR, which is the annual process for evaluating capital adequacy of the largest firms that began last week. We are actively involved in other horizontal programs such as the Comprehensive Liquidity Analysis and Review (CLAR), and the Supervisory Assessment of Recovery and Resolution Preparedness (SRP). CLAR is the Federal Reserve's annual, horizontal, forward-looking program to evaluate the liquidity position and liquidity risk management practices of LISCC firms. SRP is the Federal Reserve's annual horizontal review of the LISCC firms' options to support recovery and progress in removing impediments to orderly resolution. These three programs form the foundation for the horizontal aspects of the System's supervisory program for the largest firms. The Supervision Group at the New York Fed plays a strong role in developing and executing these initiatives.\n\nIn parallel to the horizontal work for the largest firms, the Federal Reserve has a dedicated supervisory team for each of the largest firms. These teams execute supervisory strategies for each firm that align with Federal Reserve System priorities. Through the execution of firm-specific supervision, these dedicated supervisory teams play a critical role in helping to achieve these objectives. Given the location of the headquarters of many of the largest firms in New York, the Supervision Group at the New York Fed plays a central role within the Federal Reserve System in the execution of supervisory activities through the dedicated supervisory teams.\n\nTo conclude this section, I'll note that my colleagues in the Research Group at the New York Fed recently hosted a conference entitled \"Supervising Large Complex Financial Institutions: Defining Objectives and Measuring Effectiveness.\"1 This conference brought together academic economists, policymakers and senior supervisors to talk about the goals and objectives of supervision and different approaches to assessing supervisory success and effectiveness. While this conference focused on the largest institutions, the main themes apply more broadly to the objectives of supervision.\n\nOne important theme of the conference was the conceptual distinction between regulation and supervision. Both are designed to help the Fed achieve its objectives and they are mutually reinforcing, complementary efforts. Broadly speaking, regulation involves writing the rules that govern what financial institutions can and cannot do. Supervision focuses on monitoring, oversight and enforcing compliance with law, and supervisory expectations for firms' governance, internal processes and controls, and financial condition.\n\nWhy Is Supervision Necessary?\n\nI think it is useful to take a step back and talk about what motivates the need for public sector intervention in the form of ongoing supervision. Why doesn't the market provide sufficient discipline on financial intermediaries as is presumed for some other industries? In the language of an economist, what are the frictions that lead to a market failure so that financial institutions, left to their own incentives and choices, won't necessarily make optimal choices from a societal perspective?\n\nI'll talk about two types of market frictions that create the need for public sector intervention. The first is asymmetric information—the difference in what the borrower knows about an investment opportunity and what the lender or funder of that investment knows. The second is externalities—the ability of one firm's decisions or actions to affect other unrelated market participants. Both of these forces can drive a wedge between private and socially—desired outcomes and motivate the need for public sector intervention. Conceptually, either supervision or regulation can help solve these problems and their relative efficacy depends, in part, on the type of information being assessed or the problem being solved.\n\nThere is considerable academic research describing how asymmetric information can create a special role for financial intermediaries. The basic issue is that lending requires the production of private information about things like the probability of a successful investment outcome, but this can be difficult to convey to third parties who might be providing the funding. This creates a role for a financial institution to bridge the gap between borrowers and savers.\n\nMoreover, these information asymmetries create the scope for two problems endemic to credit markets: adverse selection where the riskiest borrowers are the most likely to seek credit and moral hazard where borrowers pursue riskier behavior after credit has been extended because their own funds are not invested. In both cases, mitigation requires costly screening and monitoring, which creates a role for a specialized financial intermediary.\n\nAsymmetric information also creates issues on the liability side of a bank's balance sheet. The banks' creditors—depositors or providers of funding—don't have as much information about the health of the institution as insiders do. This asymmetry makes banks opaque and inherently fragile, which can subject them to runs. This type of \"run risk\" has been largely mitigated, at least in the case of retail depositors, by deposit insurance, access to lender of last resort facilities, and other elements of the bank safety net. Importantly, however, a broad safety net weakens market discipline of banks and can create incentives for excessive risk-taking. This contributes to the need for risk-sensitive regulation and supervision.\n\nRegulations can control risk-taking through a variety of means such as setting minimum requirements for capital and liquidity positions, while supervision can identify and constrain activities that are not well-captured through regulation but still affect a firm's risk management, governance and control infrastructure. One common objective is to counter the incentive for excessive risk-taking by intermediaries.\n\nExternalities\n\nA second and distinct type of market failure that creates a need for public sector intervention stems from the externalities that the failure of a large, systemically important bank failure can have on the rest of the financial sector or the economy as whole. For example, the financial crisis vividly demonstrated how the distress of a large financial firm can destabilize broader financial markets and impact other financial firms. These effects can be seen through direct linkages via counterparty exposures. They can also be seen through less direct linkages such as confidence-related behaviors that are manifest in higher haircuts and margins or reluctance to do business with certain firms or types of firms, uncertainty about the true health of a firm, or the impact of fire sales on the valuation of a broad class of assets held by a broad range of investors. The fact that these costs are external to a given financial firm can lead to excess risk-taking.\n\nThe realization of these externalities during the financial crisis has had a profound impact on the Federal Reserve's approach to supervision. Most specifically, they provided the practical experience to support the conceptual case for tightened prudential requirements for the largest, most systemically important institutions. This focus is seen in the mandate from the Dodd-Frank Act for the Federal Reserve to implement enhanced prudential standards, including higher capital standards, to mitigate the risks posed to financial stability by systemically important financial institutions.\n\nFor example, both the Basel Committee for Banking Supervision and the Federal Reserve System have identified factors that contribute to a financial firm's systemic importance through these types of externalities. These mechanisms are proxied for by observable metrics such as the size of the balance sheet, interconnectedness between firms, complexity of the business, cross-jurisdictional activity, short-term wholesale funding, and substitutability of certain activities. These metrics then determine the capital surcharge for systemically important banks as outlined in the Federal Reserve rule and white paper from July 2015.2\n\nMore generally, these types of differences across firms provide one of the conceptual underpinnings for differentiating supervisory expectations for different types of financial institutions. This concept—often referred to as \"tailoring\"—has long been part of the supervisory approach, and has received considerable attention recently. It is reflected in our organizational structure as seen in things like the LISCC and in application of supervisory guidance across different types of firms.\n\nAs a specific example, the Federal Reserve issued two Supervisory and Regulation letters in December of 2015 that consolidate previously-issued guidance on capital planning and more clearly differentiate supervisory expectations between financial institutions with more complex operations and those with less complex operations. This tailoring of guidance applies to capital planning expectations related to governance, risk management, internal controls, capital policy, scenario design and projection methods. This guidance further clarifies how capital planning expectations are moderated for firms that pose more limited risk to financial stability.\n\nI've discussed some of the conceptual underpinning of supervision and I'll now turn to some of the practical implications and issues that I'm thinking about.\n\nWe continue to strive to provide clarity and transparency on our supervisory expectations and concerns. This can be seen through a variety of mechanisms, including public discussion and private communications with supervised firms. I've mentioned some of the most useful sources of information such as Supervision and Regulation letters that provide public guidance on specific topics to supervised firms, rules and white papers that outline and implement the underlying conceptual thinking on key issues such as the surcharge for systemically important banks or the expectations around CCAR, and public speeches by policymakers.\n\nOf course, we engage regularly in private communications with supervised firms. This is both to provide feedback on a firm's progress relative to supervisory expectations and to gain information about the developments in the financial industry and the efficacy or our supervisory programs. We do this on a regular and continuous basis through both formal and informal channels, and this is a key part of the supervisory program for firms of all sizes.\n\nI also want to highlight three specific developments that are relevant for us as we execute our supervisory responsibilities. I expect these topics—cybersecurity, fintech and reputational risk—will be familiar and I'll provide a supervisory perspective on each.\n\nThe Federal Reserve, in its supervisory role, collaborates with the Federal Financial Institutions Examination Council (FFIEC) to coordinate work on enhancing cybersecurity supervisory programs.  We are also interested in working with the industry on a coordinated approach for cybersecurity to strengthen the resiliency of the financial services sector against cyberattacks.\n\nInternally, the Federal Reserve has organized a supervisory program on cybersecurity that consists of subject matter experts from across the Federal Reserve System. Work is underway to enhance the analysis of IT examination data in conjunction with threat information to enhance risk-focused reviews during examinations.\n\nThrough our discovery work with financial institutions, we have found that the state of cybersecurity readiness varies across firms. Supervisors are seeking to develop common principles to address cybersecurity issues and will work with other agencies to ensure that firms are dedicating resources and taking the appropriate actions to defend the operations that are critical to the financial services sector against cyber threats. This is particularly true for those risks that could be channeled through interconnected organizations and that could potentially have an impact on other firms and more broadly across the sector.\n\nFintech\n\nThe financial sector, like the broader U.S. economy and society, is becoming increasingly digital and there is considerable discussion of \"fintech\"—defined simply as the intersection between financial services and new technologies. A recent industry study, for example, reports that investment in fintech companies grew from $1.8 billion in 2010 to $19 billion in 2015.3 It is unclear, however, whether fintech will enhance or fundamentally disrupt the financial service and payments industries, or perhaps it is some combination of both. In any of these cases, however, supervisors will have a keen interest in monitoring the impact on the business models and risks of financial institutions and the way financial intermediation is done.\n\nWe observe financial firms engaging in a variety of ways with fintech companies. This can be through the development of in-house projects such as setting up innovation labs or encouraging developers to code on open application program interfaces (APIs). This can also be through collaboration with industry consortiums to come up with unified industry standards that enhance operability and enable future technological advancements, or it can be through partnering with or investing directly in fintech companies.\n\nSome of the interesting questions for supervision revolve around things like: what new benefits might these new technologies bring? Will they enhance existing processes or bring about entirely new solutions to existing financial sector problems? Will they create risk, mitigate risk, or simply reallocate existing risks? How should the current supervisory framework evolve to incorporate these types of developments?\n\nThe New York Fed is assessing the implications of fintech developments through participation in a wide range of Federal Reserve System, interagency, and international efforts. In addition, we meet regularly with supervised firms, fintech companies, and industry groups to follow these developments. Our supervisory program will continue to evolve with the underlying technologies.\n\nReputation Risk\n\nOver the past few years, the New York Fed has been challenging the financial industry to consider the many factors that have contributed to recent, widespread misconduct and the perceived lack of trust in the financial sector.\n\nThe various forms of misconduct impose direct costs such as fines, legal fees in investigating allegations and defending lawsuits, and internal monitoring costs that should matter to firms. In this view, a principal benefit to a financial firm of having a strong culture that builds trustworthiness is the avoidance of the bad behavior that can carry significant monetary costs and can inflict destructive damage to the organization's reputation. Firms should have every incentive to manage and internalize those potential costs.\n\nMore broadly, there may be additional adverse effects on other firms or the financial sector as a whole if there is a widespread lack of trust in financial services. Similarly, my colleague Michael Strine has argued that the financial industry exists, in part, to enhance the public good and thus faces a higher degree of social responsibility due to its role in supporting the broader economy.4 Both of these effects suggest a role for the public sector in facilitating appropriate behavior and conduct in the financial services industry. Supervision can contribute to this through support for the development of effective internal governance regimes, prudent risk management policies, and strong compliance and control structures, all within a framework of effective oversight from the Board of Directors.\n\nTo conclude, I've spoken today about the conceptual rationale for public sector intervention through supervision and some practical issues related to how we execute on our supervisory responsibilities at the Federal Reserve Bank of New York. Our goal is to promote a safe, sound, and stable banking and financial system that supports the growth and stability of the U.S. economy, and we will continue to strive to meet these responsibilities.\n\nThank you very much for your attention this afternoon. I'd be happy to take some questions.\n\n1 Supervising Large, Complex Financial Institutions: Defining Objectives and Measuring Effectiveness\n\n2 Calibrating the GSIB Surcharge, July 20, 2015, Board of Governors of the Federal Reserve System.\n\n3 \"Digital Disruption: How Fintech is Forcing Banking to a Tipping Point,\" March 2016.\n\n4 Forming the Next Generation of Bankers: The Future of Business Education and Ethics, Michael Strine, March 22, 2016."
    },
    {
        "title": "The National and Local Economic Outlook: An Update",
        "date": "Apr 8, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud160408",
        "content": "Good morning.  I am very pleased to be in Bridgeport and to speak with you today.  I would like to give a special thanks to the University of Bridgeport for their part in making this event possible.\n\nMy visit today is part of our ongoing efforts at the Federal Reserve Bank of New York to better understand the regional economy.  We plan trips like this so that we can meet with a diverse array of stakeholders in the region to gain insights and perspectives on the local economy.  These trips also allow us to hear from Main Street about the major issues and concerns affecting people and businesses in the area.  Fairfield County is our first destination in 2016, and this is the second time we have visited the area in the past three years.\n\nI am looking forward to meeting with a number of different groups today. After starting our day here at the University of Bridgeport, we will meet with an organization called The WorkPlace to hear about some of the nationally-recognized programs they have successfully developed to help the long-term unemployed find jobs.  Long-term unemployment is a significant problem in the broader economy, and something that I will discuss in more detail later in my remarks.  Next, we will take a tour of the American Job Center, an organization that provides workforce assistance to job seekers in conjunction with local businesses.  Later we will attend, a roundtable lunch with members of the Bridgeport Regional Business Community.  And, finally, we will meet with leadership from the Housing Development Fund, New Horizons and NeighborWorks America to hear about the state of affordable housing and community development in Fairfield County.\n\nThese trips are valuable to us, and represent one of the many ways in which the New York Fed engages with people and businesses in our region.  Another one of our efforts in this vein is our Small Business Credit Survey―the latest version of which was released last month.  This survey―which includes responses from across 26 states―helps us understand the challenges facing small businesses in the region, focusing on financing needs and impediments to obtaining credit.  We partnered with multiple organizations throughout Connecticut to produce this survey―some of which are right here in Bridgeport―and received responses from nearly 200 small business owners from around the state.  I’d like to personally thank those who participated in the survey.\n\nThe remainder of my remarks will focus on recent developments in the national and local economy.  As always, what I have to say today reflects my own views and not necessarily those of the Federal Open Market Committee (FOMC) or the Federal Reserve System.1\n\nU.S. Economic Outlook\n\nDespite the swings we have seen in financial markets over the past few months and the recent divergences across different economic and financial market indicators, recent developments have not led me to make a fundamental change in my outlook for the U.S. economy.  I continue to expect that the economy will expand over the course of this year at a pace slightly above its long-term trend—sufficient to push the unemployment rate down a bit further and to more fully utilize the nation’s labor resources.  On inflation, although recent data have been somewhat firmer, it still appears that inflation for this year will fall short of our 2 percent objective for the personal consumption expenditure (PCE) deflator.  However, I anticipate an eventual return to our objective as the transitory factors that have held down inflation dissipate over time.\n\nIn assessing the current state of the U.S. economy, recent news has generated divergent signals.  Several sectors have been showing signs of softness.  Real consumer spending growth appears to have moderated somewhat from the relatively robust pace of the second half of 2015.  Both new and existing home sales have flattened since the middle of last year.  Finally, indicators of real business investment spending point to continued softness.  In contrast, manufacturing production—which had been a particular weak spot of the U.S. economy in 2015—rose in the first two months of this year.  Recent survey data, including our own Empire State Manufacturing Survey, indicate further improvement in conditions for the manufacturing sector in March.\n\nThrough it all, the U.S. labor market has remained healthy.  Payroll employment increased an average of about 229,000 per month over the course of 2015, and the gains in the first quarter of this year were nearly as high.  Even though the unemployment rate ticked up in March to 5 percent, the amount of slack in the labor market still appears to be diminishing.  Both the share of the population that is employed and the labor force participation rate have risen a bit over the past several months, with the employment-to-population ratio now at its highest level since the end of the recession.  However, measures of aggregate wage growth have remained quite subdued, which suggests there is still some slack in the labor market.\n\nAn important question is how to reconcile these cross-currents.  I continue to anticipate that consumption and housing activity will expand at a moderate pace this year.  Continued job and wage gains, combined with still-low energy prices, should sustain real disposable income growth and support consumer spending.  The housing market should remain on a solid trajectory, supported by rising employment and low mortgage rates.  In contrast to previous years, fiscal policy should also provide a moderate stimulus this year.  I believe that these positive factors will be sufficient to offset weakness in other areas, such as net exports and fixed business investment that will continue to be adversely impacted by the still-strong dollar, weak foreign growth, and low energy and commodity prices.\n\nPutting this all together, I expect real GDP growth of about 2 percent in 2016, slightly below the average pace of growth in this expansion, but a bit above my estimate of the potential growth of the U.S. economy.  If this materializes, then we should see some further reduction in the unemployment rate to around 4¾ percent—my estimate of the rate that I view as consistent with stable inflation over the long term.\n\nTurning to the outlook for inflation, on a year-over-year basis, both headline and core inflation have recently risen above the low levels that prevailed through most of 2015.  Nevertheless, inflation still remains below the Federal Reserve’s 2 percent objective.  As the FOMC noted in its March statement, this continued low inflation is partly due to declines in energy prices and non-energy import prices.  Although energy prices have stopped falling and the dollar has stopped appreciating, earlier movements in these factors still appear to be weighing on inflation.\n\nA concern with this long period of low inflation is that it has the potential to reduce inflation expectations, which, in turn, would tend to continue to hold down inflation.  As past experience shows, it is difficult to push inflation back up to the central bank’s objective if inflation expectations fall meaningfully below that objective.  Japan’s experience is cautionary in this regard.\n\nRecent evidence on inflation expectations suggests some firming, but there is still cause for concern.  Although both measures of longer-term inflation compensation based on nominal and inflation-indexed Treasury securities as well as some survey measures of inflation expectations have risen modestly since mid-February, the levels of many of these measures remain quite low.\n\nNow, as I have noted, the low level of market-based inflation compensation probably reflects factors besides inflation expectations, in particular lower term premiums.  Still, it would be prudent to consider the possibility that longer-term inflation expectations of market participants may have declined somewhat.  Even after a rise in its most recent reading, the median of the three-year inflation expectations from the New York Fed’s Survey of Consumer Expectations remains near its lowest reading over its short history.  Renewed declines in survey measures would be worrisome.  However, at this point, my conclusion is that inflation expectations have not become unanchored.\n\nIn sum, I anticipate that the combination of decreasing resource slack and anchored longer-term inflation expectations will help push inflation up to our 2 percent objective over the medium term.  The recent rise in inflation and in measures of inflation expectations have increased my confidence around this outlook compared to earlier in the year, but it is still possible that the return of inflation to our objective could take longer than I anticipate.\n\nThe forecast that I have just described is my best assessment of how the U.S. economy will evolve this year.  As always, there is uncertainty and risk relative to this forecast, which also needs to be taken into consideration in assessing the implications of the outlook for monetary policy.  I see the uncertainties around my forecast to be somewhat greater than usual.  This assessment reflects the divergent economic signals I highlighted earlier.  In addition, the factors behind the financial turbulence we saw earlier this year do not yet appear to be resolved fully.\n\nAlthough the downside risks have diminished since earlier in the year, I still judge the balance of risks to my inflation and growth outlooks to be tilted slightly to the downside.  The low levels of energy and commodity prices may signal more persistent disinflationary pressures than I currently anticipate, while renewed tightening of financial market conditions could have a greater negative impact on the U.S. economy. Also, there is significant uncertainty about economic growth prospects abroad and how this will affect the U.S. economic outlook.  I continue to monitor global economic and financial market developments closely to assess their implications for the outlook.\n\nGiven my outlook and risk assessment, I judge that a cautious and gradual approach to policy normalization is appropriate.  Moreover, caution is also called for because of our limited ability to reduce the policy rate to respond to adverse developments, recognizing that we could also use forward guidance and balance sheet policies to provide additional accommodation if that proved warranted.  Of course, the trajectory of short-term interest rates and the timing of future monetary policy adjustments will continue to be informed by the incoming data—both economic and financial—and how that data influences the outlook.\n\nThe Problem of Long-Term Unemployment\n\nBefore turning to the outlook for the local economy, I would like to spend a few minutes talking about the problem of long-term unemployment. The Great Recession was very severe, resulting in a large decline in output and damage to the labor market.  From peak to trough, non-farm payroll employment declined by 8.7 million.  The unemployment rate peaked at 10 percent.  At the same time, the average duration of unemployment increased significantly.  The number of individuals classified as long-term unemployed—that is, those who were unemployed for at least 27 weeks—peaked in December 2010 at 6.4 million, an increase of 5.3 million from the cyclical low in May 2007.  Long-term unemployment as a fraction of total unemployment increased from 16.7 percent to 44.8 percent.  The hardships of prolonged unemployment touched many families around the country and this is tragic.\n\nThe very accommodative monetary policy that we implemented in response was aimed at helping to heal the labor market and promote price stability.  Slowly, the economy began to respond and to recover.  In October 2014, the FOMC announced the end of its latest asset purchase program citing that “there has been substantial improvement in the outlook for the labor market since the inception of its current asset purchase program.”  In March 2015, the FOMC set out conditions for the decision to begin to normalize monetary policy, stating “The Committee anticipates that it will be appropriate to raise the target range for the federal funds rate when it has seen further improvement in the labor market and is reasonably confident that inflation will move back to its 2 percent objective over the medium term.”  The FOMC judged at its December 2015 meeting that those conditions had been met.\n\nThe improvement that we have seen in the labor market is very welcome. It took until May 2014 for the labor market to recover the jobs lost in the recession, but now there are 5.3 million jobs beyond the prior cyclical peak.  The unemployment rate has declined to 5.0 percent. With this improvement, the number of long-term unemployed has been reduced to 2.2 million.  Despite these gains, it is important to emphasize that more needs to be done to improve the labor market.  The current number of long-term unemployed is still higher than the prior cyclical peak in June 2003, and is more than a million above the prior cyclical low in October 2006.\n\nFurther progress on transitioning these unemployed workers back into jobs requires sustained effort on many fronts.  Essential to this effort are programs such as the Platform to Employment developed by The WorkPlace.  I am looking forward to my visit there later today to hear more about this program.  The Platform to Employment program has been replicated in nearly 20 communities across the country from Newark to San Diego.  Helping the long-term unemployed regain employment needs to be a priority for all of us.\n\nThe ongoing challenge in reducing the number of long-term unemployed is also a reminder of the importance of financial stability and sustainable economic growth.  Recessions can quickly undo years’ worth of hard-earned progress in the labor market.  Making the financial system and the economy more resilient so we can avoid such deep downturns in the future is a responsibility that I take very seriously.\n\nLocal Economic Outlook\n\nTurning now to the region, while Fairfield County’s economy has recovered from the Great Recession, its job growth has lagged behind both New York City and the nation.  Only recently has employment approached its pre-recession levels, and it is still well shy of where it was back in 2000.  Other barometers of the Fairfield County economy, such as home prices, also indicate a slow recovery from the recession.  There are still a sizable number of properties that are either in foreclosure or up for foreclosure sale, which has weighed down the local housing market.  Yet, Fairfield County has not been the only place in the metro region to see a sluggish recovery—notably, both northern New Jersey and the Mid-Hudson Valley have yet to see employment return to their pre-recession peaks.\n\nThis picture is in contrast with New York City’s economic performance, which has been exceptionally strong throughout this expansion, and has been a driver of growth for the whole tri-state region.  Why is New York City seeing so much stronger job growth than Fairfield County?  After all, it wasn’t so long ago that Stamford was attracting finance-sector jobs away from Manhattan.  The answer lies in a mix of cyclical and secular trends. On the cyclical side, while finance employment has been sluggish in both New York City and Fairfield County, New York City is benefiting from strong employment gains in technology, construction, retailing and advertising—sectors that are not contributing to the same degree in Fairfield County.  In addition, Stamford has had to deal with the relocations of UBS and RBS, while Fairfield faces the prospective relocation of GE.\n\nOne potential long-term trend that is likely affecting the region is a renewed urbanization: that is, a gradual but broad-based shift of preferences to urban living, especially for younger generations.  Throughout the second half of the 20th century, we saw a steady and widespread migration from cities to suburbs.  More recently, this suburbanization trend seems to have halted; and there are signs that cities, especially major cities, have been gaining relative to their suburbs.  It’s too early to say if this is the start of a secular trend, and the implications for Fairfield County are unclear.  However, Stamford and Bridgeport stand to benefit from a shift in preferences toward more urban living, given their high population densities and strong transit links to New York City.\n\nEven now, while Fairfield County may have lagged in job growth, population growth has remained quite sturdy—not only across the county, but here in Bridgeport as well.  In fact, Bridgeport’s population has grown by about 6 percent since 2000—its first sustained increase since the 1940s —while Stamford’s has grown by almost 10 percent.  \n\nFairfield County can leverage its proximity to the New York City job market.  While most residents of Fairfield County rely on job opportunities locally, many residents commute to jobs in Manhattan.  So the strength in New York City’s economy should be of significant help to Fairfield County.  Over the past year, New York City’s economy has, on average, added more than twice as many jobs each month as the total expected job losses from the relocations of GE, UBS and RBS.  While a majority of these jobs may not have been in high-paying-industry sectors, some of them were—especially in technology-related industries.  So, as long as New York City keeps adding jobs at such a brisk pace, Fairfield County will benefit.  In addition, research indicates that a strong central city is critical for the well-being of its suburbs, meaning that a healthy and growing New York City economy is critically important to Fairfield County.\n\nWhat else can the region do to foster economic growth going forward?  It certainly helps for cities like Bridgeport and Stamford to be attractive locations for businesses to locate and grow.  It is also important to focus on quality of life issues that make people want to live here—like good schools and low crime, and also amenities ranging from nice parks to good restaurants.  Bridgeport has clearly been making progress in this regard, both along the waterfront at Steelpointe Harbor and along Main Street.  With New York City’s large and booming economy nearby, and Metro North providing a convenient mass transit link, anything that makes people more inclined to live here—and perhaps even set up shop here—is bound to boost the local economy.  And finally, one of the most important ingredients for a healthy urban economy is a well-trained and motivated work force.  It cannot be overstated how important it is to help provide residents with the skills and tools they need to be productive and successful, so that they can realize their full potential.\n\nCollege Graduates Finding Good Jobs\n\nI would like to close with a message for the students in attendance today, especially those of you who will be graduating soon.  If you started college four or five years ago, you may have heard about the difficult time that graduates have had finding good jobs, perhaps even making you wonder about the value of obtaining a college degree.  Indeed, in the years following the Great Recession, unemployment and underemployment among recent college graduates reached highs not seen in decades.  So, it wouldn’t be surprising if many of you are concerned about your ability to find a good job after graduation.\n\nI want to make two points that should temper these concerns.  First, it is not unusual for college graduates to take some time to find the right job when they transition from school to the working world.  In good economic times, as well as in bad times, there is often an adjustment period as newly-minted graduates search for a job that best fits their skills and interests.  Second, as the job market has continued to strengthen through the expansion, it has gotten easier for those graduating from college to find a good job.2\n\nWhile job opportunities for those with a college degree were flat between 2011 and early 2014, demand for college graduates has been on the rise for about two years now.  As a result, for the first time during this recovery, both unemployment and underemployment among recent college graduates has generally been falling.  This means that, not only has it become easier to find a job upon graduation, but more college graduates are taking jobs that better utilize their degrees.  In addition, starting salaries for recent college graduates have been increasing for the past couple of years, reflecting the growing demand for their skills in a tighter job market.\n\nTo conclude, while there are economic challenges facing both the nation and the region, I am optimistic that we will continue to make progress on both fronts over the next few years.  Achieving our dual mandate of maximum sustainable employment and price stability will help those who will graduate soon to achieve their own aspirations.\n\nThank you for your kind attention.  I would be happy to take some questions.\n\n1 Jaison Abel, Jason Bram, Tony Davis, Richard Deitz, Jonathan McCarthy and Joseph Tracy assisted in preparing these remarks.\n\n2 We have been tracking these developments using data our economists have developed to help us better understand the job prospects facing recent college graduates. These data are available on our website and are updated regularly. See The Labor Market for Recent College Graduates."
    },
    {
        "title": "The Role of the Federal Reserve—Lessons from Financial Crises",
        "date": "Mar 31, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud160331",
        "content": "It is a great pleasure to have the opportunity to speak here today as part of the Virginia Association of Economists annual meeting at Virginia Military Institute and Washington and Lee University.  This is an appropriate setting for the topic I will be addressing—the role of the Federal Reserve as the central bank of the United States.  When the Federal Reserve Act was enacted in 1913, H. Parker Willis, who had been a professor at Washington and Lee University, played a critical role.  He worked closely with Representative Carter Glass of Virginia in crafting the legislative proposal that established the Federal Reserve, and Willis became the first Secretary of the Federal Reserve Board in 1914. \n\nOf course, I’m tackling this subject today not just because H. Parker Willis was a professor here in Lexington.  Instead, I’m addressing this issue because of the ongoing debate about the role of the Federal Reserve and its structure and governance.  My purpose is to demystify the nation’s central bank and to respond to some of the critiques that we continue to face.  I see this as necessary because there is a risk that the Federal Reserve could be changed in ways that might impair our ability to achieve our primary objectives—namely, full employment and price stability.  As always, what I have to say today reflects my own views and not necessarily those of the Federal Open Market Committee or the Federal Reserve System.1\n\nThe current debate surrounding this nation’s central bank is not new, but rather dates back to our nation’s independence from Great Britain.  The notion of a central bank in the United States has nearly always been controversial—in concept, design and practice.  So much so that the charters of the first two U.S. central banks, the First and Second Banks of the United States, were allowed to lapse. \n\nThe United States, in contrast to many of our European counterparts, had no central bank from 1836 to 1913.  During this period, the economy was prone to financial panics.  The rapid development of the 19th century American economy, including the westward expansion made possible by railroads, outstripped the ability of the private banking system to satisfy the nation’s needs for an elastic currency and a stable supply of credit. Ultimately, the Panic of 1907―in which J.P. Morgan, the leading financier of the day, played an outsized role in responding to the crisis―was the catalyst that turned the tide back in favor of establishing a central bank. \n\nThe debate preceding the passage of the Federal Reserve Act in 1913 was protracted and intense.  Early drafts of reform proposals were careful not to even mention a “central bank”—the 75-year-old scars from the Second Bank’s failed reauthorization battle had still not fully healed.  Although there was a strong recognition that the country needed a central bank to forestall and mitigate financial panics, there was considerable disagreement about how it should be structured.  Important considerations included what role should be played by private bankers versus government officials, how centralized the new bank should be and the extent of its powers.2  \n\nDuring this period, the U.S. economy was susceptible to financial panics.  Lacking a central bank, the country had no reliable lender of last resort that could provide currency and credit on demand against high-quality assets.  Banks had to self-insure against bank runs by holding a high fraction of deposits as cash, and by entering into private liquidity agreements with other banks.  Self-insurance was not only expensive relative to the cost of a public backstop provided by a central bank, but it was also limited in its effectiveness.  When the business cycle turned down and financial stress intensified, bank runs often ensued as bank customers rushed to convert their deposits into cash.  Without a true lender of last resort, cash that banks individually and collectively kept in reserve often proved inadequate to meet demand.  At that point, a bank under duress had to either close its doors or quickly sell assets.  Of course, either action worked to intensify the panic and to increase the stress on the economy.\n\nThe Federal Reserve was established so that there would be a lender of last resort that could lend against high-quality collateral and provide a reliable and elastic currency.3  The knowledge that such a lender of last resort existed reduced considerably the incentives for depositors to race to their banks to make withdrawals at the first sign of trouble.4   At that time, the provision of an elastic currency was especially important because the agricultural sector represented a significant proportion of the U.S. economy.  As a result, the demand for credit increased significantly each fall when farmers incurred the costs associated with harvesting their crops.  Prior to the establishment of the Federal Reserve, accommodating this seasonal demand necessitated a flow of currency and credit from the major financial markets such as New York and Chicago that put upward pressure on interest rates.  With its ability to provide an elastic currency, the Federal Reserve would be able to dampen such seasonal swings in interest rates. \n\nAt its inception, the Federal Reserve differed from central banks in Europe in that it was decentralized.  The design reflected long-standing concerns in our country about the concentration of power in a single authority. These concerns are reflected in the federated system established by the Constitution in 1787.  Similarly, the design adopted for the central bank in 1913 was also federated, with twelve independent Reserve Banks, each with its own capital and board of directors, overseen by a seven member Federal Reserve Board in Washington, with the Secretary of the Treasury serving as chair of the Board.  This decentralized structure allayed the prevailing fear of concentrating power in either New York or Washington and, especially within the banking community, of ceding too much power to the federal government.\n\nPaul Warburg was an American banker who had emigrated to the United States from Europe.  He was deeply involved in the debate about the appropriate structure of the central bank and described the concern as follows:\n\nThe view was generally held that centralization of banking would inevitably result in one of two alternatives: either complete governmental control, which meant politics in banking, or control by “Wall Street,” which meant banking in politics.5\n\nImportantly, the Federal Reserve was designed to have budgetary independence from the federal government.  The System funds itself through its business operations, and not through Congressional appropriations.6  This formed an important foundation in ensuring that the conduct of monetary policy would be independent from political influence.  A wide range of research has shown that central banks achieve better outcomes with respect to employment and inflation when they are insulated from short-term political pressures in their conduct of monetary policy.7  \n\nThe Federal Reserve System was soon tested, first by the Depression of 1920-21, and then by the Great Depression.  The searing experience of the Great Depression, including the 1933 Banking Holiday and thousands of bank failures, revealed significant shortcomings.  This led to the Banking Act of 1935, which made two important changes—establishing a federal deposit insurance system and making the Federal Reserve System more centralized.  The Great Depression made it clear that having a lender of last resort was not sufficient for preventing bank runs.  Deposit insurance, which had been debated but not adopted during the drafting of the Federal Reserve Act, was instituted with the creation of the Federal Deposit Insurance Corporation.\n\nA second lesson was that monetary policy would be more effective if it were coordinated at the national level, rather than conducted individually by the Reserve Banks.8  To accomplish this, the Banking Act replaced the Federal Reserve Board with the Board of Governors of the Federal Reserve System, and established the Federal Open Market Committee (FOMC) to oversee the conduct of monetary policy.  The governors in Washington were made dominant because they were given the majority of votes on the FOMC.9 Over time, the role of the executive branch and the U.S. Treasury was reduced.  Beginning in 1936, the Secretary of the Treasury no longer served on the FOMC.  The Federal Reserve Board moved to its own building in 1937, and the Federal Reserve gained independence in the setting of monetary policy with the Treasury-Federal Reserve Accord in 1951.\n\nA century following the Panic of 1907, another financial crisis led to the Great Recession.  As was the case following the Great Depression, weaknesses exposed by the crisis led to further changes to the Federal Reserve System.  Some of these changes were legislated while others were initiated by the Federal Reserve itself.  \n\nBefore discussing the lessons learned from the financial crisis and the changes that ensued, I think it is important to recount the Fed’s efforts to support the U.S. economy during this recent period of severe economic and financial distress.  As you may recall, the Federal Reserve responded to the financial crisis in two ways.  First, it intervened to prevent the failure of several systemically important institutions, including firms that it did not supervise―namely Bear Stearns and AIG.10   Second, it established a number of special liquidity facilities to ensure that domestic banks, securities dealers, commercial paper issuers, money market mutual funds and foreign banks had sufficient access to dollar liquidity so they could sustain their lending and investment activities, and thereby continue to supply credit to households and businesses. \n\nI strongly believe that these interventions were necessary to prevent a systemic collapse of the global financial system.  If such a collapse had occurred, I am convinced that the consequences would have been a global depression.  If the Federal Reserve hadn’t done everything in its authority to prevent a collapse, we would have been derelict in our duty to the country.\n\nLet me be clear: The interventions by the Federal Reserve during the crisis were designed to safeguard the economy, for the benefit of all Americans, while protecting the taxpayer.  I am proud of our record on both accounts.  In the end, the Federal Reserve’s extraordinary interventions achieved their intended purposes to support economic activity and, incidentally, taxpayers were compensated for the risks undertaken in these interventions.  Total profits from these interventions were more than $30 billion.11 One important condition for lending in these programs was that the loan needed to be secured to the satisfaction of the Federal Reserve Bank extending the loan.  Despite the degree of dislocation in the financial system at the time and the severity of the Great Recession, there were no losses for any of the Fed’s programs.   \n\nAt the same time, I agree with those critics who argue that there was something fundamentally unfair about the disparity in treatment between the few large financial institutions that were saved versus the millions of individuals who lost their homes or their jobs.  My response is not particularly satisfying.  Recessions inflict considerable pain on innocent bystanders in the economy.  Depressions greatly compound this pain. Given the Federal Reserve’s role and authority, what we knew at the time and the powers and tools that were available to us, I think we made good choices.  If the large systemic banking organizations had failed, the hardships inflicted on households and small business would have been far worse.12\n\nFrom my perspective, I believe that any critique of the Fed or other agencies should be focused more on the regulatory and supervisory shortcomings—some of which, I admit, were ours—that created the economic and financial market circumstances in which the Fed’s extraordinary interventions proved necessary.  This leads me back to some of those lessons learned and the changes that have resulted.\n\nThe crisis showed that the regulatory community did not fully grasp the vulnerability of the financial system.  In particular, critical financial institutions were not resilient enough to cope with large scale disruptions without assistance, and problems in one institution quickly spread to others.  In response, the Federal Reserve has made significant changes in how we regulate and supervise financial institutions.  We have raised capital and liquidity requirements, put banks through annual stress tests, established the Large Institution Supervision Coordination Committee (LISCC) to enable us to evaluate the largest firms collectively and relative to one another, and set up the Office of Financial Stability to enable us to look at the financial system more holistically.  Financial stability now receives the attention it deserves.  For example, there are now regular briefings and discussions on financial stability at FOMC meetings. \n\nAs discussed earlier, the Federal Reserve has the responsibility to be the lender of last resort.  In normal times, this occurs through discount window lending to depository institutions.  During the financial crisis, Section 13(3) of the Federal Reserve Act enabled extensions of credit in “unusual and exigent” circumstances to individuals, partnerships and corporations.13  The extraordinary interventions that were undertaken using our emergency powers under Section 13(3) of the Federal Reserve Act were warranted and within our authority.  However, I suspect that the scale and scope of these interventions went considerably further than envisioned by the public and Congress prior to the crisis.\n\nIn response, Congress narrowed the scope of Section 13(3) with the Dodd-Frank Act.  In particular, Dodd-Frank limits the Federal Reserve’s authority to extend credit through facilities with broad-based eligibility, and constrains the Federal Reserve’s authority to extend credit to a single company.  The Dodd-Frank Act also modified the governance that would apply to any future intervention in an effort to increase public accountability. \n\nIn addition, the Dodd-Frank Act addressed the broader issue exposed by the crisis―that some large financial institutions had become “too-big-to-fail” (TBTF).  To end TBTF, Title II of the Dodd-Frank Act established a process to ensure that any financial firm could be resolved without threatening the viability of the financial system and without putting taxpayer funds at risk.14  In addition, financial intermediaries designated as systemically important by the Financial Stability Oversight Council are subject to tougher prudential standards and enhanced supervision by the Federal Reserve.  The intent behind these measures is to reduce the likelihood of a failure of a large financial firm, and the consequence of such a failure for the financial system, should one occur.\n\nThe Dodd-Frank Act left intact the decentralized structure of the original federated Federal Reserve System—the twelve Reserve Banks and the Board of Governors in Washington.  Each Reserve Bank continues to operate as a separately capitalized corporation with its own Board of Directors.15   But, to limit the role of bankers on the Federal Reserve Bank boards, the Dodd-Frank Act prohibited the bankers on each board from participating in the selection of its Reserve Bank president.16    \n\nEven prior to the Dodd-Frank Act, it is important to emphasize that the directors played no significant role in setting policy.17 Instead, their role has been to advise the Reserve Bank president about economic, community and business developments in the District, and to ensure that the Bank is a well-governed and well-managed institution.18   And, although member banks hold capital in the Federal Reserve and receive a dividend set by law against this capital, member banks do not influence the Fed’s monetary and regulatory policy decisions, how policy is implemented or the supervisory oversight of banks. \n\nThe Dodd-Frank Act also addressed the need for enhanced transparency by establishing disclosure requirements for participation in Federal Reserve facilities.  In terms of transparency, I do think it is a fair critique that, in the past, the Federal Reserve has not always been sufficiently transparent.  For example, prior to 1994, we typically didn’t even announce changes that were being implemented in monetary policy.  Market participants had to infer what we were doing from our daily open market operations and where the federal funds rate was trading.  During the crisis, I also believe we could have done more to explain the motivations for our extraordinary interventions.  At times, while the motivations and objectives might have been obvious to us, they weren’t always as readily apparent to Congress or to the public.  I think this created uncertainty about what we were trying to accomplish, and made it more difficult for outside observers to assess the appropriateness of our actions and our motives.  \n\nToday the situation is quite different.  We have made significant enhancements with respect to transparency.  After each meeting, the FOMC issues a statement that sets the current target range for the federal funds rate and explains its monetary policy decision.  Four times a year, the chair holds a press conference explaining the decision and the FOMC releases its Summary of Economic Projections (SEP).  The SEP provides the FOMC participants’ forecasts for key economic variables and the federal funds rate over the next few years.  In addition, the FOMC participants give numerous speeches explaining their views on monetary policy and other issues, and the chair regularly testifies about monetary policy and the Federal Reserve’s other activities before Congress.  Overall, I have found that this move towards greater transparency has held us in good stead.   \n\nNearly six years since the enactment of the Dodd-Frank Act, there still remains considerable debate about whether further changes to the role and structure of the Federal Reserve System are warranted.  Some of the issues being debated include how much discretion the Federal Reserve should have in the conduct of monetary policy, whether the Federal Reserve is sufficiently transparent in how it operates, and the role of the New York Fed.  I would caution that any further changes to the Federal Reserve System should be grounded on what we learned from the crisis and based on the needs of our evolving economy.  Changes should be undertaken if they would make the Federal Reserve more effective in its mission.  In contrast, changes based on an emotional, unreasoned response in reaction to the pain associated with the financial crisis and the Great Recession would likely be counter-productive. \n\nSome argue that the Federal Reserve has too much discretion in its implementation of monetary policy and, consequently, would have the Fed stick to a formal rule in its conduct of monetary policy.  This is a poor idea because adherence to a simple rule would undoubtedly lead to significant policy errors.  The world is simply too complex to put monetary policy on autopilot.19  \n\nConcerns over the FOMC’s degree of discretion in monetary policy likely reflect, in part, innovations in monetary policy enacted in recent years that added accommodation at a time when short-term interest rates were stuck very close to the zero lower bound.  In particular, the Federal Reserve’s balance sheet grew substantially as the Federal Reserve enacted a series of large-scale asset purchase programs designed to support economic activity.  With respect to the argument that these programs have distorted financial markets, I would simply respond that monetary policy always affects financial markets and financial asset valuations.   The expected path of the federal funds rate is an important factor influencing the level of bond yields, and the level of bond yields and bond term premia have implications for the valuations of other financial assets, such as equities, and influence the foreign exchange value of the dollar. \n\nOf course, the impact of monetary policy may have been greater this time.  But this is mainly because more monetary policy stimulus has been required during the current economic cycle in order to push the U.S. economy towards the Fed’s dual mandate objectives.  I believe that if we had not responded as forcefully, the recovery would have been slower, the unemployment rate would have been higher and there would have been a greater risk of deflation. \n\nWhile it is much too soon to claim success, the U.S. economy today is in a relatively good place compared to most other countries.  The economic expansion is in its seventh year and we have made considerable progress relative to our employment and inflation objectives.  Also, the tools that we designed to control monetary policy even with a very large balance sheet have been shown to be effective.  In December, when we raised our target range for the federal funds rate to 25 to 50 basis points, we also raised the interest rate we pay on bank reserves to 50 basis points and the rate we pay on our overnight reverse repo facility to 25 basis points.  Not only have these new tools been effective in moving the federal funds rate up into the middle of its new target range, but the entire complex of money market rates has moved up as well. \n\nAlso, the fears expressed by some that the Federal Reserve’s very large balance sheet would lead to runaway inflation have proven to be unwarranted.  Now that we have demonstrated that we have monetary control even with a very large balance sheet, I would hope that this issue has been put to rest.   \n\nAs I noted earlier, I am a committed advocate for enhanced transparency and we are always looking for ways to improve.  But, of course, there are limits.  One could argue, for example, that a live broadcast of a Federal Open Market Committee meeting would be more transparent.  But, such a broadcast would do considerable harm by constraining candor and debate over the appropriate monetary policy decision.  I believe it would likely lead to less informed decisions and, consequently, poorer execution of our monetary policy responsibilities.  Similarly, disclosing in real time who was borrowing from the Federal Reserve’s Discount Window would likely undercut the efficacy of the window.  Banks might be reluctant to borrow if it were immediately made public because such borrowing might be construed in the market as a sign of weakness.20   Stigmatizing Discount Window use by banks would make this tool less effective as a lender of last resort backstop for bank liquidity needs.  While there obviously is a point of diminishing returns in terms of increased transparency, I think it’s reasonable to evaluate what further steps the Federal Reserve could make in this direction, judging whether additional steps would enhance versus diminish the effectiveness of our tools and policies, and our ability to achieve our objectives.  \n\nWith regard to the Federal Reserve Bank of New York, which I have the responsibility of leading, I’d observe that the New York Fed, like the other eleven reserve banks, serves to support the overall mission of the Federal Reserve System.  But, by virtue of its location, the New York Fed has a unique responsibility on behalf of the System—namely to serve as the operational arm of the Federal Open Market Committee with respect to the implementation of monetary policy.  This separation of the policy arm and the operational arm of monetary policy in two distinct institutions is a very unusual setup among central banks.21\n\nThere are good reasons why these important functions are assigned to the New York Fed.  Monetary policy works by affecting financial market conditions in order to promote the economic outcomes consistent with the Fed’s dual mandate.  As such, effective and efficient monetary policy execution requires a practical and thorough understanding of the broad range of the financial markets through which monetary policy operates.\n\nNew York City is the nation’s principal financial center.  By locating the duties related to the execution of monetary policy at the New York Fed, the System gains a number of benefits. These include the New York Fed’s ability to develop expertise in and understanding of the broad spectrum of financial markets and institutions, and its access to a deep pool of specialized financial talent in the place where it is most concentrated.\n\nThe New York Fed’s institutional knowledge and experience in financial markets and its related disciplines are a critical input at all times, but never more so than during periods of duress.  This was evidenced during the financial crisis, when the New York Fed’s unique insights and expertise helped the Federal Reserve respond quickly and effectively to unfolding events.\n\nThe authors who crafted the Federal Reserve Act such as Carter Glass—assisted by H. Parker Willis—deliberately created a central bank structure that fits the U.S. political system, with a series of important checks and balances—between Washington and the rest of the country, between Washington and New York, and with respect to the role of the federal government versus the private sector.  These checks and balances exist to ensure that the central bank acts in the broadest interests of the nation, and is appropriately insulated from short-term political pressures and considerations.  In this construction, New York does have a special role, but one that stems from where it sits, in New York City, the nation’s most important financial center. \n\nThe Federal Reserve exists to serve Main Street, not Wall Street.  The dual mandate objectives established by Congress—full employment and price stability—are clear on this point.  There is no mention of Wall Street.  At the same time, it is broadly recognized that without a well-functioning financial system it would be difficult, if not impossible, for the Federal Reserve to achieve those objectives.\n\nThe Federal Reserve has evolved considerably since its inception in 1913. This evolution has been informed by lessons from past financial crises. The goal has been to make the Federal Reserve even more effective in carrying out its mission.  This process of learning from experience is as critical for institutions as it as individuals.  To paraphrase Winston Churchill: People always make mistakes, but only wise ones learn from their mistakes.\n\nThank you for your kind attention.  I would be happy to take a few questions.\n\n1 Kenneth Garbade, Jack Gutt, Joyce Hansen, Michael Held and Joseph Tracy assisted in preparing these remarks.\n\n2 See America’s Bank: The Epic Struggle to Create the Federal Reserve, Roger Lowenstein, 2015.\n\n3 President Wilson, who signed the Federal Reserve Act, described this new feature for the currency. “Suffice it here to say that it provides a currency which expands as it is needed and contracts when it is not needed: a currency which comes into existence in response to the call of every man who can show a going business and a concrete basis for extending credit to him, however obscure or prominent he may be, however big or little his business transactions.” Letter to Congressman Oscar Underwood, October 17, 1914, Wilson Papers, vol. 31.\n\n4 In addition, the Federal Reserve was given the authority to clear checks on a nationwide basis.  This helped knit the country’s commerce together better and ensured that checks would be payable at par rather than discounted by differing amounts depending on how far one was from the bank of issue and how little one knew about a particular bank’s financial condition and viability. \n\n5 Paul Warburg, The Federal Reserve System: Its Origins and Growth, vol. 1, 1930.\n\n6 The Federal Reserve Banks are funded through their respective operations. The Federal Reserve Board―and later, the Board of Governors―is funded through semiannual assessments on the Banks [Section 10(3) of the Federal Reserve Act]. The Board’s financial independence was strengthened in the 1933 Banking Act [Section 6(b)] which stipulated that these semiannual assessments “[…] shall not be construed to be Government funds or appropriated moneys.”\n\n7 For example, see Alesina and Summers, “Central Bank Independence and Macroeconomic Performance.” Journal of Money, Credit and Banking, vol. 25, no. 2 pg. 157-62, 1993.\n\n8 H. Parker Willis described this independence given to the Reserve Banks in the conduct of policy. “There is nothing, either in the Federal Reserve Act or in the regulations of the Federal Reserve Board, to indicate that the reserve banks are to be operated in groups or through communications with one another, resulting in the establishment of a single policy as to detail.” H. Parker Willis, The Federal Reserve, pg. 128, 1915.\n\n9 This assumes no vacancies on the seven-member Board of Governors.  Currently, with five members of the Board of Governors and five votes among the Reserve Banks, the balance is split equally.  \n\n10 The Federal Reserve was unable to prevent the bankruptcy of Lehman Brothers.\n\n11 For example, see “Federal Reserve: Emergency Lending,” Marc Labonte, Congressional Research Service, January 6, 2016.\n\n12 In retrospect, I believe more government support could have been provided to homeowners.  Although the Federal Reserve did not have the authority to implement a mortgage modification loan program, we did implement a mortgage-backed securities purchase program to lower mortgage interest rates.   \n\n13 Several other conditions had to be satisfied as well.\n\n14 Although considerable progress has been made in ending TBTF, I think it is premature to declare victory.  Some banks still have work to do to make their resolution plans credible, and there needs to be more work on cross-border issues to ensure that such a resolution would proceed smoothly.\n\n15 Each Board is made up of nine members divided evenly into Class A, B, and C directors.  The Class A directors represent the banks of the District that are Federal Reserve member banks, the Class B and C directors represent the broader public interest.  Only eligible Class B and Class C directors can participate in the process for selecting Reserve Bank presidents.\n\n16 The selection of the President consists of two steps: Appointment of the Reserve Bank president by the Board of Directors, and approval of the appointment by the Board of Governors. For details see: http://www.federalreserve.gov/faqs/how-is-a-federal-reserve-bank-president-selected.htm\n\n17 There is one exception, but it has little actual significance.  Boards of Directors do make recommendations concerning changes in the primary credit (discount) rate.  However, these recommendations only go into effect by an affirmative vote by the Board of Governors.  Also, if the Board of Governors wanted to change the primary credit rate and no recommendations were available to act upon, the Board legally could move unilaterally.  In fact, what would likely happen in this unlikely circumstance would be a phone call to one or more Reserve Banks to ask for the desired recommendation and rapid compliance with the Board of Governors’ request.  \n\n18 The Board of Governors plays an important role in the oversight of the Reserve Banks.  The Board of Governors regularly evaluates Reserve Bank operations and each Bank’s budget The Reserve Banks conduct supervision under delegated authority from the Board of Governors.  Reserve Bank directors continue to have no role in bank supervision.   \n\n19 See The Fed at a crossroads, Where to go next? Brookings Institution panel remarks, October 15, 2015.\n\n20 A historical example was the disclosure of Reconstruction Finance Corporation loans to banks in the Great Depression. This disclosure often created runs at the respective banks. See James Butkiewicz, “The Reconstruction Finance Corporation, the Gold Standard, and the Banking Panic of 1933,” Southern Economic Journal, vol. 66, no. 2, October 1999, pg. 271.\n\n21 In the case of the European Central Bank, the member national central banks are involved in policy implementation."
    },
    {
        "title": "Forming the Next Generation of Bankers: The Future of Business Education and Ethics",
        "date": "Mar 22, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/str160322",
        "content": "Thank you Professor Viswanathan.  It is a pleasure to celebrate the Zarb School of Business 50th anniversary. I also want to thank Dean Herman Berliner for his invitation.  Dean Berliner once referred to himself as “an unlikely administrator.”  I know the feeling.  But he is no doubt quite an accomplished one who has contributed greatly and sustainably to advancing the Zarb School and Hofstra’s development.\n\nIt’s an honor to have this opportunity to speak about business education and financial services—their nexus and their future.  On a personal level, I’m delighted to speak at a university.  I began my career as a professor.  I then became a university administrator.  The future of higher education—and its capacity to create social good—is a point of great interest and importance to me and to our society. \n\nWorking now at the Federal Reserve Bank of New York, part of my job is to focus on the stability of the nation’s financial system.  That requires a forward-looking perspective, which I hope will inform today’s discussions.\n\nOne caveat before I continue:  My remarks are based on my personal experience in academia and at the Fed.  My views, however, are my own.  They may differ from the official views of the Federal Reserve Bank of New York or the Federal Reserve System.1\n\nI plan to talk about four topics today:\n\nThe dynamic forces shaping the future of financial services in many ways are not unique to that industry.  Speed, technology, globalism, complexity, connectedness, uncertainty, risk, proximity, customization—these themes arise in many sectors of the economy.  I suspect they will be resonant in other discussions today.\n\nI will focus my remarks today on how these themes apply within the financial services sector.  In particular, I will focus on three categories—structure, reciprocity and trust—as a model for organizing future changes in financial services.\n\nLet’s take structure first.  Trading platforms, information delivery systems, and communication tools will continue to evolve at an astounding pace.  Indeed, new technologies are already affecting the way markets behave and market participants engage with each other—as, for example, the “flash crash” of 2010 and the rise of high frequency trading have shown. \n\nMore people than ever will have access to financial services.  Transactions will occur with unimaginable speed across global distances.  We will be ever more interconnected and more rapidly connected.  We may also grow increasingly less dependent on financial intermediaries, or the structure and nature of those financial intermediaries may be quite different. \n\nWith these changes come new opportunities, but also significant new vulnerabilities.  We’re just beginning to appreciate cyber-risks.  Increased access and connectivity may become accelerants in a financial crisis.  With these changes to the structure of finance, will we be prepared to mitigate attendant risks?\n\nNext, let’s consider reciprocity.  When I taught undergraduates, I marveled at the differences between my generation and the generation seated before me.  I imagine you feel those differences even more keenly now.  The students in our universities today are arguably more self-assured and less reliant on established institutions than their predecessors.  What social contract do they envision?  What degree of personal responsibility in banking will they demand?  Is there room for a financial safety net? \n\nAnd then there’s trust—keeping one’s word.  Financial services will, in my view, become increasingly commoditized.  That is, finance will increasingly deal more in products, less in relationships.  How will our expectations of trustworthiness differ in a growing, off-the-shelf market?  And who will want to go into that business?  How will perceptions of trustworthiness affect the talent that financial services attracts?\n\nConsider the recent rise of “fintech.”  Why is it that start-up companies are as trusted—perhaps more trusted—than established banks?  Is this just a generational anomaly?  Or is there a more fundamental change at work? \n\nI identified these three categories—structure, reciprocity and trust—because these are the factors that, in my view, distinguish financial services from other industries. \n\nUnlike many businesses, “[f]inancial firms exist, in part, to benefit the public, not simply their shareholders, employees and corporate clients.”2\n\nThis role demands a degree of social responsibility not imposed on all industries equally. \n\nThere are reasons for this.  First, financial institutions are indispensable intermediaries in the economy.  This is the element of structure.  Because of this role, problems in banks do not tend to stay in banks.  They can impact firms and households across the country and abroad.\n\nSecond, banks in particular receive public operating benefits—deposit insurance and access to the Discount Window, among others.3   In exchange for these benefits, they owe a heightened degree of “other-regarding” decisions and behaviors.  This is the element of reciprocity.  Other industries do not have this type of safety net, and we do not expect as much from them.\n\nFinally—and most important—is trust.  Finance is complex and opaque.  It requires trust in the professionals who provide these services.  That’s why financial firms market themselves as dealers in trust.  We should expect these dealers in trust to keep their word—to meet legitimate expectations of trustworthiness.  Finance, after all, is the business of credit.  And the word “credit” derives from the Latin credo—“I trust.” \n\nOther enterprises may market themselves under the slogan, “trust us.” But in financial services that promise is too important to break.  People entrust the financial system with their hard-earned wages, their nest eggs, and their children’s and grandchildren’s college tuition. \n\nIn short, the future of the financial industry, its shape and evolution, perhaps more than other industries, and especially in the current environment, depends critically on restoring trust, reaffirming the reciprocal relationship of finance and society, and ensuring our structures are resilient in the face of crisis.\n\nSo, we need to explore more deeply basic questions about what we want from financial service providers, not just in terms of commodities or services, but more deeply in terms of values and quality. \n\nThese questions drive the debate over the “culture” of finance.  That word—culture—is for some, the end of the discussion.  Some view it as too inchoate for rigorous academic discussion.  Others see it as too “squishy” for serious business consideration.  For now, let’s set aside the term and focus on the meaning behind it. \n\nI agree with Bill Dudley, the New York Fed’s president —and not just because he’s my boss.  He sees “deep-seated” and interrelated causes behind the recent scandals at many large financial institutions.4   The pattern of misconduct is too prevalent to be mere coincidence, and too pervasive to be diagnosed as just a few “bad apples.”  I’ll spare you the full litany. \n\nSuffice it to say, too often financial services firms ceased to function as a service—that is, work for others.  The industry largely lacked “other-regarding” norms and behavior.  In the financial crisis, and in its wake, many failed to consider “the private and social costs of their decisions.”5   In contrast to the volumes written about the financial crisis, there is comparatively little academic review of post-crisis scandals.  We need to better understand—beyond intuition and anecdote—the causes of this rather extraordinary pattern of behavior. \n\nWhen we do, we’ll likely discover how much in common they had with earlier financial scandals.  We may find more than common causes.  We may also find common responses.  For the last one-hundred years, the official sector’s response to financial crisis has been a vow: “Never again.” But then, another crisis occurs.  We would do well to ask whether the recent reforms of the Dodd-Frank Act can live up to that same promise. \n\nLet me be absolutely clear:  I believe Dodd-Frank’s reforms show great progress on many fronts.  But, I am a student of history.  And I am a political realist.  Regulation is not a panacea.  We cannot expect too much from it.  My colleagues and I believe that external controls are essential. But we also know they are not sufficient. In light of our history, they do not offer adequate assurance of good decision-making.  The fact is, over-reliance on external controls may even discourage personal responsibility.  We must avoid the implication that it’s someone else’s responsibility to spot mistakes. \n\nLet me place a marker here with this question:  How might business education contribute to offsetting a tendency to see ethical questions as the responsibility of a control function?  Of a regulator?  Of an ethicist?\n\nThe financial crisis made clear that there are social costs inherent in business decisions.  Financial decision-makers must be much more aware of these costs.  This goal is ambitious, to say the least.  But it’s necessary.  Well-run financial services support our economic well-being.  Poorly-run financial services have been shown, time and again, to be economically ruinous. \n\nLet me say it again:  I am a realist.  I also worry about placing too great a reliance on self-restraint.  After all, Princeton’s Alan Blinder has called self-regulation in finance an oxymoron.6   More recently, two Nobel laureates have argued that “pressures for less than scrupulous behavior” are unavoidable in competitive markets.7  \n\nFinance is not alone, of course.  Tensions between competition and self-restraint arise in other industries too.  Financial services do not have a monopoly on scandal.  In any industry, decision-makers may become consumed with near-term profit instead of long-term value.  Any industry can suffer from a competitive myopia that blinds many to storm signals.  What makes finance different are the three elements I mentioned—structure, reciprocity and trust. \n\nFinance is hard-wired—both literally and metaphorically—into the nation’s economy.  Other industries depend on it.  And crises in finance have a tendency to spread to other sectors.  An ebb in financial stability or productivity signals low tide for the rest of the economy.\n\nFinance also receives operating benefits available to no other industry.  These benefits are given most directly to banks.  But other firms, which interact with and depend on banks, are third-party beneficiaries. \n\nAnd then there is trust.  Absent that trust, credit will be withheld in cases where otherwise it would be extended.  As a result, economic activity will diminish.  Left unabated, another crisis will emerge.  Extending credit—that is, extending trust—is what financial services is all about.  Other industries make cars or computers or ice cream cones.  We trust them to make good products.  But in finance, trust is inarguably more central or should be the essence of the product. \n\nSo, what might we do together to contribute constructively to the future of the financial services sector?\n\nBusiness schools like Zarb hold stereoscopic influence over financial services.  By this I mean, they inform both the demand for—and the supply of—financial professionals. \n\nHere is what I mean by demand—and this may not be economic orthodoxy.  Business schools educate and advise the leading consumers of financial services.  They can also shape the views of shareholders in financial firms.  In both respects, business schools can affect the demand for certain qualities in financial service. \n\nNow, supply.  The financial services industry employs professionals with MBA degrees.  Locally, the financial services industry is one of the region’s largest employers.  It’s also the highest paying.8   Nationwide, financial services firms employ roughly a third of graduates of leading MBA programs.9   The CEOs of half of the large, systemically important banks hold MBA degrees.  Business schools, of course, do more than confer degrees.  They create networks of professionals and germinate the norms that bind them.  They supply a culture.\n\nBusiness education, therefore, serves as a fulcrum for finance.  It supports and balances the future of financial services.  But that does not mean its role is purely passive or reactive.  Business education can—and should—help guide the qualitative changes that will occur in the financial services industry in the future.  Thoughtful, independent discussion of the future of finance is at home in business schools.  Such a discussion may include any number of questions.  Here are a few examples.\n\nThe same questions occupy my colleagues in the official sector.  And, they are increasingly on the agenda of many financial firms.  We see evidence that some of the largest financial services firms are expressing a commitment to change their culture.  They may not have fully endorsed Christine Lagarde’s call for capital and culture to share equal billing.10 But, they are espousing greater alignment of principle and practice. \n\nAs that continues, reform agendas at financial firms should soon reach back to recruiting.  What are the qualities and skills that are needed in newly minted MBAs?  What additional training do experienced employees need?  How can business schools better supply what the changing industry demands?\n\nI do not mean to suggest that business schools should teach students right from wrong.  Your students are not blank slates.  Still, the purpose of business school is to prepare students for the business world.  How will your programs change to keep pace with changes in business?\n\nPermit me to suggest a few learned skills that have particular relevance for financial services.  Issue-spotting is one.  It’s a critical skill that is too often missing in finance.  Scandals often arise when people overlook—or, do not want to see—the signs of trouble.  Sometimes those signs are missed as a result of personal bias—whether apparent or latent.  Those with training in identifying bias and risk will add greater value to firms looking to minimize cost and make realistic assessments of risk. \n\nOf course, issue-spotting is valuable only if identified issues are appropriately escalated.  So speaking up is another beneficial skill.  Candor—both its gift and its receipt—is a skill that needs to be practiced.  Inculcating a culture of candor relies, in turn, on skills like mentoring and coaching.  These skills can be developed through experiential or situational learning.\n\nStudents need to practice these skills.  A greater and earlier emphasis on experiential or situational learning is indicated.  In law and medicine, clinical education is no longer delayed until core courses are completed.  Why should business be any different?  Perhaps first-year business students need their own laboratories to experiment in resolving difficult situations:\n\nOf course, clinics are not the only answer.  Classroom training remains a central part of business education.  But that training is also dynamic.  It changes in response to the needs of industry.  As I have said, financial services firms appear to be moving toward greater personal responsibility for outcomes.  It’s the responsibility of every financial services professional to serve on the front line of good decision-making.  Accordingly, discussions of ethics should not be delivered in a single ethics class.  After all, most case studies—most business decisions—contain ethical dimensions. \n\nIn clinics and in the classroom, we need to train people entering finance to develop “ethics memory” so that they can instinctively make good judgments in difficult situations.  Financial regulators share the same goal as business educators: helping others develop a habit of making good choices.11  \n\nI don’t want to be an impolite lunch guest.  I cannot ask you to consider changing the way you prepare students for finance without proposing how the official sector can help. \n\nI believe there are three ways:  First, by convening discussions.  Second, by brokering opportunities for continuing education.  Third, by promoting research.\n\nA few minutes ago, I sketched some thoughts on the trajectory of the financial services industry.  Would it help to make that discussion more regular?  More structured?  More inclusive of other views?  The official sector could create a forum for the ongoing discussion of the future of financial innovation.  I see a need for bankers, regulators and educators to participate in the discussion—in the same room at the same time.  How do we expect the industry to change?  And, how can we best prepare students for those changes?  After all, it’s in everyone’s interest to have the best prepared MBAs available to staff our nation’s financial infrastructure.\n\nNext, the official sector could identify areas where there is a need for executive education.  I’m speaking here of retraining experienced professionals.  I mentioned earlier the need for greater expertise in cyber-security.  Regulatory changes may also create a need for new learning—especially in the areas of compliance, capital and liquidity management, and risk culture.  If financial supervisors identify a need for new training, can business schools supply continuing education programs?  That would be a valuable and well-compensated service.  Business schools can develop programs responsive to an evolving need for training.  Those programs would be tailored to new circumstances and evolving social responsibilities.  \n\nFinally, business schools are ideally situated to conduct excellent research—in tandem with the official sector—in the areas of behavior and decision-making in finance.  Some issues in need of greater attention include:\n\nBusiness schools may also take advantage of a natural strength that comes from their setting in a university.  They may consider partnering with departments that study culture, or other cultures, for input on different considerations of ethics.  For example: \n\nLet’s work together to identify the most fruitful areas of research.\n\nIn closing, I want to again thank the Zarb School of Business and Dean Berliner.  Your leadership in convening these discussions is laudable.  My colleagues and I look forward to collaborating with you, and to contributing jointly to the future of finance and business education.\n\nThank you for your kind attention.\n\n1 Nick Balamaci, Stephanie Chaly, Ed Cheney, James Hennessy, Jacqueline McCormack, Anand Marri, Thomas Noone, Kevin Stiroh and Joseph Tracy assisted in preparing these remarks.\n\n2 William C. Dudley, Enhancing Financial Stability by Improving Culture in the Financial Services Industry, Remarks at the Workshop on Reforming Culture and Behavior in the Financial Services Industry, October 20, 2014.\n\n3 See E. Gerald Corrigan, “Are Banks Special?,” Federal Reserve Bank of Minneapolis Annual Report, January 1983.\n\n4 William C. Dudley, Ending Too Big to Fail, Remarks at the Global Economic Policy Forum, November 7, 2013.\n\n5 Dan Awrey, William Blair, and David Kershaw, “Between Law and Markets: Is There a Role for Culture and Ethics in Financial Regulation?” 38 Del. J. Corp. L. 217 (2013).\n\n6 Alan S. Blinder, After the Music Stopped 434 (2013).\n\n7 George A. Akerlof and Robert J. Shiller, Phishing for Phools: The Economics of Manipulation and Deception xi (2015).\n\n8 New York State Bureau of Labor Market Information, Division of Research and Statistics, Significant Industries: A Report to the Workforce Development System, September 2015, 2-3.\n\n9 “Banks?  No, thanks!” The Economist, Oct. 11, 2014.\n\n10 Christine Lagarde, “Economic Inclusion and Financial Integrity,” Address to the Conference on Inclusive Capitalism, May 27, 2014 (“[F]inancial leaders [must] tak[e] values as seriously as valuation, culture as seriously as capital.”).\n\n11 Aristotle, Nicomachean Ethics, Book II, Chapter 1 (describing moral virtue as a habit (ethos))."
    },
    {
        "title": "Opening Remarks at the Conference on Supervising Large, Complex Financial Institutions: Defining Objectives and Measuring Effectiveness",
        "date": "Mar 18, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud160318",
        "content": "Welcome.  It is great to see all of you here today to discuss the objectives and measurement of supervision for large, complex financial institutions.\n\nNearly eight years have passed since the financial crisis hit, pushing the financial system and the U.S. economy to the brink, and leaving scars that are still evident today. The hardships of the financial crisis are unfortunately still open wounds for too many people, especially those who lost their jobs, homes or businesses, or those who struggled with the sharp fall in value of their homes and with staying current on their debts and bills.  Understanding what went wrong and how to avoid severe financial crises in the future is, I am sure, an issue foremost in the minds of all the distinguished speakers here today, and I am no exception.  Before continuing, let me indicate that what I have to say today reflects my own views and not necessarily those of the Federal Open Market Committee or the Federal Reserve System.1\n\nA central part of the financial crisis was the failure or near-failure of many large financial institutions—both institutions that were supervised by the Federal Reserve and many that were not—which constrained their ability to play their crucial role in supplying credit and financial services to businesses and consumers.  In the wake of the crisis, considerable attention has been devoted to the role that supervisors—including those here at the New York Fed—played in this meltdown.  But much of this discussion has taken place without a broad understanding of what supervisors actually do—the scope, breadth, and limits of their activities and authorities.  Within the official sector, we make a distinction between regulation—the rules governing what financial institutions must and must not do, and supervision—which involves monitoring, oversight and enforcing compliance with law, regulation and supervisory expectations for firms’ governance, internal processes and controls, and financial condition.  This distinction between regulation and supervision is often not well-understood, and thus their complementary roles are not always fully appreciated. \n\nAddressing this distinction, as we see it, is one important motivation for this conference.  Through the papers presented today and the discussions by noted academic researchers, policymakers and senior supervisors from the U.S. and overseas, we hope to broaden the understanding of the role and objectives of supervision.  The goal is to spur conversation and new research on what supervision should be trying to achieve and the best means for achieving those ends.  Just as important, we want to generate ideas for how to better assess the effectiveness of supervisory activities.  In a world of limited resources, how do we deploy our people and technology in the most effective way to limit disruption and distress at individual firms and to the financial system, while still fostering an efficient and innovative financial system?\n\nIn considering these questions, a good starting point is to understand more fully what supervisors do and how they do it in the current environment.  The papers that are being presented here today are largely aimed at achieving this goal.  As described in the first paper—“Supervising Large, Complex Financial Institutions: What do Supervisors Do?”—bank supervision has long been an integral component of the Federal Reserve’s responsibilities.  At a broad level, Federal Reserve supervision of large financial institutions is guided by two key objectives: “enhancing the resiliency of a firm to lower the probability of its failure or inability to serve as a financial intermediary [and] reducing the impact on the financial system and the broader economy in the event of a firm’s failure or material weakness.”2 In addressing these objectives, bank supervisors enforce laws and regulations, but there is much more to supervision than just enforcement.  The activities and practices at large, complex financial organizations are simply too intricate and evolve too quickly to be fully described ex ante in regulation.  Banking supervision works in concert with regulation as the more flexible element of banking policy.  Supervisory policy and standards can, and do, evolve over time to reflect the evolution in practice in the banking industry and in financial markets.\n\nOverall, supervisors are guided by the mandate to identify any practices or conditions at supervised firms that are a threat to the safety and soundness of those firms—and, of course, to ensure that the firms take all necessary steps to promptly remediate any such conditions.  Critically, the definition of what constitutes “safe and sound” is not hard-coded into regulation, but is guided by the information and analysis done by supervisors and other Federal Reserve staff, such as economists, attorneys and market analysts.  Supervisory expectations and standards are expressed through public guidance such as Supervision and Regulation Letters (SR Letters), publicly-posted examination manuals and other guidance. \n\nOn a day-to-day basis, banking supervisors collect information on financial institutions through examinations and analysis.  Examiners look at key aspects of a supervised firm’s businesses and risk management functions.  They use this information to assess the adequacy of the firm’s systems and processes for identifying, measuring, monitoring and controlling risks at the firm and in the financial sector more generally.  That said, the ultimate responsibility for risk identification and risk management remains with the supervised institution.  The Federal Reserve’s role is to ensure that the institution has the necessary strong processes in place to achieve this objective.  As risks emerge, supervisors intervene, within the realms of their safety and soundness mandates, to require that banks take corrective actions as necessary.  Supervision can reduce the chance that a financial firm fails, but it can never provide a guarantee against failure.\n\nSupervisory policies have undergone important changes over the past few years, especially in the context of the largest financial institutions.  Banking supervision no longer focuses solely on the safety and soundness of individual institutions—so-called micro-prudential supervision—but now also focuses on the implications for financial stability more generally.  The evolving supervisory framework has resulted in a number of organizational changes in the way supervision is conducted at the New York Fed and in the Federal Reserve System more broadly. A very important change in the Federal Reserve System is that supervision of the largest, most systemically important financial institutions are now coordinated across districts through the Large Institution Supervision Coordinating Committee (LISCC).  The LISCC is a System-wide committee, chaired by the director of the Board of Governors of the Federal Reserve’s Division of Banking Supervision and Regulation, and composed of senior officers at the Board and Reserve Banks. \n\nIn addition, the Federal Reserve has instituted three major horizontal evaluations, which are a key element of the new enhanced supervisory framework for large banking companies.  These horizontal evaluations, which focus on capital adequacy, liquidity resiliency and preparedness for recovery and resolution, examine practices and conditions across a group of firms simultaneously, enabling supervisors to develop peer perspectives about best practices and approaches.  These programs involve a consistent and systematic assessment of the financial strength and operational resiliency of the largest and most complex financial institutions, addressing both micro-prudential safety-and-soundness supervisory goals and broader macro-prudential financial stability objectives. \n\nSpecifically, in the Comprehensive Capital Analysis and Review (CCAR), which applies to firms with at least $50 billion in total assets, Federal Reserve supervisors assess whether bank holding companies have sufficient capital to continue operations through times of economic and financial stress, and whether they have robust, forward-looking capital-planning processes that take into account whatever unique risks they might face.  By including forward-looking elements, the CCAR complements the enhanced Basel capital requirements now being implemented in the United States.  In addition, the Comprehensive Liquidity Analysis and Review (CLAR), which applies to LISCC firms, reviews the liquidity positions and liquidity risk management of large, complex banking organizations, including internal stress-testing practices.  Finally, the Federal Reserve does an annual evaluation of the LISCC firms' options to support recovery and progress in removing impediments to orderly resolution in the Supervisory Assessment of Recovery and Resolution Preparedness (SRP).\n\nWhile these evaluations form the core of the supervisory program for large, complex banking companies, they are far from the only important activities pursued by supervisors.  Day-to-day oversight of individual firms is a critical complement to these cross-firm, horizontal evaluations.  There are three key channels that generate insight on firms: 1) continuous monitoring through meetings with bank senior management, business line management, risk managers, auditors and others; 2) review and analysis of internal reports; and 3) independent analysis by supervisory teams.  These insights guide supervisory assessments and supervisory actions for individual institutions, as well as contributing to the assessments made as part of CCAR, CLAR and other horizontal exercises.\n\nWe have come a long way since the financial crisis, and the financial sector today is much more resilient as a result of the evolving supervisory framework.  However, more work still lies ahead.  A key challenge in assessing the effectiveness of supervision is that much of what supervisors do, by necessity, is confidential.  This reliance on confidential information and confidential actions can make supervision seem mysterious to outsiders, which complicates the evaluation of what policies may be effective or ineffective.  For example, if a bank fails, is this evidence of poor supervision, or instead evidence that even good supervision can’t prevent all bank failures? Improving our ability to do this diagnosis is critical.  I am confident that today's panels and paper discussions will help us move forward in this direction.  \n\nThank you for participating in this conference.  \n\n1 Beverly Hirtle, David Lucca and Joseph Tracy assisted in preparing these remarks.\n\n2 Board of Governors of the Federal Reserve System. “Consolidated Supervision Framework for Large Financial Institutions.”  Supervision and Regulation Letter 12-17. December 17, 2012. http://www.federalreserve.gov/bankinforeg/srletters/sr1217.htm"
    },
    {
        "title": "Operational Risk Management at the Federal Reserve Bank of New York",
        "date": "Mar 15, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/ros160315",
        "content": "Good morning.  I would like to thank the organizers for inviting me to speak at this year’s OpRisk North America conference.  Today, I will share my perspectives on operational risk based on my experiences at the Federal Reserve Bank of New York including those in my current role as the Bank’s Chief Risk Officer.  Before I begin, I wanted to state that the views I will be expressing are mine and do not necessarily reflect those of the Federal Reserve Bank of New York or the Federal Reserve System.\n\nAs you know, operational risk is defined as the risk of loss resulting from inadequate or failed internal processes, people and systems, or external events.  The New York Fed faces operational risks because, like other public and private institutions, it relies on people, processes, and systems to execute its objectives, and, in the same way, the Bank is subject to external events that can impact the effectiveness of our people, processes, and systems.\n\nTo give you a better sense of the New York Fed as an operational entity, I wanted to touch briefly on the broad range of the Bank’s operations.  Most familiar are likely to be the Bank’s execution of market operations to implement monetary policy directives of the Federal Open Market Committee and financial supervisory activities.  As significant in the Bank’s operational profile are the financial services that the Bank provides to the U.S. government, financial institutions and businesses, and to foreign central banks and international institutions.  Notably, the Bank operates the Fedwire® electronic payments and securities transfer service, which is a critical part of the nation’s payment system infrastructure.  The Bank performs fiscal agency functions for the U.S. government including the auction of Treasury securities, and it provides correspondent banking and custody services to foreign central banks and international institutions.1\n\nThe Bank’s Risk Framework\n\nOperations imply operational risk, so how does the Bank approach operational risk management?  Let me back up for a moment, since a useful starting point to answer this question is to begin by describing the Bank’s overall approach to managing risk within the Bank’s risk framework.\n\nIn particular, the risk framework is designed to enable the Bank to understand and communicate its risk profile to key stakeholders, assess how risks may change in response to planned activities or changes in the environment, take action to ensure risks remain at acceptable levels, recover quickly and effectively from risk events, and continuously improve the effectiveness and efficiency of the risk management.\n\nAs part of the risk framework, the Bank defines risk management roles and responsibilities using the three lines of defense model.2  The three lines of defense model creates a set of layered defenses that align responsibility for risk taking with accountability for risk control and provide effective, independent risk oversight and escalation.  In the three lines model, the assignment of risk management roles is clear and comprehensive in order to prevent gaps, ambiguities, or overlaps in responsibility.  More specifically, the business areas are the first line of defense, independent risk management units are the second line of defense, and internal audit is the third line of defense.\n\nIn the Bank, the first line of defense is comprised of the business areas that execute and support the execution of the Bank’s mission.  These first line units are responsible for both the operational activities that result in risk as well as control of the resulting risks.  The first line “owns” its risk in the sense that it is accountable for both positive and negative outcomes and is empowered to manage the distribution of outcomes.\n\nAt its core, the three lines model recognizes the strong incentives for effective risk management created by aligning accountability and responsibility.  In other words, from the perspective of the first line, there is “skin in the game,” and risk management is not viewed as someone else’s problem.  In addition, putting the first line in charge of risk management has efficiency and effectiveness benefits, since the first line can adapt risk solutions to its specific needs and nimbly respond when the risk environment changes.\n\nThe Bank’s second line of defense—independent risk management areas including my Group, the Risk Group—provides independent assessment and oversight of the risks taken by the first line as well as frameworks that provide a common structure for risk management processes and practices.  The second line also provides an integrated view of risk to senior stakeholders through consolidated analysis of the Bank's risks.\n\nA second key insight of the three lines model is that a layered defense increases safety and reliability by providing an independent check that reduces the chance of error.  The second line is the second layer, and provides a view of risk that is independent of the business area’s assessment.  What is the value-add from having this type of second opinion?  To start, while the business has the deepest understanding of its environment, operations, and objectives, the second line offers a perspective based on expertise in risk identification, analysis, and mitigation that is informed by its institution-wide view of operations and risks.  This allows the second line to provide unique insights by aggregating risks across business lines, identifying interactions across risk types, and spotting anomalies in risk exposures or risk practices that deserve additional attention.  There also may be externalities to actions taken in one business line that have implications for the risk of the institution as a whole; it is the second line’s responsibility to articulate that perspective to senior management.\n\nThe second line’s responsibility for establishing a common framework for risk management creates benefits in terms of both efficiency and effectiveness.  The second line is an institution’s center of excellence in risk management and is in an ideal position to identify and promulgate best practices and standards.  In terms of efficiency, the second line is well-positioned to collect and analyze risk information coming from all of the business lines, to provide common risk management tools, and to provide guidance to business areas to improve their risk management capabilities.\n\nThe Bank’s third line of defense, Internal Audit, provides assurance to senior management and the Board of Directors that first and second line risk management and control activities are effective.  Although I did not say it before, the second line is not fully independent of management.  This is because second line risk management (in our case, the Risk Group) is involved in a broad range of management decisions related to the Bank’s risk controls, and, in terms of reporting lines, I am part of the Bank’s Management Committee and report to the Bank’s president.  Our Audit Group, however, provides a fully independent, final layer of defense. Internal Audit does not participate in management decisions, and our General Auditor reports to the Chair of the Audit and Risk Subcommittee of the Bank’s Board of Directors.\n\nIn addition to defining clear and comprehensive risk management roles and responsibilities using the three lines of defense model, the Bank’s risk framework establishes governance, escalation, and reporting processes around risk exposures, risk decisions, and risk events.  A strong governance regime provides assurance to stakeholders (Management Committee, the President, and the Board) who delegate risk-taking authority to the business lines.\n\nFrom first-line businesses and support functions, risk information flows to the second line, and then to the Risk Subcommittee of our Bank’s Management Committee.  The Risk Subcommittee discusses significant emerging and existing risks, provides perspectives on managing and mitigating risks, and supports Management Committee review and decision making.  The Audit and Risk Subcommittee of the Bank’s Board of Directors and the Board of Directors itself are at the top of the governance chain, although there are limitations on their roles with respect to supervisory and monetary policy issues.3\n\nOperational Risk Management\n\nThe Bank’s operational risk approach sits within, and is shaped by the Bank’s overall risk framework. Roles and responsibilities as well as escalation and reporting follow the general structure I previously described.  At this point, though, I can drill down into operational risk specifics.\n\nIn the first line of defense, operational risk management is conducted by staff in the front line units with support from first-line’s centralized risk management resources (usually as part of a business’ shared services function).  The first line is responsible and accountable for the identification, analysis, management, and monitoring of the operational risks that arise as a result of its activities.\n\nSecond line operational risk management is led by the Central Operational Risk Function in the Risk Group.  Central Operational Risk provides independent assessment and oversight of first-line operational risks and risk management practices and is responsible for maintaining and enhancing the Bank’s operational risk framework.\n\nOur third line, Internal Audit, assesses the design and implementation of the operational risk framework and provides feedback on risk issues as an observer on governance committees.\n\nThe foundations of the Bank’s operational risk program are the risk event reporting process and the risk and control self-assessment process.  These processes are central to risk identification, analysis, and response.  The risk event reporting process involves real-time collection, classification, analysis, and escalation of operational risk events including establishment of root causes and determination of remediation plans.  You are all likely familiar with reporting risk events that have impacted your institution and categorizing them as low, moderate or significant.  We do this as well.  In addition, we require reporting of near miss events, which is when a control failure has occurred but we’ve been fortunate that there was no impact to the Bank.  These near miss events are a window into vulnerabilities that could cause an adverse impact in the future.\n\nThe risk and control self-assessment process establishes a methodology for business areas to periodically assess their operational risks and ensure that appropriate mitigation is in place for key business processes.  Importantly, the Central Operational Risk area is responsible for synthesizing the outputs of these two processes, along with follow-up discussions with business areas, to identify significant Bank-wide residual operational risks.\n\nWe have found it useful to view operational risk through the lens of processes, risks, and controls. We have defined a comprehensive set of processes to characterize the Bank’s primary services and transactions.  Along with that, we have articulated a standardized set of risk drivers, risk events, risk event impacts, and, finally, controls that prevent, detect, or mitigate the consequences of a risk event.  We are using this risk taxonomy on a forward-looking basis to assess the risks of Bank processes and as an analysis tool to investigate risk events after the fact.\n\nUsing standardized process, risk, and control types allows us to aggregate as well as slice and dice our risks around dimensions of interest.  For example, we identify processes associated with the highest levels of residual risk and the largest number of risk events as areas for additional attention.  It may be the case that additional mitigation is required or there are weaknesses in controls.  We identify common control profiles for process types and assess their effectiveness based on risk ratings and past risk events.  That informs the second line’s recommendations to business areas on control protocols.  We also use the common control profiles to assess where we are over-controlled and have the opportunity to shift resources to other priorities.\n\nThe risk information collected and the risk analysis performed by the second line is the basis for operational risk reporting in the Bank’s risk profile report that is presented to the Risk Subcommittee, the Management Committee, and the Audit and Risk Subcommittee.  The risk profile report shows the Bank’s most significant risks (including operational risks) along with a risk rating and the status of mitigating actions and mitigation plans.  In addition, the report highlights significant and notable risk events along with the event’s impact, root causes, and remediation activities.  We have found that the risk profile report provides the necessary flow of information to create meaningful committee dialogue around risk issues, feedback to business areas, coordination across Groups and lines of defense, and when necessary, additional escalation.\n\nPast, Present, and Future\n\nAs the theory and practice of operational risk management and the complexity of our operations have evolved, so has our approach.  In the mid-2000’s, the Bank formalized its operational risk program.  That included starting a Bank-wide Operational Risk Committee (a predecessor to the Risk Subcommittee) led by the First Vice President, establishing the risk event reporting and risk control self-assessment processes, and creating the Centralized Operational Risk Function (then located in the Bank’s Corporate Group).  Later developments included instituting new Bank-wide control policies reflecting a deeper understanding of sources of operational risk.  For example, the Bank established policies to govern end-user developed automation tools and the management of contingent workers.\n\nThe past several years have been characterized by a focus on integration of the Bank’s risk management program.  As part of a review of the Bank’s governance, the Risk Subcommittee of the Management Committee was established with a broad risk management mandate spanning financial, operational, and compliance risk.  The Bank also enhanced the coordination of risk management, resource allocation, and strategic planning through linkages between the Risk Subcommittee and our planning and resources committee.  Most indicative of this trend was the establishment in 2015 of the Risk Group’s Enterprise Risk Management Function which is responsible for providing an integrated view across all of the Bank’s risks.\n\nIn thinking about areas for future improvement, I ask myself: How can we accelerate organizational learning from our mistakes?  Does our analysis of risks and risk events sufficiently reveal organizational and structural issues that contribute to the risk?  How can I further create a positive risk culture that encourages reporting and escalating of risk issues?  Are we finding the right balance in our controls between specificity of rules and the adaptability of principles?\n\nThese and other questions will continue to focus us on improving and enhancing our policies and practices with the goal of supporting the Bank’s ability to manage risk within its risk tolerance.\n\nThank you.\n\n1https://www.newyorkfed.org/aboutthefed/fedpoints.html\n\n2 IIA Position Paper: The Three Lines of Defense in Effective Risk Management and Control, January 2013.\n\n3 https://www.newyorkfed.org/aboutthefed/audit.html"
    },
    {
        "title": "International Spillovers and Policies",
        "date": "Mar 15, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/mus160301",
        "content": "Good morning.  Thank you for the opportunity to speak here in Hangzhou about the important theme of international spillovers.  By exchanging perspectives and experiences we can forge relationships and a shared understanding that can lead to better economic outcomes.  I will focus mostly on the interaction of international spillovers with policy frameworks within and across countries.  I will do this from the vantage point of a domestic economy experiencing foreign spillovers.  I will then briefly turn to considerations from a U.S. perspective. As always, my remarks reflect my own views and not necessarily those of the Federal Reserve Bank of New York, the Federal Open Market Committee (FOMC), or the Federal Reserve System.1\n\nInternational spillovers encompass a broad topic with respect to their sources and channels.  Before narrowly focusing on monetary sources and associated channels, it seems appropriate to first take a broader view of the various sources.  Foreign spillovers can emanate from monetary, fiscal, regulatory, currency or trade policies.  Spillovers can also arise from endogenous productivity changes, from financial stability developments, and from a global financial cycle.2\n\nThe Mundell-Fleming logic remains relevant for thinking about policy options of open economies.  It provides lessons on channels of transmission, basic tradeoffs across objectives, and the effectiveness of policy responses.3  The channels of macroeconomic and financial transmission among open and interdependent economies occur through the current account of the balance of payments, the capital account, or both.  From the vantage point of the domestic economy, there are three main channels of transmission: the exchange rate; the external demand; and the interest rate or risk-premia channel.  The last can be more broadly defined as the financial conditions channel.\n\nThe intensity of transmission of spillovers will depend on the monetary policy, currency policy and the real and financial openness of the economy being affected by external developments.  Other relevant characteristics include the flexibility of labor and product markets; the depth, safety and soundness of the financial system; and the economy’s position as an international creditor or debtor.\n\nThe Mundell-Fleming framework suggests that a country with a high degree of capital mobility and a flexible currency will be in a favorable position to use monetary policy to attain its internal growth and inflation objectives.  When foreign interest rates increase and financial conditions tighten, changes in relative prices brought about by currency fluctuations can help to absorb the external shocks.  The positive expenditure switching effect from currency depreciation improves the trade balance and supports demand.  It is sufficiently larger than the negative investment demand effect from higher interest rates, such that internal balance is attained and monetary policy is able to pursue domestic inflation and growth objectives. 4\n\nDifferent policy options are available to a country with a high degree of capital mobility and a preference for currency stability.  A country may prefer currency stability as an effective nominal anchor, or because of concerns about the broader uncertainties associated with currency volatility.  Regardless of the reason and because the exchange rate is not available to absorb external shocks, this country will be in an unfavorable position to achieve domestic macroeconomic objectives by using monetary policy.  Instead, monetary policy is aimed at external adjustment.  Tighter foreign monetary policy and financial conditions need to be validated by tighter domestic monetary policy to maintain currency stability.  With domestic monetary conditions effectively imported from abroad, they may sometimes conflict with domestic growth and inflation objectives. Simply put, currency stability is prioritized.\n\nThese policy considerations are summarized in the well-known international policy trilemma.  It says that a country cannot simultaneously maintain independent monetary policy focused on domestic objectives, a high degree of capital mobility, and currency stability.  Any two of these regime characteristics can be chosen, but not all three.  While the trilemma suggests sharp tradeoffs, additional policy tools can soften the tradeoffs.\n\nFiscal policy is one tool that serves this purpose.  When foreign credit conditions tighten, expansionary fiscal policy can allow a country with a high degree of capital mobility, and a policy preference for currency stability, to effectively manage towards domestic macroeconomic objectives and internal balance.  In fact, fiscal policy can also be used to rebalance the composition of demand towards the desired mix of investment and consumption.\n\nSofter tradeoffs than implied by the trilemma are suggested by some of the empirical evidence on interest rate co-movements.  For example, there is ample evidence that domestic short-term interest rates become progressively less correlated with short-term interest rates in large foreign countries, and that monetary policy is more autonomous, as countries move from high capital mobility to low capital mobility, and from rigid currencies to flexible currencies.5\n\nUsing foreign exchange reserves to support currency stability is unlikely to be as effective in enabling the country to attain domestic growth and inflation objectives.  If losses of foreign reserves are not sterilized, the effect will be similar to that of domestic monetary tightening, and as already discussed, might work against domestic growth and inflation objectives.  If reserve losses are sterilized and the underlying cause of capital outflows is not addressed, reserves losses can be persistent.\n\nMacroprudential policy provides an additional tool that can add degrees of freedom and help soften the tradeoffs of the trilemma. Macroprudential policy can potentially temper spillovers by modulating the type of buildup of financial imbalances that contributed to the Asian crisis of 1997, countless other emerging market crisis, the global financial crisis of 2008 and the subsequent euro area crisis of 2011.  Progress continues to be made in designing macroprudential policy frameworks, in definition of objectives, and in developing appropriate instruments, calibration and communication.\n\nCapital flow management can be viewed as the part of the macroprudential toolkit that operates across borders.  Over the last five years, capital flow management has received increased attention, first in managing inflows and, more recently, in managing outflows.6  A consensus has emerged that capital flow management can be useful as part of a broader set of policy tools.  In fact, the effectiveness of capital flow management may be limited if not appropriately supported by monetary, fiscal, exchange rate and prudential policies, or the warranted macroeconomic adjustment.  Also, capital flow management practices that are transparent, temporary, and targeted seem to work best.\n\nSpillovers from foreign countries are not confined only to conventional monetary policies, nor is the U.S. the sole source of transmission.  There also is evidence that the unconventional monetary policies of the ECB and the BOJ have spilled over onto the U.S. yield curve and broader financial conditions.7  This is interesting because all three monetary areas share the characteristics of flexible currencies and capital mobility.\n\nI will now conclude with a few related comments from the U.S. perspective.\n\nThe Fed’s statutory mandate, like that of other central banks, is domestic.  The Federal Reserve Act states that the Fed should “promote effectively the goals of maximum employment, stable prices, and moderate long-term interest rates”.  Within this mandate, there are good reasons to consider the international effects of monetary and regulatory policies.  The U.S. economy and the economies of the rest of the world have important feedback effects on each other, and adverse international spillovers can harm U.S. prosperity.\n\nLike other central banks, the Federal Reserve can help promote global prosperity and stability, by promoting growth and stability at home.  The FOMC has taken a number of steps in recent years to increase transparency and improve communications, to effectively convey policy intentions to market participants and policymakers. The Fed has also made considerable progress in strengthening the safety and soundness of the U.S. financial system, through improved regulation and supervision of banks and a broader group of systemically important institutions.  A stronger U.S. banking system better protects against future shocks, provides a more solid foundation for domestic growth, and therefore also enhances prospects for growth and financial stability abroad.\n\nI look forward to a discussion with fellow panelists and also to engaging with conference participants over the next two days.\n\nThank you for your attention.\n\nReferences\n\nEhrmann, M. & M. Fratzscher (2005). Equal Size, Equal Role? Interest Rate Interdependence Between the Euro Area and the United States, Royal Economic Society, vol. 115 (506), pages 928-948, October.\n\nFarhi, E. and I. Werning (2014). Dilemma not Trilemma? Capital Controls and Exchange Rates with Volatile Capital Flows, IMF Economic Review, pp. 62: 569-605.\n\nFederal Open Market Committee (2016), Statement on Longer-Run Goals and Monetary Policy Strategy, Adopted effective January 24, 2012; as amended effective January 26, 2016\n\nFederal Open Market Committee (2016), Policy Statement, January 27, 2016\n\nFleming, M. (1962). Domestic Financial Policies under Fixed and under Floating Exchange Rates, IMF staff papers, 9 (3), pp. 369-380.\n\nG20 (2010), Summit Leaders’ Declaration, Seoul, Korea, November 11 – 12, 2010\n\nGoldberg, L. (2013). Banking Globalization, Transmission, and Monetary Policy Autonomy, Sveriges Riksbank Economic Review (Special Issue), pp. 161-193.\n\nIMF (2105), The IMF’s Approach to Capital Account Liberalization Revisiting the 2005 IEO Evaluation, Independent Evaluation Office, Washington, D.C.\n\nIMF (2015), 2015 Spillover Report, Washington, D.C.\n\nIMF (2012), The Liberalization and Management of Capital Flows: an Institutional View, Washington, D.C.\n\nKlein, M. and Shambaugh, J. (2015). “Rounding the Corners of the Policy Trilemma: Sources of Monetary Policy Autonomy”, American Economic Journal: Macroeconomics 2015, 7 (4): 33–66, http://dx.doi.org/10.1257/mac.20130237\n\nKrugman, P. (1999). Balance Sheets, the Transfer Problem, and Financial Crises, in International Finance and Financial Crises: Essays in Honor of Robert Flood, edited by Peter Isard, Assaf Razin and Andrew Rose (Springer Netherlands) pp. 31-55.\n\nMundell, R. (1963). Capital Mobility and Stabilization Policy under Fixed and Flexible Exchange Rates, Canadian Journal of Economic and Political Science, 29 (4), pp.475–485.\n\nObstfeld, M. (2015). Trilemmas and tradeoffs: living with financial globalization, BIS Working Papers No 480.\n\nObstfeld, M., Shambaugh, J., and Taylor, A. (2005). The Trilemma in History: Tradeoffs among Exchange Rates, Monetary Policies, and Capital Mobility, Review of Economics and Statistics, vol. 87 (3).\n\nRey, H. (2014). International Channels of Transmission of Monetary Policy and the Mundellian Trilemma, London Business School, Paper presented at the 15th Jacques Polak Annual Research Conference Hosted by the International Monetary Fund, Washington, D.C., November 13–14.\n\nUnited States Congress (1913), Federal Reserve Act, Amended 1977, Section 2A\n\n1 Linda Goldberg, Paolo Pesenti, Joseph Tracy and Matthew Higgins contributed to these remarks.\n\n2 A global financial cycle refers to the global co-movement of credit by banks and capital markets, asset prices, term premia, and risk premia.\n\n3 Fleming (1962) and Mundell (1963).\n\n4 Krugman (1999). The balance sheet effects imparted from a large international debtor position may work against achievement of an improved external balance and attainment of internal balance.\n\n5 Obstfeld, Shambaugh and Taylor (2005), Goldberg (2013), Klein and Shambaugh (2015) and Obstfeld (2015).\n\n6 G20 (2010), IMF (2012), Farhi and Werning (2014) and IMF (2015).\n\n7 http://www.federalreserve.gov/aboutthefed/section2a.htm"
    },
    {
        "title": "The U.S. Economic Outlook and Implications for Monetary Policy",
        "date": "Feb 29, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud160229",
        "content": "Thank you, Deputy Governor Chen, for the opportunity to speak at this inaugural People’s Bank of China and Federal Reserve Bank of New York 2016 Joint Symposium. It is a wonderful occasion for our two central banks to meet and share ideas on important global macroeconomic and monetary policy issues.\n\nIt is especially satisfying to me to be here in Hangzhou to see the result of my discussion last February with Governor Zhou about this kind of policy and research exchange. I thank you and the staff of The People’s Bank of China for developing this idea into such an ambitious work agenda, and for assembling such a distinguished group of contributors and participants for this symposium. We at the New York Fed look forward to future collaborative efforts to strengthen the relationship between our two institutions, especially given the key roles that China and the United States play in the global economy. By working together I believe we can achieve better outcomes for our countries and for the global economy. Today, although I will focus mainly on the U.S. economic outlook, I also will discuss how global economic and financial market developments factor into my thinking about U.S. monetary policy. As always, what I have to say today reflects my own views and not necessarily those of the Federal Open Market Committee (FOMC) or the Federal Reserve System.1\n\nU.S. Economic Outlook\nRecent economic and financial developments have not yet led me to make a fundamental change in my outlook for U.S. growth in 2016. At this point, I have marked down my growth outlook very modestly. However, financial market conditions have tightened since the start of the year, mostly in response to international developments. If this tightening of financial conditions were to persist, it could potentially lead to a more significant downgrade to my outlook.\n\nI continue to expect that the economy will expand over the course of this year at a pace slightly above its long-term trend—sufficient to push the unemployment rate down a bit further and to more fully utilize the nation’s labor resources. On inflation, we continue to fall short of our 2 percent objective for the personal consumption expenditure (PCE) deflator. This shortfall looks likely to continue longer than I had earlier anticipated due to the persistent strength of the dollar and weakness in energy prices. However, I continue to expect a gradual return to our 2 percent objective as the transitory factors that have held down inflation dissipate.\n\nIn assessing the current state of the U.S. economy, there are divergent signals, with weakness in some spending and production indicators, but robustness in a number of labor market indicators. In the fourth quarter, investment spending for both equipment and structures fell, and net exports and inventory investment were drags on growth. These developments partly reflect the slowing of global growth and the impact of the dollar’s appreciation since mid-2014. This weakness has been particularly evident in U.S. manufacturing. However, the latest ISM non-manufacturing index suggests the possibility of a slower pace of growth also in the services sector, which has been relatively robust. Consumer spending growth—which had been a brighter spot for most of 2015—also slowed notably in the fourth quarter.\n\nIn contrast, the U.S. labor market has remained healthy and supportive of growth. Over the course of 2015, payroll employment increased an average of about 228,000 per month. Although the January increase of 151,000 was below last year’s pace, other aspects of the report were strong. The January data indicated substantial gains in wages and hours worked, which implies healthy real income growth for the household sector. Moreover, the amount of slack in the labor market continues to diminish based on a number of measures. The unemployment rate declined to 4.9 percent. This was accompanied by further declines in both the share of people working part-time that would like full-time work, as well as the share of people not part of the labor force. Consistent with a tighter labor market, the share of the working-age population that is employed has risen gradually, and the labor force participation rate has stabilized.\n\nWhile the weakness in some indicators has led me to make small reductions in my forecast, my overall outlook has not changed substantially. In making this determination, a central question is how to reconcile these cross-currents. One has to also factor in that the real GDP growth rate in the fourth quarter was only 1.0 percent and the four-quarter change was 1.9 percent—low even by the subdued standards of this expansion. It is important, though, to recognize that a number of transitory factors held down growth in the fourth quarter—most notably lower utility spending due to warm weather and a decline in the rate of inventory investment.\n\nAs these transitory factors depressing the pace of economic growth dissipate, the core strengths for the economy should reassert themselves. I anticipate that consumption and housing activity will expand at a moderate pace. Continued job and wage gains, as well as low energy prices, should support real disposable income growth and consumer spending. Retail sales in January provide evidence of this support. The housing market should remain on a solid trajectory, supported by rising employment and low mortgage rates. In addition, the fiscal 2016 budget package passed at the end of last year, which extended a number of tax breaks and eased caps on spending, means that fiscal policy should provide a moderate stimulus this year. I believe that these positive factors should be sufficient to offset weakness in manufacturing and business-fixed investment.\n\nPutting this all together, I expect real GDP growth of about 2 percent in 2016, slightly below the average pace of growth in this expansion, and a bit above my estimate of the potential growth of the U.S. economy. If this materializes, then we should see some further reduction in the unemployment rate to around my estimate of the rate—about 4¾ percent—that I view as consistent with stable inflation over the long term.\n\nTurning to the outlook for inflation, headline inflation on a year-over-year basis has begun to rise as the sharp falls in energy prices in late 2014 and early 2015 are removed from the calculations. However, inflation still remains well below the Federal Reserve’s 2 percent objective. As the FOMC has noted in its statements, this continued low inflation is partly due to recent further declines in energy prices and ongoing impacts of a stronger dollar on non-energy import prices. Although energy prices will eventually stop falling and the dollar will stop appreciating, these factors appear to have had a more persistent depressing influence on inflation than previously anticipated.\n\nThis continued period of low headline inflation is a concern, in part, because it could lead to significantly lower inflation expectations. If this drop in inflation expectations were to occur, it would, in turn, tend to depress future inflation. Evidence on the inflation expectations front suggests some cause for concern. In particular, market-based measures of longer-term inflation compensation derived from nominal and inflation-indexed Treasury securities have fallen to very low levels. In addition, some survey measures of household inflation expectations have recently moved lower.\n\nWith respect to the market-based measures, there are some reasons to discount the decline. Several term structure models, including those estimated at the New York Fed, attribute most of this decline to a decrease in term premiums—what investors demand for insurance against inflation risks—and not to a decrease in long-term inflation expectations. These models suggest that, over the indicated horizon, investors anticipate that low growth will occur along with low inflation. Still, given the extent to which inflation compensation has fallen since mid-2014, I believe that it is prudent to consider the possibility that longer-term inflation expectations of market participants may have declined somewhat.\n\nWhat I find more concerning is the decline in some household survey measures of longer-term inflation expectations. For example, the median of three-year expectations from the New York Fed’s Survey of Consumer Expectations has declined over the past year to its lowest reading in the survey’s short history, and the longer-running University of Michigan measure is at the bottom of its historical range. Further declines in either measure would be worrisome. To date, these declines have not been sufficiently large for me to conclude that inflation expectations have become unanchored. However, these developments merit close scrutiny, as past experience shows that it is difficult to push inflation back up to the central bank’s objective if inflation expectations fall meaningfully below that objective. Japan’s experience is cautionary in this regard.\n\nIn sum, I still anticipate that the combination of decreasing resource slack and anchored longer-term inflation expectations will contribute to inflation rising to our 2 percent objective over the medium term. Even so, because of the more persistent effects of energy and commodity price declines and U.S. dollar appreciation, the return of inflation to that goal may be slower than I earlier anticipated. This does not deny the possibility of some upside surprise―such as a sharp upswing in wage growth triggered by low unemployment. But, on balance, I am somewhat less confident than I was before. Partly, this reflects my assessment that uncertainty to the outlook has increased and that downside risks have crept up, a topic to which I now turn.\n\nAssessment of Risks to the Outlook\nThe forecast that I have just described is my best assessment of how the U.S. economy will evolve over 2016. But there is uncertainty and risk relative to this baseline forecast, which also needs to be taken into consideration in assessing the implications of the outlook for monetary policy.\n\nThere are two key considerations to this risk assessment. First, are the perceived uncertainties about the forecast higher, lower or about average as compared to the past? Second, are the risks symmetric around the baseline forecast or skewed upward or downward? The level of uncertainty is important in that it relates to the expected magnitude of forecast errors. The balance of risks is important in that it relates to the expected sign of those forecast errors.\n\nAt the New York Fed, we use surveys, internal models and judgment in forming our risk assessment. Some surveys, such as the U.S. Survey of Professional Forecasters (SPF), ask respondents for both point and density forecasts. The density forecasts take the form of respondents filling out a histogram of their probability assessments that the forecasted outcome will fall into given intervals. These histograms can be aggregated and the resulting aggregate distribution can be used to assess both the level of uncertainty as well as the balance of risks. Recent U.S. SPF surveys have not indicated any significant changes in the overall degree of uncertainty around growth or inflation. The balance of risks have been roughly symmetric for inflation and tilted to the downside for growth.\n\nWe supplement this survey evidence with assessments from internal models. These models are data-driven and estimate percentiles of the distribution of possible forecast outcomes for growth and inflation. Our estimated models have indicated a greater degree of uncertainty than reflected in the U.S. SPF, and suggest downside risk to both the growth and inflation forecast. But, I would caution that these models are still relatively new in terms of their development and their results need to be supplemented by judgment.\n\nNow, putting these inputs and my judgment together, I see the uncertainties around my forecast to be greater than the typical levels of the past. This assessment reflects the divergent economic signals I highlighted earlier, and is consistent with the turbulence we have seen in global financial markets. At this moment, I judge that the balance of risks to my growth and inflation outlooks may be starting to tilt slightly to the downside. The recent tightening of financial market conditions could have a greater negative impact on the U.S. economy should this tightening prove persistent and the continuing decline in energy and commodity prices may signal greater and more persistent disinflationary pressures in the global economy than I currently anticipate. I am closely monitoring global economic and financial market developments to assess their implications for my outlook and the balance of risks.\n\nU.S. Monetary Policy in a Global Environment\nThus far, my remarks have focused on the U.S. outlook and the risks relative to this outlook. This focus is appropriate. Like other central banks, our monetary policy mandate concerns domestic objectives: maximum sustainable employment and price stability. Our monetary policy actions, however, often have global consequences that, in turn, influence the U.S. economy and financial markets. At the same time, external factors can impact the monetary policy transmission mechanism in the U.S. and influence the effectiveness of our monetary policy in achieving our objectives. We cannot appropriately calibrate policy without keeping these spillover and feedback effects in mind.\n\nIn some cases, these effects can be significant. An example is the market volatility we saw in the spring and summer of 2013 during the so-called “taper tantrum.” In the U.S., we saw a spike in Treasury yields, with the 10-year rate rising by more than 100 basis points from early May of that year before peaking in September. Financial markets in emerging market economies (EME) were hit hard as well, with declines in equity prices, a widening in sovereign debt spreads and a sharp increase in foreign exchange volatility.\n\nVolatility in EME financial markets reemerged last summer and has continued into this year. For EMEs as a group, declines in equity prices and currency values have been larger than what we experienced during the taper tantrum. The recent episode, however, appears to largely reflect concerns about risks emanating from EMEs themselves, rather than concerns about the direction of Fed policy.\n\nThree related concerns appear to be at the forefront. First, there has been a broad-based slowdown in EME growth. This slowdown is not new. Each year since 2012, EME growth has come in well short of expectations that existed at the start of the year. It is difficult, however, for market participants to recognize and adjust to large structural shifts in real time. There is still considerable uncertainty about the trajectory of long-term EME growth, and therefore the appropriate pricing of EME financial assets. In that context, heightened market volatility is not surprising.\n\nSecond, this slowdown in EME growth is happening after a long upswing in credit expansion in these countries. Based on BIS data for 23 of the largest EMEs, since the end of 2007 credit to their private nonfinancial sectors has risen from roughly 80 to 135 percent of GDP. In several countries, the increase in private credit penetration exceeds 40 percent of GDP—roughly the same order of magnitude that we witnessed in other periods that preceded financial crises. Although the magnitude of the EME credit boom has been apparent for some time, with EME growth now on a more subdued trajectory and the global financial environment less forgiving, market participants have begun to sharpen their focus on the risk of larger than expected credit losses as the credit cycle reverses.\n\nThird, China has entered a period of economic transition, from a growth model based on investment, manufacturing and rapid state-directed credit growth to one emphasizing consumption, services and more sustainable market-directed credit intermediation. This transition also involves a downshift in China’s trend growth rate, which is inevitable given the level of economic development in China and the shrinking margin of underutilized labor market resources. Economic transitions are always difficult to manage, and invariably present policymakers with unexpected challenges. It is not surprising, then, that market participants are focused on the risk that the downshift in Chinese growth could prove to be more pronounced than anticipated. I want to be clear. The rebalancing process now underway in China is both necessary and appropriate, and will unfold over a period of years. How to pragmatically strike the best balance between progress on long-term reform and support for short-term growth is, of course, a matter for policymakers in China. You can be sure that you will have the backing of the official international community as you oversee this important transformation.\n\nFinally, the slowdown in Chinese and broader EME growth has been a key factor behind the sharp drop in global commodity prices since mid-2014. The income streams accruing to commodity export countries have been adversely impacted, requiring sometimes painful downward adjustments in investment and economic activity in these countries. Here, too, it may take time for markets to find a new equilibrium, as investment projects that were based on the price structure prevailing a few years ago are still coming online.\n\nIn most cases, EMEs appear to have the tools and the resources to help manage their way through the current period of difficulties. There are fewer of the pegged exchange rate regimes that often came violently undone during past periods of acute market stress. Monetary policy regimes are generally better structured and more coherent. Banking systems are better capitalized and supported by stronger regulatory and supervisory frameworks. Foreign exchange reserve cushions are larger and generally still ample. I think we can remain cautiously optimistic about EMEs’ prospects overall in the years ahead, although particular countries will face considerable challenges over the near term.\n\nHow will we at the Fed take in and respond to these developments? As I’ve already noted, foreign developments factor into our assessment of the U.S. outlook. Recent FOMC statements have sometimes made this connection explicit, noting that the Committee will be monitoring foreign economic and financial conditions closely. The weakness we have been seeing in EME economies and markets is one of the considerations behind the Committee’s assessment that the appropriate pace of policy normalization is likely to be gradual.\n\nThe federal funds rate is only one element of the broader set of financial conditions affecting the U.S. growth and inflation outlook. Tighter financial conditions abroad do spill back into the U.S. economy, and policymakers must take this into account in their assessment of appropriate monetary policy. Of course, this does not mean that we will let market volatility dictate our policy stance. There is no such a thing as a “Fed put.” What we care about is the country’s growth and inflation prospects, and we take financial market developments into consideration only to the extent that they affect the economic outlook.\n\nOur focus on the global implications of our policies also has a broader dimension. Given the central place of U.S. markets in the global financial system and the dollar’s status as the leading global reserve currency, we in the United States have a special responsibility to be good stewards of the global economic commons. This makes it all the more important that we conduct policy transparently and according to clear principles.\n\nThis highlights the importance of effective Fed communication. I noted earlier our attempts in the spring of 2013 to provide guidance about the potential timing and pace of tapering may have served to confuse market participants. Market participants seemed to conflate the prospective tapering of asset purchases with monetary policy tightening, and pulled forward their expectations about the likely timing of liftoff, thus raising their expected path for the federal funds rate. Since then, we have adjusted our approach with improved results. The actual tapering of the Fed’s asset purchase program went smoothly and liftoff of our policy rate from the zero lower bound was in line with widely held market expectations. While the future path of policy is not on a pre-set course and must remain data dependent, we will strive to be similarly effective in our communications going forward.\n\nAs you know, we’ve taken a number of steps in recent years to increase transparency and improve our communications. This includes regular press conferences by the Fed chair following FOMC meetings; the publishing of growth, inflation, unemployment and federal funds rate projections by FOMC participants; a concerted attempt to lay out the guideposts that the FOMC will look at to assess progress toward our mandate; and an equally concerted attempt to lay out how our various policy tools―such as interest on reserves, overnight reverse repurchase (ON RRP) operations and reinvestment of principal payments from our asset holdings—will be used to pursue our objectives.\n\nWe are also working to be better stewards in safeguarding financial stability. Simply put, we failed to act early enough and decisively enough to stem the credit excesses that spawned the financial crisis and Great Recession. The U.S. was not alone in this shortcoming, but, given our position in the global financial system, we should have done better. We’ve taken important steps through new legislative mandates and a broader effort to rethink our regulatory and supervisory framework. In particular, systemically important banking organizations must now hold capital and liquidity buffers that are better aligned with their risk profiles. Many other changes have also been implemented, such as the central clearing of standardized over-the-counter (OTC) derivatives contracts, which should help make the global financial system more resilient and robust.\n\nWhile these efforts remain a work in progress, I think they will reduce the risk of repeating the mistakes of the past decade, and enable us to take a more proactive stance toward addressing potential future vulnerabilities. Of course, we at the Fed are not alone here. Since the financial crisis, central banks worldwide have been engaged in a broad rethinking of how better to fulfill their mandates.\n\nLet me close with a final thought. Adam Smith in the Wealth of Nations introduced the concept of the “invisible hand.” Smith argued that individuals acting in their self-interest can collectively promote the public interest. This concept, I believe, also often applies to international monetary policy.2 The biggest problems that countries create for others often stem from getting policy wrong domestically. Recession or instability at home is often quickly exported abroad. Equally important, growth and stability abroad makes it easier to set policy at home. Central banks, therefore, by individually acting on their domestic economy mandates, can collectively promote the global economy.\n\nThank you for your kind attention.\n\n1 Matthew Higgins, Jonathan McCarthy, Paolo Pesenti and Joseph Tracy assisted in preparing these remarks.\n\n2 There are important exceptions of course. Two noteworthy exceptions include “beggar-thy-neighbor” policies in which countries engage in unilateral competitive currency devaluation and trade protectionism."
    },
    {
        "title": "Money Markets after Liftoff: Assessment to Date and the Road Ahead",
        "date": "Feb 22, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/pot160222",
        "content": "Thank you. It’s an honor to speak to you today as part of the 70th Anniversary Celebration of the School of International and Public Affairs at Columbia University, and I would like to thank my former colleague and longtime friend Trish Mosser for the invitation. The programs offered by SIPA, which aim to train young leaders to address the world’s most pressing public policy problems, are especially relevant to the mission of the New York Fed, and many members of the Bank’s Markets Group are alumni.1\n\nI’m here to offer a few thoughts on money markets and the implementation of monetary policy. Because of the large size of the Federal Reserve’s balance sheet, the staff at the Federal Reserve spent the past several years developing a new and innovative framework to control money market rates, and working to increase the amount of data we receive about trading in those markets. In my remarks today, I will review the performance of this new framework and discuss key insights from those new data collections.\n\nBefore 2008, bank reserves were scarce, and the Federal Reserve influenced overnight interest rates by making small adjustments in the supply of reserves. The expansion in the Federal Reserve’s balance sheet during and after the financial crisis means that reserves are now abundant, and small adjustments in the quantity of reserves will not have much influence on overnight interest rates. Instead, the Federal Reserve’s new framework is premised on the payment of interest on reserves and on ensuring sufficient competition in money markets so that the rate of interest paid on reserves is passed through to other money market rates and thus to deposit rates offered to households and firms.2\n\nImplementing policy in this new way has required a significant shift in the staff’s analytical focus, toward quantitative, data-driven analysis of money market structure. This work was accomplished by a large roster of highly talented staff at the Board of Governors, the New York Fed, and elsewhere in the Federal Reserve System. Of course, the development of this new framework entailed frequent interaction with the Board of Governors and the Federal Open Market Committee (FOMC), which are responsible for the tools used to implement monetary policy.\n\nI’ll focus my remarks on a few key questions.\n\nFirst, what is this new framework and why was it necessary? In this section, I will explain why the Federal Reserve was able to lift rates off zero without requiring large-scale “reserve draining.”3\n\nNext, what do the new data on money market trading say about how this new framework is affecting market conditions? As part of this discussion, I’ll walk through the enhanced data that, starting in March, the New York Fed will use to calculate the effective federal funds rate.4\n\nFinally, what questions remain, and what can be done to learn more about the issues they raise? These include the level of capacity required in the Federal Reserve’s reverse repo operations and the consequences of changes in the regulatory rules for banks and money funds.\n\nAs always, these views are mine alone and do not necessarily reflect those of the New York Fed or the Federal Reserve System.\n\nA New Approach for Policy Implementation\n\nWhy are we following a new approach to implementing monetary policy?\n\nBefore the financial crisis, the Federal Reserve followed a well-worn playbook for monetary policy implementation that was based on scarcity of reserves. Reserves left at the Federal Reserve earned zero percent interest. As a result, to achieve the FOMC’s desired level of the federal funds rate, the Open Market Desk at the New York Fed (Desk) estimated the amount of reserves to add or subtract that would intersect supply and demand at the target interest rate directed by the FOMC.5 All else equal, if the Desk wanted interest rates to go up, it slightly reduced the amount of reserves; if it wanted rates to go down, it slightly increased the amount.6 So long as the Desk was able to effectively forecast the appropriate amount of reserves to add or subtract, temporary open market operations provided it with an efficient means to add or subtract them, and therefore allowed for good control over the federal funds rate.7 These operations were conducted as repos or reverse repos secured by Treasury securities or other high-quality collateral.8\n\nBack then, a typical day on the Desk focused on forecasting the demand for reserves created by reserve requirements and other sources, and projecting autonomous factors affecting reserves. 9, 10 With very small amounts of excess reserves in the system, there was a straightforward relationship between rates in a range of money markets. Thus, the Desk was able to target a rate in one part of the unsecured market by conducting small secured operations as needed with primary dealers.\n\nSince the financial crisis, the environment has changed in two important ways. First, by the end of 2014, following the large-scale asset purchase programs, the Federal Reserve balance sheet was funded by about $3.1 trillion in liabilities other than Federal Reserve notes, which were mostly in the form of reserves in excess of the amount banks were required to hold; in contrast, there were only $64 billion of non-Federal Reserve note liabilities in June 2007, of which only about $2 billion were excess reserves. As a result, reserves are now anything but scarce. Second, Congress granted the Federal Reserve an important new tool: the authority to pay interest on reserve balances, or IOR.11\n\nIn many countries, and in theory, a standing facility that pays IOR provides a floor on interest rates. The reason is simple: if a private counterparty can invest cash at the central bank, then given the safety and convenience of this investment, it will dominate any other overnight investment at that interest rate. Further, if banks can acquire funds in the wholesale market at rates below IOR, then competition for these funds will bid up these rates close to that paid on reserves.\n\nThe Federal Reserve instituted interest on reserves in late 2008, and the interest rate on reserves turned out not to be a floor; market interest rates fell significantly below it. With the large levels of excess reserves in the system, certain institutional aspects of money markets in the United States—including bank-only access to IOR, credit limits imposed by lenders and other impediments to market competition, and the costs of balance sheet expansion—appear to create frictions that have made IOR act more like a magnet that pulls up short-term interest rates than a firm floor beneath them. Thus, in considering how to implement policy normalization, the Federal Reserve evaluated a variety of ways to increase competition for funds among borrowers and thereby improve the control provided by IOR.12\n\nThe Federal Reserve initially considered an approach to policy normalization that was based on a combination of term operations and asset sales or redemptions aimed at making reserves sufficiently scarce so that, once the FOMC decided to tighten monetary policy, this tightening could be implemented using IOR within a framework that was otherwise similar to that used before the crisis.13 As part of this approach, the Federal Reserve acquired a new set of reverse repo counterparties, including money market funds and government-sponsored enterprises, in addition to the primary dealers with which it had traditionally transacted. These new counterparties were major investors of cash in overnight markets and had capacity to facilitate the “draining” of substantial reserve balances, if offered attractive term investments with the Federal Reserve. In addition, the Federal Reserve developed a term deposit facility to drain banks’ reserve balances.14 This playbook of draining reserves back to reserve scarcity to support the transmission of interest on reserves into market rates is standard among central banks. If policymakers had chosen to do so, the Federal Reserve could have made such a term reserve draining approach work to help control money market rates in the United States.15\n\nSo, why did the Federal Reserve not follow this approach of large-scale reserve draining? As the balance sheet continued to grow as a result of asset purchase programs, it became apparent that such reserve draining operations would have been very large in comparison to the size of private money markets. As a result, they probably would have had lasting effects on the structure of those markets—for example, by reshaping the structure of term money market activity according to the structure of the Federal Reserve’s operations. These lasting effects could have been especially large if the balance sheet did not run off quickly.\n\nFor this reason, the Federal Reserve instead pursued offering an overnight investment opportunity that could, with sufficient capacity, intensify competition in money markets and, without necessarily draining reserves, enhance the transmission of IOR into other overnight money market rates. This approach entailed a twist on a traditional Federal Reserve tool, reverse repos: Instead of running quantity-based, term operations aimed at altering reserve levels, the Desk would run interest-rate-based overnight operations aimed directly at influencing market rates.\n\nA number of operational features were required to implement such an overnight reverse repo, or ON RRP, facility: It would need same-day settlement;16 the operation would need to be run predictably, every day, and as late in the day as possible, to give lenders time to bargain with other counterparties using the outside option of investing with the Federal Reserve;17 an appropriate spread below IOR would be required to ensure that the facility neither induced large changes in the structure of money markets nor lost the ability to support interest rate control;18 and the operations would need enough unused capacity that lenders could credibly propose to leave borrowers that did not offer an adequate interest rate.19\n\nThis approach works in theory, and, with the support of staff across the Federal Reserve System, a theoretical framework was developed around money market rate control through the use of this framework.20 The ON RRP facility was also tested for more than two years. This theoretical and empirical examination gave the Federal Reserve confidence that it could effectively raise rates when the time came while limiting undesirable effects on financial market structure, and also ensured that additional term tool options were available if the combination of the overnight tools – IOR and ON RRP – was not sufficient to provide interest rate control.21\n\nAt this point, you might have a question: from late 2008 to late 2015, the federal funds rate traded well within the FOMC’s target range of zero to 25 basis points.22 Shouldn’t this have been conclusive evidence as to the ability of the operational framework to provide sufficient control of interest rates?\n\nUnfortunately, the answer is no, because there are natural features of the financial system that likely provide support to interest rates near the zero lower bound, or ZLB. These features include the availability of physical cash and a behavioral aversion by some money market investors to investing at negative rates, and also encompass certain unique features of money markets in the United States, such as legal and regulatory incentives applicable to money market mutual funds and the ability of the government-sponsored enterprises to leave unremunerated deposits at the Federal Reserve.23\n\nAs a result, while the pre-liftoff testing suggested that the tools were likely to work well, it wasn’t possible to determine with complete certainty the extent to which market rates were driven by the tools or by ZLB effects. So we couldn’t completely rule out that the federal funds rate and other money market rates might not go up one for one with rises in administered rates. One might also worry that money market rates might not move together as rates rise, meaning that, for example, a disconnect might emerge between secured and unsecured rates, or between overnight and term instruments.24 Either situation could result in impaired transmission of monetary policy into broad financial conditions.\n\nAssessing the Performance of the New Framework\n\nTo summarize my conclusion about the performance of the operating framework to date: I am extremely pleased with what we’ve seen so far.\n\nIn discussing why I view the framework as effective, it’s helpful to start by explaining what it means for the tools to work well. I think about this issue through three lenses: interest rate control, avoiding unintended impact on the structure of the financial system, and avoiding financial instability. I’ll touch on what each of these mean, and then I’ll review the quantitative evidence since liftoff on how well things have gone.\n\nInterest Rate Control\n\nClearly, interest rate control is the paramount objective. It is of great importance that the public is confident that the federal funds rate will be, on average over time, within the target range set forth by the FOMC, and that other money market rates will continue to move closely with changes in the federal funds rate. This way, expectations for the FOMC’s future policy stance will be properly incorporated into the term structure of interest rates, and thereby appropriately affect broad financial conditions and the broader economy.\n\nIt’s not necessary that the effective rate stay within the target range each and every day. Before the crisis, there was usually a bit of variability in the effective rate owing to limitations on the Desk’s ability to achieve completely precise daily forecasts for reserve demand. Now, as I said earlier, the FOMC sets the target as a range, not as a point value. The federal funds rate can vary within that range, and it could even move outside that range on occasion, without materially affecting the economy. 25\n\nWe have instituted a data-driven, cross-market framework for evaluating interest rate control. In unsecured markets, we gather daily, transaction-level data through a new Federal Reserve statistical collection called the FR 2420. These rich data give us much greater insight than we had before the crisis about how these markets are structured, which is important to developing and maintaining an operating regime based on improving competition among borrowers in money markets, in the context of a large amount of reserves and substantial ongoing regulatory and business model changes.\n\nMonetary control does not stop at the effective federal funds rate: The Federal Reserve also wants to ensure that the stance of monetary policy is passed through into other money market rates. For example, we evaluate such transmission in the Treasury tri-party repo market, the market in which ON RRP is conducted, by collecting similar trade-by-trade data from clearing banks. In addition to these overnight markets, we pay close attention to the Eurodollar market, which I’ll address later, as well as term money market rates.\n\nAvoiding Unintended Effects on the Structure of the Financial System\n\nA second lens for evaluating the operational framework is the extent to which it avoids creating incentives that might result in undesirable changes in the structure of the financial system. This means that the framework shouldn’t have lasting unintended effects on how people invest their money or on how financial institutions interact with each other.\n\nThe Federal Reserve is operating the ON RRP on a temporary basis, primarily because the balance sheet is currently large. The FOMC’s policy normalization principles and plans make the temporary nature of the ON RRP clear by stating that it will be discontinued when it is no longer needed to help control the federal funds rate.26 This intention was noted again in the minutes to the January FOMC meeting. As a result, we expect that firms would not make structural changes in how they operate in response to the ON RRP, to avoid the cost of unwinding those changes when it is discontinued. The intention of the FOMC to impose an aggregate cap on the ON RRP, as described in the January minutes, will mitigate this risk.\n\nWe closely monitor industry structure and fund flows to detect whether the framework is having any unintended impact. For example, we watch banking data and money fund holdings to see if savings are shifting between the two, because if such a shift had to reverse as we wound down the ON RRP, those flows could potentially create costs to the firms concerned and also unnecessary problems for market functioning. We also keep track of prime money funds converting into government-only money funds, because if they do so on the mistaken impression that the Federal Reserve would provide a risk-free investment opportunity indefinitely and on a very large scale, they might later have to unwind that change when their error became apparent.\n\nAvoiding Financial Instability\n\nThe third lens for evaluating the framework is the extent to which the framework avoids augmenting risks of financial instability. In particular, the framework shouldn’t create a risk that, in times of stress, money market lenders will rapidly disintermediate their usual counterparties and come to the Federal Reserve instead, such as through the ON RRP facility.27 Of course, flight-to-quality effects existed in financial markets long before we put the new framework into place—what I’m referring to is a situation where the framework facilitates those effects or makes them larger.\n\nWe address the risk of sudden surges in take-up in a couple of ways. For example, we can employ vigilant market monitoring to detect any such shifts and allow the FOMC to formulate a response. We also use per-counterparty caps, and can use an aggregate cap.\n\nEvidence to Date on Control\n\nNow, let’s discuss what the data tell us about how well the framework performed.\n\nThe distribution of traded rates in the federal funds market, shown in Figure 1, essentially made a parallel shift higher after liftoff. These data are drawn from the new FR 2420 data collection I discussed earlier, and they show an average over all days from liftoff to this past Wednesday, leaving out December 31. Nearly all trading occurred within the new range—it’s normal for there to be a little bit above the range—suggesting that our framework was broadly effective in moving traded rates upward.\n\nBecause traded rates moved upward in this way, we have so far achieved an excellent level of control over the federal funds rate. Since December 17, the day after the FOMC meeting, the effective federal funds rate, calculated under its current methodology as a volume-weighted mean, has traded within the FOMC’s new 25-to-50-basis-point range on all but one day, which I’ll come back to. This is shown in Figure 2. The rate has averaged around 36 basis points, about 25 basis points above its average level over the analogous year-ago period, when the target range was zero to 25 basis points.\n\nLet’s consider the calculation of the effective rate for a moment. Last year, we announced some changes to that calculation, the most significant of which is that, starting in March, we will switch to calculating it as a volume-weighted median using data drawn from the FR 2420 data collection, rather than a volume-weighed mean using data provided by federal funds brokers. The change in the data source was made to make the calculation process more robust.28 We changed the calculation methodology because, while the mean and median are usually very close to one another, when they differ the median is a better reflection of money market activity, and because using a median enhances the reliability and integrity of the rate.29 The New York Fed published a technical note this past July that describes in detail the benefits of this change.30\n\nOur recent experience has confirmed the expectation that the transition to the median in March will result in little overall impact on the level of the effective rate over time. Figure 3 shows a time series of the mean and median rates computed from the FR 2420 data. As you can see, they remain quite close to one another.\n\nThe FR 2420 data also allow us to examine closely the distribution of traded rates in the overnight Eurodollar deposit market. For those who are not native speakers of money market jargon, the federal funds and Eurodollar deposit markets are very similar: They are wholesale funds taken in by banks, with definitional differences that I’ll leave to the footnotes to my speech.31 Just this past October, following amendments to the FR 2420 collection, we began to receive trade-level data for the bulk of activity in the Eurodollar market. 32\n\nThe Eurodollar market is considerably larger than the federal funds market. According to FR 2420 data, on average in the last two months of 2015 there was $233 billion of overnight borrowing in the Eurodollar market; in contrast, over that same period, there was $70 billion overnight borrowing in the federal funds market.33 I’ll show some data on the composition of activity in these markets a bit later, but it’s worthwhile noting that many of the Desk’s nonbank counterparties, such as the money funds, are active lenders in the Eurodollar market.\n\nWe observed a similar response in yields in the Eurodollar market as we did in the federal funds market, with the distribution of traded rates making a parallel shift upward of about 25 basis points, as shown in Figure 4. Since liftoff, the overnight Eurodollar volume-weighted mean rate as calculated from FR 2420 data, as seen in Figure 5, has averaged about 35 basis points.34 This is in line with the effective federal funds rate. The volume-weighted median, also shown in the figure, moved upward by a like amount. This is an encouraging development because it indicates that, as we have moved up from the zero lower bound, money market rates are continuing to move together.\n\nWe can look into that further by examining the overnight Treasury repo market. Rates on these transactions as reported by the tri-party clearing banks, shown in Figure 6, exhibited a similar shift upward, although they settled modestly lower in the target range than they had in the pre-liftoff period. In part, this shift likely reflects the fact that the ON RRP offered rate was 5 basis points before liftoff—that is, 5 basis points above the bottom of the target range—and it is now set to 25 basis points, or the bottom of the target range.\n\nIt also looks as though the increase in the federal funds rate passed through effectively into term money market instruments. Figure 7 shows the recent progression of rates on three-month Treasury bills and three-month commercial paper. Also shown for comparison is a three-month overnight index swap quote for the effective federal funds rate. All of these rates rose going into the December FOMC meeting, which makes quite a bit of sense, given that most market participants expected the FOMC to tighten policy at that meeting.35 We also gather information about rates on term unsecured borrowing in our FR 2420 collection, and about term secured transactions from the clearing banks, and these data tell a similar story.\n\nTreasury bill yields rose a bit less than other rates on instruments of comparable term and now trade at yields somewhat below the ON RRP rate. Treasury bills have some attractive qualities in comparison to ON RRP, such as round-the-clock liquidity. In addition, there seems to be strong demand for bills from investors who don’t have direct access to the ON RRP or IOR, and it appears that most Treasury bills are owned by such investors. Also, bills have typically traded below other money market rates during tightening cycles, as they do now; periods where bills trade at or above other rates have been the exception and not the rule.36 Thus, the smaller increase in bill yields than in rates on other term instruments is not surprising, and I do not read it as undermining the general conclusion that the policy rate increase was effective in firming money market conditions.37\n\nDecember 31\n\nNow, I mentioned earlier that fed funds traded within the target range on all but one day. Let’s talk about that day, which was December 31.\n\nOn that day, the effective federal funds rate, calculated as a mean from the FR 2420 data, printed at 20 basis points, 15 basis points below its prior-day value and 5 basis points below the FOMC’s target range. The median dropped a bit more, by 20 basis points to a level of 15 basis points. In addition, as shown in Figures 8 and 9, the dispersion of traded overnight rates in federal funds and Eurodollars increased on that day, with a substantial amount of trading activity at fairly low rates. These year-end effects were transitory, and the level and distribution of rates in the federal funds and Eurodollar markets returned to pre-year-end conditions on the following day.\n\nAlthough the effects were transitory and did not adversely affect policy implementation, spending a bit more time on this episode can contribute to our understanding of money market relationships. Why did we see the year-end drop in unsecured rates? Recall that the framework functions by creating opportunities for investors to borrow funds in money markets to earn IOR whenever there are significant differences between market rates and the IOR rate. On quarter-ends, the actual and perceived marginal balance sheet costs of a number of depository institutions increase as they publish financial statements and calculate regulatory ratios.38 These increased balance sheet costs mean that depository institutions borrowing funds in money markets to earn IOR must do so at a lower interest rate to account for these costs. Further, the reduction in balance sheet capacity can temporarily lower the bargaining power of lenders, again allowing borrowers to obtain funds at lower rates.\n\nMany lenders in the federal funds and Eurodollar markets with access to the ON RRP facility responded to these low rates by increasing their use of the facility, as shown in Figure 10. However, other lenders remained in these markets, including those without ON RRP access, those who received funds late in the afternoon after the ON RRP operation had already occurred, and those for which the ON RRP was not an acceptable substitute. On this last point, for some institutions, the ON RRP is an imperfect substitute to lending in private unsecured markets because, in the tri-party repo system through which the ON RRP is settled, cash is not returned at maturity until late the next day, whereas in private unsecured markets, earlier return of funds can be negotiated.39\n\nThese quarter-end effects, as measured by the effective federal funds rate, are likely to appear a bit larger when we switch in early March to a median calculation. As I mentioned, on December 31, the median fell a bit more than the mean did, 20 basis points in the median versus 15 for the mean. A larger relative decline isn’t unusual; averaging across the quarter-end dates in March, June, and September 2015, the median declined by 7 basis points while the mean declined by 6. The median declines more because it is, in most cases, a better measure of the center of the distribution of traded rates—an issue the Desk analyzed at some length in the technical note I mentioned earlier. This difference between the two might grow a bit more as rates move further from zero.\n\nIn secured markets, rates in the tri-party repo market on year-end were relatively stable. This makes sense, as ON RRP is a good substitute for tri-party repo with private counterparties, and therefore this facility forms a fairly firm floor on overnight repo rates for lenders that have access to the facility.40 As I’ll show in a moment, volumes declined in the private repo market, as is typical on quarter-ends.\n\nEvidence to Date on Other “Lenses”\n\nTurning to the second lens I noted earlier, we have achieved excellent control while avoiding any unintended change in the structure of the financial system. Figure 11 shows RRP take-up over time, grouped by counterparty type. Take-up, both in aggregate and at the counterparty-type level, has been stable since liftoff. Among the money funds shown in this figure, government-only funds make up more of the demand on typical days, with funds that can invest in a broader universe of money market instruments, called prime funds, accounting for the balance. This has also been stable since liftoff.41 Indeed, more recently, usage has come down as unsecured and secured rates have firmed some. Even with zero usage, the ON RRP facility could still be effective in underpinning the level for money market rates by supporting lenders’ bargaining power.\n\nIn addition, as shown in Figure 12, lending volumes in secured and unsecured money markets have been stable, suggesting that the Federal Reserve’s facilities have not displaced activity in the private sector.42 Figure 13 shows activity in unsecured markets by select lender types. This shows that, post-liftoff, the composition of activity in these markets also appears to have been stable.\n\nFinally, I’d like to speak briefly to the third lens, avoiding financial instability. As you know, there has been quite a bit of volatility and risk aversion in financial markets lately, and there has been no corresponding surge in ON RRP demand. This continues to be an issue that the FOMC is thinking closely about, as reflected in the January FOMC minutes.\n\nLessons Learned on Operational Structure\n\nWe have learned a lot about the structure of our operations from our experience with liftoff. I will discuss a few of my observations.\n\nLet’s begin with quantifying the aggregate capacity limit. At the December meeting, the FOMC directed that the ON RRP’s capacity be “limited only by the value of Treasury securities held outright in the System Open Market Account that are available for such operations.” As a result, in practice, the ON RRP is currently being run with an aggregate capacity limit of around $2 trillion, which is far in excess of typical daily demand and well above the $300 billion capacity limit the facility had had since September 2014. As the FOMC made clear in the minutes to its March 2015 meeting, this elevated capacity was intended to ensure a smooth liftoff, but the absence of an aggregate cap is only temporary. This is why the recent January minutes included a discussion of when and how it will be appropriate to reinstate an aggregate cap.\n\nSo, what have we learned about the appropriate level of the cap? It seems likely that having a very elevated aggregate capacity was helpful in controlling market rates initially, perhaps because it showed the FOMC’s commitment to achieving interest rate control, but it’s unclear exactly how much available capacity, or “headroom,” is needed to maintain such control. By way of background, determining the appropriate level of capacity involves striking a balance between the “lenses” I discussed earlier. Imposing a limit on the facility’s capacity reduces the extent to which there can be a disruptive surge of funds into the facility and the extent to which private-sector investors can change their lending patterns by using the facility. Thus, such a limit helps avoid financial instability and unintended effects on the structure of the financial system. However, a limit that is perceived as likely to bind weakens our control of interest rates, because when demand exceeds capacity, the interest rate paid on the ON RRP facility is determined by an auction and is likely to be below the offering rate, thus reducing the efficacy of the facility in supporting the federal funds rate.43\n\nOur recent experience suggests that having reasonably high aggregate capacity can help improve control without necessarily encouraging greater use of the facility. When that capacity is great enough, money market lenders are confident that they will be able to place funds with us at the administered rate, even on financial statement reporting dates. That confidence empowers them to demand rates from their borrowers that are above the ON RRP offering rate. Conversely, when the capacity is perceived as low, lenders worry that they might not be able to place all their funds at reasonable rates. This means that they might accept relatively low rates in money markets. In addition, because of switching costs and other frictions as well as risk aversion on the part of lenders, rates can decline days or weeks ahead of a date on which the ON RRP is anticipated to reach its capacity, to levels so soft that lenders reallocate funds to the ON RRP.44 So, having a high capacity could actually reduce ON RRP take-up. But as I mentioned earlier, it isn’t clear exactly how high the capacity needs to be to achieve the confidence of which I speak, and the benefits of high capacity must be carefully balanced against the benefits of a tighter aggregate cap in terms of avoiding financial instability and unintended effects on the structure of the financial system.\n\nLooking at the recent data, it’s hard to precisely identify the extent to which the very high capacity, relative to the previous limit of $300 billion, helped to facilitate control beyond instilling confidence, as I discussed earlier. One tentative indication that high capacity may be playing an important role is that unsecured rates appear to have settled in about the same position relative to IOR as before liftoff, even though one might have expected the widening in the IOR-ON RRP spread and the lower support from the ZLB to result in some downward shift.\n\nThe ON RRP also has a per-counterparty cap, which was, and is, $30 billion. It seems that the current level of the per-counterparty cap was also adequately large for perceived individual “headroom,” thereby supporting lender bargaining power. Figure 14 shows summary data on how frequently we receive large bids in our operations at the ON RRP and term RRP facilities. As you can see, we rarely see bids of more than $10 billion, and combined individual counterparty usage across overnight and term operations has never exceeded $30 billion.\n\nRecently, we learned a bit about the term RRP operations. We have been conducting these over quarter-ends as a way of providing additional RRP capacity. Additional capacity is available because term RRP operations are subject to a separate overall size limit, and because there is no per-counterparty cap in those operations. Counterparties appear to regard these as close substitutes for ON RRPs, and as shown in Figure 15, we’ve typically seen that overall RRP take-up, across overnight and term RRP, has been fairly stable across quarter-ends.\n\nAt this past year-end, there was essentially no take-up at the term RRP operations. The reason is that the aggregate and per-counterparty caps were sufficiently high that market participants were confident that they would not bind, and also because we conducted the term operations without any interest rate premium relative to the overnight operations. At prior quarter-ends, term RRPs were typically offered with maximum bid rates a few basis points above the ON RRP rate.\n\nThe year-end experience suggests that market participants generally prefer overnight RRPs to term RRPs if they offer the same rate, and that, if desired, adequate control over money market rates can be attained without term RRPs, as long as there is adequate ON RRP capacity.\n\nFinally, I’ll offer a few observations on a daily repo operation I haven’t yet mentioned. As a long-standing service to foreign central banks, foreign governments, and international official institutions, the New York Fed runs an overnight investment facility known as the foreign repo pool.45 In this operation, at the end of each business day, account holders’ cash balances are invested in an overnight reverse repo secured by the Federal Reserve’s securities holdings.46\n\nThe foreign pool is an autonomous factor affecting reserves and, as a result, in the pre-crisis operating framework, the Desk would have to essentially sterilize variations in the pool's size with repo operations. Accordingly, the interest rate on the pool was slightly below overnight Treasury repo rates in the market.47 To ease the Desk’s job in the daily forecasting of autonomous factors, tight limits were imposed on customers’ ability to rapidly vary the size of their investment in the pool.\n\nSince the crisis, the New York Fed has continued to provide the pool as a service. Investments in the pool continue to be paid the market-based rate I mentioned. Figure 16 contains some statistics on this rate relative to a measure of the broad market rate. We do not use the foreign pool as a means for implementing monetary policy, and the change in the composition of Federal Reserve liabilities it can generate has little to no impact in unsecured markets because of the large amount of reserves in the system. Recall that the current operating framework is not based on reserves scarcity.\n\nUse of the foreign pool has grown over the last year and a half, from a bit under $100 billion in mid-2014 to about $247 billion on February 17. This growth isn’t associated with liftoff, and it isn’t because we have changed the way in which the interest rate is calculated. Instead, use of the pool has increased because over time the constraints imposed on customers’ ability to vary the size of their investments have been removed, the supply of balance sheet offered by the private sector to foreign central banks appears to have declined, and some central banks desire to maintain robust dollar liquidity buffers.\n\nOpen Questions\n\nCompared with the relative calm of the pre-crisis period, money markets have recently gone through a sustained period of substantial change. Some of these changes are the result of regulations to improve the safety and soundness of the financial system, others relate to business model enhancements, and, of course, some relate to Federal Reserve activity. The Federal Reserve continues to collect data on the structure of money markets to determine whether any changes are needed in its operating framework to ensure that its objectives continue to be met.\n\nWe are continuing to watch developments as business models adapt to the numerous changes in bank regulation introduced over the past few years. We are interested in the extent to which these business model changes impact the structure of money markets and thereby affect the form of monetary policy implementation. For example, we are monitoring developments in bank behavior that could have implications for the transmission of monetary policy into market rates. Thus, we are paying close attention to the behavior of foreign banks on financial statement reporting dates. We are also keeping a close eye on how domestic banks continue to respond to enhanced requirements on nonoperating deposits, which, for some institutions, are fairly large relative to the size of their reserve holdings. Should these business model changes increase the spread banks demand to borrow funds in money markets to earn IOR, or should business model decisions apparently related to regulatory changes alter broader dynamics in money markets, this could result in upward pressure on ON RRP take-up over time.\n\nWe are also paying close attention to developments in the money market mutual fund industry and their potential impact on policy implementation.48 The Securities and Exchange Commission recently announced new regulations for these funds aimed at enhancing financial stability. One important feature of these new rules is that government-only money funds receive different regulatory treatment than prime funds, possibly making government-only funds more attractive for some institutional investors. If the assets under management in government-only money funds were to grow significantly, that could put upward pressure on ON RRP take-up, since most fund managers consider the facility to be a government investment.49 As I noted earlier, we would not want to see growth in government-only money funds if it were predicated on a mistaken impression that ON RRP would be around indefinitely and with high capacity. Also, if assets under management in prime funds were to decline sharply, this could possibly lead to less efficient transmission of monetary policy.\n\nFinally, we want to keep an eye on relative value relationships in money markets. For example, the difference between customer-to-dealer and interdealer repo rates has been quite volatile in recent months, apparently because of growth in dealer balance sheet costs.\n\nThe Road Ahead\n\nTo summarize, in the period since liftoff, we have achieved excellent control over the effective federal funds rate, and we have done so while avoiding unintended effects on the financial system or financial stability. The policy rate increase has passed through to other money market rates, suggesting that the increase is affecting broader financial conditions as expected and intended.\n\nWe still have a lot of work to do, both to observe ongoing structural changes in financial markets and to formulate and implement any necessary response in the Federal Reserve’s monetary policy operations. I believe we have thoughtfully considered these uncertainties, and I am confident that we have appropriate policy tools available to respond as necessary to them.\n\nThank you. I would be happy to take a few questions.\n\nFull Presentation Updated February 26, 2016\nData Updated March 1, 2016\n\n1 I would like to thank James Egelhof and Patricia Zobel for their excellent assistance in the preparation of these remarks, and colleagues in the Federal Reserve System for numerous insightful comments and suggestions. In addition, I would like to acknowledge the leadership of James Clouse, William English, and Lorie Logan in the construction of this new framework and James McAndrews for his intellectual leadership on U.S. money markets.\n\n2 For more details on implementation frameworks, see for example Ihrig, Meade, and Weinbach (2015).\n\n3 With the commencement of the policy normalization process in December 2015, the Federal Reserve began issuing an implementation note that provides the operational settings of the Federal Reserve's policy tools. This note serves to separate communications about the policy stance from those about policy implementation. It also promotes greater cohesion and transparency in communications about policy implementation by consolidating information that was previously provided in FOMC statements, FOMC minutes, Desk statements, and elsewhere.\n\n4 An estimated historical series for an effective federal funds rate, produced using the new data source and calculation methodology, is available on the New York Fed’s website. This series will be updated at the end of February.\n\n5 Demand for loans of reserves was driven, for example, by reserve requirements imposed on depository institutions.\n\n6 I emphasize “all else equal.” There were many subtleties to the pre-crisis relationship between the operations and money market rates. In particular, money market rates would often move to a new target rate even before operations were conducted, because of the Federal Reserve’s credibility. Of course, that credibility was underpinned by the Federal Reserve’s actual operational capability. For a detailed exposition on pre-crisis monetary policy implementation, see Bernanke, “Implementing Monetary Policy,” March 30, 2005.\n\n7 Of course, the Desk also controlled the level of reserves through permanent open market operations.\n\n8 When the Desk conducts a reverse repo transaction, it sells securities held in the System Open Market Account (SOMA) under an agreement to repurchase the securities at a predetermined price. The difference between the sale price and the repurchase price, together with the length of time between the two legs of the transaction, implies a rate of interest (the reverse repo rate) paid by the Federal Reserve to its counterparty. The transaction leaves the size of the SOMA securities portfolio unchanged but shifts some of the liabilities on the Fed’s balance sheet from bank reserves to reverse repos while the trade is outstanding. A repo (as opposed to a reverse repo) is the opposite. Before the crisis, repos were more common than reverse repos. The eligible collateral for the Desk’s repo and reverse repo operations is set forth in the FOMC’s Authorization for Domestic Open Market Operations.\n\n9 Autonomous factors are those components of the Federal Reserve’s balance sheet that affect reserve balances but are not directly under the central bank’s control. The largest of these, by far, is currency in circulation. Others include U.S. Treasury deposits, Federal Reserve Bank “float,” and overnight reverse repos conducted with foreign central banks. See, for example, Hilton (2008).\n\n10 The 2005 speech by then-Governor Bernanke that I noted earlier provides great detail on a day in the life on the Desk prior to the financial crisis.\n\n11 Congress granted this authority to the Federal Reserve in the Financial Services Regulatory Relief Act of 2006, with an October 2011 effective date, but accelerated its implementation to October 2008 as part of the legislative response to the financial crisis. A similar tool is critical in the implementation regime of most advanced economy central banks.\n\n12 The FOMC also considered an arrangement under which depository institutions could pledge funds held in a segregated account at the Federal Reserve as collateral in borrowing transactions with private creditors. This approach aimed to reduce credit-related barriers to depository institutions’ borrowing funds in money markets and thereby intensify competition among borrowers. The FOMC decided that these accounts involved operational, regulatory, and policy issues that would have been difficult to resolve in a timely fashion, and it shelved further work. See the minutes to the FOMC’s December 2014 meeting.\n\n13 This approach was discussed in detail in the FOMC’s June 2011 Exit Strategy Principles.\n\n14 The Federal Reserve continues to test its Term Deposit Facility (TDF). In late 2014 and early 2015, the Federal Reserve conducted rounds of testing of the TDF that on two occasions resulted in cumulative take-up of more than $400 billion. These operations reportedly had no significant impact on conditions in money markets, suggesting that TDF operations would have to be significantly larger to put upward pressure on rates.\n\n15 This approach has been used heavily, for example, by central banks that have large foreign exchange reserves.\n\n16 In July 2015, the FOMC announced that changes in administered rates would go into effect on the day after its policy decision. Otherwise, the intraday timing of a policy change could result in various operations being conducted with inconsistent settings.\n\n17 ON RRP is currently offered to a broad set of counterparties; these counterparties make up a fairly large proportion of the lending community in overnight money markets. By offering this large group of counterparties an alternative, risk-free investment option, ON RRP should improve lenders’ market power and help keep money market rates near or above the ON RRP offered rate. Even if money markets were perfectly competitive, ON RRP would likely still be effective at supporting the floor provided by IOR, for example through addressing segmentation between secured and unsecured money markets.\n\n18 The preservation of enough activity in the federal funds market to maintain the viability of the effective federal funds rate was one component of this.\n\n19 A final operational change to improve the transmission of IOR into market rates was a revision to the interest calculation methodology that became effective in July 2015. As originally implemented, interest on excess reserves (IOER) was paid on each bank’s average balances over a two-week maintenance period, at the average interest rate. This meant that if the interest rate changed during a maintenance period, the change would not have been fully reflected in the interest payments to depository institutions until the beginning of a new maintenance period. This methodology likely would have resulted in slow adjustment of market rates to changes in the IOER rate. The revised calculation bases interest payments to depository institutions on the IOER rate in effect each day and the level of balances held each day.\n\n20 See Clouse, Ihrig, Klee, and Chen, “The Federal Reserve’s Tools for Policy Normalization in a Preferred Habitat Model of Financial Markets,” October 2014; Martin, McAndrews, Palida, and Skeie, “Federal Reserve Tools for Managing Rates and Reserves,” September 2013; and Armenter and Lester, “Excess Reserves and Monetary Policy Normalization,” September 2015.\n\n21 Testing focused on two types of term operations. Term RRPs are similar to ON RRP in most respects; for example, both operations use the same expanded set of counterparties. The key differences are that term RRPs are conducted for longer than overnight, they each have their own, separate, aggregate capacity limit, and the term RRP does not incorporate a per-counterparty cap. Term RRPs can be used in a variety of ways. In the current framework, they have been used to offer increased RRP capacity, using an interest-rate-oriented approach, around periods when larger-than-usual demand is expected, but they could also be used in a quantity-oriented way to drain reserves. Term deposits are likewise similar to IOR; TDF offers a somewhat higher rate to banks that are willing to lock up their funds for a time.\n\n22 The FOMC’s decision to use a target range for the federal funds rate, instead of a point target as was used prior to December 2008, further supported the use of this operating framework. The FOMC’s use of a range supports this framework by underscoring the fact that variation of the effective rate within that range is expected and is consistent with the level of interest rate control it desires.\n\n23 Frost, Logan, Martin, McCabe, Natalucci, and Remache, “Overnight RRP Operations as a Monetary Policy Tool: Some Design Considerations,” February 2015.\n\n24 It seems particularly appropriate to pay close attention to the integration of money markets in light of the increase in bank and dealer balance sheet costs that has ensued from post-crisis changes in financial regulations and other factors.\n\n25 See Chair Yellen’s September 17, 2014, press conference. “To begin normalization, the Committee will raise its target range for the federal funds rate. The Committee expects that the effective federal funds rate may vary within the target range, and could even move outside of that range on occasion, but such movements should have no material effect on financial conditions or the broader economy.”\n\n26 See the September 2014 Policy Normalization Principles and Plans.\n\n27 It is possible that the ON RRP facility might, in certain circumstances, have beneficial effects on financial stability by providing confidence that there will be an elastic supply of risk-free assets available to nonbanks.\n\n28 “Statement Regarding Planned Changes to the Calculation of the Federal Funds Effective Rate and the Publication of an Overnight Bank Funding Rate,” February 2, 2015.\n\n29 “Statement Regarding the Calculation Methodology for the Effective Federal Funds Rate and Overnight Bank Funding Rate,” July 8, 2015.\n\n30 “Technical Note Concerning the Methodology for Calculating the Effective Federal Funds Rate,” July 8, 2015.\n\n31 Eurodollars are unsecured U.S. dollar deposits held at a bank or bank branch outside of the United States or domestically through an international banking facility (IBF), whereas federal funds are domestic borrowings by U.S. banks and U.S. branches and agencies of foreign banking organizations (FBOs). Although the Federal Reserve can impose reserve requirements on net Eurodollar deposits of U.S.-based banks, it has imposed a zero reserve requirement since 1990, making the treatment of Eurodollar deposits effectively the same as federal funds borrowings.\n\n32 Since revisions were implemented to the FR 2420 on October 20, 2015, both domestic banks and FBOs have been required to report Eurodollar transactions on the FR 2420. Domestic banks report the activity of all offshore branches with more than $2 billion in assets, while FBOs report activity of offshore branches that are managed and controlled by offices based in the United States. Additionally, both domestic banks and FBOs report the activity of their IBFs. Prior to the revisions, only domestic banks reported Eurodollar transactions, and there was no reporting by IBFs.\n\n33 In addition to enhancing the Eurodollar collection, the revised reporting instructions, effective October 20, 2015, modified and clarified the definition of federal funds transactions and expanded the number of reporting entities. The effect of these changes was an increase in the amount of reported federal funds activity.\n\n34 The data file for this speech, available on the New York Fed’s website, includes a historical calculation of the OBFR based on data provided by federal funds and Eurodollar brokers until October 19, 2015 and based on FR 2420 data, from October 20th, 2015 until February 17th, 2016.\n\n35 See “Responses to Survey of Primary Dealers,” December 2015, and “Responses to Survey of Market Participants,” December 2015.\n\n36 One such exceptional period was in mid-October 2015, amid some market concern about the debt ceiling that eventually abated.\n\n37 The Treasury has recently begun to ramp up its issuance of Treasury bills in part to support the maintenance of a larger cash balance, which it keeps in its “checking account” at the Federal Reserve. This action increases the amount of Treasury bills in circulation, thereby creating a greater stock of investible assets for nonbank money market investors—an outcome that tends to put upward pressure on Treasury bill rates and potentially other term money market rates.\n\n38 Domestic depository institutions are subject to a regulatory leverage ratio that is calculated as an average over the quarter and thus experience less variation in balance sheet costs at the end of quarters.\n\n39 The Federal Home Loan Banks (FHLBs) are one type of institution that requires this feature for some of their cash holdings. Prior to liftoff, the FHLBs, which have access to unremunerated deposit accounts at the Federal Reserve, were able to use this to support their negotiating position with borrowers at quarter-ends, but now with rates further above zero it is less effective in this regard.\n\n40 ON RRP does not always provide a completely firm floor on the tri-party repo market. For example, late-day funds can generate some softness. Also, there is evidence that some lenders in repo markets are reluctant to frequently switch their counterparties. However, ON RRP is otherwise a close substitute to repo with a private counterparty. In particular, since all tri-party repo is unwound at the same time, there is no possibility of early return of funds, which, as I mentioned earlier, is a significant benefit some federal funds and Eurodollar borrowers are able to offer.\n\n41 Prime funds’ share of participation is significantly larger on quarter-end dates.\n\n42 A small number of financial market commentators had opined that use of the tri-party repo infrastructure for the ON RRP facility would make it harder for market participants to borrow the Federal Reserve’s Treasuries in securities lending to facilitate settlement, and therefore result in less liquid Treasury markets. However, use of the Federal Reserve’s securities lending program has been stable since liftoff, suggesting that the design of the ON RRP facility has thus far not contributed to any increase in the scarcity of specific Treasury collateral.\n\n43 If the total amount of propositions received is less than or equal to the amount of available securities, awards are made at the specified offering rate to all counterparties that submit propositions. In the event that the value of propositions received exceeds the ON RRP’s capacity, awards are made at the rate at which the capacity was achieved (the stop-out rate), with all propositions below this rate awarded in full and all propositions equal to this rate awarded on a pro rata basis. The stop-out rate, which can be any value at or below the specified offering rate (including a negative rate), is determined by evaluating all propositions in ascending order by submitted rate up to the point at which the total quantity of propositions equals the overall size limit. The ON RRP FAQs provide more information on the auction mechanism.\n\n44 In September 2014, money market rates were soft for several days going into quarter-end, as market participants anticipated that the ON RRP would reach its aggregate capacity on that day. According to market participants, money market lenders lent overnight funds well in advance of quarter-end at rates below the ON RRP rate to informally secure balance sheet capacity for the quarter-end date.\n\n45 The foreign repo pool is authorized each year by the FOMC. For more background information about the foreign repo pool, see “Services for Central Banks and International Institutions.”\n\n46 Other major central banks provide analogous services. In managing the foreign reserves of the United States, the Federal Reserve avails itself of these services when appropriate.\n\n47 As a result, the rate paid on the pool closely tracked the interest rate achieved on temporary open market operations.\n\n48 We are also monitoring developments in money fund net yields versus retail bank deposit rates. Money fund management fees declined considerably as interest rates fell to near-zero levels, apparently as fund managers worked to preserve a non-negative yield for their investors. Net interest margins at banks were also compressed. It is natural to expect that this compression in margins, both at money funds and at banks, will reverse as rates move away from zero, but the magnitude and timing of this reversal in margin compression may vary among these two investment types. We are watching closely to see whether there is a shift in relative pricing between them that could produce a flow from one to the other and thereby have implications for the structure of financial intermediation.\n\n49 Assets under management in government-only funds could grow two ways: Prime funds could convert into government-only funds, or prime fund investors could transfer their money into government funds. Most prime funds have announced their intentions at this point, and among those that have converted to government funds, take-up has not markedly increased, so therefore we are less concerned about such conversions, but we continue to pay close attention to them, as we do to investor behavior."
    },
    {
        "title": "Remarks at the New York Fed’s Economic Press Briefing on the Household Debt and Credit Report",
        "date": "Feb 12, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud160212",
        "content": "Good morning and welcome to the Federal Reserve Bank of New York's Economic Press Briefing.  I am pleased to have this opportunity to speak with you today about the economic outlook, with a focus on the household sector.  As always, what I have to say reflects my own views and not necessarily those of the Federal Open Market Committee (FOMC) or the Federal Reserve System.1\n\nAccording to the National Bureau of Economic Research, the current expansion began in July 2009, making it nearly seven years old.  If we gathered this expansion and the 11 prior post-WWII expansions together for a family photograph, arranging them by age from youngest in front to oldest in back, this expansion would be standing third from the rear.  The only two that lasted longer were the expansion that began in February of 1961, which lasted for nearly nine years, and the one that began in March of 1991, which lasted for ten years.  Thus, the current expansion is already an elder statesman in the family of post-war U.S. expansions.  This raises the question of whether this expansion has already entered its twilight years, with the risk of recession edging higher with each passing month.\n\nAs I have said before, expansions do not simply die of old age.2 Rather, expansions end either because a significant inflation risk emerges that requires a sharp tightening of monetary policy, or the economy is adversely impacted by a large shock that cannot be offset by monetary policy in a timely manner.  While the sample of post-war U.S. expansions is still too small for reliable statistical analysis, the evidence suggests that after an expansion’s first few years its likelihood of ending is mostly independent of age, and depends mainly instead on the level of inflation.  Since the possibility is low that a significant inflation risk would emerge over the near term, this means that the main danger facing the current expansion is the risk of large, adverse shocks.\n\nGiven that the labor market still appears to have some excess slack and inflation is below the Federal Reserve’s objective, monetary policy is appropriately still quite accommodative despite the advancing age of the expansion.  While this limits to an extent the degree to which monetary policy can aggressively respond to any adverse events, the good news is that the economy is more resilient to any shocks.  Key sectors of the U.S. economy, such as the household sector, seem to be in good shape.  The financial system is also clearly much stronger, with the banking system much better capitalized and with much larger liquidity buffers than in the years preceding the financial crisis. \n\nIn today’s briefing, we will be focusing on the household sector.  Given the importance of consumption to U.S. economic growth, the health of the household sector is critical for an assessment of risks to the outlook.  Over the last several years we have devoted significant resources to better understanding the state of the household sector, and one outcome of this attention is our now well-known Quarterly Report on Household Debt and Credit.  This report, and the data that underlie it, give us direct insight into the health of the household balance sheet.  The assets and liabilities that families hold is a crucial determinant of the household sector’s ability to help immunize the economy from shocks.  We learned during the Great Recession the importance of household liabilities—in particular, mortgages, which make up more than two-thirds of the sector’s total liabilities.\n\nAs you know, the early part of the 2000s witnessed a very substantial expansion in borrowing by households.  This borrowing took many forms, but was particularly notable in mortgages and home equity lines of credit, and reflected the very high and unsustainable expectations of house price appreciation.  Household debts peaked in 2008, at which point the sector’s balance sheet was weak.  Households were highly-leveraged, with many underwater on their mortgages.  This was particularly the case for younger and lower- and middle-income households who, besides being more leveraged, were disproportionally affected by job loss and other income shocks, and had little experience managing debts―especially in times of stress.  Consequently, delinquency, foreclosure and bankruptcy rates reached very high levels.\n\nBeginning in 2008, households began a new phase to repair their balance sheets by paying down outstanding debt.  During this “Great Deleveraging,” which lasted five years, households reduced their overall liabilities by 12 percent or $1.5 trillion.  House prices began to rise again in earnest in 2012, and by mid-2013 household deleveraging appeared to have come to an end.  Net household borrowing resumed, and household debt has risen over 8 percent during the last couple of years.  But, this recent period of credit expansion looks very different from the early 2000s, and I believe that the household sector today is in much better health.  There are several reasons for my confidence.\n\nLet me begin with the situation in the residential mortgage market, which plays such a dominant role in household finance and was such a problem the last time around.  Currently, house price growth is running nationally at around 5 percent per year.  But this time, households appear to be much more cautious in how they are responding to the rise in home values.  Rather than borrowing against the rising equity in their homes, mortgage balances have instead essentially stabilized and remain well below their previous levels.  As my colleagues will describe in more detail shortly, this stabilization reflects an increase in debt paydown due to a combination of lower interest rates, shorter loan terms and aging of mortgages.  This trend is reflected in increased household saving and growing home equity, and we expect these trends to continue to help households rebuild their balance sheets over the near term, thereby further increasing the household sector’s resiliency to shocks.\n\nLow interest rates have also been important in driving the household debt service ratio to the lowest levels observed in our data.  This means that households have more capacity with which to absorb any temporary adverse income shocks.  One clear reflection of these improvements is the very significant decline in delinquency rates.  At this point, the great majority of the bad debt from the boom years has been charged off and new foreclosures are at the lowest level we’ve seen in our data.  The combination of charge-offs and tight mortgage underwriting standards over the last several years has shifted the stock of outstanding mortgage balances toward lower-risk borrowers, who are typically older with more stable incomes.\n\nFor all these reasons, the household sector looks much better positioned today than in 2008 to absorb shocks and continue to contribute to the economic expansion.\n\nNow, before asking my colleagues to discuss some of these trends in more detail, let me note that my positive overview of the household sector comes with some caveats.  First, these aggregate statistics can mask some important variations, and there are some local markets where, as in the prior boom, both house prices and mortgage growth are quite strong.  Similarly, there exist substantial differences across households in their financial situation and their ability to improve their balance sheets.  Borrowers with little equity, low credit scores and slow income growth face difficulties refinancing into low rate mortgages.  As a result, this subset of the population has been less able to deleverage and still faces relatively high debt service costs, which constrains their consumption, investment and saving behavior.  So, even within what looks to be a stable overall mortgage market, there are some things that we must continue to monitor.  Second, there are other difficult challenges that many households face, particularly with respect to a subject we’ve discussed on previous occasions―student loans.  On balance, though, households are in a much better position today than they were in 2008.  Now, I'd like to turn things over to our economists, beginning with Andrew Haughwout, who will provide a more detailed discussion of household sector finances.\n\n1 Domenico Giannone, Andrew Haughwout, Joseph Tracy and Wilbert van der Klaauw assisted in preparing these remarks.\n\n2 See William C. Dudley, The U.S. Economic Outlook and Implications for Monetary Policy, Remarks at the Economic Leadership Forum, Somerset, New Jersey."
    },
    {
        "title": "The U.S. Economic Outlook and Implications for Monetary Policy",
        "date": "Jan 15, 2016",
        "speaker": "Unknown",
        "url": "https://www.newyorkfed.org/newsevents/speeches/2016/dud160115",
        "content": "It is a pleasure to have the opportunity to join you again at the Economic Leadership Forum.  I would like to thank John McWeeney of the New Jersey Bankers Association and Rutgers University for the invitation to be here today.  Northern New Jersey is a vital part of the New York Fed’s district, and I don’t just say that because I live here.  My staff and I actively maintain ties with local community, business and banking leaders, and we always benefit from opportunities to hear about what is on your mind. \n\nI am particularly delighted to see that community banks are well represented here today.  The Federal Reserve understands the importance of a vibrant community banking sector and the crucial role community banks play in supporting the activities of local businesses and households.  While protecting the safety and soundness of the financial system is an important part of the Federal Reserve’s mandate, I believe that the regulation of community banks should be appropriately calibrated to reflect the lower degree of risk they pose to the financial system relative to larger banks.  In other words, community banks should not be subject to the same set of regulations that are applied to the most systemically important banking organizations.\n\nIn my remarks, I will discuss the U.S. economic outlook and the implications for U.S. monetary policy. I will focus primarily on last month’s Federal Open Market Committee (FOMC) “lift-off” decision—the first increase in the federal funds rate target range in nearly 10 years. I’ll explain what motivated my vote to begin to normalize U.S. monetary policy. I’ll also offer a preliminary assessment of how things are going so far―both with respect to how the U.S. bond market reacted to lift-off and how well our new tools are working as we begin to push up short-term interest rates. Both issues are pertinent, as money market rates have spent a very long time close to zero and we have never attempted to tighten monetary policy with such a large balance sheet and high level of excess reserves. To summarize my conclusions: Generally so far, so good on both fronts.\n\nLooking ahead, I’ll talk about what comes next. No surprises here—it depends on the data. As noted in the December FOMC statement, we expect that the normalization of monetary policy will be quite gradual. But, there is no commitment here. The flow of the data—broadly defined―will drive our actions as it influences our assessment of the economic outlook and our view of the stance of monetary policy best suited to achieve our dual mandate objectives of maximum sustainable employment and price stability. As always, what I have to say reflects my own views and not necessarily those of the FOMC or the Federal Reserve System.1\n\nIn terms of the economic outlook, the situation does not appear to have changed much since the last FOMC meeting. Some recent activity indicators have been on the softer side, pointing to a relatively weak fourth quarter for real GDP growth. But this needs to be weighed against the strength evident in the U.S. labor market. I continue to expect that the economy will expand at a pace slightly above its long-term trend in 2016. In other words, I anticipate sufficient economic strength to push the unemployment rate down a bit further and to more fully utilize the nation’s labor resources.\n\nTurning to inflation, we continue to fall short of our 2 percent objective for the personal consumption expenditure (PCE) deflator. But I take it as a positive sign that the core PCE inflation rate—that is, excluding food and energy—has been quite stable despite the downward pressure being exerted by lower energy prices on the prices of non-energy goods and services, as well as the drop in non-energy import prices from a firmer dollar.\n\nGoing into more detail, U.S. economic activity has areas of both strength and weakness. On the stronger side of the ledger, domestic demand is doing reasonably well. In particular, consumption and housing activity continue to expand at a moderate pace. Consumer spending has been supported by solid real disposable income growth, which has been underpinned, in turn, by sturdy job gains and falling energy prices. Residential investment has been slowly increasing for several years and that trend seems likely to continue in 2016. Housing starts are still well below the rate consistent with the nation’s population growth rate, and the fundamentals of housing demand remain positive. Rising employment is likely to boost the household formation rate and low mortgage interest rates should keep housing relatively affordable, despite the ongoing recovery in home prices. Last month’s passage of a fiscal 2016 budget package should also provide support to economic activity. Not only does this budget package reduce uncertainty about the budgetary outlook, but its extension of a number of tax breaks and easing of the caps on domestic and military spending means that fiscal policy in 2016 will likely turn somewhat stimulative.\n\nOn the weaker side, the collapse in energy prices continues to pull down domestic investment in oil and gas drilling projects. Although this adjustment is now well-advanced, I suspect that there remains a further leg down given the sector’s diminished cash flows and the reduced access to credit. In addition, manufacturing remains very soft—hurt by the drop in energy-related investment, an ongoing inventory adjustment and the loss of competitiveness caused by the persistent strength of the U.S. dollar. Even the one bright spot in manufacturing over the past year—the auto sector—seems to be close to a cyclical peak. Thus, I suspect manufacturing will continue to be soft in 2016. Overseas developments, especially with respect to the emerging market economies, pose a risk to the U.S. economic outlook—potentially exerting greater restraint on the demand for U.S. exports and contributing to greater turbulence in global financial markets. Putting these positives and negatives together, the most likely outlook seems to be more of what we have experienced in this expansion—an economy that grows at slightly above a 2 percent annual rate this year.\n\nThe inflation outlook also has not changed much. Inflation remains well below the Federal Reserve’s 2 percent objective. In my assessment, this is due mainly to weaker energy prices and the impact of a stronger dollar on non-energy import prices. However, the fact that core measures of inflation are considerably higher than the headline readings, and have been quite stable in recent months, suggests to me that we are likely to see inflation rise once energy prices stop falling and the dollar stops appreciating—clearly neither trend can persist indefinitely. Of course, this assumes that the U.S. economy grows sufficiently rapidly so that pressure on available labor and capital resources continues to increase.\n\nWith respect to the risks to the inflation outlook, the most concerning is the possibility that inflation expectations become unanchored to the downside. This would be problematic were it to occur because inflation expectations are an important driver of actual inflation. If inflation expectations become unanchored to the downside, it would become much more difficult to push inflation back up to the central bank’s objective. Japan’s difficult experience indicates the importance of avoiding such an outcome.\n\nFor this reason, we closely monitor inflation expectations. Inflation measured by the PCE deflator has been running below the FOMC’s objective since May 2012. A concern is whether these persistent underruns in inflation may be beginning to weigh on inflation expectations. Some surveys of inflation expectations have softened recently. For example, the University of Michigan measure of median long-term household inflation expectations—that is, expected inflation at a five-to ten-year horizon—is currently at 2.6 percent. This is near the very bottom end of its range over the past two decades.\n\nThe New York Fed’s Survey of Consumer Expectations also shows softness. The median of 3-year inflation expectations has declined over the past year, falling by 22 basis points to 2.8 percent. While the magnitude of this decline is small, I think it is noteworthy because the current reading is below where we have been during the survey history. Up until July 2014, the median largely stayed in the range from 3.2 to 3.4 percent, and from July 2014 to July 2015 it remained near 3 percent.\n\nWhile it has a short history, I put more weight on the New York Fed’s survey because its methodology should be more robust in accurately assessing consumer inflation expectations. Compared to the more widely followed University of Michigan survey, for example, the New York Fed survey has several advantages. The sample size is larger, most of the people that are interviewed are the same each month, and the inflation expectations question is posed differently to focus the respondent’s attention on inflation rather than on prices. We believe that all these factors lead to a more reliable estimate of inflation expectations.\n\nObviously, I didn’t think the degree of weakness we have seen in our survey measure of inflation expectations was of sufficient concern to defer the start of monetary policy normalization. And, as long as the economy continues to grow at an above-trend pace, I expect the increase in resource utilization will be sufficient to push both inflation and inflation expectations higher over time. That said, should the economy unexpectedly weaken, then this fall in inflation expectations would become more concerning.\n\nAssuming, as I anticipate, that inflation does move back towards our 2 percent target, I am often asked how tolerant would I be of an overshoot? In other words, is the 2 percent inflation target a ceiling or not? I don’t think of the 2 percent objective as a ceiling. I would be equally intolerant of misses relative to 2 percent in both directions, above or below, with my intolerance growing the further we deviated from our 2 percent objective. We will almost certainly never be precisely at our 2 percent objective for any length of time given all the forces—many of which are not under our control—that influence actual inflation outcomes over a business cycle. Thus, with a neutral monetary policy, my goal would be to spend about the same amount of time slightly above as slightly below our 2 percent objective.\n\nTurning now to U.S. monetary policy, why did I favor raising the federal funds rate target last month? Basically, my assessment was that our conditions for lift-off had been met. Recall, these two conditions were: 1) further improvement in the labor market that we anticipated would be sustained in 2016, and 2) greater confidence that inflation would begin to move back towards our 2 percent inflation objective over the medium term.\n\nThe timing of policy normalization involves a balancing of risks. I don’t disagree with our critics that there were risks from lifting off in December versus waiting a little longer. First, the economy might turn out to be more fragile than we anticipate, or economic shocks could push the economy off-course relative to our expectations. In other words, our economic projections might be too optimistic. Second, the first tightening move might itself provoke another taper tantrum characterized by higher bond yields and tighter financial market conditions that could be sufficiently strong to impede the economic recovery.\n\nMy judgment was that these risks were manageable. First, downside forecast errors are certainly possible, but the U.S. economy appears to be on sufficiently sound footing to withstand downside shocks better than was the case a few years ago. Second, I felt that the likelihood of a substantial tightening in financial market conditions due to lift-off was relatively low, in part, because the rate hike was widely anticipated. Market conditions had adjusted quite smoothly—except for some strains observed in the high-yield debt market—as market participants placed higher odds of tightening in the weeks preceding the December FOMC meeting. This reinforced that conclusion. A large market reaction would have been a surprise given that this was one of the most anticipated monetary policy events in history. Also, the policy action needs to be viewed in context. While this decision was the first upward adjustment to short-term rates in nearly 10 years, the actual move was small—only 25 basis points—which, by itself, should have only a very mild impact on the overall trajectory of the economy. As we noted in the FOMC statement and as Chair Yellen pointed out in her December press conference, even after this rate hike, the stance of monetary policy remains accommodative.\n\nMoreover, it is important to recognize that there are also significant risks from waiting longer to lift off. Upside forecast errors are also certainly possible. For example, while the pace of growth has generally been weaker than expected in recent years, the pace of labor market improvement has generally been stronger. By waiting, we would increase the risk that we would need to raise rates more aggressively in the future. This could unduly threaten the economic expansion. In balancing these risks, relatively “early and slow” seems like a better strategy than “late and fast”—especially when one is uncertain both about the degree of accommodation being provided by monetary policy and the level of unemployment consistent with our price stability mandate. Because monetary policy works with a lag, policy normalization needs to begin before the economy reaches its employment and inflation objectives. That is, if we are to get to a neutral monetary policy setting before inflation materially overshoots our 2 percent objective, then we need to get started. Once underway, the pace of policy tightening can be calibrated to how the economy and financial market conditions are responding.\n\nA particular risk of late and fast is that the unemployment rate could significantly undershoot the level consistent with price stability. If this occurred, then inflation would likely rise above our objective. At that point, history shows it is very difficult to push the unemployment rate back up just a little bit in order to contain inflation pressures. Looking at the post-war period, whenever the unemployment rate has increased by more than 0.3 to 0.4 percentage points, the economy has always ended up in a full-blown recession with the unemployment rate rising by at least 1.9 percentage points. This is an outcome to avoid, especially given that in an economic downturn the last to be hired are often the first to be fired. The goal is the maximum sustainable level of employment—in other words, the most job opportunities for the most people over the long run.\n\nSome of you may be wondering whether the risk of a recession isn’t already quite high? And, if so, doesn’t this imply a need for special care in adjusting monetary policy? After all, the current economic expansion is more than six years old—a bit long in the tooth by post-war standards.\n\nEven so, recession risk did not play a major factor in my thinking. Economic expansions don’t simply die of old age. They primarily end either because monetary policy is kept too loose for too long, thereby necessitating a subsequent sharp tightening in monetary policy to prevent a significant inflation overshoot, or because some large adverse shock hits the economy that the central bank cannot easily offset. Mitigating the first risk of being forced to choke off the expansion argues for getting started with policy normalization now rather than holding off. With respect to the second risk of unanticipated shocks, this is obviously very difficult for the central bank to insulate the economy from. Making sure the financial system is robust and resilient is probably the most important thing the central bank can do in this respect.\n\nI would like to turn now to the issue of how the initial step in normalization is going. The U.S. bond market response to lift-off has been very mild. There has been no bond market “taper tantrum” such as what occurred in 2013 when Chairman Bernanke discussed the possibility of tapering Federal Reserve asset purchases. Normalization is also going very well in the sense that, even with an extraordinarily large balance sheet, the tools we have developed to raise the federal funds rate (and other money market rates) have so far worked well. The federal funds rate is trading close to the middle of the new target range of 25 to 50 basis points and other money market rates have moved up in tandem.\n\nWhy is this noteworthy? To explain, I’ll first have to provide some background on how monetary policy used to work before the crisis and then compare that regime with how it works now.\n\nBefore the financial crisis, banks valued reserves―even though the Federal Reserve paid no interest on them―because the Federal Reserve kept their supply scarce. When the FOMC wanted to adjust its federal funds rate target—that is, the interest rate banks earn or pay when lending or borrowing overnight reserves with another bank—it directed the System Open Market Account (the Desk) manager in New York to alter the supply of reserves within the banking system (up or down) as needed to match the estimated demand for reserves in order to keep the federal funds rate very close to the FOMC’s target.2 In this setup, the amount of banking reserves in the system—required and excess—was very small and the Desk typically conducted open market operations that added or drained no more than a few billion dollars of reserves from the banking system to ensure that the federal funds rate traded around the FOMC’s target.\n\nIn October 2008, after passage of the TARP legislation, the Federal Reserve was authorized to implement a new tool, the ability to pay interest on required and excess reserves.3 This tool was significant because it meant that the FOMC would be able to control interest rate policy even with a much larger balance sheet and a much larger amount of excess reserves in the banking system. This was important because it gave us more scope to expand the Federal Reserve’s balance sheet to help address the financial crisis and to provide support for the economic recovery, knowing that later, when the recovery took hold, we could raise interest rates even with an enlarged balance sheet. The Fed wouldn’t necessarily be forced to return to the much smaller balance sheet we had prior to 2008 before we could begin the monetary policy normalization process.\n\nWhen interest is paid on reserves, these reserves retain value even when they are no longer scarce. Banks may be able to borrow funds at lower interest rates from financial entities such as money market funds that are not permitted to hold deposits at the Federal Reserve and place these borrowed funds with the Federal Reserve to earn the higher interest rate paid on reserve balances.\n\nHow well the ability to pay interest on reserves works in practice in raising the entire constellation of short-term interest rates depends critically on the willingness of banks to engage in such arbitrage activity. Prior to lift-off, we were uncertain how much friction there might be that would limit the willingness of banks to expand their balance sheets. These frictions include limits on bank leverage that can make the use of balance sheet capacity costly, competitive frictions given the relatively narrow range of banks that are viewed as sufficiently creditworthy to warrant being recipients of large uncollateralized loans and, for those banks that accept FDIC insured deposits, insurance premiums that increase when they borrow reserves and their total liabilities increase.\n\nTo help ensure that money market rates would track the federal funds rate regardless of these frictions, the FOMC developed a second tool—the overnight, fixed-rate, reverse repurchase facility (overnight RRP). In this facility, a number of financial entities, such as money market funds, that cannot hold reserves at the Federal Reserve can lend funds overnight to the Federal Reserve against the Fed’s Treasury collateral and receive the overnight RRP rate—currently 25 basis points. The overnight RRP provides these institutions with an alternative investment if the interest rate offered by banks is unattractive. Consequently, these financial entities should be unwilling to lend funds to banks and others at lower rates than the overnight RRP rate. The overnight RRP rate should act, therefore, as a floor on money market rates.\n\nAs I noted earlier, the tools are working as anticipated. Not only is the federal funds rate trading close to the middle of the new 25 to 50 basis point target range, but the entire complex of money market rates, such as LIBOR deposits and GCF repo, has also risen as well. Moreover, apart from a temporary spike in usage around year end, which was expected, the usage of the overnight, fixed rate reverse repo facility has fallen back to levels similar to what we saw during the testing phase. In recent days, usage has averaged less than $100 billion per day. Thus, the facility has been absorbing only a small fraction of the $2.4 trillion of excess reserves in the banking system. This demonstrates that to firm interest rates, we don’t necessarily have to drain reserves or shrink the size of our balance sheet.\n\nWe are very pleased by how well our tools are working, and this has reinforced our confidence that they will support the policy normalization process going forward. This is what we expected given our extensive testing, but there are always uncertainties that only can be resolved by actually getting underway. For example, there was some residual uncertainty about how important a factor the zero lower bound of interest rates had been in supporting the federal funds rate and other money market rates. This uncertainty now has been at least partially resolved. Even as we have moved away from the zero lower bound, the relative yield relationships we saw before December’s federal funds rate hike have persisted.\n\nSo what’s on the docket for monetary policy in 2016? The answer is that it depends on how the incoming data weighs on the outlook, and how changes in the outlook influence our views on the appropriate setting for monetary policy. What I can say is that our expectation at the December FOMC meeting was for further interest rate hikes in 2016 and beyond. Participants anticipated that the federal funds rate would likely continue on a gradual upward path. Over the longer term, FOMC participants expected that the federal funds rate would eventually reach 3 to 4 percent as inflation rose back to our 2 percent objective and the headwinds from the financial crisis that had been restraining economic activity fully dissipated.\n\nEven though this path is shallow relative to previous tightening cycles, the median federal funds rate path of FOMC participants in the December Summary of Economic Projections (SEP) is well above the path implied by the federal funds futures market. Should this be a concern? Does this imply that there is a significant risk of an abrupt future spike in short- and long-term interest rates as market rates realign to levels more consistent with the median FOMC participants’ projections?\n\nI don’t think so for several reasons. First, the SEP projections are modal forecasts—that is, what the participants believe is most likely to happen—whereas those embodied in market prices are a mean—that is, an average across all possible outcomes. One might reasonably expect these modal forecasts to be above the mean when inflation is low and the economic outlook is uncertain. Second, the median federal funds rate forecasts for primary dealers and for buyside participants surveyed just prior to the December FOMC meeting differed only marginally from the December SEP median projections of FOMC participants. This reinforces my judgment that the difference between means and modes is the main factor for the gap between the federal funds futures market and the SEP paths. Third, the differences between the interest rates implied by futures markets and the SEP have been quite small at shorter-term time horizons, such as the end of 2016, and grow much larger as the time horizon lengthens. I think this is noteworthy because the confidence one has at longer horizons should be much lower than at shorter horizons.\n\nBecause I do not know what the federal funds rate target range will be at the end of 2017 or 2018 with any confidence, I am not very concerned if others have a different modal forecast. Projections will adjust as incoming information changes the economic outlook. I would expect convergence over time of the SEP and market expectations as new information informs the outlook.\n\nLet me close with some observations about my current thinking concerning our reinvestment of maturing Treasury securities and paydowns in our agency MBS holdings. As we noted in the December FOMC statement, we anticipate that we will continue reinvestment “until normalization of the federal funds rate is well underway.” I think this policy makes sense not only because the decision to end reinvestment will represent a further tightening of monetary policy, but also because it is difficult to assess ahead of time the impact of such a decision on financial market conditions given the lack of historical experience.\n\nI also believe that continuing reinvestment until the federal funds rate reaches a higher level makes sense. We want to ensure that we have the ability to respond to adverse shocks by easing monetary policy by lowering the policy rate. Having more “dry powder” in the form of higher short-term interest rates seems more desirable than less dry powder and a smaller balance sheet.\n\nNow the words “well underway” in the FOMC statement are vague—what does that mean in terms of the level of the federal funds rate? Reiterating the disclaimer that I am speaking for myself, my view is that we should not set a numerical tripwire for ending reinvestment. If the economy were growing very quickly and the risks of an early return to the zero lower bound for the federal funds rate were deemed to be low, then I could see ending reinvestment at a relatively low federal funds rate. In contrast, if the economy lacked forward momentum and the risks of a return to the zero lower bound were judged to be considerably higher, I would want to continue reinvestment until the federal funds rate was higher. Consistent with the general principles I mentioned before, the evolution of the overall monetary policy stance—both interest rate decisions and balance sheet developments—should be data dependent.\n\nIn my view, good monetary policy-making requires ongoing assessment and judgment, not the adherence to mechanical rules. I know market participants desire certainty, but in the uncertain world in which we live, that desire is not consistent with the policy that would best achieve our objectives. We will strive to communicate as clearly as we can so you can think along with us and alter your expectations just as we do in response to incoming information.\n\nThank you for kind attention. I would be happy to take a few questions.\n\n1 Antoine Martin, Jamie McAndrews, Jonathan McCarthy, Paolo Pesenti and Joseph Tracy assisted in preparing these remarks.\n\n2 In this arrangement, the size of the Federal Reserve’s balance sheet was mainly driven by the amount of currency in circulation. As the amount of currency in circulation increased, this drained reserves from the banking system. The Federal Reserve offset this by increasing its holdings of Treasury securities.\n\n3 The Federal Reserve gained the authority to pay interest on reserves earlier as part of the Financial Services Regulatory Relief Act of 2006; that legislation authorized the Federal Reserve Banks to begin payment of interest on reserves in 2011. As part of the TARP legislation, Congress moved up the date at which the Federal Reserve could exercise that authority to October 2008."
    }
]